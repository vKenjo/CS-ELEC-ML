{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Hours Studied</th>\n",
       "      <th>Previous Scores</th>\n",
       "      <th>Extracurricular Activities</th>\n",
       "      <th>Sleep Hours</th>\n",
       "      <th>Sample Question Papers Practiced</th>\n",
       "      <th>Performance Index</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>7</td>\n",
       "      <td>99</td>\n",
       "      <td>1</td>\n",
       "      <td>9</td>\n",
       "      <td>1</td>\n",
       "      <td>91</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>4</td>\n",
       "      <td>82</td>\n",
       "      <td>0</td>\n",
       "      <td>4</td>\n",
       "      <td>2</td>\n",
       "      <td>65</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>8</td>\n",
       "      <td>51</td>\n",
       "      <td>1</td>\n",
       "      <td>7</td>\n",
       "      <td>2</td>\n",
       "      <td>45</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>5</td>\n",
       "      <td>52</td>\n",
       "      <td>1</td>\n",
       "      <td>5</td>\n",
       "      <td>2</td>\n",
       "      <td>36</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>7</td>\n",
       "      <td>75</td>\n",
       "      <td>0</td>\n",
       "      <td>8</td>\n",
       "      <td>5</td>\n",
       "      <td>66</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Hours Studied  Previous Scores  Extracurricular Activities  Sleep Hours  \\\n",
       "0              7               99                           1            9   \n",
       "1              4               82                           0            4   \n",
       "2              8               51                           1            7   \n",
       "3              5               52                           1            5   \n",
       "4              7               75                           0            8   \n",
       "\n",
       "   Sample Question Papers Practiced  Performance Index  \n",
       "0                                 1                 91  \n",
       "1                                 2                 65  \n",
       "2                                 2                 45  \n",
       "3                                 2                 36  \n",
       "4                                 5                 66  "
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# =============================================================================\n",
    "# 1. Load Data\n",
    "# =============================================================================\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "import math\n",
    "\n",
    "# Load the data \n",
    "df = pd.read_csv('./../Student_Performance.csv')\n",
    "df.head()  \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# =============================================================================\n",
    "# 2. Select a unique randomization seed\n",
    "# =============================================================================\n",
    "\n",
    "seed = 39\n",
    "np.random.seed(seed)\n",
    "\n",
    "# let n be the target sample of observations\n",
    "n = 9000\n",
    "\n",
    "# let tn be the target sample of observations\n",
    "tn = 30"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Sample:\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Hours Studied</th>\n",
       "      <th>Previous Scores</th>\n",
       "      <th>Extracurricular Activities</th>\n",
       "      <th>Sleep Hours</th>\n",
       "      <th>Sample Question Papers Practiced</th>\n",
       "      <th>Performance Index</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>9229</th>\n",
       "      <td>2</td>\n",
       "      <td>96</td>\n",
       "      <td>0</td>\n",
       "      <td>9</td>\n",
       "      <td>6</td>\n",
       "      <td>74</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7476</th>\n",
       "      <td>6</td>\n",
       "      <td>95</td>\n",
       "      <td>1</td>\n",
       "      <td>8</td>\n",
       "      <td>4</td>\n",
       "      <td>84</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2702</th>\n",
       "      <td>5</td>\n",
       "      <td>79</td>\n",
       "      <td>0</td>\n",
       "      <td>8</td>\n",
       "      <td>5</td>\n",
       "      <td>65</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3941</th>\n",
       "      <td>1</td>\n",
       "      <td>91</td>\n",
       "      <td>1</td>\n",
       "      <td>5</td>\n",
       "      <td>0</td>\n",
       "      <td>63</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3540</th>\n",
       "      <td>1</td>\n",
       "      <td>50</td>\n",
       "      <td>0</td>\n",
       "      <td>4</td>\n",
       "      <td>6</td>\n",
       "      <td>22</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      Hours Studied  Previous Scores  Extracurricular Activities  Sleep Hours  \\\n",
       "9229              2               96                           0            9   \n",
       "7476              6               95                           1            8   \n",
       "2702              5               79                           0            8   \n",
       "3941              1               91                           1            5   \n",
       "3540              1               50                           0            4   \n",
       "\n",
       "      Sample Question Papers Practiced  Performance Index  \n",
       "9229                                 6                 74  \n",
       "7476                                 4                 84  \n",
       "2702                                 5                 65  \n",
       "3941                                 0                 63  \n",
       "3540                                 6                 22  "
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# =============================================================================\n",
    "# 3. Sample Train Data\n",
    "# =============================================================================\n",
    "\n",
    "features = ['Hours Studied', 'Previous Scores', \n",
    "            'Extracurricular Activities', \n",
    "            'Sample Question Papers Practiced', \n",
    "            'Performance Index']\n",
    "target = 'Performance Index'\n",
    "\n",
    "# Randomly sample 30 observations for training using the seed.\n",
    "train_df = df.sample(n, random_state=seed)\n",
    "\n",
    "# Check that the training sample looks representative\n",
    "print(\"Training Sample:\")\n",
    "train_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "scaler = StandardScaler()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = train_df[features].values\n",
    "y_train = train_df[target].values\n",
    "X_train_scaled = scaler.fit_transform(X_train)\n",
    "\n",
    "X_train_aug = np.c_[np.ones(X_train_scaled.shape[0]), X_train_scaled]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# =============================================================================\n",
    "# 4. Building the Weight Update (Gradient Descent) Function\n",
    "# =============================================================================\n",
    "def gradient_descent(X, y, weights, learning_rate=0.01, iterations=50):\n",
    "    weight_history = []\n",
    "    for i in range(iterations):\n",
    "        # Compute predictions\n",
    "        predictions = X.dot(weights)\n",
    "        \n",
    "        # Compute errors and Mean Squared Error (MSE) loss\n",
    "        errors = predictions - y\n",
    "        loss = np.mean(errors ** 2)\n",
    "        \n",
    "        # Compute gradients (partial derivatives)\n",
    "        gradients = (1 / len(y)) * X.T.dot(errors)\n",
    "        \n",
    "        # Update weights using gradient descent rule\n",
    "        weights = weights - learning_rate * gradients\n",
    "        \n",
    "        # Save current weights in history and print iteration details\n",
    "        weight_history.append(weights.copy())\n",
    "        print(f\"Iteration {i+1}: Weights = {weights}, Loss = {loss:.4f}\")\n",
    "    return weights, weight_history\n",
    "\n",
    "# Initialize weights (one for intercept plus one per feature)\n",
    "initial_weights = np.zeros(X_train_aug.shape[1])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 1: Weights = [0.55303333 0.07057988 0.17564165 0.00456974 0.00812747 0.19195161], Loss = 3426.9129\n",
      "Iteration 2: Weights = [1.10053633 0.13978054 0.34778338 0.00903144 0.01606743 0.38011248], Loss = 3351.6410\n",
      "Iteration 3: Weights = [1.6425643  0.2076291  0.51649465 0.01338743 0.02382385 0.56455772], Loss = 3278.1455\n",
      "Iteration 4: Weights = [2.17917199 0.27415209 0.6818435  0.01763996 0.03140063 0.74536095], Loss = 3206.3803\n",
      "Iteration 5: Weights = [2.71041361 0.33937555 0.84389664 0.02179127 0.03880157 0.92259433], Loss = 3136.3004\n",
      "Iteration 6: Weights = [3.2363428  0.40332501 1.00271947 0.02584353 0.04603043 1.0963286 ], Loss = 3067.8621\n",
      "Iteration 7: Weights = [3.75701271 0.46602547 1.15837607 0.02979887 0.05309085 1.26663308], Loss = 3001.0232\n",
      "Iteration 8: Weights = [4.27247592 0.52750146 1.31092926 0.03365938 0.05998643 1.43357573], Loss = 2935.7423\n",
      "Iteration 9: Weights = [4.78278449 0.58777702 1.46044062 0.03742711 0.06672067 1.59722316], Loss = 2871.9797\n",
      "Iteration 10: Weights = [5.28798998 0.64687572 1.6069705  0.04110405 0.07329702 1.75764067], Loss = 2809.6964\n",
      "Iteration 11: Weights = [5.78814341 0.70482064 1.75057805 0.04469218 0.07971884 1.91489224], Loss = 2748.8549\n",
      "Iteration 12: Weights = [6.28329531 0.76163444 1.89132127 0.04819341 0.08598944 2.0690406 ], Loss = 2689.4184\n",
      "Iteration 13: Weights = [6.77349569 0.81733931 2.02925698 0.05160962 0.09211205 2.22014725], Loss = 2631.3513\n",
      "Iteration 14: Weights = [7.25879407 0.87195701 2.16444089 0.05494266 0.09808982 2.36827245], Loss = 2574.6192\n",
      "Iteration 15: Weights = [7.73923946 0.92550888 2.2969276  0.05819432 0.10392587 2.51347527], Loss = 2519.1884\n",
      "Iteration 16: Weights = [8.2148804  0.97801583 2.42677064 0.06136639 0.10962324 2.65581362], Loss = 2465.0261\n",
      "Iteration 17: Weights = [8.68576493 1.02949834 2.55402245 0.06446058 0.11518488 2.79534424], Loss = 2412.1008\n",
      "Iteration 18: Weights = [9.15194061 1.07997652 2.67873446 0.06747859 0.12061373 2.93212277], Loss = 2360.3815\n",
      "Iteration 19: Weights = [9.61345454 1.12947007 2.80095707 0.07042208 0.12591263 3.06620374], Loss = 2309.8383\n",
      "Iteration 20: Weights = [10.07035333  1.17799828  2.92073968  0.07329267  0.13108437  3.1976406 ], Loss = 2260.4419\n",
      "Iteration 21: Weights = [10.52268313  1.2255801   3.0381307   0.07609197  0.1361317   3.32648573], Loss = 2212.1641\n",
      "Iteration 22: Weights = [10.97048963  1.27223408  3.15317759  0.07882152  0.1410573   3.45279048], Loss = 2164.9772\n",
      "Iteration 23: Weights = [11.41381807  1.31797841  3.26592688  0.08148285  0.14586378  3.5766052 ], Loss = 2118.8544\n",
      "Iteration 24: Weights = [11.85271322  1.36283091  3.37642415  0.08407745  0.15055372  3.6979792 ], Loss = 2073.7696\n",
      "Iteration 25: Weights = [12.28721942  1.40680907  3.48471411  0.08660679  0.15512965  3.81696086], Loss = 2029.6975\n",
      "Iteration 26: Weights = [12.71738056  1.44993003  3.59084055  0.08907231  0.15959402  3.93359757], Loss = 1986.6134\n",
      "Iteration 27: Weights = [13.14324009  1.49221058  3.6948464   0.09147539  0.16394925  4.04793579], Loss = 1944.4932\n",
      "Iteration 28: Weights = [13.56484102  1.53366719  3.79677377  0.09381743  0.1681977   4.16002106], Loss = 1903.3137\n",
      "Iteration 29: Weights = [13.98222594  1.574316    3.89666389  0.09609975  0.1723417   4.26989802], Loss = 1863.0520\n",
      "Iteration 30: Weights = [14.39543702  1.61417284  3.9945572   0.09832369  0.17638351  4.37761043], Loss = 1823.6860\n"
     ]
    }
   ],
   "source": [
    "# =============================================================================\n",
    "# 5. Displaying Weights per Iteration\n",
    "# =============================================================================\n",
    "# Run gradient descent\n",
    "final_weights, weight_history = gradient_descent(X_train_aug, y_train, \n",
    "                                                 initial_weights,\n",
    "                                                 learning_rate=0.01, \n",
    "                                                 iterations=30) # sameple is 30 given n = 30 as per instructions\n",
    "                                                  # please check the last cell for all iterations n = 10,000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[14.39543702  1.61417284  3.9945572   0.09832369  0.17638351  4.37761043]\n"
     ]
    }
   ],
   "source": [
    "print(final_weights)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 1200x1200 with 6 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# =============================================================================\n",
    "# 6. Plot the Value of Weights per Iteration\n",
    "# =============================================================================\n",
    "\n",
    "# Convert weight_history list to a NumPy array for plotting\n",
    "weight_history = np.array(weight_history)  # shape: (iterations, number_of_weights)\n",
    "\n",
    "# Define names for each weight: first weight is the Intercept, then the feature names\n",
    "weight_names = ['Intercept'] + features\n",
    "\n",
    "# Create a 3x2 grid of subplots\n",
    "fig, axs = plt.subplots(3, 2, figsize=(12, 12))\n",
    "axs = axs.flatten()  # Flatten the grid for easy indexing\n",
    "\n",
    "# Plot the evolution of each weight over iterations\n",
    "for j in range(weight_history.shape[1]):\n",
    "    axs[j].plot(range(1, len(weight_history) + 1), weight_history[:, j],\n",
    "                marker='o', linestyle='-')\n",
    "    axs[j].set_xlabel('Iteration')\n",
    "    axs[j].set_ylabel(weight_names[j])\n",
    "    axs[j].set_title(f'Variation of {weight_names[j]} over Iterations')\n",
    "    axs[j].grid(True)\n",
    "\n",
    "# Hide any unused subplots (if any)\n",
    "for k in range(weight_history.shape[1], len(axs)):\n",
    "    axs[k].axis('off')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Final Regression Model:\n",
      "y = 14.3954 + 1.6142*Hours Studied + 3.9946*Previous Scores + 0.0983*Extracurricular Activities + 0.1764*Sample Question Papers Practiced + 4.3776*Performance Index\n"
     ]
    }
   ],
   "source": [
    "# =============================================================================\n",
    "# 7. Build a Function for the Final Regression Model\n",
    "# =============================================================================\n",
    "# The equation will be in the form: y = intercept + coef1*G1 + coef2*G2\n",
    "# reminder that final_weights carry the values of each feature vector\n",
    "equation = f\"{final_weights[0]:.4f}\"  # Start with the intercept\n",
    "for idx, feature in enumerate(features, start=1):\n",
    "    coef = final_weights[idx]\n",
    "    if coef >= 0:\n",
    "        equation += f\" + {coef:.4f}*{feature}\"\n",
    "    else:\n",
    "        equation += f\" - {abs(coef):.4f}*{feature}\"\n",
    "        \n",
    "print(\"Final Regression Model:\")\n",
    "print(\"y =\", equation)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Sample:\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Hours Studied</th>\n",
       "      <th>Previous Scores</th>\n",
       "      <th>Extracurricular Activities</th>\n",
       "      <th>Sleep Hours</th>\n",
       "      <th>Sample Question Papers Practiced</th>\n",
       "      <th>Performance Index</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>2052</th>\n",
       "      <td>4</td>\n",
       "      <td>95</td>\n",
       "      <td>1</td>\n",
       "      <td>9</td>\n",
       "      <td>4</td>\n",
       "      <td>75</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>723</th>\n",
       "      <td>6</td>\n",
       "      <td>82</td>\n",
       "      <td>0</td>\n",
       "      <td>4</td>\n",
       "      <td>6</td>\n",
       "      <td>63</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6180</th>\n",
       "      <td>7</td>\n",
       "      <td>90</td>\n",
       "      <td>1</td>\n",
       "      <td>7</td>\n",
       "      <td>5</td>\n",
       "      <td>79</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6913</th>\n",
       "      <td>3</td>\n",
       "      <td>66</td>\n",
       "      <td>1</td>\n",
       "      <td>7</td>\n",
       "      <td>9</td>\n",
       "      <td>44</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>173</th>\n",
       "      <td>8</td>\n",
       "      <td>62</td>\n",
       "      <td>0</td>\n",
       "      <td>6</td>\n",
       "      <td>8</td>\n",
       "      <td>53</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      Hours Studied  Previous Scores  Extracurricular Activities  Sleep Hours  \\\n",
       "2052              4               95                           1            9   \n",
       "723               6               82                           0            4   \n",
       "6180              7               90                           1            7   \n",
       "6913              3               66                           1            7   \n",
       "173               8               62                           0            6   \n",
       "\n",
       "      Sample Question Papers Practiced  Performance Index  \n",
       "2052                                 4                 75  \n",
       "723                                  6                 63  \n",
       "6180                                 5                 79  \n",
       "6913                                 9                 44  \n",
       "173                                  8                 53  "
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# =============================================================================\n",
    "# 8. Sampling Test Data\n",
    "# =============================================================================\n",
    "\n",
    "# Remove training observations from the original dataframe\n",
    "remaining_df = df.drop(train_df.index)\n",
    "\n",
    "# Randomly sample 30 test observations (use a different seed to avoid overlap)\n",
    "test_df = remaining_df.sample(tn, random_state=seed+1)\n",
    "\n",
    "\n",
    "print(\"Test Sample:\")\n",
    "test_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_test = test_df[features].values\n",
    "y_test = test_df[target].values\n",
    "\n",
    "X_test_scaled = scaler.transform(X_test)\n",
    "X_test_aug = np.c_[np.ones(X_test_scaled.shape[0]), X_test_scaled]\n",
    "\n",
    "X_test_aug = np.c_[np.ones(X_test_scaled.shape[0]), X_test_scaled]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predictions on Test Set:\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([24.18927586, 19.63633694, 25.88364856, 10.13452439, 14.12637115,\n",
       "       27.7293717 , 17.17045401, 31.78697138, 28.88831257, 23.45263457,\n",
       "        4.99349427,  7.34782617, 12.05868862, 19.05888518, 24.03445145,\n",
       "        2.88573001, 16.8219188 ,  1.2370268 , 12.02438125, 15.07260278,\n",
       "        4.76998266, 15.83856509,  1.3915144 , 19.42121694, 20.26907245,\n",
       "        5.55421881,  7.34782617,  0.61334397, 16.5024963 ,  2.05648515])"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# =============================================================================\n",
    "# 9. Use the Regression Function for Prediction\n",
    "# =============================================================================\n",
    "\n",
    "# Predict using the final weights for all test observations\n",
    "predictions = X_test_aug.dot(final_weights)\n",
    "\n",
    "# Show a few predictions alongside actual values\n",
    "predictions = X_test_aug.dot(final_weights)\n",
    "print(\"Predictions on Test Set:\")\n",
    "predictions\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean Squared Error on Test Set: 1766.5769\n",
      "Root Mean Squared Error on Test Set: 42.0307\n"
     ]
    }
   ],
   "source": [
    "# =============================================================================\n",
    "# 10. Calculate for Errors\n",
    "# =============================================================================\n",
    "\n",
    "mse = mean_squared_error(y_test, predictions)\n",
    "rmse = math.sqrt(mse)\n",
    "print(f\"Mean Squared Error on Test Set: {mse:.4f}\")\n",
    "print(f\"Root Mean Squared Error on Test Set: {rmse:.4f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 1: Weights = [0.55303333 0.07057988 0.17564165 0.00456974 0.00812747 0.19195161], Loss = 3426.9129\n",
      "Iteration 2: Weights = [1.10053633 0.13978054 0.34778338 0.00903144 0.01606743 0.38011248], Loss = 3351.6410\n",
      "Iteration 3: Weights = [1.6425643  0.2076291  0.51649465 0.01338743 0.02382385 0.56455772], Loss = 3278.1455\n",
      "Iteration 4: Weights = [2.17917199 0.27415209 0.6818435  0.01763996 0.03140063 0.74536095], Loss = 3206.3803\n",
      "Iteration 5: Weights = [2.71041361 0.33937555 0.84389664 0.02179127 0.03880157 0.92259433], Loss = 3136.3004\n",
      "Iteration 6: Weights = [3.2363428  0.40332501 1.00271947 0.02584353 0.04603043 1.0963286 ], Loss = 3067.8621\n",
      "Iteration 7: Weights = [3.75701271 0.46602547 1.15837607 0.02979887 0.05309085 1.26663308], Loss = 3001.0232\n",
      "Iteration 8: Weights = [4.27247592 0.52750146 1.31092926 0.03365938 0.05998643 1.43357573], Loss = 2935.7423\n",
      "Iteration 9: Weights = [4.78278449 0.58777702 1.46044062 0.03742711 0.06672067 1.59722316], Loss = 2871.9797\n",
      "Iteration 10: Weights = [5.28798998 0.64687572 1.6069705  0.04110405 0.07329702 1.75764067], Loss = 2809.6964\n",
      "Iteration 11: Weights = [5.78814341 0.70482064 1.75057805 0.04469218 0.07971884 1.91489224], Loss = 2748.8549\n",
      "Iteration 12: Weights = [6.28329531 0.76163444 1.89132127 0.04819341 0.08598944 2.0690406 ], Loss = 2689.4184\n",
      "Iteration 13: Weights = [6.77349569 0.81733931 2.02925698 0.05160962 0.09211205 2.22014725], Loss = 2631.3513\n",
      "Iteration 14: Weights = [7.25879407 0.87195701 2.16444089 0.05494266 0.09808982 2.36827245], Loss = 2574.6192\n",
      "Iteration 15: Weights = [7.73923946 0.92550888 2.2969276  0.05819432 0.10392587 2.51347527], Loss = 2519.1884\n",
      "Iteration 16: Weights = [8.2148804  0.97801583 2.42677064 0.06136639 0.10962324 2.65581362], Loss = 2465.0261\n",
      "Iteration 17: Weights = [8.68576493 1.02949834 2.55402245 0.06446058 0.11518488 2.79534424], Loss = 2412.1008\n",
      "Iteration 18: Weights = [9.15194061 1.07997652 2.67873446 0.06747859 0.12061373 2.93212277], Loss = 2360.3815\n",
      "Iteration 19: Weights = [9.61345454 1.12947007 2.80095707 0.07042208 0.12591263 3.06620374], Loss = 2309.8383\n",
      "Iteration 20: Weights = [10.07035333  1.17799828  2.92073968  0.07329267  0.13108437  3.1976406 ], Loss = 2260.4419\n",
      "Iteration 21: Weights = [10.52268313  1.2255801   3.0381307   0.07609197  0.1361317   3.32648573], Loss = 2212.1641\n",
      "Iteration 22: Weights = [10.97048963  1.27223408  3.15317759  0.07882152  0.1410573   3.45279048], Loss = 2164.9772\n",
      "Iteration 23: Weights = [11.41381807  1.31797841  3.26592688  0.08148285  0.14586378  3.5766052 ], Loss = 2118.8544\n",
      "Iteration 24: Weights = [11.85271322  1.36283091  3.37642415  0.08407745  0.15055372  3.6979792 ], Loss = 2073.7696\n",
      "Iteration 25: Weights = [12.28721942  1.40680907  3.48471411  0.08660679  0.15512965  3.81696086], Loss = 2029.6975\n",
      "Iteration 26: Weights = [12.71738056  1.44993003  3.59084055  0.08907231  0.15959402  3.93359757], Loss = 1986.6134\n",
      "Iteration 27: Weights = [13.14324009  1.49221058  3.6948464   0.09147539  0.16394925  4.04793579], Loss = 1944.4932\n",
      "Iteration 28: Weights = [13.56484102  1.53366719  3.79677377  0.09381743  0.1681977   4.16002106], Loss = 1903.3137\n",
      "Iteration 29: Weights = [13.98222594  1.574316    3.89666389  0.09609975  0.1723417   4.26989802], Loss = 1863.0520\n",
      "Iteration 30: Weights = [14.39543702  1.61417284  3.9945572   0.09832369  0.17638351  4.37761043], Loss = 1823.6860\n",
      "Iteration 31: Weights = [14.80451598  1.65325322  4.09049334  0.10049052  0.18032536  4.48320116], Loss = 1785.1943\n",
      "Iteration 32: Weights = [15.20950415  1.69157234  4.18451113  0.10260152  0.18416941  4.58671226], Loss = 1747.5558\n",
      "Iteration 33: Weights = [15.61044245  1.72914512  4.27664866  0.10465792  0.1879178   4.68818493], Loss = 1710.7501\n",
      "Iteration 34: Weights = [16.00737135  1.76598616  4.36694326  0.10666092  0.19157262  4.78765955], Loss = 1674.7574\n",
      "Iteration 35: Weights = [16.40033097  1.8021098   4.45543149  0.10861171  0.1951359   4.88517572], Loss = 1639.5583\n",
      "Iteration 36: Weights = [16.789361    1.83753008  4.54214921  0.11051146  0.19860966  4.98077223], Loss = 1605.1340\n",
      "Iteration 37: Weights = [17.17450072  1.87226076  4.62713156  0.1123613   0.20199584  5.07448712], Loss = 1571.4662\n",
      "Iteration 38: Weights = [17.55578905  1.90631534  4.71041299  0.11416234  0.20529638  5.16635767], Loss = 1538.5371\n",
      "Iteration 39: Weights = [17.93326449  1.93970705  4.79202726  0.11591568  0.20851313  5.25642042], Loss = 1506.3291\n",
      "Iteration 40: Weights = [18.30696518  1.97244887  4.87200747  0.11762237  0.21164796  5.34471119], Loss = 1474.8254\n",
      "Iteration 41: Weights = [18.67692886  2.0045535   4.95038605  0.11928346  0.21470265  5.43126508], Loss = 1444.0095\n",
      "Iteration 42: Weights = [19.0431929   2.03603341  5.02719479  0.12089997  0.21767898  5.51611651], Loss = 1413.8653\n",
      "Iteration 43: Weights = [19.40579431  2.06690082  5.10246486  0.12247291  0.22057866  5.59929921], Loss = 1384.3772\n",
      "Iteration 44: Weights = [19.7647697   2.09716771  5.1762268   0.12400325  0.2234034   5.68084623], Loss = 1355.5298\n",
      "Iteration 45: Weights = [20.12015534  2.12684582  5.24851055  0.12549196  0.22615484  5.76078999], Loss = 1327.3084\n",
      "Iteration 46: Weights = [20.47198712  2.15594666  5.31934546  0.12693996  0.22883461  5.83916225], Loss = 1299.6984\n",
      "Iteration 47: Weights = [20.82030058  2.18448151  5.38876029  0.12834818  0.23144429  5.91599414], Loss = 1272.6856\n",
      "Iteration 48: Weights = [21.16513091  2.21246145  5.45678322  0.12971753  0.23398545  5.99131618], Loss = 1246.2565\n",
      "Iteration 49: Weights = [21.50651293  2.23989731  5.5234419   0.13104887  0.2364596   6.06515829], Loss = 1220.3974\n",
      "Iteration 50: Weights = [21.84448113  2.26679972  5.58876341  0.13234307  0.23886823  6.13754979], Loss = 1195.0954\n",
      "Iteration 51: Weights = [22.17906966  2.29317911  5.6527743   0.13360098  0.24121281  6.20851941], Loss = 1170.3377\n",
      "Iteration 52: Weights = [22.51031229  2.3190457   5.71550059  0.13482342  0.24349475  6.27809533], Loss = 1146.1118\n",
      "Iteration 53: Weights = [22.8382425   2.34440951  5.77696778  0.13601121  0.24571547  6.34630515], Loss = 1122.4057\n",
      "Iteration 54: Weights = [23.16289341  2.36928034  5.83720087  0.13716512  0.24787632  6.41317595], Loss = 1099.2075\n",
      "Iteration 55: Weights = [23.48429781  2.39366783  5.89622437  0.13828594  0.24997866  6.47873425], Loss = 1076.5056\n",
      "Iteration 56: Weights = [23.80248817  2.41758142  5.9540623   0.13937442  0.25202379  6.54300606], Loss = 1054.2890\n",
      "Iteration 57: Weights = [24.11749662  2.44103035  6.01073819  0.14043131  0.25401299  6.60601686], Loss = 1032.5465\n",
      "Iteration 58: Weights = [24.42935498  2.4640237   6.06627512  0.14145734  0.25594753  6.66779163], Loss = 1011.2675\n",
      "Iteration 59: Weights = [24.73809477  2.48657036  6.12069571  0.14245321  0.25782864  6.72835487], Loss = 990.4415\n",
      "Iteration 60: Weights = [25.04374715  2.50867904  6.17402212  0.14341961  0.25965753  6.78773057], Loss = 970.0585\n",
      "Iteration 61: Weights = [25.34634302  2.53035828  6.22627608  0.14435724  0.26143537  6.84594227], Loss = 950.1084\n",
      "Iteration 62: Weights = [25.64591292  2.55161647  6.27747889  0.14526676  0.26316332  6.90301302], Loss = 930.5815\n",
      "Iteration 63: Weights = [25.94248712  2.57246182  6.32765143  0.14614882  0.26484251  6.95896542], Loss = 911.4685\n",
      "Iteration 64: Weights = [26.23609558  2.59290238  6.37681415  0.14700406  0.26647404  7.01382164], Loss = 892.7600\n",
      "Iteration 65: Weights = [26.52676796  2.61294603  6.42498711  0.1478331   0.26805902  7.06760339], Loss = 874.4471\n",
      "Iteration 66: Weights = [26.81453362  2.63260053  6.47218996  0.14863656  0.26959848  7.12033196], Loss = 856.5211\n",
      "Iteration 67: Weights = [27.09942161  2.65187345  6.51844198  0.14941504  0.27109348  7.17202821], Loss = 838.9732\n",
      "Iteration 68: Weights = [27.38146073  2.67077222  6.56376204  0.15016911  0.27254503  7.22271259], Loss = 821.7952\n",
      "Iteration 69: Weights = [27.66067946  2.68930415  6.60816866  0.15089936  0.27395412  7.27240515], Loss = 804.9789\n",
      "Iteration 70: Weights = [27.937106    2.70747637  6.65167998  0.15160634  0.27532174  7.32112554], Loss = 788.5162\n",
      "Iteration 71: Weights = [28.21076827  2.7252959   6.69431378  0.1522906   0.27664882  7.36889302], Loss = 772.3995\n",
      "Iteration 72: Weights = [28.48169392  2.74276959  6.7360875   0.15295269  0.27793632  7.41572646], Loss = 756.6211\n",
      "Iteration 73: Weights = [28.74991031  2.7599042   6.77701821  0.15359312  0.27918513  7.46164437], Loss = 741.1736\n",
      "Iteration 74: Weights = [29.01544454  2.77670631  6.81712266  0.15421241  0.28039616  7.50666489], Loss = 726.0498\n",
      "Iteration 75: Weights = [29.27832343  2.7931824   6.85641727  0.15481106  0.28157028  7.5508058 ], Loss = 711.2425\n",
      "Iteration 76: Weights = [29.53857353  2.80933882  6.89491812  0.15538957  0.28270834  7.59408452], Loss = 696.7448\n",
      "Iteration 77: Weights = [29.79622113  2.82518178  6.93264098  0.15594842  0.28381119  7.63651813], Loss = 682.5501\n",
      "Iteration 78: Weights = [30.05129225  2.84071739  6.9696013   0.15648808  0.28487965  7.67812338], Loss = 668.6517\n",
      "Iteration 79: Weights = [30.30381266  2.85595161  7.00581425  0.15700901  0.28591451  7.71891667], Loss = 655.0431\n",
      "Iteration 80: Weights = [30.55380787  2.87089033  7.04129465  0.15751166  0.28691657  7.75891409], Loss = 641.7182\n",
      "Iteration 81: Weights = [30.80130312  2.88553927  7.07605707  0.15799647  0.28788659  7.7981314 ], Loss = 628.6706\n",
      "Iteration 82: Weights = [31.04632342  2.89990407  7.11011577  0.15846388  0.28882533  7.83658407], Loss = 615.8945\n",
      "Iteration 83: Weights = [31.28889352  2.91399027  7.14348474  0.15891431  0.28973352  7.87428723], Loss = 603.3840\n",
      "Iteration 84: Weights = [31.52903792  2.92780326  7.17617767  0.15934817  0.29061189  7.91125574], Loss = 591.1333\n",
      "Iteration 85: Weights = [31.76678088  2.94134836  7.20820801  0.15976586  0.29146114  7.94750415], Loss = 579.1369\n",
      "Iteration 86: Weights = [32.0021464   2.95463077  7.23958891  0.16016779  0.29228195  7.98304672], Loss = 567.3892\n",
      "Iteration 87: Weights = [32.23515827  2.9676556   7.27033327  0.16055433  0.29307502  8.01789744], Loss = 555.8850\n",
      "Iteration 88: Weights = [32.46584002  2.98042785  7.30045376  0.16092588  0.29384098  8.05207002], Loss = 544.6189\n",
      "Iteration 89: Weights = [32.69421495  2.99295242  7.32996277  0.16128279  0.29458051  8.08557788], Loss = 533.5860\n",
      "Iteration 90: Weights = [32.92030614  3.00523411  7.35887244  0.16162543  0.29529422  8.11843419], Loss = 522.7811\n",
      "Iteration 91: Weights = [33.14413641  3.01727765  7.3871947   0.16195415  0.29598273  8.15065187], Loss = 512.1996\n",
      "Iteration 92: Weights = [33.36572838  3.02908766  7.4149412   0.16226931  0.29664666  8.18224356], Loss = 501.8364\n",
      "Iteration 93: Weights = [33.58510443  3.04066865  7.4421234   0.16257123  0.29728659  8.21322166], Loss = 491.6871\n",
      "Iteration 94: Weights = [33.80228672  3.05202508  7.46875251  0.16286026  0.2979031   8.24359833], Loss = 481.7471\n",
      "Iteration 95: Weights = [34.01729718  3.06316131  7.49483953  0.16313671  0.29849677  8.27338548], Loss = 472.0119\n",
      "Iteration 96: Weights = [34.23015755  3.07408159  7.52039522  0.16340091  0.29906815  8.30259479], Loss = 462.4771\n",
      "Iteration 97: Weights = [34.4408893   3.08479011  7.54543016  0.16365316  0.29961777  8.33123771], Loss = 453.1385\n",
      "Iteration 98: Weights = [34.64951374  3.09529099  7.56995468  0.16389376  0.30014618  8.35932545], Loss = 443.9920\n",
      "Iteration 99: Weights = [34.85605194  3.10558824  7.59397894  0.16412302  0.30065389  8.38686902], Loss = 435.0335\n",
      "Iteration 100: Weights = [35.06052475  3.1156858   7.61751288  0.16434122  0.30114141  8.41387918], Loss = 426.2589\n",
      "Iteration 101: Weights = [35.26295284  3.12558754  7.64056624  0.16454865  0.30160924  8.44036652], Loss = 417.6645\n",
      "Iteration 102: Weights = [35.46335664  3.13529726  7.66314859  0.16474559  0.30205786  8.46634138], Loss = 409.2465\n",
      "Iteration 103: Weights = [35.66175641  3.14481868  7.68526929  0.1649323   0.30248776  8.49181392], Loss = 401.0010\n",
      "Iteration 104: Weights = [35.85817218  3.15415543  7.7069375   0.16510905  0.30289939  8.51679409], Loss = 392.9245\n",
      "Iteration 105: Weights = [36.05262379  3.16331108  7.72816224  0.1652761   0.30329322  8.54129164], Loss = 385.0134\n",
      "Iteration 106: Weights = [36.24513089  3.17228915  7.74895231  0.16543371  0.30366968  8.56531612], Loss = 377.2642\n",
      "Iteration 107: Weights = [36.43571291  3.18109307  7.76931637  0.16558212  0.30402921  8.58887692], Loss = 369.6736\n",
      "Iteration 108: Weights = [36.62438912  3.1897262   7.78926287  0.16572157  0.30437225  8.61198321], Loss = 362.2382\n",
      "Iteration 109: Weights = [36.81117856  3.19819185  7.80880013  0.16585231  0.3046992   8.63464401], Loss = 354.9548\n",
      "Iteration 110: Weights = [36.99610011  3.20649325  7.82793628  0.16597456  0.30501048  8.65686812], Loss = 347.8201\n",
      "Iteration 111: Weights = [37.17917244  3.21463357  7.84667929  0.16608854  0.30530648  8.67866421], Loss = 340.8311\n",
      "Iteration 112: Weights = [37.36041405  3.22261594  7.86503699  0.16619449  0.3055876   8.70004074], Loss = 333.9848\n",
      "Iteration 113: Weights = [37.53984324  3.23044339  7.88301704  0.16629262  0.30585421  8.72100604], Loss = 327.2780\n",
      "Iteration 114: Weights = [37.71747814  3.23811892  7.90062695  0.16638313  0.30610669  8.74156825], Loss = 320.7081\n",
      "Iteration 115: Weights = [37.89333669  3.24564546  7.91787407  0.16646625  0.3063454   8.76173534], Loss = 314.2720\n",
      "Iteration 116: Weights = [38.06743666  3.25302589  7.93476564  0.16654216  0.3065707   8.78151515], Loss = 307.9670\n",
      "Iteration 117: Weights = [38.23979563  3.26026302  7.95130871  0.16661106  0.30678294  8.80091535], Loss = 301.7904\n",
      "Iteration 118: Weights = [38.410431    3.26735962  7.96751023  0.16667316  0.30698246  8.81994346], Loss = 295.7394\n",
      "Iteration 119: Weights = [38.57936003  3.2743184   7.98337697  0.16672863  0.30716959  8.83860684], Loss = 289.8116\n",
      "Iteration 120: Weights = [38.74659976  3.28114202  7.99891562  0.16677766  0.30734466  8.85691273], Loss = 284.0043\n",
      "Iteration 121: Weights = [38.91216709  3.28783307  8.01413268  0.16682043  0.30750799  8.8748682 ], Loss = 278.3151\n",
      "Iteration 122: Weights = [39.07607876  3.29439412  8.02903456  0.16685713  0.30765989  8.89248018], Loss = 272.7414\n",
      "Iteration 123: Weights = [39.2383513   3.30082767  8.04362754  0.16688791  0.30780066  8.9097555 ], Loss = 267.2809\n",
      "Iteration 124: Weights = [39.39900112  3.30713618  8.05791774  0.16691295  0.30793061  8.92670081], Loss = 261.9313\n",
      "Iteration 125: Weights = [39.55804445  3.31332205  8.07191121  0.16693242  0.30805002  8.94332265], Loss = 256.6903\n",
      "Iteration 126: Weights = [39.71549733  3.31938763  8.08561384  0.16694647  0.30815917  8.95962742], Loss = 251.5555\n",
      "Iteration 127: Weights = [39.87137569  3.32533526  8.09903142  0.16695525  0.30825835  8.97562141], Loss = 246.5249\n",
      "Iteration 128: Weights = [40.02569527  3.33116719  8.11216963  0.16695894  0.30834783  8.99131077], Loss = 241.5963\n",
      "Iteration 129: Weights = [40.17847165  3.33688565  8.12503402  0.16695766  0.30842788  9.00670154], Loss = 236.7676\n",
      "Iteration 130: Weights = [40.32972027  3.34249282  8.13763004  0.16695157  0.30849875  9.02179963], Loss = 232.0366\n",
      "Iteration 131: Weights = [40.4794564   3.34799084  8.14996304  0.16694082  0.30856069  9.03661083], Loss = 227.4015\n",
      "Iteration 132: Weights = [40.62769517  3.35338181  8.16203824  0.16692553  0.30861397  9.05114083], Loss = 222.8602\n",
      "Iteration 133: Weights = [40.77445155  3.35866778  8.17386078  0.16690586  0.30865881  9.0653952 ], Loss = 218.4108\n",
      "Iteration 134: Weights = [40.91974037  3.36385077  8.18543569  0.16688193  0.30869545  9.07937939], Loss = 214.0514\n",
      "Iteration 135: Weights = [41.0635763   3.36893276  8.1967679   0.16685386  0.30872414  9.09309876], Loss = 209.7801\n",
      "Iteration 136: Weights = [41.20597387  3.37391569  8.20786224  0.1668218   0.30874509  9.10655856], Loss = 205.5952\n",
      "Iteration 137: Weights = [41.34694746  3.37880144  8.21872344  0.16678585  0.30875852  9.11976391], Loss = 201.4949\n",
      "Iteration 138: Weights = [41.48651132  3.3835919   8.22935615  0.16674615  0.30876466  9.13271986], Loss = 197.4775\n",
      "Iteration 139: Weights = [41.62467954  3.38828889  8.23976492  0.16670281  0.30876371  9.14543136], Loss = 193.5412\n",
      "Iteration 140: Weights = [41.76146608  3.39289419  8.24995419  0.16665595  0.30875588  9.15790324], Loss = 189.6844\n",
      "Iteration 141: Weights = [41.89688475  3.39740956  8.25992836  0.16660568  0.30874137  9.17014024], Loss = 185.9054\n",
      "Iteration 142: Weights = [42.03094924  3.40183674  8.26969168  0.1665521   0.30872038  9.18214702], Loss = 182.2027\n",
      "Iteration 143: Weights = [42.16367308  3.40617739  8.27924838  0.16649533  0.30869309  9.19392814], Loss = 178.5747\n",
      "Iteration 144: Weights = [42.29506968  3.41043319  8.28860256  0.16643548  0.30865971  9.20548807], Loss = 175.0199\n",
      "Iteration 145: Weights = [42.42515232  3.41460576  8.29775825  0.16637264  0.30862041  9.21683118], Loss = 171.5368\n",
      "Iteration 146: Weights = [42.55393413  3.41869668  8.30671941  0.16630691  0.30857537  9.22796178], Loss = 168.1239\n",
      "Iteration 147: Weights = [42.68142812  3.42270753  8.31548991  0.16623839  0.30852476  9.23888406], Loss = 164.7797\n",
      "Iteration 148: Weights = [42.80764717  3.42663982  8.32407355  0.16616718  0.30846877  9.24960216], Loss = 161.5030\n",
      "Iteration 149: Weights = [42.93260403  3.43049506  8.33247405  0.16609337  0.30840755  9.26012012], Loss = 158.2923\n",
      "Iteration 150: Weights = [43.05631133  3.43427473  8.34069506  0.16601705  0.30834126  9.27044189], Loss = 155.1462\n",
      "Iteration 151: Weights = [43.17878155  3.43798026  8.34874016  0.1659383   0.30827008  9.28057138], Loss = 152.0635\n",
      "Iteration 152: Weights = [43.30002706  3.44161307  8.35661285  0.16585723  0.30819414  9.29051237], Loss = 149.0429\n",
      "Iteration 153: Weights = [43.42006013  3.44517456  8.36431657  0.1657739   0.30811362  9.30026862], Loss = 146.0830\n",
      "Iteration 154: Weights = [43.53889286  3.44866607  8.37185467  0.1656884   0.30802864  9.30984377], Loss = 143.1827\n",
      "Iteration 155: Weights = [43.65653726  3.45208895  8.37923048  0.16560081  0.30793936  9.31924141], Loss = 140.3408\n",
      "Iteration 156: Weights = [43.77300522  3.4554445   8.3864472   0.16551121  0.30784593  9.32846505], Loss = 137.5560\n",
      "Iteration 157: Weights = [43.88830851  3.458734    8.39350803  0.16541968  0.30774847  9.33751814], Loss = 134.8272\n",
      "Iteration 158: Weights = [44.00245875  3.46195872  8.40041606  0.16532629  0.30764712  9.34640407], Loss = 132.1533\n",
      "Iteration 159: Weights = [44.1154675   3.46511989  8.40717434  0.16523111  0.30754202  9.35512613], Loss = 129.5331\n",
      "Iteration 160: Weights = [44.22734616  3.46821871  8.41378585  0.16513421  0.30743329  9.36368757], Loss = 126.9656\n",
      "Iteration 161: Weights = [44.33810603  3.47125638  8.42025352  0.16503567  0.30732106  9.37209158], Loss = 124.4497\n",
      "Iteration 162: Weights = [44.4477583   3.47423405  8.42658022  0.16493555  0.30720544  9.38034127], Loss = 121.9843\n",
      "Iteration 163: Weights = [44.55631405  3.47715287  8.43276876  0.16483391  0.30708657  9.38843969], Loss = 119.5684\n",
      "Iteration 164: Weights = [44.66378425  3.48001396  8.4388219   0.16473081  0.30696455  9.39638985], Loss = 117.2011\n",
      "Iteration 165: Weights = [44.77017974  3.48281841  8.44474232  0.16462633  0.30683951  9.40419467], Loss = 114.8812\n",
      "Iteration 166: Weights = [44.87551127  3.48556729  8.45053268  0.16452052  0.30671154  9.41185703], Loss = 112.6080\n",
      "Iteration 167: Weights = [44.97978949  3.48826167  8.45619558  0.16441345  0.30658076  9.41937976], Loss = 110.3804\n",
      "Iteration 168: Weights = [45.08302493  3.49090258  8.46173355  0.16430516  0.30644728  9.42676562], Loss = 108.1975\n",
      "Iteration 169: Weights = [45.18522802  3.49349103  8.46714908  0.16419571  0.30631119  9.43401732], Loss = 106.0583\n",
      "Iteration 170: Weights = [45.28640907  3.49602801  8.47244463  0.16408517  0.3061726   9.44113751], Loss = 103.9621\n",
      "Iteration 171: Weights = [45.38657831  3.49851451  8.47762257  0.16397358  0.30603161  9.4481288 ], Loss = 101.9080\n",
      "Iteration 172: Weights = [45.48574586  3.50095148  8.48268526  0.16386099  0.3058883   9.45499375], Loss = 99.8950\n",
      "Iteration 173: Weights = [45.58392174  3.50333985  8.487635    0.16374746  0.30574278  9.46173484], Loss = 97.9224\n",
      "Iteration 174: Weights = [45.68111585  3.50568054  8.49247404  0.16363303  0.30559514  9.46835454], Loss = 95.9894\n",
      "Iteration 175: Weights = [45.77733803  3.50797447  8.49720459  0.16351775  0.30544546  9.47485525], Loss = 94.0951\n",
      "Iteration 176: Weights = [45.87259798  3.5102225   8.50182881  0.16340168  0.30529383  9.48123931], Loss = 92.2387\n",
      "Iteration 177: Weights = [45.96690533  3.51242552  8.50634882  0.16328485  0.30514033  9.48750906], Loss = 90.4196\n",
      "Iteration 178: Weights = [46.06026961  3.51458436  8.51076671  0.16316731  0.30498505  9.49366673], Loss = 88.6369\n",
      "Iteration 179: Weights = [46.15270025  3.51669988  8.51508452  0.1630491   0.30482806  9.49971456], Loss = 86.8899\n",
      "Iteration 180: Weights = [46.24420658  3.51877287  8.51930423  0.16293026  0.30466945  9.50565472], Loss = 85.1780\n",
      "Iteration 181: Weights = [46.33479785  3.52080415  8.52342782  0.16281084  0.30450929  9.51148934], Loss = 83.5003\n",
      "Iteration 182: Weights = [46.4244832   3.5227945   8.52745719  0.16269088  0.30434765  9.51722051], Loss = 81.8562\n",
      "Iteration 183: Weights = [46.51327171  3.5247447   8.53139423  0.1625704   0.30418461  9.52285028], Loss = 80.2450\n",
      "Iteration 184: Weights = [46.60117232  3.52665549  8.53524078  0.16244946  0.30402023  9.52838065], Loss = 78.6661\n",
      "Iteration 185: Weights = [46.68819393  3.52852763  8.53899865  0.16232809  0.30385459  9.53381359], Loss = 77.1188\n",
      "Iteration 186: Weights = [46.77434533  3.53036184  8.54266962  0.16220632  0.30368775  9.53915103], Loss = 75.6025\n",
      "Iteration 187: Weights = [46.85963521  3.53215883  8.5462554   0.16208419  0.30351978  9.54439486], Loss = 74.1165\n",
      "Iteration 188: Weights = [46.94407219  3.53391931  8.54975772  0.16196173  0.30335074  9.54954693], Loss = 72.6603\n",
      "Iteration 189: Weights = [47.0276648   3.53564396  8.55317823  0.16183898  0.30318069  9.55460907], Loss = 71.2332\n",
      "Iteration 190: Weights = [47.11042148  3.53733345  8.55651856  0.16171596  0.30300968  9.55958304], Loss = 69.8347\n",
      "Iteration 191: Weights = [47.1923506   3.53898845  8.55978033  0.16159271  0.30283779  9.5644706 ], Loss = 68.4641\n",
      "Iteration 192: Weights = [47.27346043  3.5406096   8.5629651   0.16146925  0.30266507  9.56927345], Loss = 67.1209\n",
      "Iteration 193: Weights = [47.35375916  3.54219753  8.5660744   0.16134562  0.30249156  9.57399328], Loss = 65.8046\n",
      "Iteration 194: Weights = [47.4332549   3.54375288  8.56910975  0.16122185  0.30231734  9.57863172], Loss = 64.5147\n",
      "Iteration 195: Weights = [47.51195569  3.54527625  8.57207263  0.16109796  0.30214243  9.58319039], Loss = 63.2505\n",
      "Iteration 196: Weights = [47.58986946  3.54676823  8.57496448  0.16097397  0.30196691  9.58767085], Loss = 62.0116\n",
      "Iteration 197: Weights = [47.6670041   3.54822943  8.57778672  0.16084992  0.30179082  9.59207468], Loss = 60.7975\n",
      "Iteration 198: Weights = [47.74336739  3.54966041  8.58054074  0.16072584  0.3016142   9.59640337], Loss = 59.6076\n",
      "Iteration 199: Weights = [47.81896705  3.55106174  8.58322792  0.16060173  0.30143711  9.60065841], Loss = 58.4416\n",
      "Iteration 200: Weights = [47.89381071  3.55243397  8.58584958  0.16047764  0.30125958  9.60484127], Loss = 57.2988\n",
      "Iteration 201: Weights = [47.96790594  3.55377765  8.58840703  0.16035357  0.30108167  9.60895337], Loss = 56.1789\n",
      "Iteration 202: Weights = [48.04126021  3.55509331  8.59090156  0.16022956  0.30090342  9.61299611], Loss = 55.0813\n",
      "Iteration 203: Weights = [48.11388095  3.55638147  8.59333443  0.16010562  0.30072486  9.61697087], Loss = 54.0057\n",
      "Iteration 204: Weights = [48.18577547  3.55764265  8.59570687  0.15998178  0.30054605  9.62087899], Loss = 52.9516\n",
      "Iteration 205: Weights = [48.25695105  3.55887735  8.59802008  0.15985806  0.30036701  9.62472178], Loss = 51.9186\n",
      "Iteration 206: Weights = [48.32741487  3.56008607  8.60027526  0.15973447  0.30018779  9.62850055], Loss = 50.9061\n",
      "Iteration 207: Weights = [48.39717406  3.56126928  8.60247357  0.15961104  0.30000842  9.63221655], Loss = 49.9139\n",
      "Iteration 208: Weights = [48.46623565  3.56242746  8.60461614  0.15948778  0.29982894  9.63587103], Loss = 48.9415\n",
      "Iteration 209: Weights = [48.53460663  3.56356108  8.60670409  0.15936471  0.29964939  9.63946521], Loss = 47.9886\n",
      "Iteration 210: Weights = [48.60229389  3.5646706   8.60873851  0.15924186  0.2994698   9.64300028], Loss = 47.0547\n",
      "Iteration 211: Weights = [48.66930429  3.56575646  8.61072047  0.15911922  0.2992902   9.6464774 ], Loss = 46.1394\n",
      "Iteration 212: Weights = [48.73564458  3.5668191   8.61265103  0.15899683  0.29911063  9.64989774], Loss = 45.2424\n",
      "Iteration 213: Weights = [48.80132147  3.56785895  8.61453121  0.1588747   0.29893112  9.6532624 ], Loss = 44.3633\n",
      "Iteration 214: Weights = [48.86634158  3.56887644  8.61636203  0.15875284  0.29875169  9.65657249], Loss = 43.5017\n",
      "Iteration 215: Weights = [48.9307115   3.56987198  8.61814446  0.15863127  0.29857238  9.6598291 ], Loss = 42.6574\n",
      "Iteration 216: Weights = [48.99443772  3.57084598  8.61987948  0.15851     0.29839321  9.66303327], Loss = 41.8299\n",
      "Iteration 217: Weights = [49.05752668  3.57179883  8.62156804  0.15838904  0.29821421  9.66618605], Loss = 41.0189\n",
      "Iteration 218: Weights = [49.11998474  3.57273093  8.62321107  0.15826842  0.29803542  9.66928845], Loss = 40.2241\n",
      "Iteration 219: Weights = [49.18181823  3.57364266  8.62480948  0.15814813  0.29785685  9.67234147], Loss = 39.4452\n",
      "Iteration 220: Weights = [49.24303338  3.5745344   8.62636417  0.1580282   0.29767853  9.67534608], Loss = 38.6818\n",
      "Iteration 221: Weights = [49.30363638  3.57540652  8.62787601  0.15790864  0.29750048  9.67830325], Loss = 37.9337\n",
      "Iteration 222: Weights = [49.36363335  3.57625937  8.62934585  0.15778945  0.29732274  9.68121391], Loss = 37.2005\n",
      "Iteration 223: Weights = [49.42303035  3.57709332  8.63077456  0.15767064  0.29714531  9.68407898], Loss = 36.4819\n",
      "Iteration 224: Weights = [49.48183338  3.57790872  8.63216294  0.15755224  0.29696823  9.68689937], Loss = 35.7776\n",
      "Iteration 225: Weights = [49.54004838  3.5787059   8.6335118   0.15743425  0.29679151  9.68967595], Loss = 35.0874\n",
      "Iteration 226: Weights = [49.59768123  3.57948519  8.63482194  0.15731667  0.29661518  9.6924096 ], Loss = 34.4110\n",
      "Iteration 227: Weights = [49.65473775  3.58024694  8.63609414  0.15719953  0.29643925  9.69510115], Loss = 33.7481\n",
      "Iteration 228: Weights = [49.7112237   3.58099147  8.63732916  0.15708282  0.29626375  9.69775146], Loss = 33.0983\n",
      "Iteration 229: Weights = [49.7671448   3.58171908  8.63852773  0.15696655  0.29608869  9.70036132], Loss = 32.4616\n",
      "Iteration 230: Weights = [49.82250669  3.58243009  8.6396906   0.15685074  0.29591409  9.70293154], Loss = 31.8375\n",
      "Iteration 231: Weights = [49.87731495  3.5831248   8.64081848  0.1567354   0.29573996  9.70546291], Loss = 31.2259\n",
      "Iteration 232: Weights = [49.93157514  3.58380352  8.64191207  0.15662052  0.29556634  9.70795619], Loss = 30.6265\n",
      "Iteration 233: Weights = [49.98529272  3.58446654  8.64297206  0.15650612  0.29539322  9.71041214], Loss = 30.0391\n",
      "Iteration 234: Weights = [50.03847312  3.58511414  8.64399912  0.1563922   0.29522064  9.71283149], Loss = 29.4633\n",
      "Iteration 235: Weights = [50.09112173  3.5857466   8.64499392  0.15627878  0.29504859  9.71521496], Loss = 28.8990\n",
      "Iteration 236: Weights = [50.14324384  3.58636421  8.6459571   0.15616585  0.2948771   9.71756328], Loss = 28.3460\n",
      "Iteration 237: Weights = [50.19484474  3.58696724  8.6468893   0.15605343  0.29470618  9.71987713], Loss = 27.8040\n",
      "Iteration 238: Weights = [50.24592962  3.58755594  8.64779115  0.15594152  0.29453585  9.72215719], Loss = 27.2729\n",
      "Iteration 239: Weights = [50.29650366  3.58813059  8.64866324  0.15583012  0.29436611  9.72440413], Loss = 26.7523\n",
      "Iteration 240: Weights = [50.34657196  3.58869144  8.64950618  0.15571924  0.29419698  9.7266186 ], Loss = 26.2421\n",
      "Iteration 241: Weights = [50.39613957  3.58923873  8.65032055  0.15560889  0.29402847  9.72880126], Loss = 25.7420\n",
      "Iteration 242: Weights = [50.44521151  3.58977273  8.65110693  0.15549907  0.29386059  9.73095272], Loss = 25.2519\n",
      "Iteration 243: Weights = [50.49379273  3.59029367  8.65186589  0.15538978  0.29369336  9.73307361], Loss = 24.7716\n",
      "Iteration 244: Weights = [50.54188813  3.59080178  8.65259796  0.15528104  0.29352678  9.73516452], Loss = 24.3009\n",
      "Iteration 245: Weights = [50.58950258  3.5912973   8.6533037   0.15517283  0.29336086  9.73722606], Loss = 23.8395\n",
      "Iteration 246: Weights = [50.63664089  3.59178046  8.65398363  0.15506518  0.29319562  9.73925879], Loss = 23.3874\n",
      "Iteration 247: Weights = [50.68330782  3.59225148  8.65463827  0.15495807  0.29303106  9.7412633 ], Loss = 22.9442\n",
      "Iteration 248: Weights = [50.72950807  3.59271059  8.65526813  0.15485152  0.2928672   9.74324013], Loss = 22.5099\n",
      "Iteration 249: Weights = [50.77524632  3.593158    8.65587371  0.15474553  0.29270403  9.74518984], Loss = 22.0843\n",
      "Iteration 250: Weights = [50.82052719  3.59359392  8.6564555   0.15464009  0.29254157  9.74711296], Loss = 21.6671\n",
      "Iteration 251: Weights = [50.86535526  3.59401855  8.65701398  0.15453522  0.29237983  9.74901001], Loss = 21.2582\n",
      "Iteration 252: Weights = [50.90973504  3.59443211  8.65754961  0.15443092  0.29221881  9.75088152], Loss = 20.8575\n",
      "Iteration 253: Weights = [50.95367102  3.59483479  8.65806286  0.15432718  0.29205852  9.75272798], Loss = 20.4648\n",
      "Iteration 254: Weights = [50.99716764  3.5952268   8.65855417  0.15422402  0.29189896  9.75454989], Loss = 20.0798\n",
      "Iteration 255: Weights = [51.0402293   3.59560831  8.65902399  0.15412142  0.29174015  9.75634774], Loss = 19.7026\n",
      "Iteration 256: Weights = [51.08286034  3.59597952  8.65947275  0.1540194   0.29158209  9.758122  ], Loss = 19.3329\n",
      "Iteration 257: Weights = [51.12506507  3.59634062  8.65990088  0.15391796  0.29142478  9.75987314], Loss = 18.9705\n",
      "Iteration 258: Weights = [51.16684775  3.59669179  8.66030879  0.15381709  0.29126824  9.76160161], Loss = 18.6154\n",
      "Iteration 259: Weights = [51.20821261  3.59703321  8.66069689  0.15371681  0.29111245  9.76330787], Loss = 18.2673\n",
      "Iteration 260: Weights = [51.24916382  3.59736505  8.66106558  0.1536171   0.29095744  9.76499234], Loss = 17.9262\n",
      "Iteration 261: Weights = [51.28970551  3.59768748  8.66141525  0.15351797  0.2908032   9.76665547], Loss = 17.5918\n",
      "Iteration 262: Weights = [51.32984179  3.59800068  8.66174628  0.15341943  0.29064974  9.76829766], Loss = 17.2641\n",
      "Iteration 263: Weights = [51.3695767   3.5983048   8.66205906  0.15332146  0.29049706  9.76991935], Loss = 16.9430\n",
      "Iteration 264: Weights = [51.40891427  3.59860001  8.66235394  0.15322409  0.29034517  9.77152092], Loss = 16.6282\n",
      "Iteration 265: Weights = [51.44785846  3.59888648  8.66263129  0.15312729  0.29019406  9.77310279], Loss = 16.3197\n",
      "Iteration 266: Weights = [51.48641321  3.59916435  8.66289146  0.15303108  0.29004374  9.77466533], Loss = 16.0174\n",
      "Iteration 267: Weights = [51.52458241  3.59943378  8.6631348   0.15293545  0.28989422  9.77620892], Loss = 15.7210\n",
      "Iteration 268: Weights = [51.56236992  3.59969492  8.66336165  0.15284041  0.2897455   9.77773396], Loss = 15.4306\n",
      "Iteration 269: Weights = [51.59977955  3.59994792  8.66357235  0.15274595  0.28959758  9.77924079], Loss = 15.1460\n",
      "Iteration 270: Weights = [51.63681509  3.60019292  8.66376722  0.15265207  0.28945045  9.78072978], Loss = 14.8670\n",
      "Iteration 271: Weights = [51.67348027  3.60043007  8.66394657  0.15255878  0.28930413  9.78220128], Loss = 14.5936\n",
      "Iteration 272: Weights = [51.70977881  3.6006595   8.66411073  0.15246607  0.28915862  9.78365563], Loss = 14.3256\n",
      "Iteration 273: Weights = [51.74571435  3.60088136  8.66425999  0.15237395  0.28901391  9.78509318], Loss = 14.0630\n",
      "Iteration 274: Weights = [51.78129054  3.60109577  8.66439467  0.15228241  0.28887001  9.78651426], Loss = 13.8056\n",
      "Iteration 275: Weights = [51.81651097  3.60130287  8.66451506  0.15219145  0.28872692  9.78791919], Loss = 13.5533\n",
      "Iteration 276: Weights = [51.85137919  3.60150279  8.66462144  0.15210107  0.28858464  9.78930829], Loss = 13.3060\n",
      "Iteration 277: Weights = [51.88589873  3.60169565  8.6647141   0.15201127  0.28844317  9.79068187], Loss = 13.0637\n",
      "Iteration 278: Weights = [51.92007308  3.60188159  8.66479332  0.15192204  0.2883025   9.79204023], Loss = 12.8261\n",
      "Iteration 279: Weights = [51.95390568  3.60206072  8.66485936  0.1518334   0.28816265  9.79338369], Loss = 12.5934\n",
      "Iteration 280: Weights = [51.98739996  3.60223316  8.6649125   0.15174533  0.28802361  9.79471253], Loss = 12.3652\n",
      "Iteration 281: Weights = [52.02055929  3.60239903  8.664953    0.15165784  0.28788539  9.79602704], Loss = 12.1416\n",
      "Iteration 282: Weights = [52.05338703  3.60255844  8.66498111  0.15157093  0.28774797  9.79732751], Loss = 11.9224\n",
      "Iteration 283: Weights = [52.0858865   3.60271152  8.66499709  0.15148458  0.28761136  9.7986142 ], Loss = 11.7076\n",
      "Iteration 284: Weights = [52.11806096  3.60285837  8.66500117  0.15139881  0.28747557  9.7998874 ], Loss = 11.4971\n",
      "Iteration 285: Weights = [52.14991369  3.6029991   8.66499361  0.15131361  0.28734058  9.80114737], Loss = 11.2908\n",
      "Iteration 286: Weights = [52.18144788  3.60313382  8.66497463  0.15122897  0.28720641  9.80239436], Loss = 11.0885\n",
      "Iteration 287: Weights = [52.21266674  3.60326263  8.66494448  0.15114491  0.28707304  9.80362865], Loss = 10.8903\n",
      "Iteration 288: Weights = [52.2435734   3.60338564  8.66490337  0.1510614   0.28694048  9.80485046], Loss = 10.6961\n",
      "Iteration 289: Weights = [52.274171    3.60350295  8.66485153  0.15097846  0.28680872  9.80606006], Loss = 10.5057\n",
      "Iteration 290: Weights = [52.30446263  3.60361465  8.66478917  0.15089608  0.28667777  9.80725768], Loss = 10.3191\n",
      "Iteration 291: Weights = [52.33445133  3.60372085  8.66471652  0.15081427  0.28654762  9.80844356], Loss = 10.1362\n",
      "Iteration 292: Weights = [52.36414015  3.60382165  8.66463377  0.150733    0.28641827  9.80961793], Loss = 9.9569\n",
      "Iteration 293: Weights = [52.39353209  3.60391713  8.66454114  0.1506523   0.28628973  9.81078102], Loss = 9.7812\n",
      "Iteration 294: Weights = [52.4226301   3.60400739  8.66443883  0.15057214  0.28616198  9.81193305], Loss = 9.6090\n",
      "Iteration 295: Weights = [52.45143713  3.60409252  8.66432702  0.15049254  0.28603503  9.81307423], Loss = 9.4403\n",
      "Iteration 296: Weights = [52.47995609  3.60417261  8.66420592  0.15041349  0.28590887  9.81420479], Loss = 9.2749\n",
      "Iteration 297: Weights = [52.50818987  3.60424774  8.66407572  0.15033498  0.2857835   9.81532492], Loss = 9.1127\n",
      "Iteration 298: Weights = [52.5361413   3.60431801  8.6639366   0.15025702  0.28565893  9.81643484], Loss = 8.9539\n",
      "Iteration 299: Weights = [52.56381322  3.60438349  8.66378874  0.1501796   0.28553514  9.81753475], Loss = 8.7981\n",
      "Iteration 300: Weights = [52.59120842  3.60444428  8.66363231  0.15010272  0.28541214  9.81862484], Loss = 8.6455\n",
      "Iteration 301: Weights = [52.61832967  3.60450044  8.6634675   0.15002638  0.28528992  9.81970531], Loss = 8.4959\n",
      "Iteration 302: Weights = [52.64517971  3.60455206  8.66329448  0.14995057  0.28516849  9.82077635], Loss = 8.3493\n",
      "Iteration 303: Weights = [52.67176124  3.60459923  8.66311341  0.14987529  0.28504783  9.82183813], Loss = 8.2056\n",
      "Iteration 304: Weights = [52.69807696  3.60464201  8.66292445  0.14980055  0.28492794  9.82289086], Loss = 8.0647\n",
      "Iteration 305: Weights = [52.72412953  3.60468048  8.66272778  0.14972633  0.28480883  9.82393469], Loss = 7.9267\n",
      "Iteration 306: Weights = [52.74992157  3.60471471  8.66252353  0.14965264  0.28469049  9.82496982], Loss = 7.7914\n",
      "Iteration 307: Weights = [52.77545568  3.60474479  8.66231188  0.14957947  0.28457292  9.8259964 ], Loss = 7.6588\n",
      "Iteration 308: Weights = [52.80073446  3.60477077  8.66209297  0.14950682  0.28445611  9.82701461], Loss = 7.5288\n",
      "Iteration 309: Weights = [52.82576045  3.60479273  8.66186694  0.14943468  0.28434006  9.82802461], Loss = 7.4014\n",
      "Iteration 310: Weights = [52.85053618  3.60481074  8.66163395  0.14936306  0.28422477  9.82902657], Loss = 7.2766\n",
      "Iteration 311: Weights = [52.87506415  3.60482487  8.66139414  0.14929196  0.28411024  9.83002064], Loss = 7.1542\n",
      "Iteration 312: Weights = [52.89934684  3.60483518  8.66114765  0.14922136  0.28399646  9.83100697], Loss = 7.0343\n",
      "Iteration 313: Weights = [52.92338671  3.60484173  8.6608946   0.14915127  0.28388342  9.83198573], Loss = 6.9167\n",
      "Iteration 314: Weights = [52.94718617  3.6048446   8.66063515  0.14908168  0.28377113  9.83295705], Loss = 6.8015\n",
      "Iteration 315: Weights = [52.97074764  3.60484384  8.66036941  0.14901259  0.28365959  9.83392109], Loss = 6.6886\n",
      "Iteration 316: Weights = [52.9940735   3.60483952  8.66009752  0.148944    0.28354878  9.83487798], Loss = 6.5779\n",
      "Iteration 317: Weights = [53.0171661   3.60483169  8.65981961  0.14887591  0.28343871  9.83582788], Loss = 6.4694\n",
      "Iteration 318: Weights = [53.04002777  3.60482042  8.65953579  0.1488083   0.28332937  9.8367709 ], Loss = 6.3631\n",
      "Iteration 319: Weights = [53.06266083  3.60480577  8.65924619  0.14874119  0.28322075  9.8377072 ], Loss = 6.2589\n",
      "Iteration 320: Weights = [53.08506755  3.60478778  8.65895092  0.14867457  0.28311286  9.83863691], Loss = 6.1568\n",
      "Iteration 321: Weights = [53.10725021  3.60476653  8.65865011  0.14860842  0.2830057   9.83956014], Loss = 6.0567\n",
      "Iteration 322: Weights = [53.12921104  3.60474205  8.65834387  0.14854276  0.28289925  9.84047704], Loss = 5.9586\n",
      "Iteration 323: Weights = [53.15095226  3.60471442  8.6580323   0.14847758  0.28279351  9.84138771], Loss = 5.8624\n",
      "Iteration 324: Weights = [53.17247608  3.60468367  8.65771552  0.14841287  0.28268849  9.8422923 ], Loss = 5.7682\n",
      "Iteration 325: Weights = [53.19378465  3.60464987  8.65739364  0.14834863  0.28258417  9.84319091], Loss = 5.6758\n",
      "Iteration 326: Weights = [53.21488013  3.60461306  8.65706676  0.14828487  0.28248055  9.84408366], Loss = 5.5852\n",
      "Iteration 327: Weights = [53.23576467  3.60457329  8.65673498  0.14822157  0.28237763  9.84497067], Loss = 5.4965\n",
      "Iteration 328: Weights = [53.25644035  3.60453062  8.65639841  0.14815873  0.28227541  9.84585205], Loss = 5.4095\n",
      "Iteration 329: Weights = [53.27690928  3.6044851   8.65605713  0.14809635  0.28217388  9.84672791], Loss = 5.3243\n",
      "Iteration 330: Weights = [53.29717352  3.60443676  8.65571126  0.14803443  0.28207304  9.84759837], Loss = 5.2407\n",
      "Iteration 331: Weights = [53.31723512  3.60438566  8.65536088  0.14797297  0.28197288  9.84846351], Loss = 5.1588\n",
      "Iteration 332: Weights = [53.3370961   3.60433185  8.65500609  0.14791195  0.2818734   9.84932346], Loss = 5.0785\n",
      "Iteration 333: Weights = [53.35675848  3.60427536  8.65464698  0.14785139  0.28177459  9.85017831], Loss = 4.9999\n",
      "Iteration 334: Weights = [53.37622422  3.60421625  8.65428364  0.14779127  0.28167646  9.85102816], Loss = 4.9228\n",
      "Iteration 335: Weights = [53.39549532  3.60415455  8.65391615  0.14773159  0.28157899  9.85187311], Loss = 4.8472\n",
      "Iteration 336: Weights = [53.4145737   3.60409032  8.65354461  0.14767235  0.28148219  9.85271325], Loss = 4.7731\n",
      "Iteration 337: Weights = [53.43346129  3.60402359  8.65316908  0.14761355  0.28138605  9.85354869], Loss = 4.7005\n",
      "Iteration 338: Weights = [53.45216001  3.6039544   8.65278967  0.14755519  0.28129057  9.8543795 ], Loss = 4.6293\n",
      "Iteration 339: Weights = [53.47067175  3.60388279  8.65240645  0.14749725  0.28119573  9.85520579], Loss = 4.5596\n",
      "Iteration 340: Weights = [53.48899836  3.60380881  8.65201949  0.14743974  0.28110155  9.85602764], Loss = 4.4912\n",
      "Iteration 341: Weights = [53.50714171  3.6037325   8.65162887  0.14738266  0.28100801  9.85684514], Loss = 4.4242\n",
      "Iteration 342: Weights = [53.52510363  3.60365388  8.65123468  0.147326    0.2809151   9.85765837], Loss = 4.3585\n",
      "Iteration 343: Weights = [53.54288592  3.60357301  8.65083698  0.14726976  0.28082284  9.85846742], Loss = 4.2942\n",
      "Iteration 344: Weights = [53.5604904   3.60348991  8.65043585  0.14721393  0.2807312   9.85927236], Loss = 4.2311\n",
      "Iteration 345: Weights = [53.57791883  3.60340462  8.65003136  0.14715852  0.2806402   9.86007329], Loss = 4.1692\n",
      "Iteration 346: Weights = [53.59517297  3.60331719  8.64962357  0.14710351  0.28054981  9.86087027], Loss = 4.1086\n",
      "Iteration 347: Weights = [53.61225458  3.60322764  8.64921257  0.14704892  0.28046005  9.86166338], Loss = 4.0492\n",
      "Iteration 348: Weights = [53.62916536  3.603136    8.6487984   0.14699472  0.2803709   9.86245271], Loss = 3.9910\n",
      "Iteration 349: Weights = [53.64590704  3.60304232  8.64838115  0.14694093  0.28028237  9.86323832], Loss = 3.9339\n",
      "Iteration 350: Weights = [53.66248131  3.60294663  8.64796087  0.14688754  0.28019444  9.86402028], Loss = 3.8780\n",
      "Iteration 351: Weights = [53.67888983  3.60284896  8.64753764  0.14683454  0.28010711  9.86479868], Loss = 3.8232\n",
      "Iteration 352: Weights = [53.69513426  3.60274934  8.6471115   0.14678194  0.28002039  9.86557357], Loss = 3.7694\n",
      "Iteration 353: Weights = [53.71121625  3.6026478   8.64668252  0.14672973  0.27993426  9.86634503], Loss = 3.7167\n",
      "Iteration 354: Weights = [53.72713742  3.60254437  8.64625076  0.1466779   0.27984872  9.86711312], Loss = 3.6651\n",
      "Iteration 355: Weights = [53.74289938  3.60243909  8.64581628  0.14662645  0.27976377  9.86787791], Loss = 3.6145\n",
      "Iteration 356: Weights = [53.75850372  3.60233199  8.64537913  0.14657539  0.2796794   9.86863946], Loss = 3.5649\n",
      "Iteration 357: Weights = [53.77395202  3.60222309  8.64493937  0.14652471  0.27959561  9.86939784], Loss = 3.5163\n",
      "Iteration 358: Weights = [53.78924583  3.60211242  8.64449706  0.1464744   0.27951239  9.87015311], Loss = 3.4687\n",
      "Iteration 359: Weights = [53.80438671  3.60200002  8.64405225  0.14642446  0.27942975  9.87090532], Loss = 3.4220\n",
      "Iteration 360: Weights = [53.81937617  3.6018859   8.64360499  0.14637489  0.27934767  9.87165454], Loss = 3.3762\n",
      "Iteration 361: Weights = [53.83421574  3.6017701   8.64315533  0.14632569  0.27926616  9.87240083], Loss = 3.3313\n",
      "Iteration 362: Weights = [53.84890692  3.60165265  8.64270333  0.14627685  0.27918521  9.87314425], Loss = 3.2873\n",
      "Iteration 363: Weights = [53.86345118  3.60153357  8.64224903  0.14622838  0.27910481  9.87388484], Loss = 3.2442\n",
      "Iteration 364: Weights = [53.87785001  3.60141288  8.64179248  0.14618026  0.27902496  9.87462267], Loss = 3.2020\n",
      "Iteration 365: Weights = [53.89210484  3.60129062  8.64133373  0.1461325   0.27894566  9.87535779], Loss = 3.1606\n",
      "Iteration 366: Weights = [53.90621712  3.6011668   8.64087282  0.14608509  0.2788669   9.87609024], Loss = 3.1200\n",
      "Iteration 367: Weights = [53.92018829  3.60104146  8.64040981  0.14603803  0.27878868  9.87682009], Loss = 3.0802\n",
      "Iteration 368: Weights = [53.93401974  3.60091462  8.63994473  0.14599132  0.278711    9.87754739], Loss = 3.0412\n",
      "Iteration 369: Weights = [53.94771287  3.60078629  8.63947763  0.14594495  0.27863385  9.87827217], Loss = 3.0030\n",
      "Iteration 370: Weights = [53.96126908  3.60065651  8.63900855  0.14589892  0.27855722  9.8789945 ], Loss = 2.9655\n",
      "Iteration 371: Weights = [53.97468972  3.6005253   8.63853753  0.14585324  0.27848112  9.87971442], Loss = 2.9288\n",
      "Iteration 372: Weights = [53.98797616  3.60039267  8.63806462  0.14580789  0.27840554  9.88043197], Loss = 2.8928\n",
      "Iteration 373: Weights = [54.00112973  3.60025866  8.63758986  0.14576287  0.27833047  9.88114721], Loss = 2.8575\n",
      "Iteration 374: Weights = [54.01415176  3.60012329  8.63711328  0.14571819  0.27825592  9.88186017], Loss = 2.8229\n",
      "Iteration 375: Weights = [54.02704358  3.59998657  8.63663492  0.14567383  0.27818187  9.88257091], Loss = 2.7890\n",
      "Iteration 376: Weights = [54.03980648  3.59984852  8.63615482  0.1456298   0.27810833  9.88327946], Loss = 2.7558\n",
      "Iteration 377: Weights = [54.05244175  3.59970918  8.63567302  0.14558609  0.27803528  9.88398586], Loss = 2.7232\n",
      "Iteration 378: Weights = [54.06495066  3.59956855  8.63518956  0.1455427   0.27796274  9.88469017], Loss = 2.6913\n",
      "Iteration 379: Weights = [54.07733449  3.59942666  8.63470446  0.14549963  0.27789068  9.88539241], Loss = 2.6600\n",
      "Iteration 380: Weights = [54.08959448  3.59928353  8.63421778  0.14545687  0.27781912  9.88609263], Loss = 2.6293\n",
      "Iteration 381: Weights = [54.10173187  3.59913918  8.63372952  0.14541443  0.27774804  9.88679087], Loss = 2.5993\n",
      "Iteration 382: Weights = [54.11374788  3.59899363  8.63323975  0.1453723   0.27767744  9.88748717], Loss = 2.5698\n",
      "Iteration 383: Weights = [54.12564373  3.59884689  8.63274847  0.14533047  0.27760732  9.88818156], Loss = 2.5409\n",
      "Iteration 384: Weights = [54.13742063  3.59869898  8.63225574  0.14528895  0.27753767  9.88887408], Loss = 2.5126\n",
      "Iteration 385: Weights = [54.14907976  3.59854993  8.63176157  0.14524773  0.2774685   9.88956476], Loss = 2.4849\n",
      "Iteration 386: Weights = [54.16062229  3.59839974  8.63126601  0.14520682  0.27739979  9.89025365], Loss = 2.4577\n",
      "Iteration 387: Weights = [54.1720494   3.59824844  8.63076907  0.1451662   0.27733154  9.89094078], Loss = 2.4310\n",
      "Iteration 388: Weights = [54.18336224  3.59809605  8.63027079  0.14512587  0.27726375  9.89162618], Loss = 2.4049\n",
      "Iteration 389: Weights = [54.19456195  3.59794258  8.6297712   0.14508584  0.27719642  9.89230988], Loss = 2.3792\n",
      "Iteration 390: Weights = [54.20564967  3.59778805  8.62927033  0.14504609  0.27712954  9.89299191], Loss = 2.3541\n",
      "Iteration 391: Weights = [54.2166265   3.59763247  8.6287682   0.14500664  0.2770631   9.89367232], Loss = 2.3295\n",
      "Iteration 392: Weights = [54.22749357  3.59747587  8.62826484  0.14496747  0.27699712  9.89435113], Loss = 2.3054\n",
      "Iteration 393: Weights = [54.23825197  3.59731825  8.62776029  0.14492858  0.27693157  9.89502836], Loss = 2.2817\n",
      "Iteration 394: Weights = [54.24890278  3.59715963  8.62725455  0.14488997  0.27686646  9.89570406], Loss = 2.2586\n",
      "Iteration 395: Weights = [54.25944709  3.59700003  8.62674767  0.14485164  0.27680178  9.89637825], Loss = 2.2358\n",
      "Iteration 396: Weights = [54.26988595  3.59683946  8.62623966  0.14481359  0.27673754  9.89705096], Loss = 2.2136\n",
      "Iteration 397: Weights = [54.28022043  3.59667794  8.62573055  0.14477581  0.27667372  9.89772222], Loss = 2.1917\n",
      "Iteration 398: Weights = [54.29045155  3.59651549  8.62522037  0.1447383   0.27661033  9.89839205], Loss = 2.1703\n",
      "Iteration 399: Weights = [54.30058037  3.59635211  8.62470913  0.14470105  0.27654735  9.89906048], Loss = 2.1494\n",
      "Iteration 400: Weights = [54.3106079   3.59618781  8.62419687  0.14466408  0.2764848   9.89972755], Loss = 2.1288\n",
      "Iteration 401: Weights = [54.32053516  3.59602263  8.62368359  0.14462737  0.27642265  9.90039327], Loss = 2.1086\n",
      "Iteration 402: Weights = [54.33036314  3.59585656  8.62316933  0.14459092  0.27636092  9.90105767], Loss = 2.0889\n",
      "Iteration 403: Weights = [54.34009284  3.59568962  8.62265411  0.14455473  0.27629959  9.90172078], Loss = 2.0695\n",
      "Iteration 404: Weights = [54.34972525  3.59552183  8.62213795  0.14451879  0.27623867  9.90238263], Loss = 2.0505\n",
      "Iteration 405: Weights = [54.35926133  3.59535319  8.62162087  0.14448312  0.27617814  9.90304322], Loss = 2.0319\n",
      "Iteration 406: Weights = [54.36870205  3.59518373  8.62110288  0.14444769  0.27611801  9.9037026 ], Loss = 2.0137\n",
      "Iteration 407: Weights = [54.37804836  3.59501344  8.62058402  0.14441252  0.27605828  9.90436078], Loss = 1.9958\n",
      "Iteration 408: Weights = [54.38730121  3.59484235  8.62006429  0.14437759  0.27599893  9.90501779], Loss = 1.9782\n",
      "Iteration 409: Weights = [54.39646153  3.59467047  8.61954372  0.14434291  0.27593997  9.90567364], Loss = 1.9611\n",
      "Iteration 410: Weights = [54.40553025  3.59449781  8.61902233  0.14430847  0.2758814   9.90632837], Loss = 1.9442\n",
      "Iteration 411: Weights = [54.41450828  3.59432438  8.61850014  0.14427428  0.27582321  9.90698198], Loss = 1.9277\n",
      "Iteration 412: Weights = [54.42339653  3.59415019  8.61797715  0.14424032  0.27576539  9.90763451], Loss = 1.9115\n",
      "Iteration 413: Weights = [54.4321959   3.59397525  8.6174534   0.14420661  0.27570795  9.90828598], Loss = 1.8956\n",
      "Iteration 414: Weights = [54.44090727  3.59379958  8.6169289   0.14417312  0.27565087  9.90893639], Loss = 1.8801\n",
      "Iteration 415: Weights = [54.44953153  3.59362318  8.61640366  0.14413988  0.27559417  9.90958578], Loss = 1.8648\n",
      "Iteration 416: Weights = [54.45806955  3.59344607  8.6158777   0.14410686  0.27553783  9.91023416], Loss = 1.8499\n",
      "Iteration 417: Weights = [54.46652219  3.59326826  8.61535104  0.14407407  0.27548185  9.91088155], Loss = 1.8352\n",
      "Iteration 418: Weights = [54.4748903   3.59308976  8.6148237   0.14404151  0.27542623  9.91152798], Loss = 1.8209\n",
      "Iteration 419: Weights = [54.48317473  3.59291057  8.61429568  0.14400918  0.27537096  9.91217345], Loss = 1.8068\n",
      "Iteration 420: Weights = [54.49137632  3.59273071  8.61376701  0.14397706  0.27531605  9.91281799], Loss = 1.7930\n",
      "Iteration 421: Weights = [54.49949589  3.59255019  8.6132377   0.14394517  0.27526148  9.91346161], Loss = 1.7795\n",
      "Iteration 422: Weights = [54.50753426  3.59236902  8.61270776  0.1439135   0.27520727  9.91410434], Loss = 1.7662\n",
      "Iteration 423: Weights = [54.51549225  3.5921872   8.61217721  0.14388205  0.27515339  9.91474618], Loss = 1.7532\n",
      "Iteration 424: Weights = [54.52337066  3.59200475  8.61164606  0.14385081  0.27509986  9.91538716], Loss = 1.7404\n",
      "Iteration 425: Weights = [54.53117029  3.59182168  8.61111433  0.14381978  0.27504666  9.91602729], Loss = 1.7279\n",
      "Iteration 426: Weights = [54.53889192  3.59163799  8.61058203  0.14378897  0.2749938   9.91666659], Loss = 1.7157\n",
      "Iteration 427: Weights = [54.54653633  3.59145369  8.61004917  0.14375837  0.27494127  9.91730507], Loss = 1.7037\n",
      "Iteration 428: Weights = [54.5541043   3.5912688   8.60951577  0.14372797  0.27488907  9.91794275], Loss = 1.6919\n",
      "Iteration 429: Weights = [54.56159659  3.59108332  8.60898184  0.14369778  0.27483719  9.91857964], Loss = 1.6804\n",
      "Iteration 430: Weights = [54.56901396  3.59089726  8.60844738  0.14366779  0.27478564  9.91921576], Loss = 1.6690\n",
      "Iteration 431: Weights = [54.57635715  3.59071063  8.60791242  0.14363801  0.27473441  9.91985112], Loss = 1.6580\n",
      "Iteration 432: Weights = [54.58362692  3.59052344  8.60737697  0.14360842  0.27468349  9.92048574], Loss = 1.6471\n",
      "Iteration 433: Weights = [54.59082398  3.59033569  8.60684103  0.14357904  0.27463289  9.92111963], Loss = 1.6364\n",
      "Iteration 434: Weights = [54.59794907  3.5901474   8.60630461  0.14354985  0.2745826   9.9217528 ], Loss = 1.6260\n",
      "Iteration 435: Weights = [54.60500292  3.58995857  8.60576774  0.14352086  0.27453263  9.92238527], Loss = 1.6157\n",
      "Iteration 436: Weights = [54.61198622  3.5897692   8.60523042  0.14349206  0.27448295  9.92301705], Loss = 1.6057\n",
      "Iteration 437: Weights = [54.61889969  3.58957932  8.60469265  0.14346345  0.27443358  9.92364815], Loss = 1.5958\n",
      "Iteration 438: Weights = [54.62574403  3.58938892  8.60415446  0.14343503  0.27438452  9.92427859], Loss = 1.5862\n",
      "Iteration 439: Weights = [54.63251992  3.58919801  8.60361585  0.14340679  0.27433575  9.92490838], Loss = 1.5767\n",
      "Iteration 440: Weights = [54.63922806  3.5890066   8.60307683  0.14337875  0.27428728  9.92553752], Loss = 1.5674\n",
      "Iteration 441: Weights = [54.64586911  3.58881469  8.60253741  0.14335089  0.2742391   9.92616604], Loss = 1.5583\n",
      "Iteration 442: Weights = [54.65244375  3.5886223   8.6019976   0.14332321  0.27419121  9.92679394], Loss = 1.5494\n",
      "Iteration 443: Weights = [54.65895265  3.58842943  8.60145741  0.14329571  0.27414361  9.92742124], Loss = 1.5406\n",
      "Iteration 444: Weights = [54.66539645  3.58823609  8.60091685  0.14326839  0.27409629  9.92804794], Loss = 1.5321\n",
      "Iteration 445: Weights = [54.67177582  3.58804228  8.60037593  0.14324125  0.27404926  9.92867406], Loss = 1.5237\n",
      "Iteration 446: Weights = [54.6780914   3.58784802  8.59983465  0.14321429  0.27400251  9.92929961], Loss = 1.5154\n",
      "Iteration 447: Weights = [54.68434382  3.5876533   8.59929304  0.1431875   0.27395603  9.92992459], Loss = 1.5073\n",
      "Iteration 448: Weights = [54.69053371  3.58745813  8.59875108  0.14316088  0.27390984  9.93054902], Loss = 1.4994\n",
      "Iteration 449: Weights = [54.69666171  3.58726253  8.5982088   0.14313444  0.27386391  9.93117291], Loss = 1.4916\n",
      "Iteration 450: Weights = [54.70272842  3.58706649  8.5976662   0.14310816  0.27381826  9.93179627], Loss = 1.4840\n",
      "Iteration 451: Weights = [54.70873447  3.58687002  8.59712328  0.14308205  0.27377287  9.93241911], Loss = 1.4766\n",
      "Iteration 452: Weights = [54.71468046  3.58667313  8.59658007  0.14305611  0.27372775  9.93304143], Loss = 1.4692\n",
      "Iteration 453: Weights = [54.72056699  3.58647583  8.59603655  0.14303034  0.27368289  9.93366325], Loss = 1.4620\n",
      "Iteration 454: Weights = [54.72639465  3.58627812  8.59549275  0.14300472  0.2736383   9.93428458], Loss = 1.4550\n",
      "Iteration 455: Weights = [54.73216404  3.58608     8.59494867  0.14297927  0.27359396  9.93490542], Loss = 1.4481\n",
      "Iteration 456: Weights = [54.73787573  3.58588149  8.59440431  0.14295398  0.27354988  9.93552578], Loss = 1.4413\n",
      "Iteration 457: Weights = [54.74353031  3.58568259  8.59385969  0.14292885  0.27350605  9.93614568], Loss = 1.4347\n",
      "Iteration 458: Weights = [54.74912834  3.5854833   8.5933148   0.14290388  0.27346248  9.93676511], Loss = 1.4282\n",
      "Iteration 459: Weights = [54.75467039  3.58528362  8.59276966  0.14287906  0.27341916  9.9373841 ], Loss = 1.4218\n",
      "Iteration 460: Weights = [54.76015702  3.58508357  8.59222428  0.1428544   0.27337608  9.93800264], Loss = 1.4156\n",
      "Iteration 461: Weights = [54.76558878  3.58488315  8.59167865  0.14282989  0.27333325  9.93862075], Loss = 1.4094\n",
      "Iteration 462: Weights = [54.77096623  3.58468237  8.59113279  0.14280553  0.27329066  9.93923843], Loss = 1.4034\n",
      "Iteration 463: Weights = [54.7762899   3.58448122  8.5905867   0.14278132  0.27324831  9.93985569], Loss = 1.3975\n",
      "Iteration 464: Weights = [54.78156033  3.58427972  8.59004039  0.14275726  0.2732062   9.94047253], Loss = 1.3917\n",
      "Iteration 465: Weights = [54.78677806  3.58407787  8.58949386  0.14273335  0.27316432  9.94108897], Loss = 1.3860\n",
      "Iteration 466: Weights = [54.79194362  3.58387568  8.58894713  0.14270958  0.27312268  9.94170502], Loss = 1.3805\n",
      "Iteration 467: Weights = [54.79705751  3.58367314  8.58840018  0.14268596  0.27308128  9.94232067], Loss = 1.3750\n",
      "Iteration 468: Weights = [54.80212027  3.58347027  8.58785304  0.14266248  0.2730401   9.94293594], Loss = 1.3697\n",
      "Iteration 469: Weights = [54.8071324   3.58326707  8.58730571  0.14263915  0.27299915  9.94355083], Loss = 1.3644\n",
      "Iteration 470: Weights = [54.81209441  3.58306354  8.58675818  0.14261595  0.27295842  9.94416535], Loss = 1.3593\n",
      "Iteration 471: Weights = [54.8170068   3.58285969  8.58621047  0.1425929   0.27291792  9.94477951], Loss = 1.3543\n",
      "Iteration 472: Weights = [54.82187007  3.58265552  8.58566259  0.14256998  0.27287765  9.94539331], Loss = 1.3493\n",
      "Iteration 473: Weights = [54.8266847   3.58245104  8.58511453  0.1425472   0.27283759  9.94600676], Loss = 1.3445\n",
      "Iteration 474: Weights = [54.83145119  3.58224625  8.5845663   0.14252455  0.27279775  9.94661986], Loss = 1.3397\n",
      "Iteration 475: Weights = [54.83617001  3.58204115  8.58401791  0.14250204  0.27275812  9.94723262], Loss = 1.3350\n",
      "Iteration 476: Weights = [54.84084164  3.58183576  8.58346936  0.14247966  0.27271871  9.94784505], Loss = 1.3305\n",
      "Iteration 477: Weights = [54.84546656  3.58163007  8.58292066  0.14245741  0.27267951  9.94845716], Loss = 1.3260\n",
      "Iteration 478: Weights = [54.85004522  3.58142408  8.5823718   0.14243529  0.27264052  9.94906894], Loss = 1.3216\n",
      "Iteration 479: Weights = [54.85457811  3.58121781  8.5818228   0.14241331  0.27260174  9.9496804 ], Loss = 1.3173\n",
      "Iteration 480: Weights = [54.85906566  3.58101126  8.58127366  0.14239145  0.27256316  9.95029155], Loss = 1.3130\n",
      "Iteration 481: Weights = [54.86350833  3.58080442  8.58072438  0.14236971  0.27252479  9.9509024 ], Loss = 1.3089\n",
      "Iteration 482: Weights = [54.86790658  3.58059731  8.58017497  0.1423481   0.27248662  9.95151295], Loss = 1.3048\n",
      "Iteration 483: Weights = [54.87226085  3.58038993  8.57962543  0.14232662  0.27244865  9.9521232 ], Loss = 1.3008\n",
      "Iteration 484: Weights = [54.87657158  3.58018228  8.57907576  0.14230526  0.27241087  9.95273316], Loss = 1.2969\n",
      "Iteration 485: Weights = [54.88083919  3.57997436  8.57852597  0.14228402  0.2723733   9.95334284], Loss = 1.2930\n",
      "Iteration 486: Weights = [54.88506414  3.57976618  8.57797607  0.1422629   0.27233592  9.95395223], Loss = 1.2893\n",
      "Iteration 487: Weights = [54.88924683  3.57955775  8.57742605  0.1422419   0.27229873  9.95456135], Loss = 1.2856\n",
      "Iteration 488: Weights = [54.89338769  3.57934906  8.57687592  0.14222102  0.27226174  9.9551702 ], Loss = 1.2820\n",
      "Iteration 489: Weights = [54.89748715  3.57914012  8.57632568  0.14220026  0.27222493  9.95577878], Loss = 1.2784\n",
      "Iteration 490: Weights = [54.90154561  3.57893093  8.57577534  0.14217961  0.27218831  9.9563871 ], Loss = 1.2749\n",
      "Iteration 491: Weights = [54.90556349  3.57872149  8.57522491  0.14215908  0.27215188  9.95699516], Loss = 1.2715\n",
      "Iteration 492: Weights = [54.90954119  3.57851182  8.57467437  0.14213867  0.27211563  9.95760297], Loss = 1.2681\n",
      "Iteration 493: Weights = [54.91347911  3.57830191  8.57412374  0.14211836  0.27207957  9.95821053], Loss = 1.2648\n",
      "Iteration 494: Weights = [54.91737765  3.57809177  8.57357302  0.14209817  0.27204368  9.95881784], Loss = 1.2616\n",
      "Iteration 495: Weights = [54.92123721  3.57788139  8.57302221  0.14207809  0.27200798  9.95942491], Loss = 1.2584\n",
      "Iteration 496: Weights = [54.92505817  3.57767079  8.57247132  0.14205811  0.27197245  9.96003175], Loss = 1.2553\n",
      "Iteration 497: Weights = [54.92884092  3.57745996  8.57192035  0.14203825  0.27193709  9.96063835], Loss = 1.2523\n",
      "Iteration 498: Weights = [54.93258584  3.57724891  8.5713693   0.14201849  0.27190192  9.96124472], Loss = 1.2493\n",
      "Iteration 499: Weights = [54.93629332  3.57703764  8.57081817  0.14199884  0.27186691  9.96185087], Loss = 1.2464\n",
      "Iteration 500: Weights = [54.93996372  3.57682616  8.57026697  0.1419793   0.27183208  9.96245679], Loss = 1.2435\n",
      "Iteration 501: Weights = [54.94359742  3.57661446  8.5697157   0.14195986  0.27179741  9.9630625 ], Loss = 1.2407\n",
      "Iteration 502: Weights = [54.94719477  3.57640256  8.56916437  0.14194052  0.27176291  9.96366799], Loss = 1.2379\n",
      "Iteration 503: Weights = [54.95075616  3.57619044  8.56861296  0.14192129  0.27172858  9.96427327], Loss = 1.2352\n",
      "Iteration 504: Weights = [54.95428193  3.57597813  8.5680615   0.14190216  0.27169442  9.96487834], Loss = 1.2325\n",
      "Iteration 505: Weights = [54.95777245  3.57576561  8.56750998  0.14188312  0.27166041  9.9654832 ], Loss = 1.2299\n",
      "Iteration 506: Weights = [54.96122805  3.57555289  8.56695839  0.14186419  0.27162657  9.96608787], Loss = 1.2273\n",
      "Iteration 507: Weights = [54.96464911  3.57533997  8.56640676  0.14184536  0.27159289  9.96669233], Loss = 1.2248\n",
      "Iteration 508: Weights = [54.96803595  3.57512687  8.56585507  0.14182662  0.27155937  9.9672966 ], Loss = 1.2223\n",
      "Iteration 509: Weights = [54.97138892  3.57491357  8.56530333  0.14180798  0.271526    9.96790068], Loss = 1.2199\n",
      "Iteration 510: Weights = [54.97470837  3.57470008  8.56475154  0.14178944  0.27149279  9.96850457], Loss = 1.2175\n",
      "Iteration 511: Weights = [54.97799462  3.57448641  8.56419971  0.14177099  0.27145974  9.96910827], Loss = 1.2152\n",
      "Iteration 512: Weights = [54.981248    3.57427255  8.56364783  0.14175263  0.27142683  9.96971179], Loss = 1.2129\n",
      "Iteration 513: Weights = [54.98446886  3.57405852  8.56309591  0.14173437  0.27139408  9.97031513], Loss = 1.2106\n",
      "Iteration 514: Weights = [54.9876575   3.5738443   8.56254395  0.1417162   0.27136148  9.97091829], Loss = 1.2084\n",
      "Iteration 515: Weights = [54.99081426  3.57362991  8.56199195  0.14169812  0.27132903  9.97152127], Loss = 1.2063\n",
      "Iteration 516: Weights = [54.99393945  3.57341535  8.56143992  0.14168014  0.27129672  9.97212408], Loss = 1.2041\n",
      "Iteration 517: Weights = [54.99703339  3.57320061  8.56088785  0.14166224  0.27126456  9.97272672], Loss = 1.2021\n",
      "Iteration 518: Weights = [55.00009639  3.57298571  8.56033575  0.14164443  0.27123255  9.9733292 ], Loss = 1.2000\n",
      "Iteration 519: Weights = [55.00312876  3.57277064  8.55978362  0.1416267   0.27120067  9.97393151], Loss = 1.1980\n",
      "Iteration 520: Weights = [55.00613081  3.5725554   8.55923146  0.14160907  0.27116894  9.97453366], Loss = 1.1960\n",
      "Iteration 521: Weights = [55.00910283  3.57234001  8.55867927  0.14159152  0.27113735  9.97513564], Loss = 1.1941\n",
      "Iteration 522: Weights = [55.01204514  3.57212445  8.55812706  0.14157405  0.2711059   9.97573747], Loss = 1.1922\n",
      "Iteration 523: Weights = [55.01495802  3.57190873  8.55757482  0.14155667  0.27107458  9.97633915], Loss = 1.1903\n",
      "Iteration 524: Weights = [55.01784177  3.57169286  8.55702257  0.14153938  0.27104341  9.97694067], Loss = 1.1885\n",
      "Iteration 525: Weights = [55.02069669  3.57147684  8.55647029  0.14152216  0.27101236  9.97754204], Loss = 1.1867\n",
      "Iteration 526: Weights = [55.02352305  3.57126066  8.55591799  0.14150503  0.27098145  9.97814326], Loss = 1.1849\n",
      "Iteration 527: Weights = [55.02632116  3.57104433  8.55536568  0.14148798  0.27095068  9.97874434], Loss = 1.1832\n",
      "Iteration 528: Weights = [55.02909128  3.57082786  8.55481335  0.14147101  0.27092003  9.97934527], Loss = 1.1815\n",
      "Iteration 529: Weights = [55.0318337   3.57061124  8.554261    0.14145412  0.27088952  9.97994606], Loss = 1.1798\n",
      "Iteration 530: Weights = [55.03454869  3.57039448  8.55370864  0.14143731  0.27085913  9.98054671], Loss = 1.1782\n",
      "Iteration 531: Weights = [55.03723654  3.57017757  8.55315627  0.14142057  0.27082887  9.98114722], Loss = 1.1766\n",
      "Iteration 532: Weights = [55.03989751  3.56996053  8.55260389  0.14140392  0.27079874  9.98174759], Loss = 1.1750\n",
      "Iteration 533: Weights = [55.04253187  3.56974335  8.5520515   0.14138734  0.27076873  9.98234783], Loss = 1.1734\n",
      "Iteration 534: Weights = [55.04513988  3.56952603  8.55149911  0.14137083  0.27073885  9.98294794], Loss = 1.1719\n",
      "Iteration 535: Weights = [55.04772182  3.56930857  8.5509467   0.1413544   0.27070909  9.98354791], Loss = 1.1704\n",
      "Iteration 536: Weights = [55.05027793  3.56909099  8.5503943   0.14133805  0.27067945  9.98414776], Loss = 1.1690\n",
      "Iteration 537: Weights = [55.05280849  3.56887327  8.54984188  0.14132176  0.27064993  9.98474748], Loss = 1.1675\n",
      "Iteration 538: Weights = [55.05531373  3.56865543  8.54928947  0.14130556  0.27062053  9.98534707], Loss = 1.1661\n",
      "Iteration 539: Weights = [55.05779393  3.56843745  8.54873705  0.14128942  0.27059125  9.98594654], Loss = 1.1647\n",
      "Iteration 540: Weights = [55.06024932  3.56821935  8.54818463  0.14127335  0.27056209  9.98654589], Loss = 1.1633\n",
      "Iteration 541: Weights = [55.06268016  3.56800113  8.54763221  0.14125736  0.27053304  9.98714512], Loss = 1.1620\n",
      "Iteration 542: Weights = [55.0650867   3.56778279  8.5470798   0.14124143  0.27050411  9.98774423], Loss = 1.1607\n",
      "Iteration 543: Weights = [55.06746916  3.56756432  8.54652738  0.14122558  0.27047529  9.98834322], Loss = 1.1594\n",
      "Iteration 544: Weights = [55.0698278   3.56734574  8.54597497  0.14120979  0.27044659  9.98894209], Loss = 1.1581\n",
      "Iteration 545: Weights = [55.07216286  3.56712704  8.54542257  0.14119407  0.27041799  9.98954085], Loss = 1.1569\n",
      "Iteration 546: Weights = [55.07447456  3.56690822  8.54487017  0.14117842  0.27038951  9.9901395 ], Loss = 1.1556\n",
      "Iteration 547: Weights = [55.07676315  3.56668929  8.54431777  0.14116284  0.27036114  9.99073804], Loss = 1.1544\n",
      "Iteration 548: Weights = [55.07902885  3.56647024  8.54376539  0.14114732  0.27033287  9.99133646], Loss = 1.1532\n",
      "Iteration 549: Weights = [55.0812719   3.56625109  8.54321301  0.14113187  0.27030471  9.99193478], Loss = 1.1521\n",
      "Iteration 550: Weights = [55.08349251  3.56603182  8.54266064  0.14111648  0.27027666  9.99253299], Loss = 1.1509\n",
      "Iteration 551: Weights = [55.08569092  3.56581245  8.54210828  0.14110115  0.27024872  9.9931311 ], Loss = 1.1498\n",
      "Iteration 552: Weights = [55.08786734  3.56559297  8.54155593  0.14108589  0.27022088  9.9937291 ], Loss = 1.1487\n",
      "Iteration 553: Weights = [55.090022    3.56537338  8.5410036   0.14107069  0.27019314  9.99432699], Loss = 1.1476\n",
      "Iteration 554: Weights = [55.09215512  3.56515369  8.54045127  0.14105556  0.2701655   9.99492479], Loss = 1.1466\n",
      "Iteration 555: Weights = [55.0942669   3.5649339   8.53989896  0.14104048  0.27013797  9.99552248], Loss = 1.1455\n",
      "Iteration 556: Weights = [55.09635756  3.564714    8.53934666  0.14102547  0.27011054  9.99612007], Loss = 1.1445\n",
      "Iteration 557: Weights = [55.09842732  3.56449401  8.53879438  0.14101052  0.2700832   9.99671757], Loss = 1.1435\n",
      "Iteration 558: Weights = [55.10047638  3.56427392  8.53824212  0.14099562  0.27005597  9.99731497], Loss = 1.1425\n",
      "Iteration 559: Weights = [55.10250495  3.56405373  8.53768986  0.14098079  0.27002883  9.99791227], Loss = 1.1415\n",
      "Iteration 560: Weights = [55.10451324  3.56383344  8.53713763  0.14096602  0.27000179  9.99850947], Loss = 1.1405\n",
      "Iteration 561: Weights = [55.10650144  3.56361306  8.53658541  0.1409513   0.26997485  9.99910659], Loss = 1.1396\n",
      "Iteration 562: Weights = [55.10846976  3.56339259  8.53603322  0.14093664  0.269948    9.99970361], Loss = 1.1387\n",
      "Iteration 563: Weights = [55.11041839  3.56317202  8.53548104  0.14092204  0.26992124 10.00030053], Loss = 1.1377\n",
      "Iteration 564: Weights = [55.11234754  3.56295137  8.53492888  0.14090749  0.26989458 10.00089737], Loss = 1.1368\n",
      "Iteration 565: Weights = [55.1142574   3.56273062  8.53437674  0.140893    0.26986801 10.00149412], Loss = 1.1360\n",
      "Iteration 566: Weights = [55.11614816  3.56250979  8.53382462  0.14087856  0.26984153 10.00209077], Loss = 1.1351\n",
      "Iteration 567: Weights = [55.11802001  3.56228887  8.53327252  0.14086418  0.26981514 10.00268734], Loss = 1.1342\n",
      "Iteration 568: Weights = [55.11987314  3.56206786  8.53272044  0.14084986  0.26978884 10.00328383], Loss = 1.1334\n",
      "Iteration 569: Weights = [55.12170774  3.56184677  8.53216839  0.14083559  0.26976263 10.00388022], Loss = 1.1326\n",
      "Iteration 570: Weights = [55.123524    3.56162559  8.53161636  0.14082137  0.26973651 10.00447654], Loss = 1.1318\n",
      "Iteration 571: Weights = [55.12532209  3.56140433  8.53106435  0.1408072   0.26971047 10.00507276], Loss = 1.1310\n",
      "Iteration 572: Weights = [55.12710221  3.56118299  8.53051237  0.14079308  0.26968452 10.00566891], Loss = 1.1302\n",
      "Iteration 573: Weights = [55.12886452  3.56096157  8.52996041  0.14077902  0.26965866 10.00626497], Loss = 1.1294\n",
      "Iteration 574: Weights = [55.13060921  3.56074007  8.52940848  0.14076501  0.26963288 10.00686095], Loss = 1.1286\n",
      "Iteration 575: Weights = [55.13233645  3.56051849  8.52885657  0.14075105  0.26960718 10.00745685], Loss = 1.1279\n",
      "Iteration 576: Weights = [55.13404642  3.56029684  8.52830468  0.14073713  0.26958157 10.00805267], Loss = 1.1272\n",
      "Iteration 577: Weights = [55.13573929  3.56007511  8.52775283  0.14072327  0.26955604 10.00864841], Loss = 1.1264\n",
      "Iteration 578: Weights = [55.13741523  3.5598533   8.527201    0.14070946  0.26953059 10.00924408], Loss = 1.1257\n",
      "Iteration 579: Weights = [55.13907441  3.55963142  8.52664919  0.1406957   0.26950522 10.00983966], Loss = 1.1250\n",
      "Iteration 580: Weights = [55.140717    3.55940947  8.52609742  0.14068198  0.26947994 10.01043517], Loss = 1.1243\n",
      "Iteration 581: Weights = [55.14234316  3.55918745  8.52554567  0.14066831  0.26945473 10.0110306 ], Loss = 1.1237\n",
      "Iteration 582: Weights = [55.14395306  3.55896535  8.52499395  0.14065469  0.2694296  10.01162596], Loss = 1.1230\n",
      "Iteration 583: Weights = [55.14554686  3.55874318  8.52444226  0.14064111  0.26940454 10.01222124], Loss = 1.1223\n",
      "Iteration 584: Weights = [55.14712473  3.55852095  8.5238906   0.14062759  0.26937957 10.01281645], Loss = 1.1217\n",
      "Iteration 585: Weights = [55.14868681  3.55829865  8.52333897  0.1406141   0.26935467 10.01341158], Loss = 1.1210\n",
      "Iteration 586: Weights = [55.15023328  3.55807628  8.52278737  0.14060067  0.26932984 10.01400664], Loss = 1.1204\n",
      "Iteration 587: Weights = [55.15176428  3.55785384  8.5222358   0.14058727  0.26930509 10.01460163], Loss = 1.1198\n",
      "Iteration 588: Weights = [55.15327997  3.55763134  8.52168426  0.14057393  0.26928041 10.01519655], Loss = 1.1192\n",
      "Iteration 589: Weights = [55.1547805   3.55740877  8.52113275  0.14056062  0.26925581 10.0157914 ], Loss = 1.1186\n",
      "Iteration 590: Weights = [55.15626603  3.55718614  8.52058127  0.14054736  0.26923128 10.01638618], Loss = 1.1180\n",
      "Iteration 591: Weights = [55.15773671  3.55696345  8.52002982  0.14053414  0.26920682 10.01698089], Loss = 1.1174\n",
      "Iteration 592: Weights = [55.15919267  3.5567407   8.5194784   0.14052097  0.26918243 10.01757553], Loss = 1.1169\n",
      "Iteration 593: Weights = [55.16063408  3.55651788  8.51892702  0.14050784  0.26915812 10.0181701 ], Loss = 1.1163\n",
      "Iteration 594: Weights = [55.16206107  3.55629501  8.51837567  0.14049475  0.26913387 10.0187646 ], Loss = 1.1157\n",
      "Iteration 595: Weights = [55.16347379  3.55607207  8.51782435  0.1404817   0.26910969 10.01935904], Loss = 1.1152\n",
      "Iteration 596: Weights = [55.16487239  3.55584908  8.51727307  0.14046869  0.26908558 10.01995341], Loss = 1.1146\n",
      "Iteration 597: Weights = [55.166257    3.55562603  8.51672181  0.14045572  0.26906154 10.02054772], Loss = 1.1141\n",
      "Iteration 598: Weights = [55.16762776  3.55540292  8.51617059  0.1404428   0.26903756 10.02114196], Loss = 1.1136\n",
      "Iteration 599: Weights = [55.16898482  3.55517976  8.51561941  0.14042991  0.26901365 10.02173613], Loss = 1.1131\n",
      "Iteration 600: Weights = [55.1703283   3.55495654  8.51506826  0.14041706  0.26898981 10.02233024], Loss = 1.1126\n",
      "Iteration 601: Weights = [55.17165835  3.55473327  8.51451714  0.14040425  0.26896603 10.02292428], Loss = 1.1121\n",
      "Iteration 602: Weights = [55.1729751   3.55450994  8.51396606  0.14039148  0.26894232 10.02351827], Loss = 1.1116\n",
      "Iteration 603: Weights = [55.17427869  3.55428656  8.51341501  0.14037875  0.26891867 10.02411218], Loss = 1.1111\n",
      "Iteration 604: Weights = [55.17556923  3.55406313  8.51286399  0.14036606  0.26889509 10.02470604], Loss = 1.1106\n",
      "Iteration 605: Weights = [55.17684687  3.55383965  8.51231302  0.1403534   0.26887156 10.02529984], Loss = 1.1101\n",
      "Iteration 606: Weights = [55.17811174  3.55361612  8.51176207  0.14034078  0.2688481  10.02589357], Loss = 1.1097\n",
      "Iteration 607: Weights = [55.17936395  3.55339253  8.51121116  0.1403282   0.2688247  10.02648724], Loss = 1.1092\n",
      "Iteration 608: Weights = [55.18060365  3.5531689   8.51066029  0.14031565  0.26880136 10.02708085], Loss = 1.1088\n",
      "Iteration 609: Weights = [55.18183094  3.55294522  8.51010946  0.14030314  0.26877809 10.0276744 ], Loss = 1.1083\n",
      "Iteration 610: Weights = [55.18304597  3.55272149  8.50955865  0.14029067  0.26875487 10.02826789], Loss = 1.1079\n",
      "Iteration 611: Weights = [55.18424884  3.55249772  8.50900789  0.14027823  0.26873171 10.02886132], Loss = 1.1074\n",
      "Iteration 612: Weights = [55.18543969  3.5522739   8.50845716  0.14026583  0.26870861 10.0294547 ], Loss = 1.1070\n",
      "Iteration 613: Weights = [55.18661862  3.55205003  8.50790647  0.14025346  0.26868557 10.03004801], Loss = 1.1066\n",
      "Iteration 614: Weights = [55.18778577  3.55182612  8.50735582  0.14024112  0.26866258 10.03064126], Loss = 1.1062\n",
      "Iteration 615: Weights = [55.18894125  3.55160216  8.5068052   0.14022882  0.26863966 10.03123446], Loss = 1.1058\n",
      "Iteration 616: Weights = [55.19008517  3.55137816  8.50625462  0.14021656  0.26861679 10.0318276 ], Loss = 1.1053\n",
      "Iteration 617: Weights = [55.19121765  3.55115412  8.50570407  0.14020432  0.26859397 10.03242068], Loss = 1.1049\n",
      "Iteration 618: Weights = [55.19233881  3.55093003  8.50515356  0.14019212  0.26857121 10.03301371], Loss = 1.1045\n",
      "Iteration 619: Weights = [55.19344875  3.5507059   8.50460309  0.14017995  0.26854851 10.03360668], Loss = 1.1042\n",
      "Iteration 620: Weights = [55.1945476   3.55048173  8.50405266  0.14016782  0.26852586 10.03419959], Loss = 1.1038\n",
      "Iteration 621: Weights = [55.19563545  3.55025752  8.50350227  0.14015572  0.26850326 10.03479245], Loss = 1.1034\n",
      "Iteration 622: Weights = [55.19671243  3.55003327  8.50295191  0.14014364  0.26848072 10.03538525], Loss = 1.1030\n",
      "Iteration 623: Weights = [55.19777864  3.54980899  8.50240159  0.1401316   0.26845823 10.035978  ], Loss = 1.1026\n",
      "Iteration 624: Weights = [55.19883419  3.54958466  8.50185131  0.14011959  0.26843579 10.03657069], Loss = 1.1023\n",
      "Iteration 625: Weights = [55.19987918  3.54936029  8.50130107  0.14010762  0.2684134  10.03716333], Loss = 1.1019\n",
      "Iteration 626: Weights = [55.20091372  3.54913589  8.50075086  0.14009567  0.26839106 10.03775591], Loss = 1.1016\n",
      "Iteration 627: Weights = [55.20193792  3.54891145  8.50020069  0.14008375  0.26836878 10.03834844], Loss = 1.1012\n",
      "Iteration 628: Weights = [55.20295187  3.54868697  8.49965057  0.14007186  0.26834655 10.03894092], Loss = 1.1008\n",
      "Iteration 629: Weights = [55.20395569  3.54846246  8.49910048  0.14006     0.26832436 10.03953334], Loss = 1.1005\n",
      "Iteration 630: Weights = [55.20494946  3.54823791  8.49855043  0.14004817  0.26830223 10.04012571], Loss = 1.1002\n",
      "Iteration 631: Weights = [55.2059333   3.54801333  8.49800041  0.14003637  0.26828014 10.04071803], Loss = 1.0998\n",
      "Iteration 632: Weights = [55.2069073   3.54778871  8.49745044  0.1400246   0.2682581  10.04131029], Loss = 1.0995\n",
      "Iteration 633: Weights = [55.20787156  3.54756406  8.4969005   0.14001286  0.26823611 10.0419025 ], Loss = 1.0992\n",
      "Iteration 634: Weights = [55.20882618  3.54733937  8.49635061  0.14000114  0.26821417 10.04249466], Loss = 1.0988\n",
      "Iteration 635: Weights = [55.20977125  3.54711465  8.49580075  0.13998946  0.26819227 10.04308677], Loss = 1.0985\n",
      "Iteration 636: Weights = [55.21070687  3.5468899   8.49525093  0.1399778   0.26817042 10.04367883], Loss = 1.0982\n",
      "Iteration 637: Weights = [55.21163314  3.54666512  8.49470115  0.13996617  0.26814862 10.04427084], Loss = 1.0979\n",
      "Iteration 638: Weights = [55.21255014  3.54644031  8.49415141  0.13995456  0.26812686 10.04486279], Loss = 1.0976\n",
      "Iteration 639: Weights = [55.21345797  3.54621547  8.49360171  0.13994298  0.26810515 10.0454547 ], Loss = 1.0973\n",
      "Iteration 640: Weights = [55.21435672  3.54599059  8.49305205  0.13993143  0.26808348 10.04604655], Loss = 1.0970\n",
      "Iteration 641: Weights = [55.21524649  3.54576569  8.49250243  0.13991991  0.26806186 10.04663836], Loss = 1.0967\n",
      "Iteration 642: Weights = [55.21612736  3.54554076  8.49195284  0.13990841  0.26804028 10.04723011], Loss = 1.0964\n",
      "Iteration 643: Weights = [55.21699942  3.5453158   8.4914033   0.13989693  0.26801875 10.04782181], Loss = 1.0961\n",
      "Iteration 644: Weights = [55.21786276  3.54509081  8.4908538   0.13988548  0.26799725 10.04841347], Loss = 1.0958\n",
      "Iteration 645: Weights = [55.21871746  3.54486579  8.49030433  0.13987406  0.26797581 10.04900508], Loss = 1.0955\n",
      "Iteration 646: Weights = [55.21956362  3.54464075  8.48975491  0.13986266  0.2679544  10.04959663], Loss = 1.0952\n",
      "Iteration 647: Weights = [55.22040132  3.54441568  8.48920552  0.13985129  0.26793303 10.05018814], Loss = 1.0949\n",
      "Iteration 648: Weights = [55.22123064  3.54419058  8.48865618  0.13983994  0.26791171 10.0507796 ], Loss = 1.0946\n",
      "Iteration 649: Weights = [55.22205167  3.54396545  8.48810687  0.13982862  0.26789043 10.05137101], Loss = 1.0944\n",
      "Iteration 650: Weights = [55.22286448  3.5437403   8.4875576   0.13981732  0.26786918 10.05196237], Loss = 1.0941\n",
      "Iteration 651: Weights = [55.22366917  3.54351513  8.48700838  0.13980604  0.26784798 10.05255369], Loss = 1.0938\n",
      "Iteration 652: Weights = [55.22446581  3.54328993  8.48645919  0.13979479  0.26782682 10.05314495], Loss = 1.0935\n",
      "Iteration 653: Weights = [55.22525449  3.54306471  8.48591005  0.13978356  0.2678057  10.05373617], Loss = 1.0933\n",
      "Iteration 654: Weights = [55.22603528  3.54283946  8.48536094  0.13977235  0.26778461 10.05432734], Loss = 1.0930\n",
      "Iteration 655: Weights = [55.22680826  3.54261419  8.48481187  0.13976117  0.26776357 10.05491846], Loss = 1.0927\n",
      "Iteration 656: Weights = [55.22757351  3.54238889  8.48426284  0.13975001  0.26774256 10.05550954], Loss = 1.0925\n",
      "Iteration 657: Weights = [55.22833111  3.54216358  8.48371386  0.13973887  0.26772159 10.05610057], Loss = 1.0922\n",
      "Iteration 658: Weights = [55.22908113  3.54193824  8.48316491  0.13972775  0.26770066 10.05669155], Loss = 1.0920\n",
      "Iteration 659: Weights = [55.22982365  3.54171288  8.482616    0.13971666  0.26767977 10.05728249], Loss = 1.0917\n",
      "Iteration 660: Weights = [55.23055875  3.54148749  8.48206714  0.13970558  0.26765891 10.05787338], Loss = 1.0915\n",
      "Iteration 661: Weights = [55.23128649  3.54126209  8.48151831  0.13969453  0.26763809 10.05846422], Loss = 1.0912\n",
      "Iteration 662: Weights = [55.23200696  3.54103667  8.48096952  0.1396835   0.26761731 10.05905501], Loss = 1.0910\n",
      "Iteration 663: Weights = [55.23272023  3.54081122  8.48042078  0.13967249  0.26759656 10.05964576], Loss = 1.0907\n",
      "Iteration 664: Weights = [55.23342636  3.54058576  8.47987207  0.1396615   0.26757585 10.06023647], Loss = 1.0905\n",
      "Iteration 665: Weights = [55.23412543  3.54036027  8.4793234   0.13965054  0.26755518 10.06082713], Loss = 1.0903\n",
      "Iteration 666: Weights = [55.23481751  3.54013477  8.47877478  0.13963959  0.26753453 10.06141774], Loss = 1.0900\n",
      "Iteration 667: Weights = [55.23550266  3.53990925  8.47822619  0.13962866  0.26751393 10.06200831], Loss = 1.0898\n",
      "Iteration 668: Weights = [55.23618097  3.5396837   8.47767765  0.13961776  0.26749335 10.06259883], Loss = 1.0895\n",
      "Iteration 669: Weights = [55.23685249  3.53945815  8.47712914  0.13960687  0.26747281 10.06318931], Loss = 1.0893\n",
      "Iteration 670: Weights = [55.2375173   3.53923257  8.47658067  0.139596    0.26745231 10.06377974], Loss = 1.0891\n",
      "Iteration 671: Weights = [55.23817546  3.53900697  8.47603225  0.13958516  0.26743184 10.06437012], Loss = 1.0889\n",
      "Iteration 672: Weights = [55.23882704  3.53878136  8.47548386  0.13957433  0.2674114  10.06496047], Loss = 1.0886\n",
      "Iteration 673: Weights = [55.2394721   3.53855573  8.47493552  0.13956352  0.26739099 10.06555076], Loss = 1.0884\n",
      "Iteration 674: Weights = [55.24011072  3.53833009  8.47438721  0.13955273  0.26737062 10.06614102], Loss = 1.0882\n",
      "Iteration 675: Weights = [55.24074294  3.53810443  8.47383895  0.13954196  0.26735027 10.06673122], Loss = 1.0880\n",
      "Iteration 676: Weights = [55.24136885  3.53787875  8.47329072  0.13953121  0.26732996 10.06732139], Loss = 1.0877\n",
      "Iteration 677: Weights = [55.24198849  3.53765306  8.47274254  0.13952047  0.26730968 10.06791151], Loss = 1.0875\n",
      "Iteration 678: Weights = [55.24260194  3.53742735  8.47219439  0.13950975  0.26728944 10.06850158], Loss = 1.0873\n",
      "Iteration 679: Weights = [55.24320925  3.53720163  8.47164629  0.13949906  0.26726922 10.06909161], Loss = 1.0871\n",
      "Iteration 680: Weights = [55.24381049  3.53697589  8.47109822  0.13948838  0.26724903 10.0696816 ], Loss = 1.0869\n",
      "Iteration 681: Weights = [55.24440572  3.53675014  8.4705502   0.13947771  0.26722888 10.07027154], Loss = 1.0867\n",
      "Iteration 682: Weights = [55.244995    3.53652438  8.47000222  0.13946707  0.26720875 10.07086144], Loss = 1.0865\n",
      "Iteration 683: Weights = [55.24557838  3.5362986   8.46945427  0.13945644  0.26718865 10.0714513 ], Loss = 1.0862\n",
      "Iteration 684: Weights = [55.24615593  3.53607281  8.46890637  0.13944583  0.26716858 10.07204111], Loss = 1.0860\n",
      "Iteration 685: Weights = [55.24672771  3.535847    8.4683585   0.13943524  0.26714855 10.07263088], Loss = 1.0858\n",
      "Iteration 686: Weights = [55.24729376  3.53562119  8.46781068  0.13942466  0.26712854 10.07322061], Loss = 1.0856\n",
      "Iteration 687: Weights = [55.24785416  3.53539535  8.4672629   0.1394141   0.26710855 10.07381029], Loss = 1.0854\n",
      "Iteration 688: Weights = [55.24840895  3.53516951  8.46671515  0.13940356  0.2670886  10.07439993], Loss = 1.0852\n",
      "Iteration 689: Weights = [55.24895819  3.53494366  8.46616745  0.13939303  0.26706868 10.07498953], Loss = 1.0850\n",
      "Iteration 690: Weights = [55.24950194  3.53471779  8.46561979  0.13938252  0.26704878 10.07557908], Loss = 1.0848\n",
      "Iteration 691: Weights = [55.25004026  3.53449192  8.46507216  0.13937202  0.26702891 10.07616859], Loss = 1.0846\n",
      "Iteration 692: Weights = [55.25057319  3.53426603  8.46452458  0.13936155  0.26700907 10.07675806], Loss = 1.0844\n",
      "Iteration 693: Weights = [55.25110079  3.53404013  8.46397704  0.13935108  0.26698925 10.07734749], Loss = 1.0842\n",
      "Iteration 694: Weights = [55.25162312  3.53381422  8.46342953  0.13934063  0.26696946 10.07793687], Loss = 1.0840\n",
      "Iteration 695: Weights = [55.25214022  3.5335883   8.46288207  0.1393302   0.2669497  10.07852621], Loss = 1.0838\n",
      "Iteration 696: Weights = [55.25265215  3.53336237  8.46233465  0.13931978  0.26692996 10.07911551], Loss = 1.0836\n",
      "Iteration 697: Weights = [55.25315896  3.53313643  8.46178726  0.13930938  0.26691025 10.07970476], Loss = 1.0835\n",
      "Iteration 698: Weights = [55.25366071  3.53291048  8.46123992  0.139299    0.26689057 10.08029398], Loss = 1.0833\n",
      "Iteration 699: Weights = [55.25415743  3.53268452  8.46069262  0.13928862  0.26687091 10.08088315], Loss = 1.0831\n",
      "Iteration 700: Weights = [55.25464919  3.53245856  8.46014535  0.13927827  0.26685128 10.08147228], Loss = 1.0829\n",
      "Iteration 701: Weights = [55.25513603  3.53223258  8.45959813  0.13926792  0.26683167 10.08206137], Loss = 1.0827\n",
      "Iteration 702: Weights = [55.25561801  3.5320066   8.45905095  0.13925759  0.26681208 10.08265041], Loss = 1.0825\n",
      "Iteration 703: Weights = [55.25609516  3.5317806   8.4585038   0.13924728  0.26679252 10.08323941], Loss = 1.0823\n",
      "Iteration 704: Weights = [55.25656754  3.5315546   8.4579567   0.13923698  0.26677299 10.08382838], Loss = 1.0821\n",
      "Iteration 705: Weights = [55.2570352   3.53132859  8.45740963  0.13922669  0.26675348 10.0844173 ], Loss = 1.0820\n",
      "Iteration 706: Weights = [55.25749818  3.53110258  8.45686261  0.13921642  0.26673399 10.08500617], Loss = 1.0818\n",
      "Iteration 707: Weights = [55.25795653  3.53087655  8.45631562  0.13920616  0.26671453 10.08559501], Loss = 1.0816\n",
      "Iteration 708: Weights = [55.2584103   3.53065052  8.45576868  0.13919592  0.26669509 10.08618381], Loss = 1.0814\n",
      "Iteration 709: Weights = [55.25885953  3.53042449  8.45522177  0.13918568  0.26667567 10.08677256], Loss = 1.0812\n",
      "Iteration 710: Weights = [55.25930427  3.53019844  8.45467491  0.13917546  0.26665627 10.08736127], Loss = 1.0810\n",
      "Iteration 711: Weights = [55.25974456  3.52997239  8.45412808  0.13916526  0.2666369  10.08794994], Loss = 1.0809\n",
      "Iteration 712: Weights = [55.26018045  3.52974634  8.4535813   0.13915507  0.26661755 10.08853857], Loss = 1.0807\n",
      "Iteration 713: Weights = [55.26061197  3.52952027  8.45303455  0.13914489  0.26659823 10.08912716], Loss = 1.0805\n",
      "Iteration 714: Weights = [55.26103919  3.52929421  8.45248784  0.13913472  0.26657892 10.08971571], Loss = 1.0803\n",
      "Iteration 715: Weights = [55.26146213  3.52906813  8.45194117  0.13912457  0.26655964 10.09030422], Loss = 1.0802\n",
      "Iteration 716: Weights = [55.26188084  3.52884205  8.45139455  0.13911442  0.26654038 10.09089268], Loss = 1.0800\n",
      "Iteration 717: Weights = [55.26229537  3.52861597  8.45084796  0.1391043   0.26652114 10.09148111], Loss = 1.0798\n",
      "Iteration 718: Weights = [55.26270575  3.52838988  8.45030141  0.13909418  0.26650192 10.09206949], Loss = 1.0796\n",
      "Iteration 719: Weights = [55.26311202  3.52816379  8.4497549   0.13908407  0.26648272 10.09265783], Loss = 1.0795\n",
      "Iteration 720: Weights = [55.26351424  3.52793769  8.44920843  0.13907398  0.26646355 10.09324613], Loss = 1.0793\n",
      "Iteration 721: Weights = [55.26391243  3.52771158  8.448662    0.1390639   0.26644439 10.09383439], Loss = 1.0791\n",
      "Iteration 722: Weights = [55.26430664  3.52748548  8.44811561  0.13905383  0.26642525 10.09442261], Loss = 1.0790\n",
      "Iteration 723: Weights = [55.2646969   3.52725937  8.44756926  0.13904377  0.26640614 10.09501079], Loss = 1.0788\n",
      "Iteration 724: Weights = [55.26508327  3.52703325  8.44702294  0.13903373  0.26638704 10.09559893], Loss = 1.0786\n",
      "Iteration 725: Weights = [55.26546577  3.52680713  8.44647667  0.1390237   0.26636797 10.09618703], Loss = 1.0784\n",
      "Iteration 726: Weights = [55.26584444  3.52658101  8.44593044  0.13901367  0.26634891 10.09677509], Loss = 1.0783\n",
      "Iteration 727: Weights = [55.26621933  3.52635489  8.44538424  0.13900366  0.26632988 10.09736311], Loss = 1.0781\n",
      "Iteration 728: Weights = [55.26659047  3.52612876  8.44483809  0.13899366  0.26631086 10.09795108], Loss = 1.0779\n",
      "Iteration 729: Weights = [55.2669579   3.52590263  8.44429197  0.13898367  0.26629186 10.09853902], Loss = 1.0778\n",
      "Iteration 730: Weights = [55.26732165  3.5256765   8.4437459   0.13897369  0.26627289 10.09912692], Loss = 1.0776\n",
      "Iteration 731: Weights = [55.26768177  3.52545036  8.44319986  0.13896373  0.26625393 10.09971477], Loss = 1.0774\n",
      "Iteration 732: Weights = [55.26803829  3.52522422  8.44265386  0.13895377  0.26623499 10.10030259], Loss = 1.0773\n",
      "Iteration 733: Weights = [55.26839124  3.52499808  8.4421079   0.13894382  0.26621606 10.10089036], Loss = 1.0771\n",
      "Iteration 734: Weights = [55.26874066  3.52477194  8.44156198  0.13893389  0.26619716 10.1014781 ], Loss = 1.0770\n",
      "Iteration 735: Weights = [55.26908659  3.5245458   8.4410161   0.13892396  0.26617827 10.10206579], Loss = 1.0768\n",
      "Iteration 736: Weights = [55.26942905  3.52431965  8.44047026  0.13891405  0.26615941 10.10265345], Loss = 1.0766\n",
      "Iteration 737: Weights = [55.2697681   3.5240935   8.43992446  0.13890414  0.26614056 10.10324106], Loss = 1.0765\n",
      "Iteration 738: Weights = [55.27010375  3.52386736  8.43937869  0.13889425  0.26612172 10.10382864], Loss = 1.0763\n",
      "Iteration 739: Weights = [55.27043604  3.52364121  8.43883297  0.13888436  0.26610291 10.10441617], Loss = 1.0761\n",
      "Iteration 740: Weights = [55.27076502  3.52341506  8.43828728  0.13887449  0.26608411 10.10500367], Loss = 1.0760\n",
      "Iteration 741: Weights = [55.2710907   3.52318891  8.43774163  0.13886463  0.26606533 10.10559112], Loss = 1.0758\n",
      "Iteration 742: Weights = [55.27141313  3.52296276  8.43719603  0.13885477  0.26604657 10.10617854], Loss = 1.0757\n",
      "Iteration 743: Weights = [55.27173233  3.5227366   8.43665046  0.13884493  0.26602782 10.10676591], Loss = 1.0755\n",
      "Iteration 744: Weights = [55.27204834  3.52251045  8.43610493  0.13883509  0.26600909 10.10735325], Loss = 1.0753\n",
      "Iteration 745: Weights = [55.27236119  3.5222843   8.43555944  0.13882526  0.26599038 10.10794054], Loss = 1.0752\n",
      "Iteration 746: Weights = [55.27267091  3.52205815  8.43501398  0.13881545  0.26597168 10.1085278 ], Loss = 1.0750\n",
      "Iteration 747: Weights = [55.27297753  3.521832    8.43446857  0.13880564  0.265953   10.10911502], Loss = 1.0749\n",
      "Iteration 748: Weights = [55.27328109  3.52160584  8.43392319  0.13879584  0.26593433 10.10970219], Loss = 1.0747\n",
      "Iteration 749: Weights = [55.27358161  3.52137969  8.43337786  0.13878605  0.26591568 10.11028933], Loss = 1.0746\n",
      "Iteration 750: Weights = [55.27387913  3.52115354  8.43283256  0.13877627  0.26589705 10.11087643], Loss = 1.0744\n",
      "Iteration 751: Weights = [55.27417367  3.52092739  8.4322873   0.1387665   0.26587843 10.11146349], Loss = 1.0742\n",
      "Iteration 752: Weights = [55.27446527  3.52070124  8.43174208  0.13875674  0.26585983 10.11205051], Loss = 1.0741\n",
      "Iteration 753: Weights = [55.27475395  3.52047509  8.4311969   0.13874698  0.26584124 10.11263748], Loss = 1.0739\n",
      "Iteration 754: Weights = [55.27503974  3.52024895  8.43065176  0.13873724  0.26582267 10.11322442], Loss = 1.0738\n",
      "Iteration 755: Weights = [55.27532268  3.5200228   8.43010665  0.1387275   0.26580412 10.11381132], Loss = 1.0736\n",
      "Iteration 756: Weights = [55.27560279  3.51979666  8.42956159  0.13871778  0.26578557 10.11439818], Loss = 1.0735\n",
      "Iteration 757: Weights = [55.27588009  3.51957051  8.42901656  0.13870806  0.26576705 10.11498501], Loss = 1.0733\n",
      "Iteration 758: Weights = [55.27615462  3.51934437  8.42847157  0.13869835  0.26574854 10.11557179], Loss = 1.0732\n",
      "Iteration 759: Weights = [55.27642641  3.51911823  8.42792662  0.13868864  0.26573004 10.11615853], Loss = 1.0730\n",
      "Iteration 760: Weights = [55.27669548  3.5188921   8.42738171  0.13867895  0.26571155 10.11674523], Loss = 1.0728\n",
      "Iteration 761: Weights = [55.27696186  3.51866596  8.42683683  0.13866926  0.26569308 10.1173319 ], Loss = 1.0727\n",
      "Iteration 762: Weights = [55.27722557  3.51843983  8.426292    0.13865958  0.26567463 10.11791852], Loss = 1.0725\n",
      "Iteration 763: Weights = [55.27748665  3.5182137   8.4257472   0.13864991  0.26565619 10.11850511], Loss = 1.0724\n",
      "Iteration 764: Weights = [55.27774512  3.51798757  8.42520244  0.13864025  0.26563776 10.11909165], Loss = 1.0722\n",
      "Iteration 765: Weights = [55.278001    3.51776144  8.42465772  0.1386306   0.26561935 10.11967816], Loss = 1.0721\n",
      "Iteration 766: Weights = [55.27825432  3.51753532  8.42411304  0.13862095  0.26560095 10.12026463], Loss = 1.0719\n",
      "Iteration 767: Weights = [55.27850511  3.5173092   8.4235684   0.13861131  0.26558256 10.12085106], Loss = 1.0718\n",
      "Iteration 768: Weights = [55.2787534   3.51708308  8.42302379  0.13860168  0.26556419 10.12143745], Loss = 1.0716\n",
      "Iteration 769: Weights = [55.2789992   3.51685697  8.42247922  0.13859205  0.26554583 10.1220238 ], Loss = 1.0715\n",
      "Iteration 770: Weights = [55.27924254  3.51663086  8.42193469  0.13858244  0.26552748 10.12261011], Loss = 1.0713\n",
      "Iteration 771: Weights = [55.27948345  3.51640475  8.4213902   0.13857283  0.26550915 10.12319638], Loss = 1.0712\n",
      "Iteration 772: Weights = [55.27972194  3.51617865  8.42084575  0.13856323  0.26549083 10.12378261], Loss = 1.0710\n",
      "Iteration 773: Weights = [55.27995806  3.51595255  8.42030134  0.13855363  0.26547252 10.12436881], Loss = 1.0709\n",
      "Iteration 774: Weights = [55.28019181  3.51572645  8.41975696  0.13854404  0.26545423 10.12495496], Loss = 1.0707\n",
      "Iteration 775: Weights = [55.28042323  3.51550036  8.41921262  0.13853446  0.26543594 10.12554108], Loss = 1.0706\n",
      "Iteration 776: Weights = [55.28065233  3.51527427  8.41866832  0.13852489  0.26541767 10.12612716], Loss = 1.0704\n",
      "Iteration 777: Weights = [55.28087914  3.51504818  8.41812406  0.13851532  0.26539942 10.1267132 ], Loss = 1.0703\n",
      "Iteration 778: Weights = [55.28110368  3.5148221   8.41757983  0.13850576  0.26538117 10.1272992 ], Loss = 1.0701\n",
      "Iteration 779: Weights = [55.28132598  3.51459602  8.41703565  0.13849621  0.26536294 10.12788516], Loss = 1.0700\n",
      "Iteration 780: Weights = [55.28154605  3.51436995  8.4164915   0.13848667  0.26534471 10.12847108], Loss = 1.0698\n",
      "Iteration 781: Weights = [55.28176392  3.51414388  8.41594739  0.13847713  0.2653265  10.12905696], Loss = 1.0697\n",
      "Iteration 782: Weights = [55.28197962  3.51391782  8.41540332  0.13846759  0.26530831 10.12964281], Loss = 1.0695\n",
      "Iteration 783: Weights = [55.28219315  3.51369176  8.41485928  0.13845807  0.26529012 10.13022861], Loss = 1.0694\n",
      "Iteration 784: Weights = [55.28240456  3.51346571  8.41431528  0.13844855  0.26527194 10.13081438], Loss = 1.0692\n",
      "Iteration 785: Weights = [55.28261384  3.51323966  8.41377133  0.13843904  0.26525378 10.13140011], Loss = 1.0691\n",
      "Iteration 786: Weights = [55.28282104  3.51301361  8.4132274   0.13842953  0.26523563 10.13198579], Loss = 1.0690\n",
      "Iteration 787: Weights = [55.28302616  3.51278757  8.41268352  0.13842003  0.26521749 10.13257145], Loss = 1.0688\n",
      "Iteration 788: Weights = [55.28322923  3.51256154  8.41213967  0.13841054  0.26519936 10.13315706], Loss = 1.0687\n",
      "Iteration 789: Weights = [55.28343027  3.51233551  8.41159587  0.13840105  0.26518124 10.13374263], Loss = 1.0685\n",
      "Iteration 790: Weights = [55.2836293   3.51210949  8.4110521   0.13839157  0.26516313 10.13432816], Loss = 1.0684\n",
      "Iteration 791: Weights = [55.28382634  3.51188347  8.41050836  0.13838209  0.26514504 10.13491366], Loss = 1.0682\n",
      "Iteration 792: Weights = [55.28402141  3.51165745  8.40996467  0.13837262  0.26512695 10.13549912], Loss = 1.0681\n",
      "Iteration 793: Weights = [55.28421453  3.51143145  8.40942101  0.13836316  0.26510887 10.13608453], Loss = 1.0679\n",
      "Iteration 794: Weights = [55.28440572  3.51120544  8.40887739  0.1383537   0.26509081 10.13666991], Loss = 1.0678\n",
      "Iteration 795: Weights = [55.284595    3.51097945  8.40833381  0.13834425  0.26507275 10.13725526], Loss = 1.0676\n",
      "Iteration 796: Weights = [55.28478238  3.51075346  8.40779026  0.13833481  0.26505471 10.13784056], Loss = 1.0675\n",
      "Iteration 797: Weights = [55.28496789  3.51052747  8.40724676  0.13832537  0.26503668 10.13842582], Loss = 1.0674\n",
      "Iteration 798: Weights = [55.28515155  3.5103015   8.40670329  0.13831593  0.26501865 10.13901105], Loss = 1.0672\n",
      "Iteration 799: Weights = [55.28533336  3.51007552  8.40615986  0.13830651  0.26500064 10.13959623], Loss = 1.0671\n",
      "Iteration 800: Weights = [55.28551336  3.50984956  8.40561646  0.13829708  0.26498264 10.14018138], Loss = 1.0669\n",
      "Iteration 801: Weights = [55.28569156  3.5096236   8.4050731   0.13828767  0.26496464 10.14076649], Loss = 1.0668\n",
      "Iteration 802: Weights = [55.28586798  3.50939764  8.40452978  0.13827825  0.26494666 10.14135156], Loss = 1.0666\n",
      "Iteration 803: Weights = [55.28604263  3.5091717   8.4039865   0.13826885  0.26492868 10.1419366 ], Loss = 1.0665\n",
      "Iteration 804: Weights = [55.28621554  3.50894576  8.40344326  0.13825945  0.26491072 10.14252159], Loss = 1.0663\n",
      "Iteration 805: Weights = [55.28638672  3.50871982  8.40290005  0.13825005  0.26489277 10.14310655], Loss = 1.0662\n",
      "Iteration 806: Weights = [55.28655618  3.5084939   8.40235688  0.13824066  0.26487482 10.14369146], Loss = 1.0661\n",
      "Iteration 807: Weights = [55.28672396  3.50826798  8.40181375  0.13823128  0.26485688 10.14427634], Loss = 1.0659\n",
      "Iteration 808: Weights = [55.28689005  3.50804206  8.40127065  0.1382219   0.26483896 10.14486118], Loss = 1.0658\n",
      "Iteration 809: Weights = [55.28705448  3.50781616  8.40072759  0.13821253  0.26482104 10.14544598], Loss = 1.0656\n",
      "Iteration 810: Weights = [55.28721727  3.50759026  8.40018457  0.13820316  0.26480313 10.14603075], Loss = 1.0655\n",
      "Iteration 811: Weights = [55.28737843  3.50736436  8.39964159  0.13819379  0.26478523 10.14661547], Loss = 1.0653\n",
      "Iteration 812: Weights = [55.28753798  3.50713848  8.39909864  0.13818443  0.26476734 10.14720016], Loss = 1.0652\n",
      "Iteration 813: Weights = [55.28769593  3.5069126   8.39855573  0.13817508  0.26474946 10.14778481], Loss = 1.0651\n",
      "Iteration 814: Weights = [55.28785231  3.50668673  8.39801286  0.13816573  0.26473159 10.14836942], Loss = 1.0649\n",
      "Iteration 815: Weights = [55.28800712  3.50646087  8.39747002  0.13815639  0.26471373 10.14895399], Loss = 1.0648\n",
      "Iteration 816: Weights = [55.28816038  3.50623501  8.39692723  0.13814705  0.26469587 10.14953852], Loss = 1.0646\n",
      "Iteration 817: Weights = [55.28831211  3.50600916  8.39638447  0.13813771  0.26467803 10.15012302], Loss = 1.0645\n",
      "Iteration 818: Weights = [55.28846232  3.50578332  8.39584174  0.13812838  0.26466019 10.15070748], Loss = 1.0643\n",
      "Iteration 819: Weights = [55.28861103  3.50555749  8.39529906  0.13811906  0.26464236 10.1512919 ], Loss = 1.0642\n",
      "Iteration 820: Weights = [55.28875826  3.50533166  8.39475641  0.13810974  0.26462454 10.15187628], Loss = 1.0641\n",
      "Iteration 821: Weights = [55.28890401  3.50510585  8.39421379  0.13810042  0.26460673 10.15246062], Loss = 1.0639\n",
      "Iteration 822: Weights = [55.2890483   3.50488004  8.39367122  0.13809111  0.26458892 10.15304492], Loss = 1.0638\n",
      "Iteration 823: Weights = [55.28919115  3.50465424  8.39312868  0.13808181  0.26457113 10.15362919], Loss = 1.0636\n",
      "Iteration 824: Weights = [55.28933257  3.50442844  8.39258618  0.13807251  0.26455334 10.15421341], Loss = 1.0635\n",
      "Iteration 825: Weights = [55.28947258  3.50420266  8.39204371  0.13806321  0.26453556 10.1547976 ], Loss = 1.0633\n",
      "Iteration 826: Weights = [55.28961119  3.50397688  8.39150129  0.13805392  0.26451779 10.15538175], Loss = 1.0632\n",
      "Iteration 827: Weights = [55.28974841  3.50375111  8.3909589   0.13804463  0.26450002 10.15596587], Loss = 1.0631\n",
      "Iteration 828: Weights = [55.28988426  3.50352535  8.39041654  0.13803534  0.26448227 10.15654994], Loss = 1.0629\n",
      "Iteration 829: Weights = [55.29001875  3.5032996   8.38987423  0.13802606  0.26446452 10.15713398], Loss = 1.0628\n",
      "Iteration 830: Weights = [55.29015189  3.50307385  8.38933195  0.13801679  0.26444678 10.15771797], Loss = 1.0626\n",
      "Iteration 831: Weights = [55.29028371  3.50284812  8.3887897   0.13800752  0.26442904 10.15830193], Loss = 1.0625\n",
      "Iteration 832: Weights = [55.2904142   3.50262239  8.3882475   0.13799825  0.26441132 10.15888586], Loss = 1.0624\n",
      "Iteration 833: Weights = [55.2905434   3.50239667  8.38770533  0.13798899  0.2643936  10.15946974], Loss = 1.0622\n",
      "Iteration 834: Weights = [55.2906713   3.50217096  8.38716319  0.13797973  0.26437589 10.16005358], Loss = 1.0621\n",
      "Iteration 835: Weights = [55.29079792  3.50194526  8.3866211   0.13797048  0.26435819 10.16063739], Loss = 1.0619\n",
      "Iteration 836: Weights = [55.29092327  3.50171957  8.38607904  0.13796123  0.26434049 10.16122116], Loss = 1.0618\n",
      "Iteration 837: Weights = [55.29104737  3.50149388  8.38553702  0.13795198  0.2643228  10.16180489], Loss = 1.0617\n",
      "Iteration 838: Weights = [55.29117023  3.50126821  8.38499503  0.13794274  0.26430512 10.16238858], Loss = 1.0615\n",
      "Iteration 839: Weights = [55.29129186  3.50104254  8.38445308  0.1379335   0.26428745 10.16297224], Loss = 1.0614\n",
      "Iteration 840: Weights = [55.29141228  3.50081689  8.38391117  0.13792426  0.26426978 10.16355586], Loss = 1.0612\n",
      "Iteration 841: Weights = [55.29153149  3.50059124  8.38336929  0.13791503  0.26425212 10.16413943], Loss = 1.0611\n",
      "Iteration 842: Weights = [55.29164951  3.5003656   8.38282746  0.13790581  0.26423447 10.16472297], Loss = 1.0610\n",
      "Iteration 843: Weights = [55.29176634  3.50013997  8.38228565  0.13789658  0.26421682 10.16530648], Loss = 1.0608\n",
      "Iteration 844: Weights = [55.29188201  3.49991435  8.38174389  0.13788736  0.26419918 10.16588994], Loss = 1.0607\n",
      "Iteration 845: Weights = [55.29199653  3.49968874  8.38120216  0.13787815  0.26418155 10.16647337], Loss = 1.0605\n",
      "Iteration 846: Weights = [55.29210989  3.49946314  8.38066047  0.13786894  0.26416392 10.16705676], Loss = 1.0604\n",
      "Iteration 847: Weights = [55.29222213  3.49923754  8.38011881  0.13785973  0.2641463  10.16764011], Loss = 1.0603\n",
      "Iteration 848: Weights = [55.29233324  3.49901196  8.37957719  0.13785053  0.26412869 10.16822342], Loss = 1.0601\n",
      "Iteration 849: Weights = [55.29244324  3.49878639  8.37903561  0.13784133  0.26411109 10.16880669], Loss = 1.0600\n",
      "Iteration 850: Weights = [55.29255214  3.49856082  8.37849406  0.13783213  0.26409349 10.16938993], Loss = 1.0598\n",
      "Iteration 851: Weights = [55.29265995  3.49833527  8.37795255  0.13782294  0.26407589 10.16997313], Loss = 1.0597\n",
      "Iteration 852: Weights = [55.29276669  3.49810972  8.37741108  0.13781375  0.26405831 10.17055629], Loss = 1.0596\n",
      "Iteration 853: Weights = [55.29287236  3.49788419  8.37686964  0.13780456  0.26404073 10.17113941], Loss = 1.0594\n",
      "Iteration 854: Weights = [55.29297696  3.49765866  8.37632824  0.13779538  0.26402315 10.17172249], Loss = 1.0593\n",
      "Iteration 855: Weights = [55.29308053  3.49743315  8.37578688  0.1377862   0.26400558 10.17230554], Loss = 1.0591\n",
      "Iteration 856: Weights = [55.29318306  3.49720764  8.37524555  0.13777702  0.26398802 10.17288855], Loss = 1.0590\n",
      "Iteration 857: Weights = [55.29328456  3.49698214  8.37470426  0.13776785  0.26397047 10.17347152], Loss = 1.0589\n",
      "Iteration 858: Weights = [55.29338505  3.49675666  8.37416301  0.13775868  0.26395292 10.17405445], Loss = 1.0587\n",
      "Iteration 859: Weights = [55.29348453  3.49653118  8.37362179  0.13774951  0.26393537 10.17463734], Loss = 1.0586\n",
      "Iteration 860: Weights = [55.29358302  3.49630572  8.37308061  0.13774035  0.26391784 10.1752202 ], Loss = 1.0585\n",
      "Iteration 861: Weights = [55.29368052  3.49608026  8.37253946  0.13773119  0.26390031 10.17580302], Loss = 1.0583\n",
      "Iteration 862: Weights = [55.29377705  3.49585481  8.37199835  0.13772204  0.26388278 10.1763858 ], Loss = 1.0582\n",
      "Iteration 863: Weights = [55.29387261  3.49562938  8.37145728  0.13771288  0.26386526 10.17696854], Loss = 1.0580\n",
      "Iteration 864: Weights = [55.29396722  3.49540395  8.37091624  0.13770373  0.26384775 10.17755124], Loss = 1.0579\n",
      "Iteration 865: Weights = [55.29406088  3.49517854  8.37037524  0.13769459  0.26383024 10.17813391], Loss = 1.0578\n",
      "Iteration 866: Weights = [55.2941536   3.49495313  8.36983428  0.13768544  0.26381274 10.17871654], Loss = 1.0576\n",
      "Iteration 867: Weights = [55.2942454   3.49472774  8.36929335  0.1376763   0.26379524 10.17929913], Loss = 1.0575\n",
      "Iteration 868: Weights = [55.29433628  3.49450235  8.36875246  0.13766717  0.26377775 10.17988168], Loss = 1.0573\n",
      "Iteration 869: Weights = [55.29442625  3.49427698  8.3682116   0.13765803  0.26376026 10.1804642 ], Loss = 1.0572\n",
      "Iteration 870: Weights = [55.29451532  3.49405161  8.36767078  0.1376489   0.26374278 10.18104667], Loss = 1.0571\n",
      "Iteration 871: Weights = [55.2946035   3.49382626  8.36713     0.13763977  0.26372531 10.18162911], Loss = 1.0569\n",
      "Iteration 872: Weights = [55.2946908   3.49360092  8.36658925  0.13763065  0.26370784 10.18221151], Loss = 1.0568\n",
      "Iteration 873: Weights = [55.29477723  3.49337559  8.36604854  0.13762153  0.26369038 10.18279388], Loss = 1.0567\n",
      "Iteration 874: Weights = [55.29486279  3.49315026  8.36550787  0.13761241  0.26367292 10.1833762 ], Loss = 1.0565\n",
      "Iteration 875: Weights = [55.29494749  3.49292495  8.36496723  0.13760329  0.26365547 10.18395849], Loss = 1.0564\n",
      "Iteration 876: Weights = [55.29503135  3.49269965  8.36442663  0.13759418  0.26363802 10.18454074], Loss = 1.0562\n",
      "Iteration 877: Weights = [55.29511437  3.49247436  8.36388606  0.13758507  0.26362058 10.18512295], Loss = 1.0561\n",
      "Iteration 878: Weights = [55.29519656  3.49224908  8.36334553  0.13757596  0.26360314 10.18570513], Loss = 1.0560\n",
      "Iteration 879: Weights = [55.29527793  3.49202382  8.36280504  0.13756686  0.26358571 10.18628726], Loss = 1.0558\n",
      "Iteration 880: Weights = [55.29535848  3.49179856  8.36226458  0.13755775  0.26356828 10.18686936], Loss = 1.0557\n",
      "Iteration 881: Weights = [55.29543823  3.49157331  8.36172416  0.13754865  0.26355086 10.18745142], Loss = 1.0556\n",
      "Iteration 882: Weights = [55.29551718  3.49134808  8.36118378  0.13753956  0.26353344 10.18803345], Loss = 1.0554\n",
      "Iteration 883: Weights = [55.29559534  3.49112285  8.36064343  0.13753046  0.26351603 10.18861543], Loss = 1.0553\n",
      "Iteration 884: Weights = [55.29567272  3.49089764  8.36010311  0.13752137  0.26349862 10.18919738], Loss = 1.0551\n",
      "Iteration 885: Weights = [55.29574933  3.49067244  8.35956284  0.13751228  0.26348122 10.18977929], Loss = 1.0550\n",
      "Iteration 886: Weights = [55.29582517  3.49044724  8.3590226   0.1375032   0.26346382 10.19036116], Loss = 1.0549\n",
      "Iteration 887: Weights = [55.29590025  3.49022206  8.35848239  0.13749412  0.26344643 10.19094299], Loss = 1.0547\n",
      "Iteration 888: Weights = [55.29597458  3.48999689  8.35794222  0.13748504  0.26342904 10.19152479], Loss = 1.0546\n",
      "Iteration 889: Weights = [55.29604817  3.48977174  8.35740209  0.13747596  0.26341166 10.19210655], Loss = 1.0545\n",
      "Iteration 890: Weights = [55.29612102  3.48954659  8.35686199  0.13746688  0.26339428 10.19268827], Loss = 1.0543\n",
      "Iteration 891: Weights = [55.29619314  3.48932145  8.35632193  0.13745781  0.26337691 10.19326995], Loss = 1.0542\n",
      "Iteration 892: Weights = [55.29626455  3.48909633  8.3557819   0.13744874  0.26335954 10.1938516 ], Loss = 1.0540\n",
      "Iteration 893: Weights = [55.29633523  3.48887121  8.35524191  0.13743967  0.26334217 10.1944332 ], Loss = 1.0539\n",
      "Iteration 894: Weights = [55.29640522  3.48864611  8.35470196  0.13743061  0.26332481 10.19501477], Loss = 1.0538\n",
      "Iteration 895: Weights = [55.2964745   3.48842102  8.35416204  0.13742154  0.26330746 10.1955963 ], Loss = 1.0536\n",
      "Iteration 896: Weights = [55.29654308  3.48819594  8.35362216  0.13741248  0.26329011 10.1961778 ], Loss = 1.0535\n",
      "Iteration 897: Weights = [55.29661099  3.48797087  8.35308231  0.13740343  0.26327276 10.19675925], Loss = 1.0534\n",
      "Iteration 898: Weights = [55.29667821  3.48774582  8.3525425   0.13739437  0.26325542 10.19734067], Loss = 1.0532\n",
      "Iteration 899: Weights = [55.29674476  3.48752077  8.35200273  0.13738532  0.26323808 10.19792205], Loss = 1.0531\n",
      "Iteration 900: Weights = [55.29681065  3.48729574  8.35146299  0.13737627  0.26322075 10.1985034 ], Loss = 1.0529\n",
      "Iteration 901: Weights = [55.29687587  3.48707072  8.35092329  0.13736722  0.26320342 10.1990847 ], Loss = 1.0528\n",
      "Iteration 902: Weights = [55.29694045  3.48684571  8.35038362  0.13735817  0.2631861  10.19966597], Loss = 1.0527\n",
      "Iteration 903: Weights = [55.29700438  3.48662071  8.34984399  0.13734913  0.26316878 10.2002472 ], Loss = 1.0525\n",
      "Iteration 904: Weights = [55.29706767  3.48639572  8.3493044   0.13734009  0.26315146 10.20082839], Loss = 1.0524\n",
      "Iteration 905: Weights = [55.29713032  3.48617074  8.34876484  0.13733105  0.26313415 10.20140955], Loss = 1.0523\n",
      "Iteration 906: Weights = [55.29719235  3.48594578  8.34822531  0.13732201  0.26311685 10.20199066], Loss = 1.0521\n",
      "Iteration 907: Weights = [55.29725376  3.48572083  8.34768583  0.13731298  0.26309954 10.20257174], Loss = 1.0520\n",
      "Iteration 908: Weights = [55.29731456  3.48549589  8.34714637  0.13730395  0.26308224 10.20315278], Loss = 1.0518\n",
      "Iteration 909: Weights = [55.29737475  3.48527096  8.34660696  0.13729492  0.26306495 10.20373379], Loss = 1.0517\n",
      "Iteration 910: Weights = [55.29743433  3.48504604  8.34606758  0.13728589  0.26304766 10.20431475], Loss = 1.0516\n",
      "Iteration 911: Weights = [55.29749332  3.48482114  8.34552823  0.13727687  0.26303037 10.20489568], Loss = 1.0514\n",
      "Iteration 912: Weights = [55.29755172  3.48459624  8.34498892  0.13726784  0.26301309 10.20547657], Loss = 1.0513\n",
      "Iteration 913: Weights = [55.29760954  3.48437136  8.34444965  0.13725882  0.26299581 10.20605742], Loss = 1.0512\n",
      "Iteration 914: Weights = [55.29766678  3.48414649  8.34391041  0.1372498   0.26297853 10.20663824], Loss = 1.0510\n",
      "Iteration 915: Weights = [55.29772344  3.48392163  8.34337121  0.13724079  0.26296126 10.20721902], Loss = 1.0509\n",
      "Iteration 916: Weights = [55.29777954  3.48369679  8.34283204  0.13723177  0.262944   10.20779976], Loss = 1.0508\n",
      "Iteration 917: Weights = [55.29783508  3.48347195  8.34229291  0.13722276  0.26292673 10.20838046], Loss = 1.0506\n",
      "Iteration 918: Weights = [55.29789006  3.48324713  8.34175382  0.13721375  0.26290947 10.20896112], Loss = 1.0505\n",
      "Iteration 919: Weights = [55.2979445   3.48302232  8.34121476  0.13720474  0.26289222 10.20954175], Loss = 1.0503\n",
      "Iteration 920: Weights = [55.29799838  3.48279752  8.34067573  0.13719573  0.26287497 10.21012234], Loss = 1.0502\n",
      "Iteration 921: Weights = [55.29805173  3.48257274  8.34013674  0.13718673  0.26285772 10.21070289], Loss = 1.0501\n",
      "Iteration 922: Weights = [55.29810455  3.48234796  8.33959779  0.13717773  0.26284048 10.21128341], Loss = 1.0499\n",
      "Iteration 923: Weights = [55.29815684  3.4821232   8.33905887  0.13716873  0.26282324 10.21186388], Loss = 1.0498\n",
      "Iteration 924: Weights = [55.2982086   3.48189845  8.33851999  0.13715973  0.262806   10.21244432], Loss = 1.0497\n",
      "Iteration 925: Weights = [55.29825985  3.48167372  8.33798115  0.13715073  0.26278877 10.21302472], Loss = 1.0495\n",
      "Iteration 926: Weights = [55.29831058  3.48144899  8.33744234  0.13714174  0.26277154 10.21360509], Loss = 1.0494\n",
      "Iteration 927: Weights = [55.29836081  3.48122428  8.33690356  0.13713274  0.26275431 10.21418541], Loss = 1.0493\n",
      "Iteration 928: Weights = [55.29841054  3.48099958  8.33636482  0.13712375  0.26273709 10.2147657 ], Loss = 1.0491\n",
      "Iteration 929: Weights = [55.29845976  3.48077489  8.33582612  0.13711476  0.26271987 10.21534595], Loss = 1.0490\n",
      "Iteration 930: Weights = [55.2985085   3.48055021  8.33528745  0.13710578  0.26270265 10.21592617], Loss = 1.0488\n",
      "Iteration 931: Weights = [55.29855675  3.48032555  8.33474882  0.13709679  0.26268544 10.21650634], Loss = 1.0487\n",
      "Iteration 932: Weights = [55.29860451  3.4801009   8.33421022  0.13708781  0.26266823 10.21708648], Loss = 1.0486\n",
      "Iteration 933: Weights = [55.2986518   3.47987626  8.33367166  0.13707883  0.26265103 10.21766658], Loss = 1.0484\n",
      "Iteration 934: Weights = [55.29869862  3.47965163  8.33313313  0.13706985  0.26263383 10.21824664], Loss = 1.0483\n",
      "Iteration 935: Weights = [55.29874497  3.47942702  8.33259464  0.13706087  0.26261663 10.21882667], Loss = 1.0482\n",
      "Iteration 936: Weights = [55.29879085  3.47920241  8.33205619  0.1370519   0.26259943 10.21940665], Loss = 1.0480\n",
      "Iteration 937: Weights = [55.29883627  3.47897782  8.33151777  0.13704292  0.26258224 10.21998661], Loss = 1.0479\n",
      "Iteration 938: Weights = [55.29888124  3.47875325  8.33097938  0.13703395  0.26256506 10.22056652], Loss = 1.0478\n",
      "Iteration 939: Weights = [55.29892577  3.47852868  8.33044104  0.13702498  0.26254787 10.22114639], Loss = 1.0476\n",
      "Iteration 940: Weights = [55.29896984  3.47830413  8.32990272  0.13701601  0.26253069 10.22172623], Loss = 1.0475\n",
      "Iteration 941: Weights = [55.29901348  3.47807959  8.32936444  0.13700704  0.26251351 10.22230603], Loss = 1.0474\n",
      "Iteration 942: Weights = [55.29905667  3.47785506  8.3288262   0.13699808  0.26249634 10.22288579], Loss = 1.0472\n",
      "Iteration 943: Weights = [55.29909944  3.47763055  8.32828799  0.13698912  0.26247917 10.22346552], Loss = 1.0471\n",
      "Iteration 944: Weights = [55.29914178  3.47740604  8.32774982  0.13698015  0.262462   10.2240452 ], Loss = 1.0469\n",
      "Iteration 945: Weights = [55.2991837   3.47718155  8.32721169  0.13697119  0.26244483 10.22462485], Loss = 1.0468\n",
      "Iteration 946: Weights = [55.29922519  3.47695708  8.32667359  0.13696224  0.26242767 10.22520447], Loss = 1.0467\n",
      "Iteration 947: Weights = [55.29926627  3.47673261  8.32613552  0.13695328  0.26241051 10.22578404], Loss = 1.0465\n",
      "Iteration 948: Weights = [55.29930694  3.47650816  8.32559749  0.13694432  0.26239336 10.22636358], Loss = 1.0464\n",
      "Iteration 949: Weights = [55.29934721  3.47628372  8.3250595   0.13693537  0.2623762  10.22694308], Loss = 1.0463\n",
      "Iteration 950: Weights = [55.29938707  3.47605929  8.32452154  0.13692642  0.26235905 10.22752254], Loss = 1.0461\n",
      "Iteration 951: Weights = [55.29942653  3.47583488  8.32398361  0.13691747  0.26234191 10.22810196], Loss = 1.0460\n",
      "Iteration 952: Weights = [55.2994656   3.47561048  8.32344572  0.13690852  0.26232476 10.22868135], Loss = 1.0459\n",
      "Iteration 953: Weights = [55.29950428  3.47538609  8.32290787  0.13689958  0.26230762 10.2292607 ], Loss = 1.0457\n",
      "Iteration 954: Weights = [55.29954257  3.47516171  8.32237005  0.13689063  0.26229049 10.22984001], Loss = 1.0456\n",
      "Iteration 955: Weights = [55.29958048  3.47493735  8.32183227  0.13688169  0.26227335 10.23041929], Loss = 1.0455\n",
      "Iteration 956: Weights = [55.299618    3.474713    8.32129452  0.13687275  0.26225622 10.23099852], Loss = 1.0453\n",
      "Iteration 957: Weights = [55.29965516  3.47448866  8.32075681  0.13686381  0.26223909 10.23157772], Loss = 1.0452\n",
      "Iteration 958: Weights = [55.29969194  3.47426434  8.32021913  0.13685487  0.26222197 10.23215688], Loss = 1.0451\n",
      "Iteration 959: Weights = [55.29972835  3.47404002  8.31968149  0.13684593  0.26220484 10.23273601], Loss = 1.0449\n",
      "Iteration 960: Weights = [55.2997644   3.47381573  8.31914389  0.13683699  0.26218772 10.23331509], Loss = 1.0448\n",
      "Iteration 961: Weights = [55.29980009  3.47359144  8.31860631  0.13682806  0.26217061 10.23389414], Loss = 1.0446\n",
      "Iteration 962: Weights = [55.29983542  3.47336717  8.31806878  0.13681913  0.26215349 10.23447316], Loss = 1.0445\n",
      "Iteration 963: Weights = [55.2998704   3.4731429   8.31753128  0.1368102   0.26213638 10.23505213], Loss = 1.0444\n",
      "Iteration 964: Weights = [55.29990503  3.47291866  8.31699381  0.13680127  0.26211927 10.23563107], Loss = 1.0442\n",
      "Iteration 965: Weights = [55.29993932  3.47269442  8.31645638  0.13679234  0.26210217 10.23620997], Loss = 1.0441\n",
      "Iteration 966: Weights = [55.29997326  3.4724702   8.31591899  0.13678341  0.26208507 10.23678883], Loss = 1.0440\n",
      "Iteration 967: Weights = [55.30000686  3.47224599  8.31538163  0.13677449  0.26206797 10.23736765], Loss = 1.0438\n",
      "Iteration 968: Weights = [55.30004012  3.47202179  8.31484431  0.13676556  0.26205087 10.23794644], Loss = 1.0437\n",
      "Iteration 969: Weights = [55.30007305  3.47179761  8.31430702  0.13675664  0.26203377 10.23852519], Loss = 1.0436\n",
      "Iteration 970: Weights = [55.30010566  3.47157344  8.31376976  0.13674772  0.26201668 10.2391039 ], Loss = 1.0434\n",
      "Iteration 971: Weights = [55.30013793  3.47134928  8.31323254  0.1367388   0.26199959 10.23968258], Loss = 1.0433\n",
      "Iteration 972: Weights = [55.30016989  3.47112514  8.31269536  0.13672989  0.26198251 10.24026121], Loss = 1.0432\n",
      "Iteration 973: Weights = [55.30020152  3.47090101  8.31215821  0.13672097  0.26196542 10.24083981], Loss = 1.0430\n",
      "Iteration 974: Weights = [55.30023284  3.47067689  8.3116211   0.13671205  0.26194834 10.24141838], Loss = 1.0429\n",
      "Iteration 975: Weights = [55.30026384  3.47045279  8.31108402  0.13670314  0.26193126 10.2419969 ], Loss = 1.0428\n",
      "Iteration 976: Weights = [55.30029454  3.47022869  8.31054698  0.13669423  0.26191419 10.24257539], Loss = 1.0426\n",
      "Iteration 977: Weights = [55.30032493  3.47000461  8.31000997  0.13668532  0.26189712 10.24315384], Loss = 1.0425\n",
      "Iteration 978: Weights = [55.30035501  3.46978055  8.309473    0.13667641  0.26188005 10.24373225], Loss = 1.0423\n",
      "Iteration 979: Weights = [55.30038479  3.4695565   8.30893606  0.1366675   0.26186298 10.24431063], Loss = 1.0422\n",
      "Iteration 980: Weights = [55.30041428  3.46933246  8.30839916  0.13665859  0.26184591 10.24488896], Loss = 1.0421\n",
      "Iteration 981: Weights = [55.30044347  3.46910843  8.30786229  0.13664969  0.26182885 10.24546726], Loss = 1.0419\n",
      "Iteration 982: Weights = [55.30047237  3.46888442  8.30732546  0.13664079  0.26181179 10.24604553], Loss = 1.0418\n",
      "Iteration 983: Weights = [55.30050098  3.46866042  8.30678866  0.13663188  0.26179473 10.24662375], Loss = 1.0417\n",
      "Iteration 984: Weights = [55.3005293   3.46843643  8.3062519   0.13662298  0.26177768 10.24720194], Loss = 1.0415\n",
      "Iteration 985: Weights = [55.30055734  3.46821246  8.30571518  0.13661408  0.26176062 10.24778009], Loss = 1.0414\n",
      "Iteration 986: Weights = [55.3005851   3.46798849  8.30517848  0.13660518  0.26174357 10.2483582 ], Loss = 1.0413\n",
      "Iteration 987: Weights = [55.30061258  3.46776455  8.30464183  0.13659629  0.26172653 10.24893628], Loss = 1.0411\n",
      "Iteration 988: Weights = [55.30063979  3.46754061  8.30410521  0.13658739  0.26170948 10.24951432], Loss = 1.0410\n",
      "Iteration 989: Weights = [55.30066673  3.46731669  8.30356862  0.1365785   0.26169244 10.25009232], Loss = 1.0409\n",
      "Iteration 990: Weights = [55.30069339  3.46709278  8.30303207  0.1365696   0.2616754  10.25067028], Loss = 1.0407\n",
      "Iteration 991: Weights = [55.30071979  3.46686889  8.30249555  0.13656071  0.26165836 10.25124821], Loss = 1.0406\n",
      "Iteration 992: Weights = [55.30074593  3.46664501  8.30195907  0.13655182  0.26164132 10.2518261 ], Loss = 1.0405\n",
      "Iteration 993: Weights = [55.3007718   3.46642114  8.30142263  0.13654293  0.26162429 10.25240395], Loss = 1.0403\n",
      "Iteration 994: Weights = [55.30079742  3.46619729  8.30088622  0.13653404  0.26160726 10.25298176], Loss = 1.0402\n",
      "Iteration 995: Weights = [55.30082278  3.46597344  8.30034984  0.13652515  0.26159023 10.25355954], Loss = 1.0401\n",
      "Iteration 996: Weights = [55.30084788  3.46574962  8.2998135   0.13651627  0.2615732  10.25413728], Loss = 1.0399\n",
      "Iteration 997: Weights = [55.30087274  3.4655258   8.29927719  0.13650738  0.26155618 10.25471498], Loss = 1.0398\n",
      "Iteration 998: Weights = [55.30089734  3.465302    8.29874092  0.1364985   0.26153916 10.25529264], Loss = 1.0397\n",
      "Iteration 999: Weights = [55.3009217   3.46507821  8.29820469  0.13648962  0.26152214 10.25587027], Loss = 1.0395\n",
      "Iteration 1000: Weights = [55.30094582  3.46485444  8.29766849  0.13648074  0.26150512 10.25644786], Loss = 1.0394\n",
      "Iteration 1001: Weights = [55.30096969  3.46463067  8.29713232  0.13647186  0.26148811 10.25702541], Loss = 1.0393\n",
      "Iteration 1002: Weights = [55.30099333  3.46440693  8.29659619  0.13646298  0.2614711  10.25760293], Loss = 1.0391\n",
      "Iteration 1003: Weights = [55.30101673  3.46418319  8.2960601   0.1364541   0.26145409 10.25818041], Loss = 1.0390\n",
      "Iteration 1004: Weights = [55.3010399   3.46395947  8.29552403  0.13644523  0.26143708 10.25875785], Loss = 1.0388\n",
      "Iteration 1005: Weights = [55.30106283  3.46373576  8.29498801  0.13643635  0.26142007 10.25933525], Loss = 1.0387\n",
      "Iteration 1006: Weights = [55.30108554  3.46351207  8.29445202  0.13642748  0.26140307 10.25991262], Loss = 1.0386\n",
      "Iteration 1007: Weights = [55.30110801  3.46328839  8.29391606  0.13641861  0.26138607 10.26048994], Loss = 1.0384\n",
      "Iteration 1008: Weights = [55.30113027  3.46306472  8.29338014  0.13640973  0.26136907 10.26106723], Loss = 1.0383\n",
      "Iteration 1009: Weights = [55.3011523   3.46284106  8.29284426  0.13640086  0.26135207 10.26164449], Loss = 1.0382\n",
      "Iteration 1010: Weights = [55.30117411  3.46261742  8.29230841  0.13639199  0.26133508 10.2622217 ], Loss = 1.0380\n",
      "Iteration 1011: Weights = [55.3011957   3.4623938   8.29177259  0.13638313  0.26131809 10.26279888], Loss = 1.0379\n",
      "Iteration 1012: Weights = [55.30121708  3.46217018  8.29123681  0.13637426  0.2613011  10.26337602], Loss = 1.0378\n",
      "Iteration 1013: Weights = [55.30123824  3.46194658  8.29070106  0.13636539  0.26128411 10.26395313], Loss = 1.0376\n",
      "Iteration 1014: Weights = [55.30125919  3.46172299  8.29016535  0.13635653  0.26126712 10.2645302 ], Loss = 1.0375\n",
      "Iteration 1015: Weights = [55.30127993  3.46149942  8.28962968  0.13634767  0.26125014 10.26510723], Loss = 1.0374\n",
      "Iteration 1016: Weights = [55.30130047  3.46127586  8.28909403  0.1363388   0.26123316 10.26568422], Loss = 1.0372\n",
      "Iteration 1017: Weights = [55.30132079  3.46105231  8.28855843  0.13632994  0.26121618 10.26626117], Loss = 1.0371\n",
      "Iteration 1018: Weights = [55.30134092  3.46082878  8.28802286  0.13632108  0.2611992  10.26683809], Loss = 1.0370\n",
      "Iteration 1019: Weights = [55.30136084  3.46060526  8.28748732  0.13631222  0.26118222 10.26741497], Loss = 1.0368\n",
      "Iteration 1020: Weights = [55.30138057  3.46038176  8.28695182  0.13630337  0.26116525 10.26799181], Loss = 1.0367\n",
      "Iteration 1021: Weights = [55.3014001   3.46015826  8.28641635  0.13629451  0.26114828 10.26856862], Loss = 1.0366\n",
      "Iteration 1022: Weights = [55.30141943  3.45993478  8.28588092  0.13628565  0.26113131 10.26914539], Loss = 1.0364\n",
      "Iteration 1023: Weights = [55.30143857  3.45971132  8.28534552  0.1362768   0.26111434 10.26972212], Loss = 1.0363\n",
      "Iteration 1024: Weights = [55.30145752  3.45948787  8.28481016  0.13626795  0.26109738 10.27029881], Loss = 1.0362\n",
      "Iteration 1025: Weights = [55.30147627  3.45926443  8.28427483  0.13625909  0.26108041 10.27087547], Loss = 1.0360\n",
      "Iteration 1026: Weights = [55.30149484  3.459041    8.28373954  0.13625024  0.26106345 10.27145209], Loss = 1.0359\n",
      "Iteration 1027: Weights = [55.30151323  3.45881759  8.28320429  0.13624139  0.26104649 10.27202867], Loss = 1.0358\n",
      "Iteration 1028: Weights = [55.30153143  3.4585942   8.28266906  0.13623254  0.26102954 10.27260521], Loss = 1.0356\n",
      "Iteration 1029: Weights = [55.30154945  3.45837081  8.28213388  0.13622369  0.26101258 10.27318172], Loss = 1.0355\n",
      "Iteration 1030: Weights = [55.30156729  3.45814744  8.28159872  0.13621485  0.26099563 10.27375819], Loss = 1.0354\n",
      "Iteration 1031: Weights = [55.30158495  3.45792408  8.2810636   0.136206    0.26097868 10.27433462], Loss = 1.0352\n",
      "Iteration 1032: Weights = [55.30160243  3.45770074  8.28052852  0.13619716  0.26096173 10.27491102], Loss = 1.0351\n",
      "Iteration 1033: Weights = [55.30161974  3.45747741  8.27999347  0.13618831  0.26094478 10.27548738], Loss = 1.0350\n",
      "Iteration 1034: Weights = [55.30163688  3.4572541   8.27945846  0.13617947  0.26092784 10.2760637 ], Loss = 1.0348\n",
      "Iteration 1035: Weights = [55.30165384  3.45703079  8.27892348  0.13617063  0.26091089 10.27663998], Loss = 1.0347\n",
      "Iteration 1036: Weights = [55.30167064  3.45680751  8.27838854  0.13616179  0.26089395 10.27721623], Loss = 1.0346\n",
      "Iteration 1037: Weights = [55.30168726  3.45658423  8.27785363  0.13615295  0.26087701 10.27779244], Loss = 1.0344\n",
      "Iteration 1038: Weights = [55.30170372  3.45636097  8.27731875  0.13614411  0.26086008 10.27836861], Loss = 1.0343\n",
      "Iteration 1039: Weights = [55.30172002  3.45613772  8.27678392  0.13613527  0.26084314 10.27894475], Loss = 1.0342\n",
      "Iteration 1040: Weights = [55.30173615  3.45591449  8.27624911  0.13612643  0.26082621 10.27952084], Loss = 1.0340\n",
      "Iteration 1041: Weights = [55.30175213  3.45569127  8.27571434  0.1361176   0.26080927 10.2800969 ], Loss = 1.0339\n",
      "Iteration 1042: Weights = [55.30176794  3.45546806  8.27517961  0.13610876  0.26079234 10.28067293], Loss = 1.0338\n",
      "Iteration 1043: Weights = [55.30178359  3.45524487  8.27464491  0.13609993  0.26077542 10.28124891], Loss = 1.0336\n",
      "Iteration 1044: Weights = [55.30179909  3.45502169  8.27411024  0.13609109  0.26075849 10.28182486], Loss = 1.0335\n",
      "Iteration 1045: Weights = [55.30181443  3.45479853  8.27357561  0.13608226  0.26074157 10.28240077], Loss = 1.0334\n",
      "Iteration 1046: Weights = [55.30182962  3.45457537  8.27304101  0.13607343  0.26072464 10.28297664], Loss = 1.0332\n",
      "Iteration 1047: Weights = [55.30184466  3.45435224  8.27250645  0.1360646   0.26070772 10.28355248], Loss = 1.0331\n",
      "Iteration 1048: Weights = [55.30185954  3.45412911  8.27197193  0.13605577  0.2606908  10.28412828], Loss = 1.0330\n",
      "Iteration 1049: Weights = [55.30187428  3.453906    8.27143744  0.13604694  0.26067389 10.28470404], Loss = 1.0328\n",
      "Iteration 1050: Weights = [55.30188887  3.4536829   8.27090298  0.13603812  0.26065697 10.28527977], Loss = 1.0327\n",
      "Iteration 1051: Weights = [55.30190332  3.45345982  8.27036856  0.13602929  0.26064006 10.28585546], Loss = 1.0326\n",
      "Iteration 1052: Weights = [55.30191762  3.45323675  8.26983417  0.13602046  0.26062315 10.28643111], Loss = 1.0324\n",
      "Iteration 1053: Weights = [55.30193177  3.4530137   8.26929982  0.13601164  0.26060624 10.28700672], Loss = 1.0323\n",
      "Iteration 1054: Weights = [55.30194579  3.45279065  8.2687655   0.13600282  0.26058933 10.2875823 ], Loss = 1.0322\n",
      "Iteration 1055: Weights = [55.30195967  3.45256763  8.26823122  0.13599399  0.26057242 10.28815783], Loss = 1.0320\n",
      "Iteration 1056: Weights = [55.3019734   3.45234461  8.26769697  0.13598517  0.26055552 10.28873334], Loss = 1.0319\n",
      "Iteration 1057: Weights = [55.301987    3.45212161  8.26716276  0.13597635  0.26053862 10.2893088 ], Loss = 1.0318\n",
      "Iteration 1058: Weights = [55.30200047  3.45189863  8.26662858  0.13596753  0.26052172 10.28988423], Loss = 1.0316\n",
      "Iteration 1059: Weights = [55.30201379  3.45167565  8.26609443  0.13595871  0.26050482 10.29045962], Loss = 1.0315\n",
      "Iteration 1060: Weights = [55.30202699  3.45145269  8.26556032  0.13594989  0.26048792 10.29103497], Loss = 1.0314\n",
      "Iteration 1061: Weights = [55.30204005  3.45122975  8.26502625  0.13594108  0.26047102 10.29161029], Loss = 1.0312\n",
      "Iteration 1062: Weights = [55.30205299  3.45100682  8.26449221  0.13593226  0.26045413 10.29218557], Loss = 1.0311\n",
      "Iteration 1063: Weights = [55.30206579  3.4507839   8.2639582   0.13592345  0.26043724 10.29276081], Loss = 1.0310\n",
      "Iteration 1064: Weights = [55.30207846  3.450561    8.26342423  0.13591463  0.26042035 10.29333601], Loss = 1.0308\n",
      "Iteration 1065: Weights = [55.30209101  3.45033811  8.2628903   0.13590582  0.26040346 10.29391118], Loss = 1.0307\n",
      "Iteration 1066: Weights = [55.30210344  3.45011523  8.2623564   0.13589701  0.26038657 10.29448631], Loss = 1.0306\n",
      "Iteration 1067: Weights = [55.30211574  3.44989237  8.26182253  0.13588819  0.26036969 10.2950614 ], Loss = 1.0304\n",
      "Iteration 1068: Weights = [55.30212791  3.44966952  8.2612887   0.13587938  0.2603528  10.29563646], Loss = 1.0303\n",
      "Iteration 1069: Weights = [55.30213997  3.44944669  8.2607549   0.13587057  0.26033592 10.29621147], Loss = 1.0302\n",
      "Iteration 1070: Weights = [55.3021519   3.44922386  8.26022114  0.13586176  0.26031904 10.29678645], Loss = 1.0300\n",
      "Iteration 1071: Weights = [55.30216371  3.44900106  8.25968741  0.13585296  0.26030216 10.2973614 ], Loss = 1.0299\n",
      "Iteration 1072: Weights = [55.30217541  3.44877826  8.25915372  0.13584415  0.26028529 10.2979363 ], Loss = 1.0298\n",
      "Iteration 1073: Weights = [55.30218699  3.44855549  8.25862006  0.13583534  0.26026841 10.29851117], Loss = 1.0296\n",
      "Iteration 1074: Weights = [55.30219845  3.44833272  8.25808643  0.13582654  0.26025154 10.29908601], Loss = 1.0295\n",
      "Iteration 1075: Weights = [55.3022098   3.44810997  8.25755285  0.13581773  0.26023467 10.2996608 ], Loss = 1.0294\n",
      "Iteration 1076: Weights = [55.30222104  3.44788723  8.25701929  0.13580893  0.2602178  10.30023556], Loss = 1.0292\n",
      "Iteration 1077: Weights = [55.30223216  3.44766451  8.25648577  0.13580013  0.26020093 10.30081028], Loss = 1.0291\n",
      "Iteration 1078: Weights = [55.30224317  3.4474418   8.25595229  0.13579132  0.26018406 10.30138496], Loss = 1.0290\n",
      "Iteration 1079: Weights = [55.30225407  3.4472191   8.25541884  0.13578252  0.2601672  10.30195961], Loss = 1.0288\n",
      "Iteration 1080: Weights = [55.30226487  3.44699642  8.25488542  0.13577372  0.26015033 10.30253422], Loss = 1.0287\n",
      "Iteration 1081: Weights = [55.30227555  3.44677375  8.25435204  0.13576492  0.26013347 10.30310879], Loss = 1.0286\n",
      "Iteration 1082: Weights = [55.30228613  3.4465511   8.25381869  0.13575612  0.26011661 10.30368333], Loss = 1.0284\n",
      "Iteration 1083: Weights = [55.3022966   3.44632846  8.25328538  0.13574733  0.26009975 10.30425783], Loss = 1.0283\n",
      "Iteration 1084: Weights = [55.30230697  3.44610583  8.2527521   0.13573853  0.26008289 10.30483229], Loss = 1.0282\n",
      "Iteration 1085: Weights = [55.30231723  3.44588322  8.25221886  0.13572973  0.26006604 10.30540671], Loss = 1.0280\n",
      "Iteration 1086: Weights = [55.30232739  3.44566062  8.25168565  0.13572094  0.26004918 10.3059811 ], Loss = 1.0279\n",
      "Iteration 1087: Weights = [55.30233745  3.44543803  8.25115248  0.13571214  0.26003233 10.30655545], Loss = 1.0278\n",
      "Iteration 1088: Weights = [55.30234741  3.44521546  8.25061934  0.13570335  0.26001548 10.30712976], Loss = 1.0276\n",
      "Iteration 1089: Weights = [55.30235727  3.44499291  8.25008623  0.13569456  0.25999863 10.30770403], Loss = 1.0275\n",
      "Iteration 1090: Weights = [55.30236703  3.44477036  8.24955317  0.13568577  0.25998178 10.30827827], Loss = 1.0274\n",
      "Iteration 1091: Weights = [55.30237669  3.44454783  8.24902013  0.13567697  0.25996494 10.30885247], Loss = 1.0272\n",
      "Iteration 1092: Weights = [55.30238626  3.44432532  8.24848713  0.13566818  0.25994809 10.30942664], Loss = 1.0271\n",
      "Iteration 1093: Weights = [55.30239573  3.44410282  8.24795416  0.13565939  0.25993125 10.31000076], Loss = 1.0270\n",
      "Iteration 1094: Weights = [55.30240511  3.44388033  8.24742123  0.13565061  0.25991441 10.31057485], Loss = 1.0268\n",
      "Iteration 1095: Weights = [55.30241439  3.44365786  8.24688834  0.13564182  0.25989757 10.31114891], Loss = 1.0267\n",
      "Iteration 1096: Weights = [55.30242358  3.4434354   8.24635547  0.13563303  0.25988073 10.31172292], Loss = 1.0266\n",
      "Iteration 1097: Weights = [55.30243268  3.44321295  8.24582265  0.13562424  0.25986389 10.3122969 ], Loss = 1.0264\n",
      "Iteration 1098: Weights = [55.30244168  3.44299052  8.24528985  0.13561546  0.25984706 10.31287084], Loss = 1.0263\n",
      "Iteration 1099: Weights = [55.3024506   3.4427681   8.24475709  0.13560667  0.25983023 10.31344474], Loss = 1.0262\n",
      "Iteration 1100: Weights = [55.30245943  3.4425457   8.24422437  0.13559789  0.25981339 10.31401861], Loss = 1.0260\n",
      "Iteration 1101: Weights = [55.30246817  3.44232331  8.24369168  0.13558911  0.25979656 10.31459244], Loss = 1.0259\n",
      "Iteration 1102: Weights = [55.30247682  3.44210094  8.24315903  0.13558032  0.25977973 10.31516623], Loss = 1.0258\n",
      "Iteration 1103: Weights = [55.30248538  3.44187857  8.2426264   0.13557154  0.25976291 10.31573999], Loss = 1.0256\n",
      "Iteration 1104: Weights = [55.30249386  3.44165623  8.24209382  0.13556276  0.25974608 10.31631371], Loss = 1.0255\n",
      "Iteration 1105: Weights = [55.30250226  3.44143389  8.24156127  0.13555398  0.25972926 10.31688739], Loss = 1.0254\n",
      "Iteration 1106: Weights = [55.30251057  3.44121157  8.24102875  0.1355452   0.25971243 10.31746103], Loss = 1.0252\n",
      "Iteration 1107: Weights = [55.30251879  3.44098927  8.24049627  0.13553642  0.25969561 10.31803464], Loss = 1.0251\n",
      "Iteration 1108: Weights = [55.30252694  3.44076698  8.23996382  0.13552765  0.25967879 10.31860821], Loss = 1.0250\n",
      "Iteration 1109: Weights = [55.302535    3.4405447   8.23943141  0.13551887  0.25966197 10.31918175], Loss = 1.0248\n",
      "Iteration 1110: Weights = [55.30254299  3.44032243  8.23889903  0.13551009  0.25964516 10.31975524], Loss = 1.0247\n",
      "Iteration 1111: Weights = [55.30255089  3.44010018  8.23836669  0.13550132  0.25962834 10.3203287 ], Loss = 1.0246\n",
      "Iteration 1112: Weights = [55.30255872  3.43987795  8.23783438  0.13549254  0.25961153 10.32090212], Loss = 1.0244\n",
      "Iteration 1113: Weights = [55.30256646  3.43965573  8.2373021   0.13548377  0.25959471 10.32147551], Loss = 1.0243\n",
      "Iteration 1114: Weights = [55.30257413  3.43943352  8.23676986  0.135475    0.2595779  10.32204886], Loss = 1.0242\n",
      "Iteration 1115: Weights = [55.30258172  3.43921133  8.23623765  0.13546623  0.25956109 10.32262217], Loss = 1.0240\n",
      "Iteration 1116: Weights = [55.30258924  3.43898915  8.23570548  0.13545745  0.25954428 10.32319544], Loss = 1.0239\n",
      "Iteration 1117: Weights = [55.30259668  3.43876698  8.23517335  0.13544868  0.25952748 10.32376868], Loss = 1.0238\n",
      "Iteration 1118: Weights = [55.30260405  3.43854483  8.23464124  0.13543991  0.25951067 10.32434188], Loss = 1.0237\n",
      "Iteration 1119: Weights = [55.30261134  3.43832269  8.23410917  0.13543114  0.25949387 10.32491504], Loss = 1.0235\n",
      "Iteration 1120: Weights = [55.30261856  3.43810057  8.23357714  0.13542238  0.25947707 10.32548816], Loss = 1.0234\n",
      "Iteration 1121: Weights = [55.30262571  3.43787846  8.23304514  0.13541361  0.25946026 10.32606125], Loss = 1.0233\n",
      "Iteration 1122: Weights = [55.30263278  3.43765636  8.23251318  0.13540484  0.25944347 10.3266343 ], Loss = 1.0231\n",
      "Iteration 1123: Weights = [55.30263979  3.43743428  8.23198125  0.13539607  0.25942667 10.32720732], Loss = 1.0230\n",
      "Iteration 1124: Weights = [55.30264672  3.43721221  8.23144935  0.13538731  0.25940987 10.3277803 ], Loss = 1.0229\n",
      "Iteration 1125: Weights = [55.30265359  3.43699016  8.23091749  0.13537854  0.25939308 10.32835324], Loss = 1.0227\n",
      "Iteration 1126: Weights = [55.30266039  3.43676812  8.23038566  0.13536978  0.25937628 10.32892614], Loss = 1.0226\n",
      "Iteration 1127: Weights = [55.30266712  3.43654609  8.22985387  0.13536102  0.25935949 10.32949901], Loss = 1.0225\n",
      "Iteration 1128: Weights = [55.30267378  3.43632408  8.22932211  0.13535225  0.2593427  10.33007184], Loss = 1.0223\n",
      "Iteration 1129: Weights = [55.30268037  3.43610209  8.22879039  0.13534349  0.25932591 10.33064463], Loss = 1.0222\n",
      "Iteration 1130: Weights = [55.3026869   3.4358801   8.2282587   0.13533473  0.25930912 10.33121738], Loss = 1.0221\n",
      "Iteration 1131: Weights = [55.30269337  3.43565813  8.22772705  0.13532597  0.25929233 10.3317901 ], Loss = 1.0219\n",
      "Iteration 1132: Weights = [55.30269977  3.43543618  8.22719543  0.13531721  0.25927555 10.33236278], Loss = 1.0218\n",
      "Iteration 1133: Weights = [55.3027061   3.43521424  8.22666384  0.13530845  0.25925876 10.33293543], Loss = 1.0217\n",
      "Iteration 1134: Weights = [55.30271238  3.43499231  8.22613229  0.1352997   0.25924198 10.33350803], Loss = 1.0215\n",
      "Iteration 1135: Weights = [55.30271859  3.4347704   8.22560077  0.13529094  0.2592252  10.33408061], Loss = 1.0214\n",
      "Iteration 1136: Weights = [55.30272473  3.4345485   8.22506929  0.13528218  0.25920842 10.33465314], Loss = 1.0213\n",
      "Iteration 1137: Weights = [55.30273082  3.43432661  8.22453784  0.13527343  0.25919164 10.33522563], Loss = 1.0211\n",
      "Iteration 1138: Weights = [55.30273684  3.43410474  8.22400643  0.13526467  0.25917487 10.33579809], Loss = 1.0210\n",
      "Iteration 1139: Weights = [55.30274281  3.43388288  8.22347505  0.13525592  0.25915809 10.33637052], Loss = 1.0209\n",
      "Iteration 1140: Weights = [55.30274871  3.43366104  8.22294371  0.13524716  0.25914132 10.3369429 ], Loss = 1.0207\n",
      "Iteration 1141: Weights = [55.30275456  3.43343921  8.2224124   0.13523841  0.25912454 10.33751525], Loss = 1.0206\n",
      "Iteration 1142: Weights = [55.30276035  3.4332174   8.22188112  0.13522966  0.25910777 10.33808756], Loss = 1.0205\n",
      "Iteration 1143: Weights = [55.30276608  3.4329956   8.22134988  0.1352209   0.259091   10.33865983], Loss = 1.0203\n",
      "Iteration 1144: Weights = [55.30277175  3.43277381  8.22081867  0.13521215  0.25907423 10.33923207], Loss = 1.0202\n",
      "Iteration 1145: Weights = [55.30277737  3.43255204  8.2202875   0.1352034   0.25905747 10.33980427], Loss = 1.0201\n",
      "Iteration 1146: Weights = [55.30278293  3.43233028  8.21975636  0.13519465  0.2590407  10.34037644], Loss = 1.0200\n",
      "Iteration 1147: Weights = [55.30278843  3.43210854  8.21922526  0.13518591  0.25902394 10.34094856], Loss = 1.0198\n",
      "Iteration 1148: Weights = [55.30279388  3.43188681  8.21869419  0.13517716  0.25900717 10.34152065], Loss = 1.0197\n",
      "Iteration 1149: Weights = [55.30279927  3.43166509  8.21816316  0.13516841  0.25899041 10.3420927 ], Loss = 1.0196\n",
      "Iteration 1150: Weights = [55.30280461  3.43144339  8.21763216  0.13515966  0.25897365 10.34266472], Loss = 1.0194\n",
      "Iteration 1151: Weights = [55.3028099   3.4312217   8.21710119  0.13515092  0.25895689 10.3432367 ], Loss = 1.0193\n",
      "Iteration 1152: Weights = [55.30281514  3.43100002  8.21657026  0.13514217  0.25894013 10.34380864], Loss = 1.0192\n",
      "Iteration 1153: Weights = [55.30282032  3.43077837  8.21603936  0.13513343  0.25892338 10.34438054], Loss = 1.0190\n",
      "Iteration 1154: Weights = [55.30282545  3.43055672  8.2155085   0.13512468  0.25890662 10.34495241], Loss = 1.0189\n",
      "Iteration 1155: Weights = [55.30283053  3.43033509  8.21497767  0.13511594  0.25888987 10.34552424], Loss = 1.0188\n",
      "Iteration 1156: Weights = [55.30283555  3.43011347  8.21444688  0.1351072   0.25887311 10.34609604], Loss = 1.0186\n",
      "Iteration 1157: Weights = [55.30284053  3.42989187  8.21391612  0.13509846  0.25885636 10.34666779], Loss = 1.0185\n",
      "Iteration 1158: Weights = [55.30284546  3.42967028  8.21338539  0.13508971  0.25883961 10.34723951], Loss = 1.0184\n",
      "Iteration 1159: Weights = [55.30285034  3.4294487   8.2128547   0.13508097  0.25882286 10.34781119], Loss = 1.0182\n",
      "Iteration 1160: Weights = [55.30285517  3.42922714  8.21232404  0.13507223  0.25880612 10.34838284], Loss = 1.0181\n",
      "Iteration 1161: Weights = [55.30285995  3.42900559  8.21179342  0.1350635   0.25878937 10.34895445], Loss = 1.0180\n",
      "Iteration 1162: Weights = [55.30286468  3.42878406  8.21126283  0.13505476  0.25877263 10.34952602], Loss = 1.0178\n",
      "Iteration 1163: Weights = [55.30286937  3.42856254  8.21073228  0.13504602  0.25875588 10.35009756], Loss = 1.0177\n",
      "Iteration 1164: Weights = [55.30287401  3.42834104  8.21020176  0.13503728  0.25873914 10.35066905], Loss = 1.0176\n",
      "Iteration 1165: Weights = [55.3028786   3.42811955  8.20967128  0.13502855  0.2587224  10.35124052], Loss = 1.0175\n",
      "Iteration 1166: Weights = [55.30288315  3.42789807  8.20914083  0.13501981  0.25870566 10.35181194], Loss = 1.0173\n",
      "Iteration 1167: Weights = [55.30288765  3.42767661  8.20861041  0.13501108  0.25868892 10.35238333], Loss = 1.0172\n",
      "Iteration 1168: Weights = [55.30289211  3.42745516  8.20808003  0.13500234  0.25867219 10.35295468], Loss = 1.0171\n",
      "Iteration 1169: Weights = [55.30289652  3.42723372  8.20754968  0.13499361  0.25865545 10.35352599], Loss = 1.0169\n",
      "Iteration 1170: Weights = [55.30290089  3.4270123   8.20701937  0.13498487  0.25863872 10.35409727], Loss = 1.0168\n",
      "Iteration 1171: Weights = [55.30290521  3.4267909   8.20648909  0.13497614  0.25862198 10.35466851], Loss = 1.0167\n",
      "Iteration 1172: Weights = [55.3029095   3.4265695   8.20595885  0.13496741  0.25860525 10.35523971], Loss = 1.0165\n",
      "Iteration 1173: Weights = [55.30291373  3.42634813  8.20542863  0.13495868  0.25858852 10.35581088], Loss = 1.0164\n",
      "Iteration 1174: Weights = [55.30291793  3.42612676  8.20489846  0.13494995  0.25857179 10.356382  ], Loss = 1.0163\n",
      "Iteration 1175: Weights = [55.30292208  3.42590541  8.20436832  0.13494122  0.25855507 10.3569531 ], Loss = 1.0161\n",
      "Iteration 1176: Weights = [55.3029262   3.42568408  8.20383821  0.13493249  0.25853834 10.35752415], Loss = 1.0160\n",
      "Iteration 1177: Weights = [55.30293027  3.42546276  8.20330814  0.13492376  0.25852162 10.35809517], Loss = 1.0159\n",
      "Iteration 1178: Weights = [55.3029343   3.42524145  8.2027781   0.13491504  0.25850489 10.35866615], Loss = 1.0157\n",
      "Iteration 1179: Weights = [55.30293829  3.42502016  8.2022481   0.13490631  0.25848817 10.35923709], Loss = 1.0156\n",
      "Iteration 1180: Weights = [55.30294224  3.42479888  8.20171813  0.13489758  0.25847145 10.359808  ], Loss = 1.0155\n",
      "Iteration 1181: Weights = [55.30294615  3.42457761  8.20118819  0.13488886  0.25845473 10.36037887], Loss = 1.0154\n",
      "Iteration 1182: Weights = [55.30295002  3.42435636  8.20065829  0.13488013  0.25843801 10.36094971], Loss = 1.0152\n",
      "Iteration 1183: Weights = [55.30295386  3.42413512  8.20012842  0.13487141  0.25842129 10.3615205 ], Loss = 1.0151\n",
      "Iteration 1184: Weights = [55.30295765  3.4239139   8.19959859  0.13486268  0.25840458 10.36209126], Loss = 1.0150\n",
      "Iteration 1185: Weights = [55.30296141  3.42369269  8.19906879  0.13485396  0.25838786 10.36266199], Loss = 1.0148\n",
      "Iteration 1186: Weights = [55.30296513  3.4234715   8.19853903  0.13484524  0.25837115 10.36323267], Loss = 1.0147\n",
      "Iteration 1187: Weights = [55.30296881  3.42325032  8.1980093   0.13483652  0.25835444 10.36380332], Loss = 1.0146\n",
      "Iteration 1188: Weights = [55.30297245  3.42302915  8.1974796   0.1348278   0.25833772 10.36437393], Loss = 1.0144\n",
      "Iteration 1189: Weights = [55.30297606  3.422808    8.19694994  0.13481908  0.25832101 10.36494451], Loss = 1.0143\n",
      "Iteration 1190: Weights = [55.30297964  3.42258686  8.19642031  0.13481036  0.25830431 10.36551505], Loss = 1.0142\n",
      "Iteration 1191: Weights = [55.30298317  3.42236574  8.19589072  0.13480164  0.2582876  10.36608555], Loss = 1.0140\n",
      "Iteration 1192: Weights = [55.30298667  3.42214463  8.19536116  0.13479292  0.25827089 10.36665601], Loss = 1.0139\n",
      "Iteration 1193: Weights = [55.30299014  3.42192353  8.19483164  0.1347842   0.25825419 10.36722644], Loss = 1.0138\n",
      "Iteration 1194: Weights = [55.30299357  3.42170245  8.19430215  0.13477548  0.25823748 10.36779683], Loss = 1.0136\n",
      "Iteration 1195: Weights = [55.30299697  3.42148138  8.19377269  0.13476677  0.25822078 10.36836719], Loss = 1.0135\n",
      "Iteration 1196: Weights = [55.30300033  3.42126033  8.19324327  0.13475805  0.25820408 10.36893751], Loss = 1.0134\n",
      "Iteration 1197: Weights = [55.30300366  3.42103929  8.19271388  0.13474934  0.25818738 10.36950779], Loss = 1.0133\n",
      "Iteration 1198: Weights = [55.30300696  3.42081827  8.19218453  0.13474062  0.25817068 10.37007803], Loss = 1.0131\n",
      "Iteration 1199: Weights = [55.30301022  3.42059725  8.19165521  0.13473191  0.25815399 10.37064824], Loss = 1.0130\n",
      "Iteration 1200: Weights = [55.30301345  3.42037626  8.19112593  0.1347232   0.25813729 10.37121841], Loss = 1.0129\n",
      "Iteration 1201: Weights = [55.30301665  3.42015528  8.19059668  0.13471448  0.25812059 10.37178854], Loss = 1.0127\n",
      "Iteration 1202: Weights = [55.30301982  3.41993431  8.19006746  0.13470577  0.2581039  10.37235864], Loss = 1.0126\n",
      "Iteration 1203: Weights = [55.30302296  3.41971335  8.18953828  0.13469706  0.25808721 10.3729287 ], Loss = 1.0125\n",
      "Iteration 1204: Weights = [55.30302606  3.41949241  8.18900913  0.13468835  0.25807052 10.37349872], Loss = 1.0123\n",
      "Iteration 1205: Weights = [55.30302913  3.41927149  8.18848002  0.13467964  0.25805383 10.3740687 ], Loss = 1.0122\n",
      "Iteration 1206: Weights = [55.30303217  3.41905057  8.18795094  0.13467093  0.25803714 10.37463865], Loss = 1.0121\n",
      "Iteration 1207: Weights = [55.30303519  3.41882968  8.1874219   0.13466222  0.25802045 10.37520857], Loss = 1.0119\n",
      "Iteration 1208: Weights = [55.30303817  3.41860879  8.18689289  0.13465351  0.25800377 10.37577844], Loss = 1.0118\n",
      "Iteration 1209: Weights = [55.30304112  3.41838792  8.18636391  0.13464481  0.25798708 10.37634828], Loss = 1.0117\n",
      "Iteration 1210: Weights = [55.30304404  3.41816707  8.18583497  0.1346361   0.2579704  10.37691808], Loss = 1.0116\n",
      "Iteration 1211: Weights = [55.30304693  3.41794623  8.18530606  0.13462739  0.25795371 10.37748785], Loss = 1.0114\n",
      "Iteration 1212: Weights = [55.3030498   3.4177254   8.18477719  0.13461869  0.25793703 10.37805757], Loss = 1.0113\n",
      "Iteration 1213: Weights = [55.30305263  3.41750459  8.18424835  0.13460998  0.25792035 10.37862726], Loss = 1.0112\n",
      "Iteration 1214: Weights = [55.30305544  3.41728379  8.18371954  0.13460128  0.25790367 10.37919692], Loss = 1.0110\n",
      "Iteration 1215: Weights = [55.30305822  3.417063    8.18319077  0.13459257  0.257887   10.37976654], Loss = 1.0109\n",
      "Iteration 1216: Weights = [55.30306097  3.41684223  8.18266203  0.13458387  0.25787032 10.38033612], Loss = 1.0108\n",
      "Iteration 1217: Weights = [55.30306369  3.41662148  8.18213333  0.13457517  0.25785365 10.38090566], Loss = 1.0106\n",
      "Iteration 1218: Weights = [55.30306639  3.41640073  8.18160466  0.13456647  0.25783697 10.38147517], Loss = 1.0105\n",
      "Iteration 1219: Weights = [55.30306906  3.41618001  8.18107603  0.13455777  0.2578203  10.38204464], Loss = 1.0104\n",
      "Iteration 1220: Weights = [55.3030717   3.41595929  8.18054743  0.13454907  0.25780363 10.38261407], Loss = 1.0102\n",
      "Iteration 1221: Weights = [55.30307432  3.41573859  8.18001886  0.13454037  0.25778696 10.38318347], Loss = 1.0101\n",
      "Iteration 1222: Weights = [55.30307691  3.41551791  8.17949033  0.13453167  0.25777029 10.38375283], Loss = 1.0100\n",
      "Iteration 1223: Weights = [55.30307947  3.41529723  8.17896183  0.13452297  0.25775362 10.38432215], Loss = 1.0099\n",
      "Iteration 1224: Weights = [55.30308201  3.41507658  8.17843337  0.13451427  0.25773695 10.38489144], Loss = 1.0097\n",
      "Iteration 1225: Weights = [55.30308453  3.41485593  8.17790494  0.13450557  0.25772029 10.38546069], Loss = 1.0096\n",
      "Iteration 1226: Weights = [55.30308701  3.4146353   8.17737655  0.13449688  0.25770362 10.3860299 ], Loss = 1.0095\n",
      "Iteration 1227: Weights = [55.30308948  3.41441469  8.17684819  0.13448818  0.25768696 10.38659908], Loss = 1.0093\n",
      "Iteration 1228: Weights = [55.30309191  3.41419409  8.17631986  0.13447949  0.2576703  10.38716822], Loss = 1.0092\n",
      "Iteration 1229: Weights = [55.30309433  3.4139735   8.17579157  0.13447079  0.25765364 10.38773732], Loss = 1.0091\n",
      "Iteration 1230: Weights = [55.30309672  3.41375293  8.17526331  0.1344621   0.25763698 10.38830639], Loss = 1.0089\n",
      "Iteration 1231: Weights = [55.30309909  3.41353237  8.17473509  0.1344534   0.25762032 10.38887541], Loss = 1.0088\n",
      "Iteration 1232: Weights = [55.30310143  3.41331183  8.1742069   0.13444471  0.25760366 10.38944441], Loss = 1.0087\n",
      "Iteration 1233: Weights = [55.30310375  3.4130913   8.17367874  0.13443602  0.25758701 10.39001336], Loss = 1.0086\n",
      "Iteration 1234: Weights = [55.30310604  3.41287078  8.17315062  0.13442733  0.25757035 10.39058228], Loss = 1.0084\n",
      "Iteration 1235: Weights = [55.30310832  3.41265028  8.17262253  0.13441863  0.2575537  10.39115116], Loss = 1.0083\n",
      "Iteration 1236: Weights = [55.30311057  3.41242979  8.17209448  0.13440994  0.25753705 10.39172001], Loss = 1.0082\n",
      "Iteration 1237: Weights = [55.30311279  3.41220932  8.17156646  0.13440125  0.25752039 10.39228882], Loss = 1.0080\n",
      "Iteration 1238: Weights = [55.303115    3.41198886  8.17103848  0.13439257  0.25750374 10.39285759], Loss = 1.0079\n",
      "Iteration 1239: Weights = [55.30311718  3.41176841  8.17051053  0.13438388  0.2574871  10.39342632], Loss = 1.0078\n",
      "Iteration 1240: Weights = [55.30311934  3.41154798  8.16998261  0.13437519  0.25747045 10.39399502], Loss = 1.0076\n",
      "Iteration 1241: Weights = [55.30312148  3.41132756  8.16945473  0.1343665   0.2574538  10.39456368], Loss = 1.0075\n",
      "Iteration 1242: Weights = [55.3031236   3.41110716  8.16892688  0.13435781  0.25743716 10.39513231], Loss = 1.0074\n",
      "Iteration 1243: Weights = [55.3031257   3.41088677  8.16839906  0.13434913  0.25742051 10.39570089], Loss = 1.0072\n",
      "Iteration 1244: Weights = [55.30312778  3.4106664   8.16787128  0.13434044  0.25740387 10.39626945], Loss = 1.0071\n",
      "Iteration 1245: Weights = [55.30312983  3.41044604  8.16734354  0.13433176  0.25738723 10.39683796], Loss = 1.0070\n",
      "Iteration 1246: Weights = [55.30313187  3.41022569  8.16681583  0.13432307  0.25737059 10.39740644], Loss = 1.0069\n",
      "Iteration 1247: Weights = [55.30313388  3.41000536  8.16628815  0.13431439  0.25735395 10.39797488], Loss = 1.0067\n",
      "Iteration 1248: Weights = [55.30313588  3.40978504  8.16576051  0.13430571  0.25733731 10.39854328], Loss = 1.0066\n",
      "Iteration 1249: Weights = [55.30313785  3.40956474  8.1652329   0.13429702  0.25732067 10.39911165], Loss = 1.0065\n",
      "Iteration 1250: Weights = [55.30313981  3.40934445  8.16470532  0.13428834  0.25730404 10.39967998], Loss = 1.0063\n",
      "Iteration 1251: Weights = [55.30314174  3.40912417  8.16417778  0.13427966  0.2572874  10.40024828], Loss = 1.0062\n",
      "Iteration 1252: Weights = [55.30314366  3.40890391  8.16365027  0.13427098  0.25727077 10.40081653], Loss = 1.0061\n",
      "Iteration 1253: Weights = [55.30314555  3.40868366  8.1631228   0.1342623   0.25725414 10.40138475], Loss = 1.0059\n",
      "Iteration 1254: Weights = [55.30314743  3.40846343  8.16259536  0.13425362  0.25723751 10.40195294], Loss = 1.0058\n",
      "Iteration 1255: Weights = [55.30314929  3.40824321  8.16206796  0.13424494  0.25722088 10.40252108], Loss = 1.0057\n",
      "Iteration 1256: Weights = [55.30315113  3.408023    8.16154059  0.13423626  0.25720425 10.40308919], Loss = 1.0056\n",
      "Iteration 1257: Weights = [55.30315295  3.40780281  8.16101325  0.13422759  0.25718762 10.40365727], Loss = 1.0054\n",
      "Iteration 1258: Weights = [55.30315476  3.40758264  8.16048595  0.13421891  0.25717099 10.40422531], Loss = 1.0053\n",
      "Iteration 1259: Weights = [55.30315654  3.40736247  8.15995868  0.13421023  0.25715437 10.40479331], Loss = 1.0052\n",
      "Iteration 1260: Weights = [55.30315831  3.40714233  8.15943145  0.13420156  0.25713774 10.40536127], Loss = 1.0050\n",
      "Iteration 1261: Weights = [55.30316006  3.40692219  8.15890425  0.13419288  0.25712112 10.4059292 ], Loss = 1.0049\n",
      "Iteration 1262: Weights = [55.30316179  3.40670207  8.15837708  0.13418421  0.2571045  10.40649709], Loss = 1.0048\n",
      "Iteration 1263: Weights = [55.30316351  3.40648197  8.15784995  0.13417553  0.25708788 10.40706494], Loss = 1.0046\n",
      "Iteration 1264: Weights = [55.30316521  3.40626187  8.15732285  0.13416686  0.25707126 10.40763276], Loss = 1.0045\n",
      "Iteration 1265: Weights = [55.30316689  3.4060418   8.15679579  0.13415819  0.25705464 10.40820054], Loss = 1.0044\n",
      "Iteration 1266: Weights = [55.30316855  3.40582173  8.15626876  0.13414951  0.25703802 10.40876828], Loss = 1.0043\n",
      "Iteration 1267: Weights = [55.3031702   3.40560168  8.15574176  0.13414084  0.25702141 10.40933599], Loss = 1.0041\n",
      "Iteration 1268: Weights = [55.30317183  3.40538165  8.1552148   0.13413217  0.25700479 10.40990366], Loss = 1.0040\n",
      "Iteration 1269: Weights = [55.30317345  3.40516163  8.15468787  0.1341235   0.25698818 10.41047129], Loss = 1.0039\n",
      "Iteration 1270: Weights = [55.30317505  3.40494162  8.15416098  0.13411483  0.25697157 10.41103889], Loss = 1.0037\n",
      "Iteration 1271: Weights = [55.30317663  3.40472163  8.15363412  0.13410616  0.25695495 10.41160645], Loss = 1.0036\n",
      "Iteration 1272: Weights = [55.3031782   3.40450165  8.1531073   0.13409749  0.25693834 10.41217397], Loss = 1.0035\n",
      "Iteration 1273: Weights = [55.30317975  3.40428168  8.1525805   0.13408883  0.25692173 10.41274146], Loss = 1.0034\n",
      "Iteration 1274: Weights = [55.30318128  3.40406173  8.15205375  0.13408016  0.25690513 10.41330891], Loss = 1.0032\n",
      "Iteration 1275: Weights = [55.3031828   3.4038418   8.15152702  0.13407149  0.25688852 10.41387632], Loss = 1.0031\n",
      "Iteration 1276: Weights = [55.30318431  3.40362188  8.15100034  0.13406283  0.25687191 10.4144437 ], Loss = 1.0030\n",
      "Iteration 1277: Weights = [55.3031858   3.40340197  8.15047368  0.13405416  0.25685531 10.41501104], Loss = 1.0028\n",
      "Iteration 1278: Weights = [55.30318727  3.40318207  8.14994706  0.13404549  0.25683871 10.41557834], Loss = 1.0027\n",
      "Iteration 1279: Weights = [55.30318873  3.40296219  8.14942047  0.13403683  0.2568221  10.41614561], Loss = 1.0026\n",
      "Iteration 1280: Weights = [55.30319018  3.40274233  8.14889392  0.13402817  0.2568055  10.41671284], Loss = 1.0024\n",
      "Iteration 1281: Weights = [55.30319161  3.40252248  8.1483674   0.1340195   0.2567889  10.41728003], Loss = 1.0023\n",
      "Iteration 1282: Weights = [55.30319303  3.40230264  8.14784092  0.13401084  0.2567723  10.41784719], Loss = 1.0022\n",
      "Iteration 1283: Weights = [55.30319443  3.40208282  8.14731447  0.13400218  0.25675571 10.41841431], Loss = 1.0021\n",
      "Iteration 1284: Weights = [55.30319582  3.40186301  8.14678805  0.13399352  0.25673911 10.41898139], Loss = 1.0019\n",
      "Iteration 1285: Weights = [55.3031972   3.40164321  8.14626167  0.13398486  0.25672251 10.41954844], Loss = 1.0018\n",
      "Iteration 1286: Weights = [55.30319856  3.40142343  8.14573532  0.1339762   0.25670592 10.42011545], Loss = 1.0017\n",
      "Iteration 1287: Weights = [55.30319991  3.40120367  8.145209    0.13396754  0.25668933 10.42068242], Loss = 1.0015\n",
      "Iteration 1288: Weights = [55.30320124  3.40098391  8.14468272  0.13395888  0.25667274 10.42124936], Loss = 1.0014\n",
      "Iteration 1289: Weights = [55.30320256  3.40076418  8.14415648  0.13395022  0.25665614 10.42181626], Loss = 1.0013\n",
      "Iteration 1290: Weights = [55.30320387  3.40054445  8.14363027  0.13394156  0.25663956 10.42238312], Loss = 1.0011\n",
      "Iteration 1291: Weights = [55.30320516  3.40032474  8.14310409  0.1339329   0.25662297 10.42294995], Loss = 1.0010\n",
      "Iteration 1292: Weights = [55.30320644  3.40010505  8.14257794  0.13392425  0.25660638 10.42351674], Loss = 1.0009\n",
      "Iteration 1293: Weights = [55.30320771  3.39988536  8.14205183  0.13391559  0.25658979 10.42408349], Loss = 1.0008\n",
      "Iteration 1294: Weights = [55.30320897  3.3996657   8.14152576  0.13390693  0.25657321 10.42465021], Loss = 1.0006\n",
      "Iteration 1295: Weights = [55.30321021  3.39944604  8.14099971  0.13389828  0.25655662 10.42521689], Loss = 1.0005\n",
      "Iteration 1296: Weights = [55.30321144  3.3992264   8.14047371  0.13388963  0.25654004 10.42578354], Loss = 1.0004\n",
      "Iteration 1297: Weights = [55.30321266  3.39900678  8.13994773  0.13388097  0.25652346 10.42635014], Loss = 1.0002\n",
      "Iteration 1298: Weights = [55.30321387  3.39878717  8.13942179  0.13387232  0.25650688 10.42691671], Loss = 1.0001\n",
      "Iteration 1299: Weights = [55.30321506  3.39856757  8.13889588  0.13386367  0.2564903  10.42748325], Loss = 1.0000\n",
      "Iteration 1300: Weights = [55.30321625  3.39834799  8.13837001  0.13385501  0.25647372 10.42804974], Loss = 0.9999\n",
      "Iteration 1301: Weights = [55.30321742  3.39812842  8.13784417  0.13384636  0.25645714 10.42861621], Loss = 0.9997\n",
      "Iteration 1302: Weights = [55.30321858  3.39790886  8.13731837  0.13383771  0.25644057 10.42918263], Loss = 0.9996\n",
      "Iteration 1303: Weights = [55.30321973  3.39768932  8.1367926   0.13382906  0.25642399 10.42974902], Loss = 0.9995\n",
      "Iteration 1304: Weights = [55.30322086  3.3974698   8.13626686  0.13382041  0.25640742 10.43031537], Loss = 0.9993\n",
      "Iteration 1305: Weights = [55.30322199  3.39725029  8.13574116  0.13381176  0.25639085 10.43088168], Loss = 0.9992\n",
      "Iteration 1306: Weights = [55.3032231   3.39703079  8.13521549  0.13380311  0.25637427 10.43144796], Loss = 0.9991\n",
      "Iteration 1307: Weights = [55.3032242   3.3968113   8.13468986  0.13379447  0.2563577  10.4320142 ], Loss = 0.9990\n",
      "Iteration 1308: Weights = [55.30322529  3.39659183  8.13416426  0.13378582  0.25634114 10.43258041], Loss = 0.9988\n",
      "Iteration 1309: Weights = [55.30322637  3.39637238  8.13363869  0.13377717  0.25632457 10.43314658], Loss = 0.9987\n",
      "Iteration 1310: Weights = [55.30322744  3.39615294  8.13311316  0.13376853  0.256308   10.43371271], Loss = 0.9986\n",
      "Iteration 1311: Weights = [55.3032285   3.39593351  8.13258766  0.13375988  0.25629143 10.4342788 ], Loss = 0.9984\n",
      "Iteration 1312: Weights = [55.30322955  3.3957141   8.1320622   0.13375124  0.25627487 10.43484486], Loss = 0.9983\n",
      "Iteration 1313: Weights = [55.30323059  3.3954947   8.13153677  0.13374259  0.25625831 10.43541088], Loss = 0.9982\n",
      "Iteration 1314: Weights = [55.30323162  3.39527531  8.13101137  0.13373395  0.25624174 10.43597687], Loss = 0.9980\n",
      "Iteration 1315: Weights = [55.30323263  3.39505594  8.13048601  0.1337253   0.25622518 10.43654282], Loss = 0.9979\n",
      "Iteration 1316: Weights = [55.30323364  3.39483658  8.12996068  0.13371666  0.25620862 10.43710873], Loss = 0.9978\n",
      "Iteration 1317: Weights = [55.30323464  3.39461724  8.12943538  0.13370802  0.25619206 10.4376746 ], Loss = 0.9977\n",
      "Iteration 1318: Weights = [55.30323562  3.39439791  8.12891012  0.13369938  0.2561755  10.43824044], Loss = 0.9975\n",
      "Iteration 1319: Weights = [55.3032366   3.3941786   8.1283849   0.13369074  0.25615895 10.43880624], Loss = 0.9974\n",
      "Iteration 1320: Weights = [55.30323757  3.3939593   8.1278597   0.1336821   0.25614239 10.43937201], Loss = 0.9973\n",
      "Iteration 1321: Weights = [55.30323853  3.39374001  8.12733454  0.13367346  0.25612584 10.43993774], Loss = 0.9971\n",
      "Iteration 1322: Weights = [55.30323947  3.39352074  8.12680942  0.13366482  0.25610928 10.44050343], Loss = 0.9970\n",
      "Iteration 1323: Weights = [55.30324041  3.39330148  8.12628433  0.13365618  0.25609273 10.44106909], Loss = 0.9969\n",
      "Iteration 1324: Weights = [55.30324134  3.39308224  8.12575927  0.13364754  0.25607618 10.44163471], Loss = 0.9968\n",
      "Iteration 1325: Weights = [55.30324226  3.39286301  8.12523425  0.1336389   0.25605963 10.44220029], Loss = 0.9966\n",
      "Iteration 1326: Weights = [55.30324317  3.39264379  8.12470926  0.13363027  0.25604308 10.44276584], Loss = 0.9965\n",
      "Iteration 1327: Weights = [55.30324407  3.39242459  8.1241843   0.13362163  0.25602653 10.44333135], Loss = 0.9964\n",
      "Iteration 1328: Weights = [55.30324497  3.39220541  8.12365938  0.133613    0.25600998 10.44389682], Loss = 0.9962\n",
      "Iteration 1329: Weights = [55.30324585  3.39198623  8.1231345   0.13360436  0.25599344 10.44446226], Loss = 0.9961\n",
      "Iteration 1330: Weights = [55.30324672  3.39176707  8.12260964  0.13359573  0.25597689 10.44502766], Loss = 0.9960\n",
      "Iteration 1331: Weights = [55.30324759  3.39154793  8.12208482  0.13358709  0.25596035 10.44559302], Loss = 0.9959\n",
      "Iteration 1332: Weights = [55.30324845  3.3913288   8.12156004  0.13357846  0.25594381 10.44615835], Loss = 0.9957\n",
      "Iteration 1333: Weights = [55.3032493   3.39110968  8.12103528  0.13356983  0.25592727 10.44672364], Loss = 0.9956\n",
      "Iteration 1334: Weights = [55.30325014  3.39089058  8.12051057  0.13356119  0.25591073 10.4472889 ], Loss = 0.9955\n",
      "Iteration 1335: Weights = [55.30325097  3.39067149  8.11998588  0.13355256  0.25589419 10.44785411], Loss = 0.9953\n",
      "Iteration 1336: Weights = [55.30325179  3.39045242  8.11946123  0.13354393  0.25587765 10.44841929], Loss = 0.9952\n",
      "Iteration 1337: Weights = [55.30325261  3.39023336  8.11893662  0.1335353   0.25586111 10.44898444], Loss = 0.9951\n",
      "Iteration 1338: Weights = [55.30325342  3.39001431  8.11841204  0.13352667  0.25584458 10.44954955], Loss = 0.9950\n",
      "Iteration 1339: Weights = [55.30325422  3.38979528  8.11788749  0.13351804  0.25582804 10.45011462], Loss = 0.9948\n",
      "Iteration 1340: Weights = [55.30325501  3.38957626  8.11736297  0.13350941  0.25581151 10.45067965], Loss = 0.9947\n",
      "Iteration 1341: Weights = [55.30325579  3.38935726  8.11683849  0.13350078  0.25579498 10.45124465], Loss = 0.9946\n",
      "Iteration 1342: Weights = [55.30325656  3.38913827  8.11631405  0.13349216  0.25577845 10.45180961], Loss = 0.9944\n",
      "Iteration 1343: Weights = [55.30325733  3.38891929  8.11578963  0.13348353  0.25576191 10.45237454], Loss = 0.9943\n",
      "Iteration 1344: Weights = [55.30325809  3.38870033  8.11526525  0.1334749   0.25574539 10.45293943], Loss = 0.9942\n",
      "Iteration 1345: Weights = [55.30325885  3.38848138  8.11474091  0.13346628  0.25572886 10.45350428], Loss = 0.9941\n",
      "Iteration 1346: Weights = [55.30325959  3.38826245  8.1142166   0.13345765  0.25571233 10.4540691 ], Loss = 0.9939\n",
      "Iteration 1347: Weights = [55.30326033  3.38804353  8.11369232  0.13344903  0.2556958  10.45463388], Loss = 0.9938\n",
      "Iteration 1348: Weights = [55.30326106  3.38782463  8.11316808  0.1334404   0.25567928 10.45519862], Loss = 0.9937\n",
      "Iteration 1349: Weights = [55.30326178  3.38760574  8.11264387  0.13343178  0.25566276 10.45576333], Loss = 0.9935\n",
      "Iteration 1350: Weights = [55.3032625   3.38738686  8.11211969  0.13342316  0.25564623 10.456328  ], Loss = 0.9934\n",
      "Iteration 1351: Weights = [55.3032632   3.387168    8.11159555  0.13341453  0.25562971 10.45689263], Loss = 0.9933\n",
      "Iteration 1352: Weights = [55.30326391  3.38694915  8.11107145  0.13340591  0.25561319 10.45745723], Loss = 0.9932\n",
      "Iteration 1353: Weights = [55.3032646   3.38673031  8.11054737  0.13339729  0.25559667 10.45802179], Loss = 0.9930\n",
      "Iteration 1354: Weights = [55.30326529  3.38651149  8.11002333  0.13338867  0.25558016 10.45858631], Loss = 0.9929\n",
      "Iteration 1355: Weights = [55.30326597  3.38629269  8.10949933  0.13338005  0.25556364 10.4591508 ], Loss = 0.9928\n",
      "Iteration 1356: Weights = [55.30326664  3.38607389  8.10897535  0.13337143  0.25554712 10.45971525], Loss = 0.9926\n",
      "Iteration 1357: Weights = [55.30326731  3.38585512  8.10845142  0.13336281  0.25553061 10.46027967], Loss = 0.9925\n",
      "Iteration 1358: Weights = [55.30326797  3.38563635  8.10792751  0.13335419  0.25551409 10.46084405], Loss = 0.9924\n",
      "Iteration 1359: Weights = [55.30326862  3.3854176   8.10740364  0.13334558  0.25549758 10.46140839], Loss = 0.9923\n",
      "Iteration 1360: Weights = [55.30326927  3.38519887  8.1068798   0.13333696  0.25548107 10.46197269], Loss = 0.9921\n",
      "Iteration 1361: Weights = [55.30326991  3.38498015  8.106356    0.13332834  0.25546456 10.46253696], Loss = 0.9920\n",
      "Iteration 1362: Weights = [55.30327054  3.38476144  8.10583223  0.13331973  0.25544805 10.46310119], Loss = 0.9919\n",
      "Iteration 1363: Weights = [55.30327117  3.38454275  8.1053085   0.13331111  0.25543154 10.46366539], Loss = 0.9917\n",
      "Iteration 1364: Weights = [55.30327179  3.38432407  8.1047848   0.1333025   0.25541503 10.46422955], Loss = 0.9916\n",
      "Iteration 1365: Weights = [55.30327241  3.3841054   8.10426113  0.13329388  0.25539853 10.46479367], Loss = 0.9915\n",
      "Iteration 1366: Weights = [55.30327302  3.38388675  8.1037375   0.13328527  0.25538202 10.46535776], Loss = 0.9914\n",
      "Iteration 1367: Weights = [55.30327362  3.38366811  8.1032139   0.13327665  0.25536552 10.46592181], Loss = 0.9912\n",
      "Iteration 1368: Weights = [55.30327422  3.38344949  8.10269033  0.13326804  0.25534902 10.46648582], Loss = 0.9911\n",
      "Iteration 1369: Weights = [55.30327481  3.38323088  8.1021668   0.13325943  0.25533252 10.4670498 ], Loss = 0.9910\n",
      "Iteration 1370: Weights = [55.30327539  3.38301229  8.1016433   0.13325082  0.25531602 10.46761374], Loss = 0.9909\n",
      "Iteration 1371: Weights = [55.30327597  3.38279371  8.10111983  0.13324221  0.25529952 10.46817765], Loss = 0.9907\n",
      "Iteration 1372: Weights = [55.30327655  3.38257514  8.1005964   0.1332336   0.25528302 10.46874151], Loss = 0.9906\n",
      "Iteration 1373: Weights = [55.30327712  3.38235659  8.10007301  0.13322499  0.25526652 10.46930535], Loss = 0.9905\n",
      "Iteration 1374: Weights = [55.30327768  3.38213805  8.09954964  0.13321638  0.25525002 10.46986914], Loss = 0.9903\n",
      "Iteration 1375: Weights = [55.30327823  3.38191953  8.09902632  0.13320777  0.25523353 10.4704329 ], Loss = 0.9902\n",
      "Iteration 1376: Weights = [55.30327879  3.38170102  8.09850302  0.13319916  0.25521704 10.47099662], Loss = 0.9901\n",
      "Iteration 1377: Weights = [55.30327933  3.38148252  8.09797976  0.13319055  0.25520054 10.47156031], Loss = 0.9900\n",
      "Iteration 1378: Weights = [55.30327987  3.38126404  8.09745653  0.13318195  0.25518405 10.47212396], Loss = 0.9898\n",
      "Iteration 1379: Weights = [55.30328041  3.38104557  8.09693334  0.13317334  0.25516756 10.47268757], Loss = 0.9897\n",
      "Iteration 1380: Weights = [55.30328093  3.38082712  8.09641018  0.13316473  0.25515107 10.47325115], Loss = 0.9896\n",
      "Iteration 1381: Weights = [55.30328146  3.38060868  8.09588705  0.13315613  0.25513458 10.47381469], Loss = 0.9894\n",
      "Iteration 1382: Weights = [55.30328198  3.38039025  8.09536396  0.13314752  0.25511809 10.47437819], Loss = 0.9893\n",
      "Iteration 1383: Weights = [55.30328249  3.38017184  8.0948409   0.13313892  0.25510161 10.47494166], Loss = 0.9892\n",
      "Iteration 1384: Weights = [55.303283    3.37995344  8.09431788  0.13313032  0.25508512 10.47550509], Loss = 0.9891\n",
      "Iteration 1385: Weights = [55.3032835   3.37973506  8.09379488  0.13312171  0.25506864 10.47606848], Loss = 0.9889\n",
      "Iteration 1386: Weights = [55.303284    3.37951669  8.09327193  0.13311311  0.25505216 10.47663184], Loss = 0.9888\n",
      "Iteration 1387: Weights = [55.30328449  3.37929833  8.092749    0.13310451  0.25503567 10.47719516], Loss = 0.9887\n",
      "Iteration 1388: Weights = [55.30328498  3.37907999  8.09222611  0.13309591  0.25501919 10.47775845], Loss = 0.9886\n",
      "Iteration 1389: Weights = [55.30328547  3.37886167  8.09170326  0.13308731  0.25500271 10.4783217 ], Loss = 0.9884\n",
      "Iteration 1390: Weights = [55.30328594  3.37864335  8.09118044  0.13307871  0.25498623 10.47888491], Loss = 0.9883\n",
      "Iteration 1391: Weights = [55.30328642  3.37842506  8.09065765  0.13307011  0.25496976 10.47944809], Loss = 0.9882\n",
      "Iteration 1392: Weights = [55.30328689  3.37820677  8.09013489  0.13306151  0.25495328 10.48001123], Loss = 0.9880\n",
      "Iteration 1393: Weights = [55.30328735  3.3779885   8.08961217  0.13305291  0.25493681 10.48057433], Loss = 0.9879\n",
      "Iteration 1394: Weights = [55.30328781  3.37777024  8.08908949  0.13304431  0.25492033 10.4811374 ], Loss = 0.9878\n",
      "Iteration 1395: Weights = [55.30328827  3.377552    8.08856683  0.13303572  0.25490386 10.48170043], Loss = 0.9877\n",
      "Iteration 1396: Weights = [55.30328872  3.37733377  8.08804421  0.13302712  0.25488739 10.48226342], Loss = 0.9875\n",
      "Iteration 1397: Weights = [55.30328916  3.37711556  8.08752163  0.13301852  0.25487091 10.48282638], Loss = 0.9874\n",
      "Iteration 1398: Weights = [55.30328961  3.37689736  8.08699908  0.13300993  0.25485444 10.4833893 ], Loss = 0.9873\n",
      "Iteration 1399: Weights = [55.30329004  3.37667917  8.08647656  0.13300133  0.25483798 10.48395219], Loss = 0.9871\n",
      "Iteration 1400: Weights = [55.30329048  3.376461    8.08595408  0.13299274  0.25482151 10.48451504], Loss = 0.9870\n",
      "Iteration 1401: Weights = [55.3032909   3.37624284  8.08543163  0.13298414  0.25480504 10.48507785], Loss = 0.9869\n",
      "Iteration 1402: Weights = [55.30329133  3.3760247   8.08490921  0.13297555  0.25478858 10.48564062], Loss = 0.9868\n",
      "Iteration 1403: Weights = [55.30329175  3.37580657  8.08438683  0.13296696  0.25477211 10.48620336], Loss = 0.9866\n",
      "Iteration 1404: Weights = [55.30329216  3.37558845  8.08386448  0.13295837  0.25475565 10.48676607], Loss = 0.9865\n",
      "Iteration 1405: Weights = [55.30329258  3.37537035  8.08334216  0.13294977  0.25473919 10.48732873], Loss = 0.9864\n",
      "Iteration 1406: Weights = [55.30329298  3.37515226  8.08281988  0.13294118  0.25472272 10.48789137], Loss = 0.9863\n",
      "Iteration 1407: Weights = [55.30329339  3.37493419  8.08229763  0.13293259  0.25470626 10.48845396], Loss = 0.9861\n",
      "Iteration 1408: Weights = [55.30329379  3.37471613  8.08177542  0.132924    0.2546898  10.48901652], Loss = 0.9860\n",
      "Iteration 1409: Weights = [55.30329418  3.37449809  8.08125324  0.13291541  0.25467335 10.48957904], Loss = 0.9859\n",
      "Iteration 1410: Weights = [55.30329457  3.37428005  8.08073109  0.13290682  0.25465689 10.49014152], Loss = 0.9857\n",
      "Iteration 1411: Weights = [55.30329496  3.37406204  8.08020898  0.13289824  0.25464043 10.49070397], Loss = 0.9856\n",
      "Iteration 1412: Weights = [55.30329535  3.37384403  8.0796869   0.13288965  0.25462398 10.49126639], Loss = 0.9855\n",
      "Iteration 1413: Weights = [55.30329573  3.37362604  8.07916485  0.13288106  0.25460753 10.49182876], Loss = 0.9854\n",
      "Iteration 1414: Weights = [55.3032961   3.37340807  8.07864284  0.13287247  0.25459107 10.4923911 ], Loss = 0.9852\n",
      "Iteration 1415: Weights = [55.30329647  3.37319011  8.07812086  0.13286389  0.25457462 10.49295341], Loss = 0.9851\n",
      "Iteration 1416: Weights = [55.30329684  3.37297216  8.07759892  0.1328553   0.25455817 10.49351567], Loss = 0.9850\n",
      "Iteration 1417: Weights = [55.30329721  3.37275423  8.07707701  0.13284672  0.25454172 10.4940779 ], Loss = 0.9849\n",
      "Iteration 1418: Weights = [55.30329757  3.37253631  8.07655513  0.13283813  0.25452527 10.4946401 ], Loss = 0.9847\n",
      "Iteration 1419: Weights = [55.30329793  3.37231841  8.07603329  0.13282955  0.25450883 10.49520226], Loss = 0.9846\n",
      "Iteration 1420: Weights = [55.30329828  3.37210052  8.07551148  0.13282097  0.25449238 10.49576438], Loss = 0.9845\n",
      "Iteration 1421: Weights = [55.30329863  3.37188264  8.0749897   0.13281238  0.25447593 10.49632646], Loss = 0.9843\n",
      "Iteration 1422: Weights = [55.30329898  3.37166478  8.07446796  0.1328038   0.25445949 10.49688851], Loss = 0.9842\n",
      "Iteration 1423: Weights = [55.30329932  3.37144693  8.07394625  0.13279522  0.25444305 10.49745053], Loss = 0.9841\n",
      "Iteration 1424: Weights = [55.30329966  3.37122909  8.07342458  0.13278664  0.25442661 10.4980125 ], Loss = 0.9840\n",
      "Iteration 1425: Weights = [55.3033      3.37101127  8.07290294  0.13277806  0.25441017 10.49857444], Loss = 0.9838\n",
      "Iteration 1426: Weights = [55.30330033  3.37079347  8.07238133  0.13276948  0.25439373 10.49913635], Loss = 0.9837\n",
      "Iteration 1427: Weights = [55.30330066  3.37057567  8.07185976  0.1327609   0.25437729 10.49969821], Loss = 0.9836\n",
      "Iteration 1428: Weights = [55.30330099  3.3703579   8.07133822  0.13275232  0.25436085 10.50026005], Loss = 0.9835\n",
      "Iteration 1429: Weights = [55.30330131  3.37014013  8.07081671  0.13274374  0.25434441 10.50082184], Loss = 0.9833\n",
      "Iteration 1430: Weights = [55.30330163  3.36992238  8.07029524  0.13273517  0.25432798 10.5013836 ], Loss = 0.9832\n",
      "Iteration 1431: Weights = [55.30330195  3.36970465  8.0697738   0.13272659  0.25431154 10.50194532], Loss = 0.9831\n",
      "Iteration 1432: Weights = [55.30330226  3.36948692  8.0692524   0.13271801  0.25429511 10.50250701], Loss = 0.9829\n",
      "Iteration 1433: Weights = [55.30330257  3.36926922  8.06873103  0.13270944  0.25427868 10.50306866], Loss = 0.9828\n",
      "Iteration 1434: Weights = [55.30330288  3.36905152  8.06820969  0.13270086  0.25426225 10.50363027], Loss = 0.9827\n",
      "Iteration 1435: Weights = [55.30330319  3.36883384  8.06768839  0.13269229  0.25424582 10.50419185], Loss = 0.9826\n",
      "Iteration 1436: Weights = [55.30330349  3.36861618  8.06716712  0.13268371  0.25422939 10.50475339], Loss = 0.9824\n",
      "Iteration 1437: Weights = [55.30330379  3.36839853  8.06664588  0.13267514  0.25421296 10.50531489], Loss = 0.9823\n",
      "Iteration 1438: Weights = [55.30330408  3.36818089  8.06612468  0.13266657  0.25419653 10.50587636], Loss = 0.9822\n",
      "Iteration 1439: Weights = [55.30330437  3.36796326  8.06560351  0.13265799  0.25418011 10.5064378 ], Loss = 0.9821\n",
      "Iteration 1440: Weights = [55.30330466  3.36774565  8.06508237  0.13264942  0.25416368 10.50699919], Loss = 0.9819\n",
      "Iteration 1441: Weights = [55.30330495  3.36752806  8.06456127  0.13264085  0.25414726 10.50756055], Loss = 0.9818\n",
      "Iteration 1442: Weights = [55.30330523  3.36731048  8.0640402   0.13263228  0.25413084 10.50812187], Loss = 0.9817\n",
      "Iteration 1443: Weights = [55.30330551  3.36709291  8.06351917  0.13262371  0.25411442 10.50868316], Loss = 0.9815\n",
      "Iteration 1444: Weights = [55.30330579  3.36687536  8.06299817  0.13261514  0.254098   10.50924441], Loss = 0.9814\n",
      "Iteration 1445: Weights = [55.30330607  3.36665782  8.0624772   0.13260657  0.25408158 10.50980563], Loss = 0.9813\n",
      "Iteration 1446: Weights = [55.30330634  3.36644029  8.06195627  0.132598    0.25406516 10.51036681], Loss = 0.9812\n",
      "Iteration 1447: Weights = [55.30330661  3.36622278  8.06143537  0.13258943  0.25404874 10.51092795], Loss = 0.9810\n",
      "Iteration 1448: Weights = [55.30330688  3.36600528  8.0609145   0.13258087  0.25403233 10.51148905], Loss = 0.9809\n",
      "Iteration 1449: Weights = [55.30330714  3.3657878   8.06039367  0.1325723   0.25401591 10.51205012], Loss = 0.9808\n",
      "Iteration 1450: Weights = [55.3033074   3.36557033  8.05987287  0.13256373  0.2539995  10.51261116], Loss = 0.9807\n",
      "Iteration 1451: Weights = [55.30330766  3.36535288  8.05935211  0.13255517  0.25398308 10.51317215], Loss = 0.9805\n",
      "Iteration 1452: Weights = [55.30330792  3.36513544  8.05883137  0.1325466   0.25396667 10.51373311], Loss = 0.9804\n",
      "Iteration 1453: Weights = [55.30330817  3.36491801  8.05831068  0.13253804  0.25395026 10.51429404], Loss = 0.9803\n",
      "Iteration 1454: Weights = [55.30330843  3.3647006   8.05779001  0.13252947  0.25393385 10.51485493], Loss = 0.9802\n",
      "Iteration 1455: Weights = [55.30330868  3.3644832   8.05726938  0.13252091  0.25391744 10.51541578], Loss = 0.9800\n",
      "Iteration 1456: Weights = [55.30330892  3.36426581  8.05674879  0.13251235  0.25390104 10.51597659], Loss = 0.9799\n",
      "Iteration 1457: Weights = [55.30330917  3.36404844  8.05622822  0.13250378  0.25388463 10.51653737], Loss = 0.9798\n",
      "Iteration 1458: Weights = [55.30330941  3.36383108  8.05570769  0.13249522  0.25386823 10.51709812], Loss = 0.9796\n",
      "Iteration 1459: Weights = [55.30330965  3.36361374  8.0551872   0.13248666  0.25385182 10.51765882], Loss = 0.9795\n",
      "Iteration 1460: Weights = [55.30330988  3.36339641  8.05466673  0.1324781   0.25383542 10.5182195 ], Loss = 0.9794\n",
      "Iteration 1461: Weights = [55.30331012  3.3631791   8.05414631  0.13246954  0.25381902 10.51878013], Loss = 0.9793\n",
      "Iteration 1462: Weights = [55.30331035  3.3629618   8.05362591  0.13246098  0.25380262 10.51934073], Loss = 0.9791\n",
      "Iteration 1463: Weights = [55.30331058  3.36274451  8.05310555  0.13245242  0.25378622 10.51990129], Loss = 0.9790\n",
      "Iteration 1464: Weights = [55.30331081  3.36252724  8.05258522  0.13244386  0.25376982 10.52046182], Loss = 0.9789\n",
      "Iteration 1465: Weights = [55.30331103  3.36230998  8.05206493  0.1324353   0.25375342 10.52102231], Loss = 0.9788\n",
      "Iteration 1466: Weights = [55.30331126  3.36209274  8.05154467  0.13242675  0.25373702 10.52158276], Loss = 0.9786\n",
      "Iteration 1467: Weights = [55.30331148  3.3618755   8.05102444  0.13241819  0.25372063 10.52214318], Loss = 0.9785\n",
      "Iteration 1468: Weights = [55.3033117   3.36165829  8.05050425  0.13240963  0.25370423 10.52270356], Loss = 0.9784\n",
      "Iteration 1469: Weights = [55.30331191  3.36144109  8.04998409  0.13240108  0.25368784 10.5232639 ], Loss = 0.9783\n",
      "Iteration 1470: Weights = [55.30331213  3.3612239   8.04946396  0.13239252  0.25367145 10.52382421], Loss = 0.9781\n",
      "Iteration 1471: Weights = [55.30331234  3.36100672  8.04894387  0.13238397  0.25365506 10.52438448], Loss = 0.9780\n",
      "Iteration 1472: Weights = [55.30331255  3.36078956  8.04842381  0.13237541  0.25363866 10.52494472], Loss = 0.9779\n",
      "Iteration 1473: Weights = [55.30331276  3.36057242  8.04790379  0.13236686  0.25362228 10.52550492], Loss = 0.9778\n",
      "Iteration 1474: Weights = [55.30331296  3.36035528  8.04738379  0.13235831  0.25360589 10.52606509], Loss = 0.9776\n",
      "Iteration 1475: Weights = [55.30331317  3.36013817  8.04686384  0.13234975  0.2535895  10.52662521], Loss = 0.9775\n",
      "Iteration 1476: Weights = [55.30331337  3.35992106  8.04634391  0.1323412   0.25357311 10.5271853 ], Loss = 0.9774\n",
      "Iteration 1477: Weights = [55.30331357  3.35970397  8.04582402  0.13233265  0.25355673 10.52774536], Loss = 0.9772\n",
      "Iteration 1478: Weights = [55.30331376  3.35948689  8.04530416  0.1323241   0.25354035 10.52830538], Loss = 0.9771\n",
      "Iteration 1479: Weights = [55.30331396  3.35926983  8.04478434  0.13231555  0.25352396 10.52886536], Loss = 0.9770\n",
      "Iteration 1480: Weights = [55.30331415  3.35905278  8.04426455  0.132307    0.25350758 10.52942531], Loss = 0.9769\n",
      "Iteration 1481: Weights = [55.30331435  3.35883575  8.04374479  0.13229845  0.2534912  10.52998522], Loss = 0.9767\n",
      "Iteration 1482: Weights = [55.30331454  3.35861873  8.04322507  0.1322899   0.25347482 10.53054509], Loss = 0.9766\n",
      "Iteration 1483: Weights = [55.30331472  3.35840172  8.04270538  0.13228136  0.25345844 10.53110493], Loss = 0.9765\n",
      "Iteration 1484: Weights = [55.30331491  3.35818473  8.04218573  0.13227281  0.25344207 10.53166473], Loss = 0.9764\n",
      "Iteration 1485: Weights = [55.30331509  3.35796775  8.0416661   0.13226426  0.25342569 10.5322245 ], Loss = 0.9762\n",
      "Iteration 1486: Weights = [55.30331528  3.35775079  8.04114652  0.13225572  0.25340931 10.53278423], Loss = 0.9761\n",
      "Iteration 1487: Weights = [55.30331546  3.35753384  8.04062696  0.13224717  0.25339294 10.53334392], Loss = 0.9760\n",
      "Iteration 1488: Weights = [55.30331564  3.3573169   8.04010744  0.13223862  0.25337657 10.53390358], Loss = 0.9759\n",
      "Iteration 1489: Weights = [55.30331581  3.35709998  8.03958795  0.13223008  0.25336019 10.5344632 ], Loss = 0.9757\n",
      "Iteration 1490: Weights = [55.30331599  3.35688307  8.0390685   0.13222154  0.25334382 10.53502279], Loss = 0.9756\n",
      "Iteration 1491: Weights = [55.30331616  3.35666618  8.03854908  0.13221299  0.25332745 10.53558234], Loss = 0.9755\n",
      "Iteration 1492: Weights = [55.30331633  3.3564493   8.03802969  0.13220445  0.25331108 10.53614185], Loss = 0.9754\n",
      "Iteration 1493: Weights = [55.3033165   3.35623243  8.03751034  0.13219591  0.25329472 10.53670133], Loss = 0.9752\n",
      "Iteration 1494: Weights = [55.30331667  3.35601558  8.03699102  0.13218737  0.25327835 10.53726077], Loss = 0.9751\n",
      "Iteration 1495: Weights = [55.30331684  3.35579874  8.03647173  0.13217882  0.25326198 10.53782017], Loss = 0.9750\n",
      "Iteration 1496: Weights = [55.303317    3.35558192  8.03595248  0.13217028  0.25324562 10.53837954], Loss = 0.9748\n",
      "Iteration 1497: Weights = [55.30331717  3.35536511  8.03543326  0.13216174  0.25322926 10.53893887], Loss = 0.9747\n",
      "Iteration 1498: Weights = [55.30331733  3.35514831  8.03491407  0.1321532   0.25321289 10.53949817], Loss = 0.9746\n",
      "Iteration 1499: Weights = [55.30331749  3.35493153  8.03439492  0.13214466  0.25319653 10.54005743], Loss = 0.9745\n",
      "Iteration 1500: Weights = [55.30331765  3.35471476  8.0338758   0.13213613  0.25318017 10.54061665], Loss = 0.9743\n",
      "Iteration 1501: Weights = [55.3033178   3.35449801  8.03335672  0.13212759  0.25316381 10.54117584], Loss = 0.9742\n",
      "Iteration 1502: Weights = [55.30331796  3.35428127  8.03283766  0.13211905  0.25314745 10.54173499], Loss = 0.9741\n",
      "Iteration 1503: Weights = [55.30331811  3.35406454  8.03231865  0.13211051  0.2531311  10.5422941 ], Loss = 0.9740\n",
      "Iteration 1504: Weights = [55.30331826  3.35384783  8.03179966  0.13210198  0.25311474 10.54285318], Loss = 0.9738\n",
      "Iteration 1505: Weights = [55.30331842  3.35363113  8.03128071  0.13209344  0.25309839 10.54341223], Loss = 0.9737\n",
      "Iteration 1506: Weights = [55.30331856  3.35341445  8.03076179  0.13208491  0.25308203 10.54397123], Loss = 0.9736\n",
      "Iteration 1507: Weights = [55.30331871  3.35319778  8.03024291  0.13207637  0.25306568 10.5445302 ], Loss = 0.9735\n",
      "Iteration 1508: Weights = [55.30331886  3.35298112  8.02972406  0.13206784  0.25304933 10.54508914], Loss = 0.9733\n",
      "Iteration 1509: Weights = [55.303319    3.35276448  8.02920524  0.1320593   0.25303298 10.54564804], Loss = 0.9732\n",
      "Iteration 1510: Weights = [55.30331915  3.35254785  8.02868646  0.13205077  0.25301663 10.5462069 ], Loss = 0.9731\n",
      "Iteration 1511: Weights = [55.30331929  3.35233123  8.02816771  0.13204224  0.25300028 10.54676573], Loss = 0.9730\n",
      "Iteration 1512: Weights = [55.30331943  3.35211463  8.02764899  0.13203371  0.25298393 10.54732452], Loss = 0.9728\n",
      "Iteration 1513: Weights = [55.30331957  3.35189805  8.02713031  0.13202518  0.25296758 10.54788327], Loss = 0.9727\n",
      "Iteration 1514: Weights = [55.30331971  3.35168148  8.02661166  0.13201665  0.25295124 10.54844199], Loss = 0.9726\n",
      "Iteration 1515: Weights = [55.30331984  3.35146492  8.02609305  0.13200812  0.25293489 10.54900067], Loss = 0.9725\n",
      "Iteration 1516: Weights = [55.30331998  3.35124837  8.02557446  0.13199959  0.25291855 10.54955932], Loss = 0.9723\n",
      "Iteration 1517: Weights = [55.30332011  3.35103184  8.02505591  0.13199106  0.25290221 10.55011793], Loss = 0.9722\n",
      "Iteration 1518: Weights = [55.30332024  3.35081533  8.0245374   0.13198253  0.25288587 10.5506765 ], Loss = 0.9721\n",
      "Iteration 1519: Weights = [55.30332037  3.35059882  8.02401892  0.131974    0.25286953 10.55123504], Loss = 0.9720\n",
      "Iteration 1520: Weights = [55.3033205   3.35038234  8.02350047  0.13196547  0.25285319 10.55179354], Loss = 0.9718\n",
      "Iteration 1521: Weights = [55.30332063  3.35016586  8.02298205  0.13195695  0.25283685 10.552352  ], Loss = 0.9717\n",
      "Iteration 1522: Weights = [55.30332076  3.3499494   8.02246367  0.13194842  0.25282051 10.55291043], Loss = 0.9716\n",
      "Iteration 1523: Weights = [55.30332088  3.34973296  8.02194533  0.13193989  0.25280418 10.55346883], Loss = 0.9715\n",
      "Iteration 1524: Weights = [55.30332101  3.34951652  8.02142701  0.13193137  0.25278784 10.55402718], Loss = 0.9713\n",
      "Iteration 1525: Weights = [55.30332113  3.3493001   8.02090873  0.13192284  0.25277151 10.55458551], Loss = 0.9712\n",
      "Iteration 1526: Weights = [55.30332125  3.3490837   8.02039048  0.13191432  0.25275518 10.55514379], Loss = 0.9711\n",
      "Iteration 1527: Weights = [55.30332137  3.34886731  8.01987227  0.1319058   0.25273884 10.55570204], Loss = 0.9710\n",
      "Iteration 1528: Weights = [55.30332149  3.34865093  8.01935409  0.13189727  0.25272251 10.55626025], Loss = 0.9708\n",
      "Iteration 1529: Weights = [55.30332161  3.34843457  8.01883594  0.13188875  0.25270618 10.55681843], Loss = 0.9707\n",
      "Iteration 1530: Weights = [55.30332173  3.34821822  8.01831783  0.13188023  0.25268986 10.55737657], Loss = 0.9706\n",
      "Iteration 1531: Weights = [55.30332185  3.34800189  8.01779975  0.13187171  0.25267353 10.55793467], Loss = 0.9705\n",
      "Iteration 1532: Weights = [55.30332196  3.34778557  8.0172817   0.13186319  0.2526572  10.55849274], Loss = 0.9703\n",
      "Iteration 1533: Weights = [55.30332207  3.34756926  8.01676369  0.13185467  0.25264088 10.55905078], Loss = 0.9702\n",
      "Iteration 1534: Weights = [55.30332219  3.34735297  8.01624571  0.13184615  0.25262455 10.55960877], Loss = 0.9701\n",
      "Iteration 1535: Weights = [55.3033223   3.34713669  8.01572777  0.13183763  0.25260823 10.56016673], Loss = 0.9699\n",
      "Iteration 1536: Weights = [55.30332241  3.34692042  8.01520985  0.13182911  0.25259191 10.56072466], Loss = 0.9698\n",
      "Iteration 1537: Weights = [55.30332252  3.34670417  8.01469197  0.13182059  0.25257559 10.56128254], Loss = 0.9697\n",
      "Iteration 1538: Weights = [55.30332263  3.34648794  8.01417413  0.13181207  0.25255927 10.5618404 ], Loss = 0.9696\n",
      "Iteration 1539: Weights = [55.30332273  3.34627171  8.01365632  0.13180356  0.25254295 10.56239821], Loss = 0.9694\n",
      "Iteration 1540: Weights = [55.30332284  3.34605551  8.01313854  0.13179504  0.25252663 10.56295599], Loss = 0.9693\n",
      "Iteration 1541: Weights = [55.30332294  3.34583931  8.01262079  0.13178653  0.25251031 10.56351374], Loss = 0.9692\n",
      "Iteration 1542: Weights = [55.30332305  3.34562313  8.01210308  0.13177801  0.252494   10.56407145], Loss = 0.9691\n",
      "Iteration 1543: Weights = [55.30332315  3.34540696  8.0115854   0.1317695   0.25247768 10.56462912], Loss = 0.9689\n",
      "Iteration 1544: Weights = [55.30332325  3.34519081  8.01106776  0.13176098  0.25246137 10.56518675], Loss = 0.9688\n",
      "Iteration 1545: Weights = [55.30332335  3.34497467  8.01055015  0.13175247  0.25244506 10.56574435], Loss = 0.9687\n",
      "Iteration 1546: Weights = [55.30332345  3.34475855  8.01003257  0.13174395  0.25242875 10.56630192], Loss = 0.9686\n",
      "Iteration 1547: Weights = [55.30332355  3.34454243  8.00951502  0.13173544  0.25241243 10.56685945], Loss = 0.9684\n",
      "Iteration 1548: Weights = [55.30332365  3.34432634  8.00899751  0.13172693  0.25239613 10.56741694], Loss = 0.9683\n",
      "Iteration 1549: Weights = [55.30332375  3.34411025  8.00848004  0.13171842  0.25237982 10.56797439], Loss = 0.9682\n",
      "Iteration 1550: Weights = [55.30332384  3.34389418  8.00796259  0.13170991  0.25236351 10.56853181], Loss = 0.9681\n",
      "Iteration 1551: Weights = [55.30332394  3.34367813  8.00744518  0.1317014   0.2523472  10.5690892 ], Loss = 0.9679\n",
      "Iteration 1552: Weights = [55.30332403  3.34346209  8.0069278   0.13169289  0.2523309  10.56964655], Loss = 0.9678\n",
      "Iteration 1553: Weights = [55.30332412  3.34324606  8.00641046  0.13168438  0.25231459 10.57020386], Loss = 0.9677\n",
      "Iteration 1554: Weights = [55.30332422  3.34303005  8.00589315  0.13167587  0.25229829 10.57076113], Loss = 0.9676\n",
      "Iteration 1555: Weights = [55.30332431  3.34281405  8.00537587  0.13166736  0.25228199 10.57131837], Loss = 0.9674\n",
      "Iteration 1556: Weights = [55.3033244   3.34259806  8.00485863  0.13165885  0.25226569 10.57187558], Loss = 0.9673\n",
      "Iteration 1557: Weights = [55.30332449  3.34238209  8.00434142  0.13165035  0.25224939 10.57243274], Loss = 0.9672\n",
      "Iteration 1558: Weights = [55.30332458  3.34216613  8.00382424  0.13164184  0.25223309 10.57298988], Loss = 0.9671\n",
      "Iteration 1559: Weights = [55.30332466  3.34195019  8.0033071   0.13163334  0.25221679 10.57354697], Loss = 0.9669\n",
      "Iteration 1560: Weights = [55.30332475  3.34173426  8.00278999  0.13162483  0.2522005  10.57410403], Loss = 0.9668\n",
      "Iteration 1561: Weights = [55.30332484  3.34151834  8.00227291  0.13161633  0.2521842  10.57466106], Loss = 0.9667\n",
      "Iteration 1562: Weights = [55.30332492  3.34130244  8.00175587  0.13160782  0.25216791 10.57521804], Loss = 0.9666\n",
      "Iteration 1563: Weights = [55.303325    3.34108655  8.00123886  0.13159932  0.25215161 10.575775  ], Loss = 0.9664\n",
      "Iteration 1564: Weights = [55.30332509  3.34087068  8.00072188  0.13159081  0.25213532 10.57633191], Loss = 0.9663\n",
      "Iteration 1565: Weights = [55.30332517  3.34065482  8.00020494  0.13158231  0.25211903 10.57688879], Loss = 0.9662\n",
      "Iteration 1566: Weights = [55.30332525  3.34043897  7.99968803  0.13157381  0.25210274 10.57744563], Loss = 0.9661\n",
      "Iteration 1567: Weights = [55.30332533  3.34022314  7.99917115  0.13156531  0.25208645 10.57800244], Loss = 0.9659\n",
      "Iteration 1568: Weights = [55.30332541  3.34000732  7.99865431  0.13155681  0.25207016 10.57855921], Loss = 0.9658\n",
      "Iteration 1569: Weights = [55.30332549  3.33979152  7.9981375   0.13154831  0.25205387 10.57911595], Loss = 0.9657\n",
      "Iteration 1570: Weights = [55.30332557  3.33957573  7.99762073  0.13153981  0.25203759 10.57967265], Loss = 0.9656\n",
      "Iteration 1571: Weights = [55.30332565  3.33935995  7.99710398  0.13153131  0.2520213  10.58022931], Loss = 0.9654\n",
      "Iteration 1572: Weights = [55.30332573  3.33914419  7.99658728  0.13152281  0.25200502 10.58078594], Loss = 0.9653\n",
      "Iteration 1573: Weights = [55.3033258   3.33892844  7.9960706   0.13151431  0.25198873 10.58134253], Loss = 0.9652\n",
      "Iteration 1574: Weights = [55.30332588  3.33871271  7.99555396  0.13150581  0.25197245 10.58189909], Loss = 0.9651\n",
      "Iteration 1575: Weights = [55.30332595  3.33849699  7.99503735  0.13149732  0.25195617 10.58245561], Loss = 0.9649\n",
      "Iteration 1576: Weights = [55.30332603  3.33828128  7.99452077  0.13148882  0.25193989 10.5830121 ], Loss = 0.9648\n",
      "Iteration 1577: Weights = [55.3033261   3.33806559  7.99400423  0.13148032  0.25192361 10.58356854], Loss = 0.9647\n",
      "Iteration 1578: Weights = [55.30332617  3.33784991  7.99348772  0.13147183  0.25190734 10.58412496], Loss = 0.9646\n",
      "Iteration 1579: Weights = [55.30332624  3.33763424  7.99297125  0.13146333  0.25189106 10.58468133], Loss = 0.9644\n",
      "Iteration 1580: Weights = [55.30332631  3.33741859  7.9924548   0.13145484  0.25187478 10.58523767], Loss = 0.9643\n",
      "Iteration 1581: Weights = [55.30332638  3.33720296  7.9919384   0.13144635  0.25185851 10.58579398], Loss = 0.9642\n",
      "Iteration 1582: Weights = [55.30332645  3.33698733  7.99142202  0.13143785  0.25184224 10.58635025], Loss = 0.9641\n",
      "Iteration 1583: Weights = [55.30332652  3.33677172  7.99090568  0.13142936  0.25182596 10.58690648], Loss = 0.9640\n",
      "Iteration 1584: Weights = [55.30332659  3.33655613  7.99038937  0.13142087  0.25180969 10.58746268], Loss = 0.9638\n",
      "Iteration 1585: Weights = [55.30332666  3.33634055  7.98987309  0.13141238  0.25179342 10.58801884], Loss = 0.9637\n",
      "Iteration 1586: Weights = [55.30332672  3.33612498  7.98935685  0.13140389  0.25177715 10.58857496], Loss = 0.9636\n",
      "Iteration 1587: Weights = [55.30332679  3.33590943  7.98884064  0.1313954   0.25176088 10.58913105], Loss = 0.9635\n",
      "Iteration 1588: Weights = [55.30332686  3.33569389  7.98832447  0.13138691  0.25174462 10.5896871 ], Loss = 0.9633\n",
      "Iteration 1589: Weights = [55.30332692  3.33547836  7.98780833  0.13137842  0.25172835 10.59024312], Loss = 0.9632\n",
      "Iteration 1590: Weights = [55.30332698  3.33526285  7.98729222  0.13136993  0.25171209 10.5907991 ], Loss = 0.9631\n",
      "Iteration 1591: Weights = [55.30332705  3.33504735  7.98677614  0.13136144  0.25169582 10.59135505], Loss = 0.9630\n",
      "Iteration 1592: Weights = [55.30332711  3.33483187  7.9862601   0.13135295  0.25167956 10.59191095], Loss = 0.9628\n",
      "Iteration 1593: Weights = [55.30332717  3.3346164   7.98574409  0.13134447  0.2516633  10.59246683], Loss = 0.9627\n",
      "Iteration 1594: Weights = [55.30332723  3.33440094  7.98522812  0.13133598  0.25164704 10.59302267], Loss = 0.9626\n",
      "Iteration 1595: Weights = [55.3033273   3.3341855   7.98471218  0.13132749  0.25163078 10.59357847], Loss = 0.9625\n",
      "Iteration 1596: Weights = [55.30332736  3.33397007  7.98419627  0.13131901  0.25161452 10.59413423], Loss = 0.9623\n",
      "Iteration 1597: Weights = [55.30332742  3.33375466  7.98368039  0.13131052  0.25159826 10.59468996], Loss = 0.9622\n",
      "Iteration 1598: Weights = [55.30332747  3.33353926  7.98316455  0.13130204  0.251582   10.59524566], Loss = 0.9621\n",
      "Iteration 1599: Weights = [55.30332753  3.33332387  7.98264874  0.13129355  0.25156575 10.59580131], Loss = 0.9620\n",
      "Iteration 1600: Weights = [55.30332759  3.3331085   7.98213297  0.13128507  0.25154949 10.59635694], Loss = 0.9618\n",
      "Iteration 1601: Weights = [55.30332765  3.33289314  7.98161723  0.13127659  0.25153324 10.59691252], Loss = 0.9617\n",
      "Iteration 1602: Weights = [55.30332771  3.3326778   7.98110152  0.13126811  0.25151699 10.59746807], Loss = 0.9616\n",
      "Iteration 1603: Weights = [55.30332776  3.33246247  7.98058584  0.13125962  0.25150074 10.59802359], Loss = 0.9615\n",
      "Iteration 1604: Weights = [55.30332782  3.33224715  7.9800702   0.13125114  0.25148449 10.59857906], Loss = 0.9613\n",
      "Iteration 1605: Weights = [55.30332787  3.33203185  7.97955459  0.13124266  0.25146824 10.59913451], Loss = 0.9612\n",
      "Iteration 1606: Weights = [55.30332793  3.33181656  7.97903902  0.13123418  0.25145199 10.59968991], Loss = 0.9611\n",
      "Iteration 1607: Weights = [55.30332798  3.33160128  7.97852348  0.1312257   0.25143574 10.60024528], Loss = 0.9610\n",
      "Iteration 1608: Weights = [55.30332803  3.33138602  7.97800797  0.13121722  0.2514195  10.60080062], Loss = 0.9608\n",
      "Iteration 1609: Weights = [55.30332809  3.33117077  7.97749249  0.13120875  0.25140325 10.60135592], Loss = 0.9607\n",
      "Iteration 1610: Weights = [55.30332814  3.33095554  7.97697705  0.13120027  0.25138701 10.60191118], Loss = 0.9606\n",
      "Iteration 1611: Weights = [55.30332819  3.33074032  7.97646164  0.13119179  0.25137076 10.60246641], Loss = 0.9605\n",
      "Iteration 1612: Weights = [55.30332824  3.33052511  7.97594627  0.13118331  0.25135452 10.6030216 ], Loss = 0.9603\n",
      "Iteration 1613: Weights = [55.30332829  3.33030992  7.97543092  0.13117484  0.25133828 10.60357675], Loss = 0.9602\n",
      "Iteration 1614: Weights = [55.30332835  3.33009474  7.97491562  0.13116636  0.25132204 10.60413187], Loss = 0.9601\n",
      "Iteration 1615: Weights = [55.30332839  3.32987958  7.97440034  0.13115789  0.2513058  10.60468696], Loss = 0.9600\n",
      "Iteration 1616: Weights = [55.30332844  3.32966443  7.9738851   0.13114941  0.25128957 10.60524201], Loss = 0.9598\n",
      "Iteration 1617: Weights = [55.30332849  3.32944929  7.97336989  0.13114094  0.25127333 10.60579702], Loss = 0.9597\n",
      "Iteration 1618: Weights = [55.30332854  3.32923417  7.97285471  0.13113247  0.25125709 10.60635199], Loss = 0.9596\n",
      "Iteration 1619: Weights = [55.30332859  3.32901906  7.97233957  0.13112399  0.25124086 10.60690693], Loss = 0.9595\n",
      "Iteration 1620: Weights = [55.30332864  3.32880397  7.97182446  0.13111552  0.25122463 10.60746184], Loss = 0.9594\n",
      "Iteration 1621: Weights = [55.30332868  3.32858889  7.97130939  0.13110705  0.25120839 10.60801671], Loss = 0.9592\n",
      "Iteration 1622: Weights = [55.30332873  3.32837382  7.97079434  0.13109858  0.25119216 10.60857154], Loss = 0.9591\n",
      "Iteration 1623: Weights = [55.30332878  3.32815877  7.97027934  0.13109011  0.25117593 10.60912634], Loss = 0.9590\n",
      "Iteration 1624: Weights = [55.30332882  3.32794373  7.96976436  0.13108164  0.2511597  10.6096811 ], Loss = 0.9589\n",
      "Iteration 1625: Weights = [55.30332887  3.32772871  7.96924942  0.13107317  0.25114347 10.61023582], Loss = 0.9587\n",
      "Iteration 1626: Weights = [55.30332891  3.3275137   7.96873451  0.1310647   0.25112725 10.61079051], Loss = 0.9586\n",
      "Iteration 1627: Weights = [55.30332896  3.3272987   7.96821963  0.13105623  0.25111102 10.61134516], Loss = 0.9585\n",
      "Iteration 1628: Weights = [55.303329    3.32708371  7.96770479  0.13104776  0.2510948  10.61189978], Loss = 0.9584\n",
      "Iteration 1629: Weights = [55.30332904  3.32686875  7.96718998  0.13103929  0.25107857 10.61245436], Loss = 0.9582\n",
      "Iteration 1630: Weights = [55.30332909  3.32665379  7.9666752   0.13103083  0.25106235 10.61300891], Loss = 0.9581\n",
      "Iteration 1631: Weights = [55.30332913  3.32643885  7.96616046  0.13102236  0.25104613 10.61356342], Loss = 0.9580\n",
      "Iteration 1632: Weights = [55.30332917  3.32622392  7.96564575  0.1310139   0.25102991 10.61411789], Loss = 0.9579\n",
      "Iteration 1633: Weights = [55.30332921  3.32600901  7.96513107  0.13100543  0.25101369 10.61467233], Loss = 0.9577\n",
      "Iteration 1634: Weights = [55.30332925  3.32579411  7.96461643  0.13099697  0.25099747 10.61522674], Loss = 0.9576\n",
      "Iteration 1635: Weights = [55.30332929  3.32557922  7.96410182  0.1309885   0.25098125 10.6157811 ], Loss = 0.9575\n",
      "Iteration 1636: Weights = [55.30332933  3.32536435  7.96358724  0.13098004  0.25096503 10.61633543], Loss = 0.9574\n",
      "Iteration 1637: Weights = [55.30332937  3.32514949  7.9630727   0.13097158  0.25094882 10.61688973], Loss = 0.9572\n",
      "Iteration 1638: Weights = [55.30332941  3.32493465  7.96255819  0.13096311  0.2509326  10.61744399], Loss = 0.9571\n",
      "Iteration 1639: Weights = [55.30332945  3.32471982  7.96204371  0.13095465  0.25091639 10.61799821], Loss = 0.9570\n",
      "Iteration 1640: Weights = [55.30332949  3.324505    7.96152927  0.13094619  0.25090018 10.6185524 ], Loss = 0.9569\n",
      "Iteration 1641: Weights = [55.30332953  3.3242902   7.96101486  0.13093773  0.25088397 10.61910655], Loss = 0.9568\n",
      "Iteration 1642: Weights = [55.30332957  3.32407541  7.96050048  0.13092927  0.25086776 10.61966067], Loss = 0.9566\n",
      "Iteration 1643: Weights = [55.30332961  3.32386063  7.95998614  0.13092081  0.25085155 10.62021475], Loss = 0.9565\n",
      "Iteration 1644: Weights = [55.30332964  3.32364587  7.95947183  0.13091235  0.25083534 10.62076879], Loss = 0.9564\n",
      "Iteration 1645: Weights = [55.30332968  3.32343112  7.95895755  0.13090389  0.25081913 10.6213228 ], Loss = 0.9563\n",
      "Iteration 1646: Weights = [55.30332972  3.32321639  7.95844331  0.13089543  0.25080293 10.62187677], Loss = 0.9561\n",
      "Iteration 1647: Weights = [55.30332975  3.32300167  7.95792909  0.13088698  0.25078672 10.62243071], Loss = 0.9560\n",
      "Iteration 1648: Weights = [55.30332979  3.32278697  7.95741492  0.13087852  0.25077052 10.62298461], Loss = 0.9559\n",
      "Iteration 1649: Weights = [55.30332982  3.32257227  7.95690077  0.13087006  0.25075431 10.62353848], Loss = 0.9558\n",
      "Iteration 1650: Weights = [55.30332986  3.3223576   7.95638666  0.13086161  0.25073811 10.62409231], Loss = 0.9556\n",
      "Iteration 1651: Weights = [55.30332989  3.32214293  7.95587258  0.13085315  0.25072191 10.6246461 ], Loss = 0.9555\n",
      "Iteration 1652: Weights = [55.30332993  3.32192828  7.95535854  0.1308447   0.25070571 10.62519986], Loss = 0.9554\n",
      "Iteration 1653: Weights = [55.30332996  3.32171365  7.95484453  0.13083624  0.25068951 10.62575358], Loss = 0.9553\n",
      "Iteration 1654: Weights = [55.30333     3.32149902  7.95433055  0.13082779  0.25067331 10.62630727], Loss = 0.9551\n",
      "Iteration 1655: Weights = [55.30333003  3.32128441  7.9538166   0.13081934  0.25065712 10.62686092], Loss = 0.9550\n",
      "Iteration 1656: Weights = [55.30333006  3.32106982  7.95330269  0.13081088  0.25064092 10.62741453], Loss = 0.9549\n",
      "Iteration 1657: Weights = [55.3033301   3.32085524  7.95278881  0.13080243  0.25062473 10.62796811], Loss = 0.9548\n",
      "Iteration 1658: Weights = [55.30333013  3.32064067  7.95227497  0.13079398  0.25060853 10.62852166], Loss = 0.9547\n",
      "Iteration 1659: Weights = [55.30333016  3.32042612  7.95176115  0.13078553  0.25059234 10.62907517], Loss = 0.9545\n",
      "Iteration 1660: Weights = [55.30333019  3.32021158  7.95124737  0.13077708  0.25057615 10.62962864], Loss = 0.9544\n",
      "Iteration 1661: Weights = [55.30333022  3.31999705  7.95073363  0.13076863  0.25055996 10.63018207], Loss = 0.9543\n",
      "Iteration 1662: Weights = [55.30333025  3.31978254  7.95021991  0.13076018  0.25054377 10.63073547], Loss = 0.9542\n",
      "Iteration 1663: Weights = [55.30333028  3.31956804  7.94970623  0.13075173  0.25052758 10.63128884], Loss = 0.9540\n",
      "Iteration 1664: Weights = [55.30333032  3.31935356  7.94919259  0.13074328  0.25051139 10.63184217], Loss = 0.9539\n",
      "Iteration 1665: Weights = [55.30333035  3.31913909  7.94867897  0.13073483  0.25049521 10.63239546], Loss = 0.9538\n",
      "Iteration 1666: Weights = [55.30333038  3.31892463  7.94816539  0.13072639  0.25047902 10.63294872], Loss = 0.9537\n",
      "Iteration 1667: Weights = [55.3033304   3.31871019  7.94765185  0.13071794  0.25046284 10.63350194], Loss = 0.9535\n",
      "Iteration 1668: Weights = [55.30333043  3.31849576  7.94713833  0.1307095   0.25044666 10.63405513], Loss = 0.9534\n",
      "Iteration 1669: Weights = [55.30333046  3.31828135  7.94662485  0.13070105  0.25043047 10.63460828], Loss = 0.9533\n",
      "Iteration 1670: Weights = [55.30333049  3.31806695  7.94611141  0.1306926   0.25041429 10.63516139], Loss = 0.9532\n",
      "Iteration 1671: Weights = [55.30333052  3.31785256  7.94559799  0.13068416  0.25039811 10.63571447], Loss = 0.9531\n",
      "Iteration 1672: Weights = [55.30333055  3.31763819  7.94508461  0.13067572  0.25038193 10.63626751], Loss = 0.9529\n",
      "Iteration 1673: Weights = [55.30333058  3.31742383  7.94457126  0.13066727  0.25036576 10.63682052], Loss = 0.9528\n",
      "Iteration 1674: Weights = [55.3033306   3.31720948  7.94405795  0.13065883  0.25034958 10.63737349], Loss = 0.9527\n",
      "Iteration 1675: Weights = [55.30333063  3.31699515  7.94354467  0.13065039  0.2503334  10.63792643], Loss = 0.9526\n",
      "Iteration 1676: Weights = [55.30333066  3.31678084  7.94303142  0.13064195  0.25031723 10.63847933], Loss = 0.9524\n",
      "Iteration 1677: Weights = [55.30333068  3.31656653  7.9425182   0.13063351  0.25030106 10.63903219], Loss = 0.9523\n",
      "Iteration 1678: Weights = [55.30333071  3.31635224  7.94200502  0.13062507  0.25028488 10.63958502], Loss = 0.9522\n",
      "Iteration 1679: Weights = [55.30333074  3.31613797  7.94149187  0.13061663  0.25026871 10.64013781], Loss = 0.9521\n",
      "Iteration 1680: Weights = [55.30333076  3.3159237   7.94097876  0.13060819  0.25025254 10.64069057], Loss = 0.9519\n",
      "Iteration 1681: Weights = [55.30333079  3.31570946  7.94046567  0.13059975  0.25023637 10.64124329], Loss = 0.9518\n",
      "Iteration 1682: Weights = [55.30333081  3.31549522  7.93995262  0.13059131  0.2502202  10.64179598], Loss = 0.9517\n",
      "Iteration 1683: Weights = [55.30333084  3.315281    7.93943961  0.13058287  0.25020404 10.64234863], Loss = 0.9516\n",
      "Iteration 1684: Weights = [55.30333086  3.31506679  7.93892662  0.13057443  0.25018787 10.64290124], Loss = 0.9515\n",
      "Iteration 1685: Weights = [55.30333089  3.3148526   7.93841367  0.130566    0.2501717  10.64345382], Loss = 0.9513\n",
      "Iteration 1686: Weights = [55.30333091  3.31463842  7.93790076  0.13055756  0.25015554 10.64400637], Loss = 0.9512\n",
      "Iteration 1687: Weights = [55.30333094  3.31442426  7.93738787  0.13054912  0.25013938 10.64455887], Loss = 0.9511\n",
      "Iteration 1688: Weights = [55.30333096  3.3142101   7.93687502  0.13054069  0.25012321 10.64511134], Loss = 0.9510\n",
      "Iteration 1689: Weights = [55.30333099  3.31399597  7.93636221  0.13053226  0.25010705 10.64566378], Loss = 0.9508\n",
      "Iteration 1690: Weights = [55.30333101  3.31378184  7.93584942  0.13052382  0.25009089 10.64621618], Loss = 0.9507\n",
      "Iteration 1691: Weights = [55.30333103  3.31356773  7.93533667  0.13051539  0.25007473 10.64676854], Loss = 0.9506\n",
      "Iteration 1692: Weights = [55.30333106  3.31335364  7.93482395  0.13050695  0.25005858 10.64732087], Loss = 0.9505\n",
      "Iteration 1693: Weights = [55.30333108  3.31313955  7.93431127  0.13049852  0.25004242 10.64787317], Loss = 0.9503\n",
      "Iteration 1694: Weights = [55.3033311   3.31292549  7.93379861  0.13049009  0.25002626 10.64842542], Loss = 0.9502\n",
      "Iteration 1695: Weights = [55.30333112  3.31271143  7.933286    0.13048166  0.25001011 10.64897765], Loss = 0.9501\n",
      "Iteration 1696: Weights = [55.30333115  3.31249739  7.93277341  0.13047323  0.24999396 10.64952983], Loss = 0.9500\n",
      "Iteration 1697: Weights = [55.30333117  3.31228336  7.93226086  0.1304648   0.2499778  10.65008198], Loss = 0.9499\n",
      "Iteration 1698: Weights = [55.30333119  3.31206935  7.93174834  0.13045637  0.24996165 10.6506341 ], Loss = 0.9497\n",
      "Iteration 1699: Weights = [55.30333121  3.31185535  7.93123585  0.13044794  0.2499455  10.65118618], Loss = 0.9496\n",
      "Iteration 1700: Weights = [55.30333123  3.31164137  7.9307234   0.13043951  0.24992935 10.65173822], Loss = 0.9495\n",
      "Iteration 1701: Weights = [55.30333125  3.31142739  7.93021098  0.13043108  0.2499132  10.65229023], Loss = 0.9494\n",
      "Iteration 1702: Weights = [55.30333127  3.31121344  7.92969859  0.13042266  0.24989705 10.6528422 ], Loss = 0.9492\n",
      "Iteration 1703: Weights = [55.30333129  3.31099949  7.92918624  0.13041423  0.24988091 10.65339414], Loss = 0.9491\n",
      "Iteration 1704: Weights = [55.30333131  3.31078556  7.92867392  0.1304058   0.24986476 10.65394604], Loss = 0.9490\n",
      "Iteration 1705: Weights = [55.30333133  3.31057164  7.92816163  0.13039738  0.24984862 10.6544979 ], Loss = 0.9489\n",
      "Iteration 1706: Weights = [55.30333135  3.31035774  7.92764938  0.13038895  0.24983248 10.65504973], Loss = 0.9487\n",
      "Iteration 1707: Weights = [55.30333137  3.31014385  7.92713715  0.13038053  0.24981633 10.65560152], Loss = 0.9486\n",
      "Iteration 1708: Weights = [55.30333139  3.30992998  7.92662497  0.1303721   0.24980019 10.65615328], Loss = 0.9485\n",
      "Iteration 1709: Weights = [55.30333141  3.30971612  7.92611281  0.13036368  0.24978405 10.65670501], Loss = 0.9484\n",
      "Iteration 1710: Weights = [55.30333143  3.30950227  7.92560069  0.13035526  0.24976791 10.65725669], Loss = 0.9483\n",
      "Iteration 1711: Weights = [55.30333145  3.30928844  7.9250886   0.13034683  0.24975177 10.65780834], Loss = 0.9481\n",
      "Iteration 1712: Weights = [55.30333147  3.30907462  7.92457654  0.13033841  0.24973564 10.65835996], Loss = 0.9480\n",
      "Iteration 1713: Weights = [55.30333149  3.30886081  7.92406452  0.13032999  0.2497195  10.65891154], Loss = 0.9479\n",
      "Iteration 1714: Weights = [55.30333151  3.30864702  7.92355253  0.13032157  0.24970337 10.65946308], Loss = 0.9478\n",
      "Iteration 1715: Weights = [55.30333153  3.30843324  7.92304057  0.13031315  0.24968723 10.66001459], Loss = 0.9476\n",
      "Iteration 1716: Weights = [55.30333154  3.30821948  7.92252865  0.13030473  0.2496711  10.66056606], Loss = 0.9475\n",
      "Iteration 1717: Weights = [55.30333156  3.30800573  7.92201676  0.13029631  0.24965497 10.6611175 ], Loss = 0.9474\n",
      "Iteration 1718: Weights = [55.30333158  3.30779199  7.9215049   0.13028789  0.24963884 10.6616689 ], Loss = 0.9473\n",
      "Iteration 1719: Weights = [55.3033316   3.30757827  7.92099308  0.13027947  0.24962271 10.66222027], Loss = 0.9472\n",
      "Iteration 1720: Weights = [55.30333161  3.30736456  7.92048129  0.13027105  0.24960658 10.6627716 ], Loss = 0.9470\n",
      "Iteration 1721: Weights = [55.30333163  3.30715086  7.91996953  0.13026264  0.24959045 10.66332289], Loss = 0.9469\n",
      "Iteration 1722: Weights = [55.30333165  3.30693718  7.9194578   0.13025422  0.24957432 10.66387415], Loss = 0.9468\n",
      "Iteration 1723: Weights = [55.30333167  3.30672351  7.91894611  0.1302458   0.2495582  10.66442538], Loss = 0.9467\n",
      "Iteration 1724: Weights = [55.30333168  3.30650986  7.91843445  0.13023739  0.24954207 10.66497657], Loss = 0.9465\n",
      "Iteration 1725: Weights = [55.3033317   3.30629622  7.91792283  0.13022897  0.24952595 10.66552772], Loss = 0.9464\n",
      "Iteration 1726: Weights = [55.30333171  3.30608259  7.91741123  0.13022056  0.24950983 10.66607883], Loss = 0.9463\n",
      "Iteration 1727: Weights = [55.30333173  3.30586898  7.91689967  0.13021215  0.24949371 10.66662992], Loss = 0.9462\n",
      "Iteration 1728: Weights = [55.30333175  3.30565538  7.91638815  0.13020373  0.24947759 10.66718096], Loss = 0.9461\n",
      "Iteration 1729: Weights = [55.30333176  3.3054418   7.91587665  0.13019532  0.24946147 10.66773197], Loss = 0.9459\n",
      "Iteration 1730: Weights = [55.30333178  3.30522822  7.91536519  0.13018691  0.24944535 10.66828295], Loss = 0.9458\n",
      "Iteration 1731: Weights = [55.30333179  3.30501467  7.91485376  0.1301785   0.24942923 10.66883388], Loss = 0.9457\n",
      "Iteration 1732: Weights = [55.30333181  3.30480112  7.91434237  0.13017009  0.24941312 10.66938479], Loss = 0.9456\n",
      "Iteration 1733: Weights = [55.30333182  3.30458759  7.91383101  0.13016167  0.249397   10.66993566], Loss = 0.9454\n",
      "Iteration 1734: Weights = [55.30333184  3.30437408  7.91331968  0.13015326  0.24938089 10.67048649], Loss = 0.9453\n",
      "Iteration 1735: Weights = [55.30333185  3.30416058  7.91280838  0.13014486  0.24936477 10.67103728], Loss = 0.9452\n",
      "Iteration 1736: Weights = [55.30333187  3.30394709  7.91229712  0.13013645  0.24934866 10.67158804], Loss = 0.9451\n",
      "Iteration 1737: Weights = [55.30333188  3.30373361  7.91178589  0.13012804  0.24933255 10.67213877], Loss = 0.9450\n",
      "Iteration 1738: Weights = [55.3033319   3.30352015  7.9112747   0.13011963  0.24931644 10.67268946], Loss = 0.9448\n",
      "Iteration 1739: Weights = [55.30333191  3.3033067   7.91076353  0.13011122  0.24930033 10.67324011], Loss = 0.9447\n",
      "Iteration 1740: Weights = [55.30333193  3.30309327  7.9102524   0.13010282  0.24928422 10.67379073], Loss = 0.9446\n",
      "Iteration 1741: Weights = [55.30333194  3.30287985  7.90974131  0.13009441  0.24926812 10.67434131], Loss = 0.9445\n",
      "Iteration 1742: Weights = [55.30333196  3.30266645  7.90923024  0.130086    0.24925201 10.67489186], Loss = 0.9443\n",
      "Iteration 1743: Weights = [55.30333197  3.30245305  7.90871921  0.1300776   0.24923591 10.67544237], Loss = 0.9442\n",
      "Iteration 1744: Weights = [55.30333198  3.30223968  7.90820821  0.13006919  0.2492198  10.67599285], Loss = 0.9441\n",
      "Iteration 1745: Weights = [55.303332    3.30202631  7.90769725  0.13006079  0.2492037  10.67654329], Loss = 0.9440\n",
      "Iteration 1746: Weights = [55.30333201  3.30181296  7.90718631  0.13005239  0.2491876  10.6770937 ], Loss = 0.9439\n",
      "Iteration 1747: Weights = [55.30333202  3.30159962  7.90667541  0.13004398  0.2491715  10.67764406], Loss = 0.9437\n",
      "Iteration 1748: Weights = [55.30333204  3.3013863   7.90616455  0.13003558  0.2491554  10.6781944 ], Loss = 0.9436\n",
      "Iteration 1749: Weights = [55.30333205  3.30117299  7.90565372  0.13002718  0.2491393  10.6787447 ], Loss = 0.9435\n",
      "Iteration 1750: Weights = [55.30333206  3.3009597   7.90514292  0.13001878  0.2491232  10.67929496], Loss = 0.9434\n",
      "Iteration 1751: Weights = [55.30333207  3.30074642  7.90463215  0.13001038  0.24910711 10.67984519], Loss = 0.9432\n",
      "Iteration 1752: Weights = [55.30333209  3.30053315  7.90412141  0.13000198  0.24909101 10.68039538], Loss = 0.9431\n",
      "Iteration 1753: Weights = [55.3033321   3.30031989  7.90361071  0.12999358  0.24907492 10.68094553], Loss = 0.9430\n",
      "Iteration 1754: Weights = [55.30333211  3.30010665  7.90310004  0.12998518  0.24905882 10.68149566], Loss = 0.9429\n",
      "Iteration 1755: Weights = [55.30333212  3.29989343  7.90258941  0.12997678  0.24904273 10.68204574], Loss = 0.9428\n",
      "Iteration 1756: Weights = [55.30333214  3.29968021  7.90207881  0.12996838  0.24902664 10.68259579], Loss = 0.9426\n",
      "Iteration 1757: Weights = [55.30333215  3.29946702  7.90156824  0.12995998  0.24901055 10.6831458 ], Loss = 0.9425\n",
      "Iteration 1758: Weights = [55.30333216  3.29925383  7.9010577   0.12995159  0.24899446 10.68369578], Loss = 0.9424\n",
      "Iteration 1759: Weights = [55.30333217  3.29904066  7.9005472   0.12994319  0.24897837 10.68424573], Loss = 0.9423\n",
      "Iteration 1760: Weights = [55.30333218  3.2988275   7.90003673  0.12993479  0.24896228 10.68479563], Loss = 0.9422\n",
      "Iteration 1761: Weights = [55.30333219  3.29861436  7.89952629  0.1299264   0.2489462  10.6853455 ], Loss = 0.9420\n",
      "Iteration 1762: Weights = [55.30333221  3.29840123  7.89901589  0.129918    0.24893011 10.68589534], Loss = 0.9419\n",
      "Iteration 1763: Weights = [55.30333222  3.29818811  7.89850552  0.12990961  0.24891403 10.68644514], Loss = 0.9418\n",
      "Iteration 1764: Weights = [55.30333223  3.29797501  7.89799518  0.12990122  0.24889795 10.68699491], Loss = 0.9417\n",
      "Iteration 1765: Weights = [55.30333224  3.29776192  7.89748487  0.12989282  0.24888186 10.68754464], Loss = 0.9415\n",
      "Iteration 1766: Weights = [55.30333225  3.29754885  7.8969746   0.12988443  0.24886578 10.68809433], Loss = 0.9414\n",
      "Iteration 1767: Weights = [55.30333226  3.29733578  7.89646436  0.12987604  0.2488497  10.68864399], Loss = 0.9413\n",
      "Iteration 1768: Weights = [55.30333227  3.29712274  7.89595415  0.12986765  0.24883363 10.68919361], Loss = 0.9412\n",
      "Iteration 1769: Weights = [55.30333228  3.2969097   7.89544398  0.12985926  0.24881755 10.6897432 ], Loss = 0.9411\n",
      "Iteration 1770: Weights = [55.30333229  3.29669668  7.89493384  0.12985086  0.24880147 10.69029275], Loss = 0.9409\n",
      "Iteration 1771: Weights = [55.3033323   3.29648368  7.89442373  0.12984248  0.2487854  10.69084227], Loss = 0.9408\n",
      "Iteration 1772: Weights = [55.30333231  3.29627068  7.89391366  0.12983409  0.24876932 10.69139175], Loss = 0.9407\n",
      "Iteration 1773: Weights = [55.30333232  3.29605771  7.89340362  0.1298257   0.24875325 10.6919412 ], Loss = 0.9406\n",
      "Iteration 1774: Weights = [55.30333233  3.29584474  7.89289361  0.12981731  0.24873717 10.69249061], Loss = 0.9404\n",
      "Iteration 1775: Weights = [55.30333234  3.29563179  7.89238363  0.12980892  0.2487211  10.69303998], Loss = 0.9403\n",
      "Iteration 1776: Weights = [55.30333235  3.29541885  7.89187369  0.12980053  0.24870503 10.69358932], Loss = 0.9402\n",
      "Iteration 1777: Weights = [55.30333236  3.29520593  7.89136378  0.12979215  0.24868896 10.69413862], Loss = 0.9401\n",
      "Iteration 1778: Weights = [55.30333237  3.29499302  7.8908539   0.12978376  0.2486729  10.69468789], Loss = 0.9400\n",
      "Iteration 1779: Weights = [55.30333238  3.29478012  7.89034406  0.12977537  0.24865683 10.69523712], Loss = 0.9398\n",
      "Iteration 1780: Weights = [55.30333239  3.29456724  7.88983425  0.12976699  0.24864076 10.69578632], Loss = 0.9397\n",
      "Iteration 1781: Weights = [55.3033324   3.29435437  7.88932447  0.12975861  0.2486247  10.69633548], Loss = 0.9396\n",
      "Iteration 1782: Weights = [55.30333241  3.29414152  7.88881472  0.12975022  0.24860863 10.69688461], Loss = 0.9395\n",
      "Iteration 1783: Weights = [55.30333242  3.29392868  7.88830501  0.12974184  0.24859257 10.6974337 ], Loss = 0.9394\n",
      "Iteration 1784: Weights = [55.30333243  3.29371585  7.88779533  0.12973346  0.24857651 10.69798275], Loss = 0.9392\n",
      "Iteration 1785: Weights = [55.30333244  3.29350304  7.88728569  0.12972507  0.24856045 10.69853177], Loss = 0.9391\n",
      "Iteration 1786: Weights = [55.30333245  3.29329024  7.88677607  0.12971669  0.24854439 10.69908076], Loss = 0.9390\n",
      "Iteration 1787: Weights = [55.30333246  3.29307745  7.88626649  0.12970831  0.24852833 10.69962971], Loss = 0.9389\n",
      "Iteration 1788: Weights = [55.30333247  3.29286468  7.88575694  0.12969993  0.24851227 10.70017862], Loss = 0.9387\n",
      "Iteration 1789: Weights = [55.30333247  3.29265192  7.88524743  0.12969155  0.24849621 10.7007275 ], Loss = 0.9386\n",
      "Iteration 1790: Weights = [55.30333248  3.29243917  7.88473795  0.12968317  0.24848016 10.70127634], Loss = 0.9385\n",
      "Iteration 1791: Weights = [55.30333249  3.29222644  7.8842285   0.12967479  0.2484641  10.70182515], Loss = 0.9384\n",
      "Iteration 1792: Weights = [55.3033325   3.29201373  7.88371908  0.12966641  0.24844805 10.70237392], Loss = 0.9383\n",
      "Iteration 1793: Weights = [55.30333251  3.29180102  7.8832097   0.12965803  0.24843199 10.70292266], Loss = 0.9381\n",
      "Iteration 1794: Weights = [55.30333252  3.29158833  7.88270035  0.12964966  0.24841594 10.70347136], Loss = 0.9380\n",
      "Iteration 1795: Weights = [55.30333252  3.29137566  7.88219103  0.12964128  0.24839989 10.70402002], Loss = 0.9379\n",
      "Iteration 1796: Weights = [55.30333253  3.29116299  7.88168175  0.1296329   0.24838384 10.70456865], Loss = 0.9378\n",
      "Iteration 1797: Weights = [55.30333254  3.29095034  7.8811725   0.12962453  0.24836779 10.70511725], Loss = 0.9377\n",
      "Iteration 1798: Weights = [55.30333255  3.29073771  7.88066328  0.12961615  0.24835175 10.7056658 ], Loss = 0.9375\n",
      "Iteration 1799: Weights = [55.30333256  3.29052509  7.88015409  0.12960778  0.2483357  10.70621433], Loss = 0.9374\n",
      "Iteration 1800: Weights = [55.30333256  3.29031248  7.87964494  0.1295994   0.24831965 10.70676282], Loss = 0.9373\n",
      "Iteration 1801: Weights = [55.30333257  3.29009989  7.87913582  0.12959103  0.24830361 10.70731127], Loss = 0.9372\n",
      "Iteration 1802: Weights = [55.30333258  3.28988731  7.87862673  0.12958266  0.24828757 10.70785969], Loss = 0.9371\n",
      "Iteration 1803: Weights = [55.30333259  3.28967474  7.87811768  0.12957428  0.24827152 10.70840807], Loss = 0.9369\n",
      "Iteration 1804: Weights = [55.30333259  3.28946219  7.87760866  0.12956591  0.24825548 10.70895641], Loss = 0.9368\n",
      "Iteration 1805: Weights = [55.3033326   3.28924965  7.87709967  0.12955754  0.24823944 10.70950472], Loss = 0.9367\n",
      "Iteration 1806: Weights = [55.30333261  3.28903713  7.87659072  0.12954917  0.2482234  10.710053  ], Loss = 0.9366\n",
      "Iteration 1807: Weights = [55.30333262  3.28882461  7.87608179  0.1295408   0.24820736 10.71060124], Loss = 0.9364\n",
      "Iteration 1808: Weights = [55.30333262  3.28861212  7.8755729   0.12953243  0.24819133 10.71114944], Loss = 0.9363\n",
      "Iteration 1809: Weights = [55.30333263  3.28839963  7.87506405  0.12952406  0.24817529 10.71169761], Loss = 0.9362\n",
      "Iteration 1810: Weights = [55.30333264  3.28818716  7.87455522  0.12951569  0.24815926 10.71224575], Loss = 0.9361\n",
      "Iteration 1811: Weights = [55.30333264  3.28797471  7.87404643  0.12950732  0.24814322 10.71279384], Loss = 0.9360\n",
      "Iteration 1812: Weights = [55.30333265  3.28776226  7.87353768  0.12949895  0.24812719 10.71334191], Loss = 0.9358\n",
      "Iteration 1813: Weights = [55.30333266  3.28754983  7.87302895  0.12949059  0.24811116 10.71388993], Loss = 0.9357\n",
      "Iteration 1814: Weights = [55.30333266  3.28733742  7.87252026  0.12948222  0.24809513 10.71443793], Loss = 0.9356\n",
      "Iteration 1815: Weights = [55.30333267  3.28712502  7.8720116   0.12947385  0.2480791  10.71498588], Loss = 0.9355\n",
      "Iteration 1816: Weights = [55.30333268  3.28691263  7.87150297  0.12946549  0.24806307 10.7155338 ], Loss = 0.9354\n",
      "Iteration 1817: Weights = [55.30333268  3.28670026  7.87099438  0.12945712  0.24804704 10.71608169], Loss = 0.9352\n",
      "Iteration 1818: Weights = [55.30333269  3.2864879   7.87048582  0.12944876  0.24803101 10.71662954], Loss = 0.9351\n",
      "Iteration 1819: Weights = [55.3033327   3.28627555  7.86997729  0.12944039  0.24801499 10.71717736], Loss = 0.9350\n",
      "Iteration 1820: Weights = [55.3033327   3.28606322  7.86946879  0.12943203  0.24799896 10.71772513], Loss = 0.9349\n",
      "Iteration 1821: Weights = [55.30333271  3.2858509   7.86896033  0.12942367  0.24798294 10.71827288], Loss = 0.9348\n",
      "Iteration 1822: Weights = [55.30333272  3.28563859  7.8684519   0.12941531  0.24796692 10.71882059], Loss = 0.9346\n",
      "Iteration 1823: Weights = [55.30333272  3.2854263   7.86794351  0.12940694  0.24795089 10.71936826], Loss = 0.9345\n",
      "Iteration 1824: Weights = [55.30333273  3.28521402  7.86743514  0.12939858  0.24793487 10.7199159 ], Loss = 0.9344\n",
      "Iteration 1825: Weights = [55.30333273  3.28500176  7.86692681  0.12939022  0.24791885 10.7204635 ], Loss = 0.9343\n",
      "Iteration 1826: Weights = [55.30333274  3.28478951  7.86641851  0.12938186  0.24790283 10.72101107], Loss = 0.9342\n",
      "Iteration 1827: Weights = [55.30333275  3.28457727  7.86591025  0.1293735   0.24788682 10.7215586 ], Loss = 0.9340\n",
      "Iteration 1828: Weights = [55.30333275  3.28436505  7.86540202  0.12936514  0.2478708  10.7221061 ], Loss = 0.9339\n",
      "Iteration 1829: Weights = [55.30333276  3.28415284  7.86489382  0.12935678  0.24785479 10.72265356], Loss = 0.9338\n",
      "Iteration 1830: Weights = [55.30333276  3.28394064  7.86438565  0.12934843  0.24783877 10.72320098], Loss = 0.9337\n",
      "Iteration 1831: Weights = [55.30333277  3.28372846  7.86387752  0.12934007  0.24782276 10.72374837], Loss = 0.9335\n",
      "Iteration 1832: Weights = [55.30333278  3.28351629  7.86336942  0.12933171  0.24780675 10.72429573], Loss = 0.9334\n",
      "Iteration 1833: Weights = [55.30333278  3.28330414  7.86286135  0.12932336  0.24779073 10.72484305], Loss = 0.9333\n",
      "Iteration 1834: Weights = [55.30333279  3.283092    7.86235331  0.129315    0.24777472 10.72539033], Loss = 0.9332\n",
      "Iteration 1835: Weights = [55.30333279  3.28287987  7.86184531  0.12930664  0.24775871 10.72593758], Loss = 0.9331\n",
      "Iteration 1836: Weights = [55.3033328   3.28266776  7.86133734  0.12929829  0.24774271 10.7264848 ], Loss = 0.9329\n",
      "Iteration 1837: Weights = [55.3033328   3.28245566  7.8608294   0.12928994  0.2477267  10.72703197], Loss = 0.9328\n",
      "Iteration 1838: Weights = [55.30333281  3.28224357  7.8603215   0.12928158  0.24771069 10.72757912], Loss = 0.9327\n",
      "Iteration 1839: Weights = [55.30333281  3.2820315   7.85981363  0.12927323  0.24769469 10.72812622], Loss = 0.9326\n",
      "Iteration 1840: Weights = [55.30333282  3.28181944  7.85930579  0.12926488  0.24767868 10.7286733 ], Loss = 0.9325\n",
      "Iteration 1841: Weights = [55.30333282  3.2816074   7.85879798  0.12925652  0.24766268 10.72922033], Loss = 0.9323\n",
      "Iteration 1842: Weights = [55.30333283  3.28139536  7.85829021  0.12924817  0.24764668 10.72976734], Loss = 0.9322\n",
      "Iteration 1843: Weights = [55.30333283  3.28118335  7.85778247  0.12923982  0.24763068 10.7303143 ], Loss = 0.9321\n",
      "Iteration 1844: Weights = [55.30333284  3.28097134  7.85727477  0.12923147  0.24761468 10.73086123], Loss = 0.9320\n",
      "Iteration 1845: Weights = [55.30333284  3.28075935  7.85676709  0.12922312  0.24759868 10.73140813], Loss = 0.9319\n",
      "Iteration 1846: Weights = [55.30333285  3.28054738  7.85625945  0.12921477  0.24758268 10.73195499], Loss = 0.9317\n",
      "Iteration 1847: Weights = [55.30333285  3.28033541  7.85575184  0.12920642  0.24756668 10.73250181], Loss = 0.9316\n",
      "Iteration 1848: Weights = [55.30333286  3.28012346  7.85524426  0.12919807  0.24755069 10.7330486 ], Loss = 0.9315\n",
      "Iteration 1849: Weights = [55.30333286  3.27991153  7.85473672  0.12918973  0.24753469 10.73359536], Loss = 0.9314\n",
      "Iteration 1850: Weights = [55.30333287  3.27969961  7.85422921  0.12918138  0.2475187  10.73414207], Loss = 0.9313\n",
      "Iteration 1851: Weights = [55.30333287  3.2794877   7.85372173  0.12917303  0.24750271 10.73468876], Loss = 0.9311\n",
      "Iteration 1852: Weights = [55.30333288  3.2792758   7.85321429  0.12916469  0.24748672 10.73523541], Loss = 0.9310\n",
      "Iteration 1853: Weights = [55.30333288  3.27906392  7.85270688  0.12915634  0.24747072 10.73578202], Loss = 0.9309\n",
      "Iteration 1854: Weights = [55.30333289  3.27885206  7.8521995   0.129148    0.24745473 10.7363286 ], Loss = 0.9308\n",
      "Iteration 1855: Weights = [55.30333289  3.2786402   7.85169215  0.12913965  0.24743875 10.73687514], Loss = 0.9307\n",
      "Iteration 1856: Weights = [55.3033329   3.27842836  7.85118484  0.12913131  0.24742276 10.73742165], Loss = 0.9305\n",
      "Iteration 1857: Weights = [55.3033329   3.27821654  7.85067756  0.12912296  0.24740677 10.73796812], Loss = 0.9304\n",
      "Iteration 1858: Weights = [55.3033329   3.27800473  7.85017031  0.12911462  0.24739079 10.73851455], Loss = 0.9303\n",
      "Iteration 1859: Weights = [55.30333291  3.27779293  7.84966309  0.12910628  0.2473748  10.73906095], Loss = 0.9302\n",
      "Iteration 1860: Weights = [55.30333291  3.27758114  7.84915591  0.12909794  0.24735882 10.73960732], Loss = 0.9301\n",
      "Iteration 1861: Weights = [55.30333292  3.27736937  7.84864876  0.1290896   0.24734284 10.74015365], Loss = 0.9299\n",
      "Iteration 1862: Weights = [55.30333292  3.27715762  7.84814164  0.12908126  0.24732686 10.74069995], Loss = 0.9298\n",
      "Iteration 1863: Weights = [55.30333292  3.27694587  7.84763456  0.12907292  0.24731087 10.7412462 ], Loss = 0.9297\n",
      "Iteration 1864: Weights = [55.30333293  3.27673414  7.84712751  0.12906458  0.2472949  10.74179243], Loss = 0.9296\n",
      "Iteration 1865: Weights = [55.30333293  3.27652242  7.84662049  0.12905624  0.24727892 10.74233862], Loss = 0.9295\n",
      "Iteration 1866: Weights = [55.30333294  3.27631072  7.8461135   0.1290479   0.24726294 10.74288477], Loss = 0.9293\n",
      "Iteration 1867: Weights = [55.30333294  3.27609903  7.84560655  0.12903956  0.24724696 10.74343089], Loss = 0.9292\n",
      "Iteration 1868: Weights = [55.30333294  3.27588736  7.84509963  0.12903122  0.24723099 10.74397697], Loss = 0.9291\n",
      "Iteration 1869: Weights = [55.30333295  3.2756757   7.84459274  0.12902289  0.24721501 10.74452302], Loss = 0.9290\n",
      "Iteration 1870: Weights = [55.30333295  3.27546405  7.84408589  0.12901455  0.24719904 10.74506903], Loss = 0.9289\n",
      "Iteration 1871: Weights = [55.30333296  3.27525241  7.84357907  0.12900621  0.24718307 10.74561501], Loss = 0.9287\n",
      "Iteration 1872: Weights = [55.30333296  3.27504079  7.84307228  0.12899788  0.2471671  10.74616095], Loss = 0.9286\n",
      "Iteration 1873: Weights = [55.30333296  3.27482919  7.84256552  0.12898954  0.24715113 10.74670686], Loss = 0.9285\n",
      "Iteration 1874: Weights = [55.30333297  3.27461759  7.8420588   0.12898121  0.24713516 10.74725273], Loss = 0.9284\n",
      "Iteration 1875: Weights = [55.30333297  3.27440601  7.8415521   0.12897287  0.24711919 10.74779857], Loss = 0.9283\n",
      "Iteration 1876: Weights = [55.30333297  3.27419445  7.84104545  0.12896454  0.24710322 10.74834437], Loss = 0.9281\n",
      "Iteration 1877: Weights = [55.30333298  3.2739829   7.84053882  0.12895621  0.24708726 10.74889014], Loss = 0.9280\n",
      "Iteration 1878: Weights = [55.30333298  3.27377136  7.84003223  0.12894788  0.24707129 10.74943587], Loss = 0.9279\n",
      "Iteration 1879: Weights = [55.30333299  3.27355983  7.83952567  0.12893955  0.24705533 10.74998156], Loss = 0.9278\n",
      "Iteration 1880: Weights = [55.30333299  3.27334832  7.83901914  0.12893121  0.24703937 10.75052722], Loss = 0.9277\n",
      "Iteration 1881: Weights = [55.30333299  3.27313682  7.83851264  0.12892288  0.24702341 10.75107285], Loss = 0.9275\n",
      "Iteration 1882: Weights = [55.303333    3.27292534  7.83800618  0.12891455  0.24700745 10.75161844], Loss = 0.9274\n",
      "Iteration 1883: Weights = [55.303333    3.27271387  7.83749975  0.12890622  0.24699149 10.75216399], Loss = 0.9273\n",
      "Iteration 1884: Weights = [55.303333    3.27250241  7.83699336  0.1288979   0.24697553 10.75270951], Loss = 0.9272\n",
      "Iteration 1885: Weights = [55.30333301  3.27229097  7.83648699  0.12888957  0.24695957 10.75325499], Loss = 0.9271\n",
      "Iteration 1886: Weights = [55.30333301  3.27207954  7.83598066  0.12888124  0.24694361 10.75380044], Loss = 0.9269\n",
      "Iteration 1887: Weights = [55.30333301  3.27186812  7.83547436  0.12887291  0.24692766 10.75434585], Loss = 0.9268\n",
      "Iteration 1888: Weights = [55.30333302  3.27165672  7.8349681   0.12886459  0.2469117  10.75489123], Loss = 0.9267\n",
      "Iteration 1889: Weights = [55.30333302  3.27144533  7.83446186  0.12885626  0.24689575 10.75543658], Loss = 0.9266\n",
      "Iteration 1890: Weights = [55.30333302  3.27123396  7.83395566  0.12884793  0.2468798  10.75598188], Loss = 0.9265\n",
      "Iteration 1891: Weights = [55.30333303  3.2710226   7.8334495   0.12883961  0.24686385 10.75652716], Loss = 0.9263\n",
      "Iteration 1892: Weights = [55.30333303  3.27081125  7.83294336  0.12883128  0.2468479  10.75707239], Loss = 0.9262\n",
      "Iteration 1893: Weights = [55.30333303  3.27059992  7.83243726  0.12882296  0.24683195 10.75761759], Loss = 0.9261\n",
      "Iteration 1894: Weights = [55.30333303  3.2703886   7.83193119  0.12881464  0.246816   10.75816276], Loss = 0.9260\n",
      "Iteration 1895: Weights = [55.30333304  3.27017729  7.83142515  0.12880631  0.24680005 10.75870789], Loss = 0.9259\n",
      "Iteration 1896: Weights = [55.30333304  3.269966    7.83091915  0.12879799  0.2467841  10.75925299], Loss = 0.9257\n",
      "Iteration 1897: Weights = [55.30333304  3.26975472  7.83041318  0.12878967  0.24676816 10.75979805], Loss = 0.9256\n",
      "Iteration 1898: Weights = [55.30333305  3.26954345  7.82990724  0.12878135  0.24675221 10.76034308], Loss = 0.9255\n",
      "Iteration 1899: Weights = [55.30333305  3.2693322   7.82940133  0.12877303  0.24673627 10.76088807], Loss = 0.9254\n",
      "Iteration 1900: Weights = [55.30333305  3.26912096  7.82889546  0.12876471  0.24672033 10.76143302], Loss = 0.9253\n",
      "Iteration 1901: Weights = [55.30333305  3.26890974  7.82838962  0.12875639  0.24670439 10.76197794], Loss = 0.9251\n",
      "Iteration 1902: Weights = [55.30333306  3.26869853  7.82788381  0.12874807  0.24668845 10.76252283], Loss = 0.9250\n",
      "Iteration 1903: Weights = [55.30333306  3.26848733  7.82737804  0.12873975  0.24667251 10.76306768], Loss = 0.9249\n",
      "Iteration 1904: Weights = [55.30333306  3.26827615  7.82687229  0.12873143  0.24665657 10.76361249], Loss = 0.9248\n",
      "Iteration 1905: Weights = [55.30333307  3.26806498  7.82636658  0.12872311  0.24664063 10.76415727], Loss = 0.9247\n",
      "Iteration 1906: Weights = [55.30333307  3.26785382  7.82586091  0.1287148   0.2466247  10.76470201], Loss = 0.9245\n",
      "Iteration 1907: Weights = [55.30333307  3.26764268  7.82535526  0.12870648  0.24660876 10.76524672], Loss = 0.9244\n",
      "Iteration 1908: Weights = [55.30333307  3.26743155  7.82484965  0.12869816  0.24659283 10.7657914 ], Loss = 0.9243\n",
      "Iteration 1909: Weights = [55.30333308  3.26722044  7.82434407  0.12868985  0.2465769  10.76633604], Loss = 0.9242\n",
      "Iteration 1910: Weights = [55.30333308  3.26700933  7.82383852  0.12868153  0.24656096 10.76688064], Loss = 0.9241\n",
      "Iteration 1911: Weights = [55.30333308  3.26679825  7.82333301  0.12867322  0.24654503 10.76742521], Loss = 0.9239\n",
      "Iteration 1912: Weights = [55.30333308  3.26658717  7.82282753  0.1286649   0.2465291  10.76796974], Loss = 0.9238\n",
      "Iteration 1913: Weights = [55.30333309  3.26637611  7.82232208  0.12865659  0.24651317 10.76851424], Loss = 0.9237\n",
      "Iteration 1914: Weights = [55.30333309  3.26616506  7.82181666  0.12864828  0.24649725 10.7690587 ], Loss = 0.9236\n",
      "Iteration 1915: Weights = [55.30333309  3.26595403  7.82131128  0.12863997  0.24648132 10.76960313], Loss = 0.9235\n",
      "Iteration 1916: Weights = [55.30333309  3.26574301  7.82080593  0.12863165  0.24646539 10.77014752], Loss = 0.9233\n",
      "Iteration 1917: Weights = [55.3033331   3.265532    7.82030061  0.12862334  0.24644947 10.77069188], Loss = 0.9232\n",
      "Iteration 1918: Weights = [55.3033331   3.26532101  7.81979533  0.12861503  0.24643355 10.7712362 ], Loss = 0.9231\n",
      "Iteration 1919: Weights = [55.3033331   3.26511003  7.81929007  0.12860672  0.24641762 10.77178049], Loss = 0.9230\n",
      "Iteration 1920: Weights = [55.3033331   3.26489907  7.81878485  0.12859841  0.2464017  10.77232474], Loss = 0.9229\n",
      "Iteration 1921: Weights = [55.30333311  3.26468812  7.81827967  0.1285901   0.24638578 10.77286895], Loss = 0.9228\n",
      "Iteration 1922: Weights = [55.30333311  3.26447718  7.81777451  0.1285818   0.24636986 10.77341313], Loss = 0.9226\n",
      "Iteration 1923: Weights = [55.30333311  3.26426625  7.81726939  0.12857349  0.24635394 10.77395728], Loss = 0.9225\n",
      "Iteration 1924: Weights = [55.30333311  3.26405534  7.8167643   0.12856518  0.24633803 10.77450139], Loss = 0.9224\n",
      "Iteration 1925: Weights = [55.30333311  3.26384444  7.81625924  0.12855687  0.24632211 10.77504547], Loss = 0.9223\n",
      "Iteration 1926: Weights = [55.30333312  3.26363356  7.81575422  0.12854857  0.24630619 10.77558951], Loss = 0.9222\n",
      "Iteration 1927: Weights = [55.30333312  3.26342269  7.81524923  0.12854026  0.24629028 10.77613351], Loss = 0.9220\n",
      "Iteration 1928: Weights = [55.30333312  3.26321183  7.81474427  0.12853196  0.24627437 10.77667748], Loss = 0.9219\n",
      "Iteration 1929: Weights = [55.30333312  3.26300099  7.81423934  0.12852365  0.24625845 10.77722142], Loss = 0.9218\n",
      "Iteration 1930: Weights = [55.30333313  3.26279016  7.81373445  0.12851535  0.24624254 10.77776532], Loss = 0.9217\n",
      "Iteration 1931: Weights = [55.30333313  3.26257935  7.81322959  0.12850704  0.24622663 10.77830918], Loss = 0.9216\n",
      "Iteration 1932: Weights = [55.30333313  3.26236855  7.81272476  0.12849874  0.24621072 10.77885301], Loss = 0.9214\n",
      "Iteration 1933: Weights = [55.30333313  3.26215776  7.81221996  0.12849044  0.24619482 10.77939681], Loss = 0.9213\n",
      "Iteration 1934: Weights = [55.30333313  3.26194698  7.8117152   0.12848214  0.24617891 10.77994057], Loss = 0.9212\n",
      "Iteration 1935: Weights = [55.30333314  3.26173622  7.81121047  0.12847383  0.246163   10.78048429], Loss = 0.9211\n",
      "Iteration 1936: Weights = [55.30333314  3.26152548  7.81070577  0.12846553  0.2461471  10.78102798], Loss = 0.9210\n",
      "Iteration 1937: Weights = [55.30333314  3.26131474  7.81020111  0.12845723  0.24613119 10.78157163], Loss = 0.9208\n",
      "Iteration 1938: Weights = [55.30333314  3.26110402  7.80969647  0.12844893  0.24611529 10.78211525], Loss = 0.9207\n",
      "Iteration 1939: Weights = [55.30333314  3.26089331  7.80919187  0.12844063  0.24609939 10.78265884], Loss = 0.9206\n",
      "Iteration 1940: Weights = [55.30333314  3.26068262  7.8086873   0.12843233  0.24608349 10.78320239], Loss = 0.9205\n",
      "Iteration 1941: Weights = [55.30333315  3.26047194  7.80818277  0.12842404  0.24606759 10.7837459 ], Loss = 0.9204\n",
      "Iteration 1942: Weights = [55.30333315  3.26026128  7.80767827  0.12841574  0.24605169 10.78428938], Loss = 0.9203\n",
      "Iteration 1943: Weights = [55.30333315  3.26005063  7.8071738   0.12840744  0.24603579 10.78483282], Loss = 0.9201\n",
      "Iteration 1944: Weights = [55.30333315  3.25983999  7.80666936  0.12839915  0.24601989 10.78537623], Loss = 0.9200\n",
      "Iteration 1945: Weights = [55.30333315  3.25962936  7.80616496  0.12839085  0.246004   10.7859196 ], Loss = 0.9199\n",
      "Iteration 1946: Weights = [55.30333316  3.25941875  7.80566058  0.12838255  0.2459881  10.78646294], Loss = 0.9198\n",
      "Iteration 1947: Weights = [55.30333316  3.25920815  7.80515624  0.12837426  0.24597221 10.78700624], Loss = 0.9197\n",
      "Iteration 1948: Weights = [55.30333316  3.25899757  7.80465194  0.12836596  0.24595632 10.78754951], Loss = 0.9195\n",
      "Iteration 1949: Weights = [55.30333316  3.258787    7.80414766  0.12835767  0.24594042 10.78809274], Loss = 0.9194\n",
      "Iteration 1950: Weights = [55.30333316  3.25857644  7.80364342  0.12834938  0.24592453 10.78863594], Loss = 0.9193\n",
      "Iteration 1951: Weights = [55.30333316  3.2583659   7.80313921  0.12834108  0.24590864 10.7891791 ], Loss = 0.9192\n",
      "Iteration 1952: Weights = [55.30333317  3.25815537  7.80263504  0.12833279  0.24589276 10.78972223], Loss = 0.9191\n",
      "Iteration 1953: Weights = [55.30333317  3.25794485  7.80213089  0.1283245   0.24587687 10.79026532], Loss = 0.9189\n",
      "Iteration 1954: Weights = [55.30333317  3.25773435  7.80162678  0.12831621  0.24586098 10.79080838], Loss = 0.9188\n",
      "Iteration 1955: Weights = [55.30333317  3.25752386  7.8011227   0.12830792  0.2458451  10.7913514 ], Loss = 0.9187\n",
      "Iteration 1956: Weights = [55.30333317  3.25731339  7.80061866  0.12829963  0.24582921 10.79189439], Loss = 0.9186\n",
      "Iteration 1957: Weights = [55.30333317  3.25710293  7.80011464  0.12829134  0.24581333 10.79243734], Loss = 0.9185\n",
      "Iteration 1958: Weights = [55.30333318  3.25689248  7.79961066  0.12828305  0.24579745 10.79298026], Loss = 0.9184\n",
      "Iteration 1959: Weights = [55.30333318  3.25668204  7.79910671  0.12827476  0.24578156 10.79352314], Loss = 0.9182\n",
      "Iteration 1960: Weights = [55.30333318  3.25647162  7.7986028   0.12826647  0.24576568 10.79406599], Loss = 0.9181\n",
      "Iteration 1961: Weights = [55.30333318  3.25626122  7.79809891  0.12825818  0.2457498  10.7946088 ], Loss = 0.9180\n",
      "Iteration 1962: Weights = [55.30333318  3.25605082  7.79759506  0.1282499   0.24573393 10.79515158], Loss = 0.9179\n",
      "Iteration 1963: Weights = [55.30333318  3.25584044  7.79709124  0.12824161  0.24571805 10.79569432], Loss = 0.9178\n",
      "Iteration 1964: Weights = [55.30333319  3.25563008  7.79658746  0.12823332  0.24570217 10.79623702], Loss = 0.9176\n",
      "Iteration 1965: Weights = [55.30333319  3.25541972  7.79608371  0.12822504  0.2456863  10.7967797 ], Loss = 0.9175\n",
      "Iteration 1966: Weights = [55.30333319  3.25520938  7.79557998  0.12821675  0.24567042 10.79732233], Loss = 0.9174\n",
      "Iteration 1967: Weights = [55.30333319  3.25499906  7.7950763   0.12820847  0.24565455 10.79786493], Loss = 0.9173\n",
      "Iteration 1968: Weights = [55.30333319  3.25478875  7.79457264  0.12820019  0.24563868 10.7984075 ], Loss = 0.9172\n",
      "Iteration 1969: Weights = [55.30333319  3.25457845  7.79406902  0.1281919   0.24562281 10.79895003], Loss = 0.9170\n",
      "Iteration 1970: Weights = [55.30333319  3.25436816  7.79356543  0.12818362  0.24560694 10.79949253], Loss = 0.9169\n",
      "Iteration 1971: Weights = [55.3033332   3.25415789  7.79306187  0.12817534  0.24559107 10.80003499], Loss = 0.9168\n",
      "Iteration 1972: Weights = [55.3033332   3.25394763  7.79255834  0.12816706  0.2455752  10.80057741], Loss = 0.9167\n",
      "Iteration 1973: Weights = [55.3033332   3.25373739  7.79205485  0.12815877  0.24555933 10.8011198 ], Loss = 0.9166\n",
      "Iteration 1974: Weights = [55.3033332   3.25352716  7.79155139  0.12815049  0.24554346 10.80166216], Loss = 0.9165\n",
      "Iteration 1975: Weights = [55.3033332   3.25331694  7.79104796  0.12814221  0.2455276  10.80220448], Loss = 0.9163\n",
      "Iteration 1976: Weights = [55.3033332   3.25310674  7.79054457  0.12813393  0.24551174 10.80274677], Loss = 0.9162\n",
      "Iteration 1977: Weights = [55.3033332   3.25289655  7.79004121  0.12812566  0.24549587 10.80328902], Loss = 0.9161\n",
      "Iteration 1978: Weights = [55.3033332   3.25268637  7.78953787  0.12811738  0.24548001 10.80383123], Loss = 0.9160\n",
      "Iteration 1979: Weights = [55.30333321  3.25247621  7.78903458  0.1281091   0.24546415 10.80437341], Loss = 0.9159\n",
      "Iteration 1980: Weights = [55.30333321  3.25226606  7.78853131  0.12810082  0.24544829 10.80491556], Loss = 0.9157\n",
      "Iteration 1981: Weights = [55.30333321  3.25205593  7.78802808  0.12809255  0.24543243 10.80545767], Loss = 0.9156\n",
      "Iteration 1982: Weights = [55.30333321  3.2518458   7.78752488  0.12808427  0.24541657 10.80599975], Loss = 0.9155\n",
      "Iteration 1983: Weights = [55.30333321  3.2516357   7.78702171  0.12807599  0.24540072 10.80654179], Loss = 0.9154\n",
      "Iteration 1984: Weights = [55.30333321  3.2514256   7.78651858  0.12806772  0.24538486 10.80708379], Loss = 0.9153\n",
      "Iteration 1985: Weights = [55.30333321  3.25121552  7.78601547  0.12805944  0.24536901 10.80762576], Loss = 0.9152\n",
      "Iteration 1986: Weights = [55.30333321  3.25100545  7.7855124   0.12805117  0.24535315 10.8081677 ], Loss = 0.9150\n",
      "Iteration 1987: Weights = [55.30333322  3.2507954   7.78500937  0.1280429   0.2453373  10.8087096 ], Loss = 0.9149\n",
      "Iteration 1988: Weights = [55.30333322  3.25058536  7.78450636  0.12803462  0.24532145 10.80925146], Loss = 0.9148\n",
      "Iteration 1989: Weights = [55.30333322  3.25037533  7.78400339  0.12802635  0.2453056  10.80979329], Loss = 0.9147\n",
      "Iteration 1990: Weights = [55.30333322  3.25016532  7.78350045  0.12801808  0.24528975 10.81033509], Loss = 0.9146\n",
      "Iteration 1991: Weights = [55.30333322  3.24995532  7.78299754  0.12800981  0.2452739  10.81087685], Loss = 0.9144\n",
      "Iteration 1992: Weights = [55.30333322  3.24974533  7.78249467  0.12800153  0.24525805 10.81141858], Loss = 0.9143\n",
      "Iteration 1993: Weights = [55.30333322  3.24953536  7.78199182  0.12799326  0.2452422  10.81196027], Loss = 0.9142\n",
      "Iteration 1994: Weights = [55.30333322  3.2493254   7.78148901  0.12798499  0.24522636 10.81250192], Loss = 0.9141\n",
      "Iteration 1995: Weights = [55.30333322  3.24911545  7.78098624  0.12797673  0.24521051 10.81304354], Loss = 0.9140\n",
      "Iteration 1996: Weights = [55.30333323  3.24890552  7.78048349  0.12796846  0.24519467 10.81358513], Loss = 0.9139\n",
      "Iteration 1997: Weights = [55.30333323  3.2486956   7.77998078  0.12796019  0.24517883 10.81412668], Loss = 0.9137\n",
      "Iteration 1998: Weights = [55.30333323  3.2484857   7.7794781   0.12795192  0.24516299 10.81466819], Loss = 0.9136\n",
      "Iteration 1999: Weights = [55.30333323  3.24827581  7.77897545  0.12794365  0.24514715 10.81520967], Loss = 0.9135\n",
      "Iteration 2000: Weights = [55.30333323  3.24806593  7.77847283  0.12793539  0.24513131 10.81575112], Loss = 0.9134\n",
      "Iteration 2001: Weights = [55.30333323  3.24785607  7.77797025  0.12792712  0.24511547 10.81629253], Loss = 0.9133\n",
      "Iteration 2002: Weights = [55.30333323  3.24764622  7.7774677   0.12791885  0.24509963 10.81683391], Loss = 0.9131\n",
      "Iteration 2003: Weights = [55.30333323  3.24743638  7.77696518  0.12791059  0.24508379 10.81737525], Loss = 0.9130\n",
      "Iteration 2004: Weights = [55.30333323  3.24722656  7.7764627   0.12790232  0.24506796 10.81791655], Loss = 0.9129\n",
      "Iteration 2005: Weights = [55.30333324  3.24701675  7.77596025  0.12789406  0.24505212 10.81845782], Loss = 0.9128\n",
      "Iteration 2006: Weights = [55.30333324  3.24680695  7.77545782  0.1278858   0.24503629 10.81899906], Loss = 0.9127\n",
      "Iteration 2007: Weights = [55.30333324  3.24659717  7.77495544  0.12787753  0.24502046 10.81954026], Loss = 0.9126\n",
      "Iteration 2008: Weights = [55.30333324  3.2463874   7.77445308  0.12786927  0.24500463 10.82008142], Loss = 0.9124\n",
      "Iteration 2009: Weights = [55.30333324  3.24617764  7.77395076  0.12786101  0.2449888  10.82062255], Loss = 0.9123\n",
      "Iteration 2010: Weights = [55.30333324  3.2459679   7.77344847  0.12785275  0.24497297 10.82116365], Loss = 0.9122\n",
      "Iteration 2011: Weights = [55.30333324  3.24575817  7.77294621  0.12784449  0.24495714 10.82170471], Loss = 0.9121\n",
      "Iteration 2012: Weights = [55.30333324  3.24554846  7.77244398  0.12783623  0.24494131 10.82224574], Loss = 0.9120\n",
      "Iteration 2013: Weights = [55.30333324  3.24533876  7.77194179  0.12782797  0.24492549 10.82278673], Loss = 0.9118\n",
      "Iteration 2014: Weights = [55.30333324  3.24512907  7.77143963  0.12781971  0.24490966 10.82332768], Loss = 0.9117\n",
      "Iteration 2015: Weights = [55.30333324  3.24491939  7.7709375   0.12781145  0.24489384 10.82386861], Loss = 0.9116\n",
      "Iteration 2016: Weights = [55.30333325  3.24470973  7.77043541  0.12780319  0.24487801 10.82440949], Loss = 0.9115\n",
      "Iteration 2017: Weights = [55.30333325  3.24450009  7.76993334  0.12779493  0.24486219 10.82495034], Loss = 0.9114\n",
      "Iteration 2018: Weights = [55.30333325  3.24429045  7.76943131  0.12778668  0.24484637 10.82549116], Loss = 0.9113\n",
      "Iteration 2019: Weights = [55.30333325  3.24408083  7.76892931  0.12777842  0.24483055 10.82603194], Loss = 0.9111\n",
      "Iteration 2020: Weights = [55.30333325  3.24387122  7.76842735  0.12777016  0.24481473 10.82657269], Loss = 0.9110\n",
      "Iteration 2021: Weights = [55.30333325  3.24366163  7.76792541  0.12776191  0.24479891 10.8271134 ], Loss = 0.9109\n",
      "Iteration 2022: Weights = [55.30333325  3.24345205  7.76742351  0.12775365  0.2447831  10.82765407], Loss = 0.9108\n",
      "Iteration 2023: Weights = [55.30333325  3.24324249  7.76692164  0.1277454   0.24476728 10.82819472], Loss = 0.9107\n",
      "Iteration 2024: Weights = [55.30333325  3.24303293  7.76641981  0.12773715  0.24475147 10.82873532], Loss = 0.9106\n",
      "Iteration 2025: Weights = [55.30333325  3.2428234   7.765918    0.12772889  0.24473565 10.82927589], Loss = 0.9104\n",
      "Iteration 2026: Weights = [55.30333325  3.24261387  7.76541623  0.12772064  0.24471984 10.82981643], Loss = 0.9103\n",
      "Iteration 2027: Weights = [55.30333325  3.24240436  7.76491449  0.12771239  0.24470403 10.83035693], Loss = 0.9102\n",
      "Iteration 2028: Weights = [55.30333326  3.24219486  7.76441278  0.12770414  0.24468822 10.8308974 ], Loss = 0.9101\n",
      "Iteration 2029: Weights = [55.30333326  3.24198538  7.76391111  0.12769588  0.24467241 10.83143783], Loss = 0.9100\n",
      "Iteration 2030: Weights = [55.30333326  3.2417759   7.76340947  0.12768763  0.2446566  10.83197823], Loss = 0.9098\n",
      "Iteration 2031: Weights = [55.30333326  3.24156645  7.76290786  0.12767938  0.24464079 10.83251859], Loss = 0.9097\n",
      "Iteration 2032: Weights = [55.30333326  3.241357    7.76240628  0.12767113  0.24462498 10.83305892], Loss = 0.9096\n",
      "Iteration 2033: Weights = [55.30333326  3.24114757  7.76190474  0.12766288  0.24460918 10.83359921], Loss = 0.9095\n",
      "Iteration 2034: Weights = [55.30333326  3.24093815  7.76140323  0.12765464  0.24459337 10.83413947], Loss = 0.9094\n",
      "Iteration 2035: Weights = [55.30333326  3.24072875  7.76090175  0.12764639  0.24457757 10.83467969], Loss = 0.9093\n",
      "Iteration 2036: Weights = [55.30333326  3.24051936  7.7604003   0.12763814  0.24456177 10.83521988], Loss = 0.9091\n",
      "Iteration 2037: Weights = [55.30333326  3.24030998  7.75989888  0.12762989  0.24454597 10.83576003], Loss = 0.9090\n",
      "Iteration 2038: Weights = [55.30333326  3.24010062  7.7593975   0.12762165  0.24453016 10.83630015], Loss = 0.9089\n",
      "Iteration 2039: Weights = [55.30333326  3.23989127  7.75889615  0.1276134   0.24451437 10.83684023], Loss = 0.9088\n",
      "Iteration 2040: Weights = [55.30333326  3.23968194  7.75839483  0.12760516  0.24449857 10.83738028], Loss = 0.9087\n",
      "Iteration 2041: Weights = [55.30333327  3.23947261  7.75789355  0.12759691  0.24448277 10.83792029], Loss = 0.9086\n",
      "Iteration 2042: Weights = [55.30333327  3.23926331  7.75739229  0.12758867  0.24446697 10.83846027], Loss = 0.9084\n",
      "Iteration 2043: Weights = [55.30333327  3.23905401  7.75689107  0.12758042  0.24445118 10.83900021], Loss = 0.9083\n",
      "Iteration 2044: Weights = [55.30333327  3.23884473  7.75638988  0.12757218  0.24443538 10.83954012], Loss = 0.9082\n",
      "Iteration 2045: Weights = [55.30333327  3.23863546  7.75588873  0.12756394  0.24441959 10.84007999], Loss = 0.9081\n",
      "Iteration 2046: Weights = [55.30333327  3.2384262   7.7553876   0.1275557   0.2444038  10.84061983], Loss = 0.9080\n",
      "Iteration 2047: Weights = [55.30333327  3.23821696  7.75488651  0.12754745  0.24438801 10.84115964], Loss = 0.9078\n",
      "Iteration 2048: Weights = [55.30333327  3.23800774  7.75438545  0.12753921  0.24437221 10.8416994 ], Loss = 0.9077\n",
      "Iteration 2049: Weights = [55.30333327  3.23779852  7.75388443  0.12753097  0.24435643 10.84223914], Loss = 0.9076\n",
      "Iteration 2050: Weights = [55.30333327  3.23758932  7.75338343  0.12752273  0.24434064 10.84277884], Loss = 0.9075\n",
      "Iteration 2051: Weights = [55.30333327  3.23738013  7.75288247  0.12751449  0.24432485 10.8433185 ], Loss = 0.9074\n",
      "Iteration 2052: Weights = [55.30333327  3.23717096  7.75238154  0.12750625  0.24430906 10.84385813], Loss = 0.9073\n",
      "Iteration 2053: Weights = [55.30333327  3.2369618   7.75188065  0.12749801  0.24429328 10.84439773], Loss = 0.9071\n",
      "Iteration 2054: Weights = [55.30333327  3.23675265  7.75137978  0.12748978  0.24427749 10.84493728], Loss = 0.9070\n",
      "Iteration 2055: Weights = [55.30333327  3.23654352  7.75087895  0.12748154  0.24426171 10.84547681], Loss = 0.9069\n",
      "Iteration 2056: Weights = [55.30333327  3.2363344   7.75037815  0.1274733   0.24424593 10.8460163 ], Loss = 0.9068\n",
      "Iteration 2057: Weights = [55.30333328  3.23612529  7.74987738  0.12746507  0.24423015 10.84655575], Loss = 0.9067\n",
      "Iteration 2058: Weights = [55.30333328  3.2359162   7.74937665  0.12745683  0.24421437 10.84709517], Loss = 0.9066\n",
      "Iteration 2059: Weights = [55.30333328  3.23570712  7.74887594  0.1274486   0.24419859 10.84763456], Loss = 0.9064\n",
      "Iteration 2060: Weights = [55.30333328  3.23549806  7.74837527  0.12744036  0.24418281 10.84817391], Loss = 0.9063\n",
      "Iteration 2061: Weights = [55.30333328  3.23528901  7.74787464  0.12743213  0.24416703 10.84871323], Loss = 0.9062\n",
      "Iteration 2062: Weights = [55.30333328  3.23507997  7.74737403  0.12742389  0.24415126 10.84925251], Loss = 0.9061\n",
      "Iteration 2063: Weights = [55.30333328  3.23487094  7.74687346  0.12741566  0.24413548 10.84979175], Loss = 0.9060\n",
      "Iteration 2064: Weights = [55.30333328  3.23466193  7.74637292  0.12740743  0.24411971 10.85033096], Loss = 0.9059\n",
      "Iteration 2065: Weights = [55.30333328  3.23445293  7.74587241  0.1273992   0.24410393 10.85087014], Loss = 0.9057\n",
      "Iteration 2066: Weights = [55.30333328  3.23424395  7.74537193  0.12739096  0.24408816 10.85140928], Loss = 0.9056\n",
      "Iteration 2067: Weights = [55.30333328  3.23403498  7.74487149  0.12738273  0.24407239 10.85194839], Loss = 0.9055\n",
      "Iteration 2068: Weights = [55.30333328  3.23382602  7.74437107  0.1273745   0.24405662 10.85248746], Loss = 0.9054\n",
      "Iteration 2069: Weights = [55.30333328  3.23361708  7.7438707   0.12736627  0.24404085 10.8530265 ], Loss = 0.9053\n",
      "Iteration 2070: Weights = [55.30333328  3.23340815  7.74337035  0.12735804  0.24402508 10.8535655 ], Loss = 0.9052\n",
      "Iteration 2071: Weights = [55.30333328  3.23319923  7.74287003  0.12734981  0.24400932 10.85410447], Loss = 0.9050\n",
      "Iteration 2072: Weights = [55.30333328  3.23299032  7.74236975  0.12734159  0.24399355 10.8546434 ], Loss = 0.9049\n",
      "Iteration 2073: Weights = [55.30333328  3.23278143  7.7418695   0.12733336  0.24397779 10.8551823 ], Loss = 0.9048\n",
      "Iteration 2074: Weights = [55.30333328  3.23257256  7.74136928  0.12732513  0.24396202 10.85572116], Loss = 0.9047\n",
      "Iteration 2075: Weights = [55.30333328  3.2323637   7.7408691   0.1273169   0.24394626 10.85625999], Loss = 0.9046\n",
      "Iteration 2076: Weights = [55.30333329  3.23215485  7.74036894  0.12730868  0.2439305  10.85679878], Loss = 0.9045\n",
      "Iteration 2077: Weights = [55.30333329  3.23194601  7.73986882  0.12730045  0.24391474 10.85733754], Loss = 0.9043\n",
      "Iteration 2078: Weights = [55.30333329  3.23173719  7.73936874  0.12729223  0.24389898 10.85787626], Loss = 0.9042\n",
      "Iteration 2079: Weights = [55.30333329  3.23152838  7.73886868  0.127284    0.24388322 10.85841495], Loss = 0.9041\n",
      "Iteration 2080: Weights = [55.30333329  3.23131958  7.73836866  0.12727578  0.24386746 10.8589536 ], Loss = 0.9040\n",
      "Iteration 2081: Weights = [55.30333329  3.2311108   7.73786866  0.12726755  0.2438517  10.85949222], Loss = 0.9039\n",
      "Iteration 2082: Weights = [55.30333329  3.23090203  7.7373687   0.12725933  0.24383595 10.86003081], Loss = 0.9038\n",
      "Iteration 2083: Weights = [55.30333329  3.23069328  7.73686878  0.12725111  0.24382019 10.86056936], Loss = 0.9036\n",
      "Iteration 2084: Weights = [55.30333329  3.23048454  7.73636888  0.12724289  0.24380444 10.86110787], Loss = 0.9035\n",
      "Iteration 2085: Weights = [55.30333329  3.23027581  7.73586902  0.12723467  0.24378869 10.86164635], Loss = 0.9034\n",
      "Iteration 2086: Weights = [55.30333329  3.23006709  7.73536919  0.12722644  0.24377293 10.8621848 ], Loss = 0.9033\n",
      "Iteration 2087: Weights = [55.30333329  3.22985839  7.73486939  0.12721822  0.24375718 10.86272321], Loss = 0.9032\n",
      "Iteration 2088: Weights = [55.30333329  3.2296497   7.73436963  0.12721     0.24374143 10.86326158], Loss = 0.9031\n",
      "Iteration 2089: Weights = [55.30333329  3.22944103  7.73386989  0.12720179  0.24372569 10.86379992], Loss = 0.9029\n",
      "Iteration 2090: Weights = [55.30333329  3.22923237  7.73337019  0.12719357  0.24370994 10.86433823], Loss = 0.9028\n",
      "Iteration 2091: Weights = [55.30333329  3.22902372  7.73287052  0.12718535  0.24369419 10.8648765 ], Loss = 0.9027\n",
      "Iteration 2092: Weights = [55.30333329  3.22881509  7.73237089  0.12717713  0.24367845 10.86541474], Loss = 0.9026\n",
      "Iteration 2093: Weights = [55.30333329  3.22860647  7.73187128  0.12716891  0.2436627  10.86595294], Loss = 0.9025\n",
      "Iteration 2094: Weights = [55.30333329  3.22839786  7.73137171  0.1271607   0.24364696 10.86649111], Loss = 0.9024\n",
      "Iteration 2095: Weights = [55.30333329  3.22818927  7.73087217  0.12715248  0.24363122 10.86702924], Loss = 0.9022\n",
      "Iteration 2096: Weights = [55.30333329  3.22798069  7.73037267  0.12714427  0.24361547 10.86756734], Loss = 0.9021\n",
      "Iteration 2097: Weights = [55.30333329  3.22777212  7.72987319  0.12713605  0.24359973 10.8681054 ], Loss = 0.9020\n",
      "Iteration 2098: Weights = [55.30333329  3.22756357  7.72937375  0.12712784  0.24358399 10.86864343], Loss = 0.9019\n",
      "Iteration 2099: Weights = [55.3033333   3.22735503  7.72887434  0.12711962  0.24356826 10.86918142], Loss = 0.9018\n",
      "Iteration 2100: Weights = [55.3033333   3.2271465   7.72837496  0.12711141  0.24355252 10.86971938], Loss = 0.9017\n",
      "Iteration 2101: Weights = [55.3033333   3.22693799  7.72787561  0.1271032   0.24353678 10.8702573 ], Loss = 0.9015\n",
      "Iteration 2102: Weights = [55.3033333   3.22672949  7.7273763   0.12709498  0.24352105 10.87079519], Loss = 0.9014\n",
      "Iteration 2103: Weights = [55.3033333   3.22652101  7.72687702  0.12708677  0.24350531 10.87133304], Loss = 0.9013\n",
      "Iteration 2104: Weights = [55.3033333   3.22631253  7.72637777  0.12707856  0.24348958 10.87187086], Loss = 0.9012\n",
      "Iteration 2105: Weights = [55.3033333   3.22610408  7.72587855  0.12707035  0.24347385 10.87240865], Loss = 0.9011\n",
      "Iteration 2106: Weights = [55.3033333   3.22589563  7.72537937  0.12706214  0.24345811 10.8729464 ], Loss = 0.9010\n",
      "Iteration 2107: Weights = [55.3033333   3.2256872   7.72488022  0.12705393  0.24344238 10.87348411], Loss = 0.9008\n",
      "Iteration 2108: Weights = [55.3033333   3.22547878  7.7243811   0.12704572  0.24342666 10.87402179], Loss = 0.9007\n",
      "Iteration 2109: Weights = [55.3033333   3.22527038  7.72388201  0.12703751  0.24341093 10.87455944], Loss = 0.9006\n",
      "Iteration 2110: Weights = [55.3033333   3.22506199  7.72338295  0.1270293   0.2433952  10.87509705], Loss = 0.9005\n",
      "Iteration 2111: Weights = [55.3033333   3.22485361  7.72288393  0.1270211   0.24337947 10.87563462], Loss = 0.9004\n",
      "Iteration 2112: Weights = [55.3033333   3.22464524  7.72238494  0.12701289  0.24336375 10.87617217], Loss = 0.9003\n",
      "Iteration 2113: Weights = [55.3033333   3.22443689  7.72188598  0.12700468  0.24334802 10.87670967], Loss = 0.9001\n",
      "Iteration 2114: Weights = [55.3033333   3.22422856  7.72138705  0.12699648  0.2433323  10.87724714], Loss = 0.9000\n",
      "Iteration 2115: Weights = [55.3033333   3.22402023  7.72088816  0.12698827  0.24331658 10.87778458], Loss = 0.8999\n",
      "Iteration 2116: Weights = [55.3033333   3.22381192  7.7203893   0.12698007  0.24330086 10.87832198], Loss = 0.8998\n",
      "Iteration 2117: Weights = [55.3033333   3.22360362  7.71989047  0.12697186  0.24328514 10.87885935], Loss = 0.8997\n",
      "Iteration 2118: Weights = [55.3033333   3.22339534  7.71939167  0.12696366  0.24326942 10.87939668], Loss = 0.8996\n",
      "Iteration 2119: Weights = [55.3033333   3.22318707  7.7188929   0.12695545  0.2432537  10.87993398], Loss = 0.8994\n",
      "Iteration 2120: Weights = [55.3033333   3.22297881  7.71839417  0.12694725  0.24323798 10.88047125], Loss = 0.8993\n",
      "Iteration 2121: Weights = [55.3033333   3.22277057  7.71789547  0.12693905  0.24322227 10.88100848], Loss = 0.8992\n",
      "Iteration 2122: Weights = [55.3033333   3.22256234  7.7173968   0.12693085  0.24320655 10.88154567], Loss = 0.8991\n",
      "Iteration 2123: Weights = [55.3033333   3.22235413  7.71689816  0.12692265  0.24319084 10.88208283], Loss = 0.8990\n",
      "Iteration 2124: Weights = [55.3033333   3.22214592  7.71639956  0.12691444  0.24317512 10.88261995], Loss = 0.8989\n",
      "Iteration 2125: Weights = [55.3033333   3.22193773  7.71590099  0.12690624  0.24315941 10.88315704], Loss = 0.8987\n",
      "Iteration 2126: Weights = [55.3033333   3.22172956  7.71540245  0.12689804  0.2431437  10.8836941 ], Loss = 0.8986\n",
      "Iteration 2127: Weights = [55.3033333   3.22152139  7.71490394  0.12688985  0.24312799 10.88423112], Loss = 0.8985\n",
      "Iteration 2128: Weights = [55.3033333   3.22131325  7.71440547  0.12688165  0.24311228 10.88476811], Loss = 0.8984\n",
      "Iteration 2129: Weights = [55.30333331  3.22110511  7.71390702  0.12687345  0.24309657 10.88530506], Loss = 0.8983\n",
      "Iteration 2130: Weights = [55.30333331  3.22089699  7.71340861  0.12686525  0.24308087 10.88584197], Loss = 0.8982\n",
      "Iteration 2131: Weights = [55.30333331  3.22068888  7.71291023  0.12685705  0.24306516 10.88637885], Loss = 0.8980\n",
      "Iteration 2132: Weights = [55.30333331  3.22048078  7.71241189  0.12684886  0.24304946 10.8869157 ], Loss = 0.8979\n",
      "Iteration 2133: Weights = [55.30333331  3.2202727   7.71191357  0.12684066  0.24303375 10.88745251], Loss = 0.8978\n",
      "Iteration 2134: Weights = [55.30333331  3.22006463  7.71141529  0.12683247  0.24301805 10.88798929], Loss = 0.8977\n",
      "Iteration 2135: Weights = [55.30333331  3.21985658  7.71091704  0.12682427  0.24300235 10.88852604], Loss = 0.8976\n",
      "Iteration 2136: Weights = [55.30333331  3.21964854  7.71041882  0.12681608  0.24298665 10.88906274], Loss = 0.8975\n",
      "Iteration 2137: Weights = [55.30333331  3.21944051  7.70992064  0.12680788  0.24297095 10.88959942], Loss = 0.8974\n",
      "Iteration 2138: Weights = [55.30333331  3.2192325   7.70942248  0.12679969  0.24295525 10.89013606], Loss = 0.8972\n",
      "Iteration 2139: Weights = [55.30333331  3.21902449  7.70892436  0.1267915   0.24293955 10.89067266], Loss = 0.8971\n",
      "Iteration 2140: Weights = [55.30333331  3.21881651  7.70842627  0.1267833   0.24292385 10.89120923], Loss = 0.8970\n",
      "Iteration 2141: Weights = [55.30333331  3.21860853  7.70792821  0.12677511  0.24290816 10.89174577], Loss = 0.8969\n",
      "Iteration 2142: Weights = [55.30333331  3.21840057  7.70743019  0.12676692  0.24289246 10.89228227], Loss = 0.8968\n",
      "Iteration 2143: Weights = [55.30333331  3.21819262  7.7069322   0.12675873  0.24287677 10.89281873], Loss = 0.8967\n",
      "Iteration 2144: Weights = [55.30333331  3.21798469  7.70643424  0.12675054  0.24286108 10.89335516], Loss = 0.8965\n",
      "Iteration 2145: Weights = [55.30333331  3.21777677  7.70593631  0.12674235  0.24284538 10.89389156], Loss = 0.8964\n",
      "Iteration 2146: Weights = [55.30333331  3.21756886  7.70543841  0.12673416  0.24282969 10.89442792], Loss = 0.8963\n",
      "Iteration 2147: Weights = [55.30333331  3.21736097  7.70494055  0.12672597  0.242814   10.89496425], Loss = 0.8962\n",
      "Iteration 2148: Weights = [55.30333331  3.21715309  7.70444272  0.12671779  0.24279832 10.89550054], Loss = 0.8961\n",
      "Iteration 2149: Weights = [55.30333331  3.21694522  7.70394492  0.1267096   0.24278263 10.8960368 ], Loss = 0.8960\n",
      "Iteration 2150: Weights = [55.30333331  3.21673737  7.70344715  0.12670141  0.24276694 10.89657302], Loss = 0.8958\n",
      "Iteration 2151: Weights = [55.30333331  3.21652953  7.70294942  0.12669322  0.24275126 10.89710921], Loss = 0.8957\n",
      "Iteration 2152: Weights = [55.30333331  3.2163217   7.70245171  0.12668504  0.24273557 10.89764536], Loss = 0.8956\n",
      "Iteration 2153: Weights = [55.30333331  3.21611389  7.70195404  0.12667685  0.24271989 10.89818148], Loss = 0.8955\n",
      "Iteration 2154: Weights = [55.30333331  3.21590609  7.7014564   0.12666867  0.2427042  10.89871757], Loss = 0.8954\n",
      "Iteration 2155: Weights = [55.30333331  3.2156983   7.7009588   0.12666048  0.24268852 10.89925362], Loss = 0.8953\n",
      "Iteration 2156: Weights = [55.30333331  3.21549053  7.70046122  0.1266523   0.24267284 10.89978963], Loss = 0.8952\n",
      "Iteration 2157: Weights = [55.30333331  3.21528277  7.69996368  0.12664412  0.24265716 10.90032561], Loss = 0.8950\n",
      "Iteration 2158: Weights = [55.30333331  3.21507503  7.69946617  0.12663593  0.24264148 10.90086156], Loss = 0.8949\n",
      "Iteration 2159: Weights = [55.30333331  3.21486729  7.69896869  0.12662775  0.24262581 10.90139747], Loss = 0.8948\n",
      "Iteration 2160: Weights = [55.30333331  3.21465957  7.69847125  0.12661957  0.24261013 10.90193335], Loss = 0.8947\n",
      "Iteration 2161: Weights = [55.30333331  3.21445187  7.69797383  0.12661139  0.24259445 10.90246919], Loss = 0.8946\n",
      "Iteration 2162: Weights = [55.30333331  3.21424418  7.69747645  0.12660321  0.24257878 10.903005  ], Loss = 0.8945\n",
      "Iteration 2163: Weights = [55.30333331  3.2140365   7.6969791   0.12659503  0.24256311 10.90354077], Loss = 0.8943\n",
      "Iteration 2164: Weights = [55.30333331  3.21382883  7.69648178  0.12658685  0.24254743 10.90407651], Loss = 0.8942\n",
      "Iteration 2165: Weights = [55.30333331  3.21362118  7.6959845   0.12657867  0.24253176 10.90461221], Loss = 0.8941\n",
      "Iteration 2166: Weights = [55.30333331  3.21341354  7.69548725  0.12657049  0.24251609 10.90514788], Loss = 0.8940\n",
      "Iteration 2167: Weights = [55.30333331  3.21320592  7.69499002  0.12656231  0.24250042 10.90568351], Loss = 0.8939\n",
      "Iteration 2168: Weights = [55.30333331  3.21299831  7.69449284  0.12655414  0.24248475 10.90621911], Loss = 0.8938\n",
      "Iteration 2169: Weights = [55.30333331  3.21279071  7.69399568  0.12654596  0.24246909 10.90675468], Loss = 0.8936\n",
      "Iteration 2170: Weights = [55.30333331  3.21258312  7.69349855  0.12653778  0.24245342 10.90729021], Loss = 0.8935\n",
      "Iteration 2171: Weights = [55.30333331  3.21237555  7.69300146  0.12652961  0.24243776 10.90782571], Loss = 0.8934\n",
      "Iteration 2172: Weights = [55.30333332  3.21216799  7.6925044   0.12652143  0.24242209 10.90836117], Loss = 0.8933\n",
      "Iteration 2173: Weights = [55.30333332  3.21196045  7.69200737  0.12651326  0.24240643 10.90889659], Loss = 0.8932\n",
      "Iteration 2174: Weights = [55.30333332  3.21175292  7.69151038  0.12650508  0.24239077 10.90943199], Loss = 0.8931\n",
      "Iteration 2175: Weights = [55.30333332  3.2115454   7.69101341  0.12649691  0.2423751  10.90996734], Loss = 0.8930\n",
      "Iteration 2176: Weights = [55.30333332  3.2113379   7.69051648  0.12648874  0.24235944 10.91050267], Loss = 0.8928\n",
      "Iteration 2177: Weights = [55.30333332  3.2111304   7.69001958  0.12648056  0.24234378 10.91103796], Loss = 0.8927\n",
      "Iteration 2178: Weights = [55.30333332  3.21092293  7.68952271  0.12647239  0.24232813 10.91157321], Loss = 0.8926\n",
      "Iteration 2179: Weights = [55.30333332  3.21071546  7.68902588  0.12646422  0.24231247 10.91210843], Loss = 0.8925\n",
      "Iteration 2180: Weights = [55.30333332  3.21050801  7.68852908  0.12645605  0.24229681 10.91264361], Loss = 0.8924\n",
      "Iteration 2181: Weights = [55.30333332  3.21030057  7.6880323   0.12644788  0.24228116 10.91317876], Loss = 0.8923\n",
      "Iteration 2182: Weights = [55.30333332  3.21009315  7.68753556  0.12643971  0.2422655  10.91371388], Loss = 0.8921\n",
      "Iteration 2183: Weights = [55.30333332  3.20988574  7.68703886  0.12643154  0.24224985 10.91424896], Loss = 0.8920\n",
      "Iteration 2184: Weights = [55.30333332  3.20967834  7.68654218  0.12642337  0.2422342  10.91478401], Loss = 0.8919\n",
      "Iteration 2185: Weights = [55.30333332  3.20947096  7.68604554  0.1264152   0.24221855 10.91531902], Loss = 0.8918\n",
      "Iteration 2186: Weights = [55.30333332  3.20926359  7.68554893  0.12640703  0.2422029  10.915854  ], Loss = 0.8917\n",
      "Iteration 2187: Weights = [55.30333332  3.20905623  7.68505235  0.12639887  0.24218725 10.91638894], Loss = 0.8916\n",
      "Iteration 2188: Weights = [55.30333332  3.20884889  7.6845558   0.1263907   0.2421716  10.91692385], Loss = 0.8915\n",
      "Iteration 2189: Weights = [55.30333332  3.20864156  7.68405929  0.12638253  0.24215595 10.91745872], Loss = 0.8913\n",
      "Iteration 2190: Weights = [55.30333332  3.20843424  7.68356281  0.12637437  0.2421403  10.91799356], Loss = 0.8912\n",
      "Iteration 2191: Weights = [55.30333332  3.20822694  7.68306636  0.1263662   0.24212466 10.91852836], Loss = 0.8911\n",
      "Iteration 2192: Weights = [55.30333332  3.20801965  7.68256994  0.12635804  0.24210902 10.91906313], Loss = 0.8910\n",
      "Iteration 2193: Weights = [55.30333332  3.20781237  7.68207355  0.12634987  0.24209337 10.91959787], Loss = 0.8909\n",
      "Iteration 2194: Weights = [55.30333332  3.20760511  7.6815772   0.12634171  0.24207773 10.92013257], Loss = 0.8908\n",
      "Iteration 2195: Weights = [55.30333332  3.20739786  7.68108088  0.12633355  0.24206209 10.92066724], Loss = 0.8907\n",
      "Iteration 2196: Weights = [55.30333332  3.20719062  7.68058459  0.12632538  0.24204645 10.92120187], Loss = 0.8905\n",
      "Iteration 2197: Weights = [55.30333332  3.2069834   7.68008833  0.12631722  0.24203081 10.92173646], Loss = 0.8904\n",
      "Iteration 2198: Weights = [55.30333332  3.20677619  7.6795921   0.12630906  0.24201517 10.92227103], Loss = 0.8903\n",
      "Iteration 2199: Weights = [55.30333332  3.20656899  7.67909591  0.1263009   0.24199953 10.92280555], Loss = 0.8902\n",
      "Iteration 2200: Weights = [55.30333332  3.20636181  7.67859975  0.12629274  0.2419839  10.92334005], Loss = 0.8901\n",
      "Iteration 2201: Weights = [55.30333332  3.20615464  7.67810362  0.12628458  0.24196826 10.92387451], Loss = 0.8900\n",
      "Iteration 2202: Weights = [55.30333332  3.20594748  7.67760752  0.12627642  0.24195263 10.92440893], Loss = 0.8898\n",
      "Iteration 2203: Weights = [55.30333332  3.20574034  7.67711145  0.12626826  0.241937   10.92494332], Loss = 0.8897\n",
      "Iteration 2204: Weights = [55.30333332  3.20553321  7.67661542  0.1262601   0.24192136 10.92547768], Loss = 0.8896\n",
      "Iteration 2205: Weights = [55.30333332  3.2053261   7.67611942  0.12625194  0.24190573 10.926012  ], Loss = 0.8895\n",
      "Iteration 2206: Weights = [55.30333332  3.20511899  7.67562345  0.12624378  0.2418901  10.92654628], Loss = 0.8894\n",
      "Iteration 2207: Weights = [55.30333332  3.2049119   7.67512751  0.12623563  0.24187447 10.92708054], Loss = 0.8893\n",
      "Iteration 2208: Weights = [55.30333332  3.20470483  7.6746316   0.12622747  0.24185885 10.92761475], Loss = 0.8892\n",
      "Iteration 2209: Weights = [55.30333332  3.20449777  7.67413573  0.12621932  0.24184322 10.92814894], Loss = 0.8890\n",
      "Iteration 2210: Weights = [55.30333332  3.20429072  7.67363989  0.12621116  0.24182759 10.92868308], Loss = 0.8889\n",
      "Iteration 2211: Weights = [55.30333332  3.20408368  7.67314408  0.12620301  0.24181197 10.9292172 ], Loss = 0.8888\n",
      "Iteration 2212: Weights = [55.30333332  3.20387666  7.6726483   0.12619485  0.24179634 10.92975128], Loss = 0.8887\n",
      "Iteration 2213: Weights = [55.30333332  3.20366965  7.67215256  0.1261867   0.24178072 10.93028532], Loss = 0.8886\n",
      "Iteration 2214: Weights = [55.30333332  3.20346265  7.67165685  0.12617854  0.2417651  10.93081933], Loss = 0.8885\n",
      "Iteration 2215: Weights = [55.30333332  3.20325567  7.67116116  0.12617039  0.24174948 10.93135331], Loss = 0.8884\n",
      "Iteration 2216: Weights = [55.30333332  3.2030487   7.67066551  0.12616224  0.24173386 10.93188725], Loss = 0.8882\n",
      "Iteration 2217: Weights = [55.30333332  3.20284175  7.6701699   0.12615409  0.24171824 10.93242116], Loss = 0.8881\n",
      "Iteration 2218: Weights = [55.30333332  3.20263481  7.66967431  0.12614594  0.24170262 10.93295503], Loss = 0.8880\n",
      "Iteration 2219: Weights = [55.30333332  3.20242788  7.66917876  0.12613779  0.24168701 10.93348887], Loss = 0.8879\n",
      "Iteration 2220: Weights = [55.30333332  3.20222096  7.66868324  0.12612964  0.24167139 10.93402267], Loss = 0.8878\n",
      "Iteration 2221: Weights = [55.30333332  3.20201406  7.66818775  0.12612149  0.24165577 10.93455644], Loss = 0.8877\n",
      "Iteration 2222: Weights = [55.30333332  3.20180717  7.66769229  0.12611334  0.24164016 10.93509017], Loss = 0.8875\n",
      "Iteration 2223: Weights = [55.30333332  3.2016003   7.66719687  0.12610519  0.24162455 10.93562387], Loss = 0.8874\n",
      "Iteration 2224: Weights = [55.30333332  3.20139343  7.66670147  0.12609704  0.24160894 10.93615754], Loss = 0.8873\n",
      "Iteration 2225: Weights = [55.30333332  3.20118659  7.66620611  0.12608889  0.24159332 10.93669117], Loss = 0.8872\n",
      "Iteration 2226: Weights = [55.30333332  3.20097975  7.66571078  0.12608075  0.24157772 10.93722476], Loss = 0.8871\n",
      "Iteration 2227: Weights = [55.30333332  3.20077293  7.66521549  0.1260726   0.24156211 10.93775833], Loss = 0.8870\n",
      "Iteration 2228: Weights = [55.30333332  3.20056612  7.66472022  0.12606446  0.2415465  10.93829185], Loss = 0.8869\n",
      "Iteration 2229: Weights = [55.30333332  3.20035933  7.66422499  0.12605631  0.24153089 10.93882535], Loss = 0.8867\n",
      "Iteration 2230: Weights = [55.30333332  3.20015254  7.66372979  0.12604817  0.24151529 10.93935881], Loss = 0.8866\n",
      "Iteration 2231: Weights = [55.30333332  3.19994578  7.66323462  0.12604002  0.24149968 10.93989223], Loss = 0.8865\n",
      "Iteration 2232: Weights = [55.30333332  3.19973902  7.66273948  0.12603188  0.24148408 10.94042562], Loss = 0.8864\n",
      "Iteration 2233: Weights = [55.30333332  3.19953228  7.66224438  0.12602373  0.24146847 10.94095897], Loss = 0.8863\n",
      "Iteration 2234: Weights = [55.30333332  3.19932555  7.6617493   0.12601559  0.24145287 10.94149229], Loss = 0.8862\n",
      "Iteration 2235: Weights = [55.30333332  3.19911884  7.66125426  0.12600745  0.24143727 10.94202558], Loss = 0.8861\n",
      "Iteration 2236: Weights = [55.30333332  3.19891214  7.66075925  0.12599931  0.24142167 10.94255883], Loss = 0.8859\n",
      "Iteration 2237: Weights = [55.30333332  3.19870545  7.66026428  0.12599117  0.24140607 10.94309205], Loss = 0.8858\n",
      "Iteration 2238: Weights = [55.30333332  3.19849877  7.65976933  0.12598303  0.24139048 10.94362523], Loss = 0.8857\n",
      "Iteration 2239: Weights = [55.30333332  3.19829211  7.65927442  0.12597489  0.24137488 10.94415838], Loss = 0.8856\n",
      "Iteration 2240: Weights = [55.30333332  3.19808546  7.65877954  0.12596675  0.24135928 10.94469149], Loss = 0.8855\n",
      "Iteration 2241: Weights = [55.30333332  3.19787883  7.65828469  0.12595861  0.24134369 10.94522457], Loss = 0.8854\n",
      "Iteration 2242: Weights = [55.30333332  3.19767221  7.65778987  0.12595047  0.24132809 10.94575762], Loss = 0.8853\n",
      "Iteration 2243: Weights = [55.30333332  3.1974656   7.65729509  0.12594233  0.2413125  10.94629063], Loss = 0.8851\n",
      "Iteration 2244: Weights = [55.30333332  3.197259    7.65680033  0.12593419  0.24129691 10.94682361], Loss = 0.8850\n",
      "Iteration 2245: Weights = [55.30333332  3.19705242  7.65630561  0.12592606  0.24128132 10.94735655], Loss = 0.8849\n",
      "Iteration 2246: Weights = [55.30333332  3.19684585  7.65581092  0.12591792  0.24126573 10.94788945], Loss = 0.8848\n",
      "Iteration 2247: Weights = [55.30333332  3.1966393   7.65531627  0.12590979  0.24125014 10.94842233], Loss = 0.8847\n",
      "Iteration 2248: Weights = [55.30333332  3.19643276  7.65482164  0.12590165  0.24123455 10.94895517], Loss = 0.8846\n",
      "Iteration 2249: Weights = [55.30333332  3.19622623  7.65432705  0.12589352  0.24121897 10.94948797], Loss = 0.8845\n",
      "Iteration 2250: Weights = [55.30333332  3.19601972  7.65383249  0.12588538  0.24120338 10.95002074], Loss = 0.8843\n",
      "Iteration 2251: Weights = [55.30333333  3.19581321  7.65333796  0.12587725  0.2411878  10.95055347], Loss = 0.8842\n",
      "Iteration 2252: Weights = [55.30333333  3.19560673  7.65284346  0.12586911  0.24117221 10.95108617], Loss = 0.8841\n",
      "Iteration 2253: Weights = [55.30333333  3.19540025  7.65234899  0.12586098  0.24115663 10.95161884], Loss = 0.8840\n",
      "Iteration 2254: Weights = [55.30333333  3.19519379  7.65185456  0.12585285  0.24114105 10.95215147], Loss = 0.8839\n",
      "Iteration 2255: Weights = [55.30333333  3.19498734  7.65136016  0.12584472  0.24112547 10.95268407], Loss = 0.8838\n",
      "Iteration 2256: Weights = [55.30333333  3.19478091  7.65086579  0.12583659  0.24110989 10.95321663], Loss = 0.8837\n",
      "Iteration 2257: Weights = [55.30333333  3.19457449  7.65037145  0.12582846  0.24109431 10.95374916], Loss = 0.8835\n",
      "Iteration 2258: Weights = [55.30333333  3.19436808  7.64987714  0.12582033  0.24107873 10.95428166], Loss = 0.8834\n",
      "Iteration 2259: Weights = [55.30333333  3.19416169  7.64938287  0.1258122   0.24106316 10.95481412], Loss = 0.8833\n",
      "Iteration 2260: Weights = [55.30333333  3.1939553   7.64888863  0.12580407  0.24104758 10.95534654], Loss = 0.8832\n",
      "Iteration 2261: Weights = [55.30333333  3.19374894  7.64839442  0.12579594  0.24103201 10.95587893], Loss = 0.8831\n",
      "Iteration 2262: Weights = [55.30333333  3.19354258  7.64790024  0.12578781  0.24101643 10.95641129], Loss = 0.8830\n",
      "Iteration 2263: Weights = [55.30333333  3.19333624  7.64740609  0.12577968  0.24100086 10.95694361], Loss = 0.8829\n",
      "Iteration 2264: Weights = [55.30333333  3.19312991  7.64691198  0.12577156  0.24098529 10.9574759 ], Loss = 0.8827\n",
      "Iteration 2265: Weights = [55.30333333  3.1929236   7.6464179   0.12576343  0.24096972 10.95800815], Loss = 0.8826\n",
      "Iteration 2266: Weights = [55.30333333  3.1927173   7.64592385  0.1257553   0.24095415 10.95854037], Loss = 0.8825\n",
      "Iteration 2267: Weights = [55.30333333  3.19251101  7.64542983  0.12574718  0.24093858 10.95907256], Loss = 0.8824\n",
      "Iteration 2268: Weights = [55.30333333  3.19230473  7.64493584  0.12573905  0.24092301 10.95960471], Loss = 0.8823\n",
      "Iteration 2269: Weights = [55.30333333  3.19209847  7.64444189  0.12573093  0.24090745 10.96013682], Loss = 0.8822\n",
      "Iteration 2270: Weights = [55.30333333  3.19189223  7.64394796  0.12572281  0.24089188 10.9606689 ], Loss = 0.8821\n",
      "Iteration 2271: Weights = [55.30333333  3.19168599  7.64345407  0.12571468  0.24087632 10.96120095], Loss = 0.8819\n",
      "Iteration 2272: Weights = [55.30333333  3.19147977  7.64296021  0.12570656  0.24086075 10.96173296], Loss = 0.8818\n",
      "Iteration 2273: Weights = [55.30333333  3.19127356  7.64246639  0.12569844  0.24084519 10.96226494], Loss = 0.8817\n",
      "Iteration 2274: Weights = [55.30333333  3.19106737  7.64197259  0.12569032  0.24082963 10.96279689], Loss = 0.8816\n",
      "Iteration 2275: Weights = [55.30333333  3.19086119  7.64147883  0.1256822   0.24081407 10.96332879], Loss = 0.8815\n",
      "Iteration 2276: Weights = [55.30333333  3.19065502  7.6409851   0.12567408  0.24079851 10.96386067], Loss = 0.8814\n",
      "Iteration 2277: Weights = [55.30333333  3.19044886  7.6404914   0.12566596  0.24078295 10.96439251], Loss = 0.8813\n",
      "Iteration 2278: Weights = [55.30333333  3.19024272  7.63999773  0.12565784  0.24076739 10.96492432], Loss = 0.8811\n",
      "Iteration 2279: Weights = [55.30333333  3.19003659  7.6395041   0.12564972  0.24075184 10.96545609], Loss = 0.8810\n",
      "Iteration 2280: Weights = [55.30333333  3.18983048  7.63901049  0.1256416   0.24073628 10.96598783], Loss = 0.8809\n",
      "Iteration 2281: Weights = [55.30333333  3.18962438  7.63851692  0.12563348  0.24072073 10.96651953], Loss = 0.8808\n",
      "Iteration 2282: Weights = [55.30333333  3.18941829  7.63802338  0.12562536  0.24070517 10.9670512 ], Loss = 0.8807\n",
      "Iteration 2283: Weights = [55.30333333  3.18921221  7.63752987  0.12561725  0.24068962 10.96758283], Loss = 0.8806\n",
      "Iteration 2284: Weights = [55.30333333  3.18900615  7.6370364   0.12560913  0.24067407 10.96811443], Loss = 0.8805\n",
      "Iteration 2285: Weights = [55.30333333  3.18880011  7.63654295  0.12560101  0.24065852 10.968646  ], Loss = 0.8804\n",
      "Iteration 2286: Weights = [55.30333333  3.18859407  7.63604954  0.1255929   0.24064297 10.96917753], Loss = 0.8802\n",
      "Iteration 2287: Weights = [55.30333333  3.18838805  7.63555616  0.12558478  0.24062742 10.96970903], Loss = 0.8801\n",
      "Iteration 2288: Weights = [55.30333333  3.18818204  7.63506281  0.12557667  0.24061187 10.97024049], Loss = 0.8800\n",
      "Iteration 2289: Weights = [55.30333333  3.18797605  7.63456949  0.12556856  0.24059633 10.97077192], Loss = 0.8799\n",
      "Iteration 2290: Weights = [55.30333333  3.18777006  7.63407621  0.12556044  0.24058078 10.97130332], Loss = 0.8798\n",
      "Iteration 2291: Weights = [55.30333333  3.1875641   7.63358296  0.12555233  0.24056524 10.97183468], Loss = 0.8797\n",
      "Iteration 2292: Weights = [55.30333333  3.18735814  7.63308973  0.12554422  0.24054969 10.972366  ], Loss = 0.8796\n",
      "Iteration 2293: Weights = [55.30333333  3.1871522   7.63259654  0.12553611  0.24053415 10.97289729], Loss = 0.8794\n",
      "Iteration 2294: Weights = [55.30333333  3.18694627  7.63210339  0.12552799  0.24051861 10.97342855], Loss = 0.8793\n",
      "Iteration 2295: Weights = [55.30333333  3.18674036  7.63161026  0.12551988  0.24050307 10.97395977], Loss = 0.8792\n",
      "Iteration 2296: Weights = [55.30333333  3.18653445  7.63111717  0.12551177  0.24048753 10.97449096], Loss = 0.8791\n",
      "Iteration 2297: Weights = [55.30333333  3.18632857  7.63062411  0.12550366  0.24047199 10.97502211], Loss = 0.8790\n",
      "Iteration 2298: Weights = [55.30333333  3.18612269  7.63013108  0.12549556  0.24045645 10.97555323], Loss = 0.8789\n",
      "Iteration 2299: Weights = [55.30333333  3.18591683  7.62963808  0.12548745  0.24044092 10.97608432], Loss = 0.8788\n",
      "Iteration 2300: Weights = [55.30333333  3.18571098  7.62914511  0.12547934  0.24042538 10.97661537], Loss = 0.8786\n",
      "Iteration 2301: Weights = [55.30333333  3.18550515  7.62865218  0.12547123  0.24040985 10.97714639], Loss = 0.8785\n",
      "Iteration 2302: Weights = [55.30333333  3.18529932  7.62815927  0.12546312  0.24039431 10.97767737], Loss = 0.8784\n",
      "Iteration 2303: Weights = [55.30333333  3.18509352  7.6276664   0.12545502  0.24037878 10.97820832], Loss = 0.8783\n",
      "Iteration 2304: Weights = [55.30333333  3.18488772  7.62717357  0.12544691  0.24036325 10.97873923], Loss = 0.8782\n",
      "Iteration 2305: Weights = [55.30333333  3.18468194  7.62668076  0.12543881  0.24034772 10.97927011], Loss = 0.8781\n",
      "Iteration 2306: Weights = [55.30333333  3.18447617  7.62618798  0.1254307   0.24033219 10.97980096], Loss = 0.8780\n",
      "Iteration 2307: Weights = [55.30333333  3.18427041  7.62569524  0.1254226   0.24031666 10.98033177], Loss = 0.8779\n",
      "Iteration 2308: Weights = [55.30333333  3.18406467  7.62520253  0.12541449  0.24030114 10.98086254], Loss = 0.8777\n",
      "Iteration 2309: Weights = [55.30333333  3.18385894  7.62470985  0.12540639  0.24028561 10.98139329], Loss = 0.8776\n",
      "Iteration 2310: Weights = [55.30333333  3.18365323  7.6242172   0.12539829  0.24027008 10.98192399], Loss = 0.8775\n",
      "Iteration 2311: Weights = [55.30333333  3.18344753  7.62372458  0.12539019  0.24025456 10.98245467], Loss = 0.8774\n",
      "Iteration 2312: Weights = [55.30333333  3.18324184  7.623232    0.12538208  0.24023904 10.98298531], Loss = 0.8773\n",
      "Iteration 2313: Weights = [55.30333333  3.18303616  7.62273945  0.12537398  0.24022351 10.98351591], Loss = 0.8772\n",
      "Iteration 2314: Weights = [55.30333333  3.1828305   7.62224693  0.12536588  0.24020799 10.98404648], Loss = 0.8771\n",
      "Iteration 2315: Weights = [55.30333333  3.18262485  7.62175444  0.12535778  0.24019247 10.98457702], Loss = 0.8769\n",
      "Iteration 2316: Weights = [55.30333333  3.18241921  7.62126198  0.12534968  0.24017695 10.98510752], Loss = 0.8768\n",
      "Iteration 2317: Weights = [55.30333333  3.18221359  7.62076956  0.12534158  0.24016143 10.98563799], Loss = 0.8767\n",
      "Iteration 2318: Weights = [55.30333333  3.18200798  7.62027716  0.12533348  0.24014592 10.98616843], Loss = 0.8766\n",
      "Iteration 2319: Weights = [55.30333333  3.18180239  7.6197848   0.12532539  0.2401304  10.98669882], Loss = 0.8765\n",
      "Iteration 2320: Weights = [55.30333333  3.1815968   7.61929247  0.12531729  0.24011489 10.98722919], Loss = 0.8764\n",
      "Iteration 2321: Weights = [55.30333333  3.18139123  7.61880018  0.12530919  0.24009937 10.98775952], Loss = 0.8763\n",
      "Iteration 2322: Weights = [55.30333333  3.18118568  7.61830791  0.1253011   0.24008386 10.98828982], Loss = 0.8762\n",
      "Iteration 2323: Weights = [55.30333333  3.18098013  7.61781568  0.125293    0.24006835 10.98882008], Loss = 0.8760\n",
      "Iteration 2324: Weights = [55.30333333  3.18077461  7.61732347  0.1252849   0.24005283 10.98935031], Loss = 0.8759\n",
      "Iteration 2325: Weights = [55.30333333  3.18056909  7.6168313   0.12527681  0.24003732 10.9898805 ], Loss = 0.8758\n",
      "Iteration 2326: Weights = [55.30333333  3.18036359  7.61633916  0.12526871  0.24002181 10.99041066], Loss = 0.8757\n",
      "Iteration 2327: Weights = [55.30333333  3.1801581   7.61584706  0.12526062  0.24000631 10.99094079], Loss = 0.8756\n",
      "Iteration 2328: Weights = [55.30333333  3.17995262  7.61535498  0.12525253  0.2399908  10.99147088], Loss = 0.8755\n",
      "Iteration 2329: Weights = [55.30333333  3.17974716  7.61486294  0.12524443  0.23997529 10.99200094], Loss = 0.8754\n",
      "Iteration 2330: Weights = [55.30333333  3.17954171  7.61437093  0.12523634  0.23995979 10.99253096], Loss = 0.8752\n",
      "Iteration 2331: Weights = [55.30333333  3.17933627  7.61387895  0.12522825  0.23994428 10.99306095], Loss = 0.8751\n",
      "Iteration 2332: Weights = [55.30333333  3.17913085  7.613387    0.12522016  0.23992878 10.9935909 ], Loss = 0.8750\n",
      "Iteration 2333: Weights = [55.30333333  3.17892544  7.61289508  0.12521207  0.23991328 10.99412082], Loss = 0.8749\n",
      "Iteration 2334: Weights = [55.30333333  3.17872004  7.6124032   0.12520398  0.23989778 10.99465071], Loss = 0.8748\n",
      "Iteration 2335: Weights = [55.30333333  3.17851466  7.61191135  0.12519589  0.23988228 10.99518056], Loss = 0.8747\n",
      "Iteration 2336: Weights = [55.30333333  3.17830929  7.61141952  0.1251878   0.23986678 10.99571037], Loss = 0.8746\n",
      "Iteration 2337: Weights = [55.30333333  3.17810393  7.61092774  0.12517971  0.23985128 10.99624016], Loss = 0.8745\n",
      "Iteration 2338: Weights = [55.30333333  3.17789859  7.61043598  0.12517162  0.23983578 10.99676991], Loss = 0.8743\n",
      "Iteration 2339: Weights = [55.30333333  3.17769326  7.60994425  0.12516354  0.23982028 10.99729962], Loss = 0.8742\n",
      "Iteration 2340: Weights = [55.30333333  3.17748794  7.60945256  0.12515545  0.23980479 10.9978293 ], Loss = 0.8741\n",
      "Iteration 2341: Weights = [55.30333333  3.17728263  7.6089609   0.12514736  0.2397893  10.99835895], Loss = 0.8740\n",
      "Iteration 2342: Weights = [55.30333333  3.17707734  7.60846927  0.12513928  0.2397738  10.99888856], Loss = 0.8739\n",
      "Iteration 2343: Weights = [55.30333333  3.17687207  7.60797767  0.12513119  0.23975831 10.99941814], Loss = 0.8738\n",
      "Iteration 2344: Weights = [55.30333333  3.1766668   7.6074861   0.1251231   0.23974282 10.99994768], Loss = 0.8737\n",
      "Iteration 2345: Weights = [55.30333333  3.17646155  7.60699457  0.12511502  0.23972733 11.00047719], Loss = 0.8736\n",
      "Iteration 2346: Weights = [55.30333333  3.17625631  7.60650306  0.12510694  0.23971184 11.00100666], Loss = 0.8734\n",
      "Iteration 2347: Weights = [55.30333333  3.17605109  7.60601159  0.12509885  0.23969635 11.00153611], Loss = 0.8733\n",
      "Iteration 2348: Weights = [55.30333333  3.17584588  7.60552015  0.12509077  0.23968086 11.00206551], Loss = 0.8732\n",
      "Iteration 2349: Weights = [55.30333333  3.17564068  7.60502874  0.12508269  0.23966538 11.00259488], Loss = 0.8731\n",
      "Iteration 2350: Weights = [55.30333333  3.1754355   7.60453737  0.12507461  0.23964989 11.00312422], Loss = 0.8730\n",
      "Iteration 2351: Weights = [55.30333333  3.17523033  7.60404602  0.12506652  0.23963441 11.00365353], Loss = 0.8729\n",
      "Iteration 2352: Weights = [55.30333333  3.17502517  7.60355471  0.12505844  0.23961892 11.0041828 ], Loss = 0.8728\n",
      "Iteration 2353: Weights = [55.30333333  3.17482002  7.60306343  0.12505036  0.23960344 11.00471203], Loss = 0.8726\n",
      "Iteration 2354: Weights = [55.30333333  3.17461489  7.60257218  0.12504228  0.23958796 11.00524123], Loss = 0.8725\n",
      "Iteration 2355: Weights = [55.30333333  3.17440977  7.60208096  0.1250342   0.23957248 11.0057704 ], Loss = 0.8724\n",
      "Iteration 2356: Weights = [55.30333333  3.17420467  7.60158978  0.12502613  0.239557   11.00629953], Loss = 0.8723\n",
      "Iteration 2357: Weights = [55.30333333  3.17399958  7.60109862  0.12501805  0.23954152 11.00682863], Loss = 0.8722\n",
      "Iteration 2358: Weights = [55.30333333  3.1737945   7.6006075   0.12500997  0.23952605 11.0073577 ], Loss = 0.8721\n",
      "Iteration 2359: Weights = [55.30333333  3.17358943  7.60011641  0.12500189  0.23951057 11.00788673], Loss = 0.8720\n",
      "Iteration 2360: Weights = [55.30333333  3.17338438  7.59962535  0.12499382  0.23949509 11.00841572], Loss = 0.8719\n",
      "Iteration 2361: Weights = [55.30333333  3.17317934  7.59913432  0.12498574  0.23947962 11.00894469], Loss = 0.8717\n",
      "Iteration 2362: Weights = [55.30333333  3.17297432  7.59864333  0.12497766  0.23946415 11.00947361], Loss = 0.8716\n",
      "Iteration 2363: Weights = [55.30333333  3.1727693   7.59815237  0.12496959  0.23944867 11.01000251], Loss = 0.8715\n",
      "Iteration 2364: Weights = [55.30333333  3.1725643   7.59766143  0.12496151  0.2394332  11.01053137], Loss = 0.8714\n",
      "Iteration 2365: Weights = [55.30333333  3.17235932  7.59717053  0.12495344  0.23941773 11.01106019], Loss = 0.8713\n",
      "Iteration 2366: Weights = [55.30333333  3.17215435  7.59667966  0.12494537  0.23940226 11.01158898], Loss = 0.8712\n",
      "Iteration 2367: Weights = [55.30333333  3.17194939  7.59618883  0.12493729  0.2393868  11.01211774], Loss = 0.8711\n",
      "Iteration 2368: Weights = [55.30333333  3.17174444  7.59569802  0.12492922  0.23937133 11.01264646], Loss = 0.8710\n",
      "Iteration 2369: Weights = [55.30333333  3.17153951  7.59520725  0.12492115  0.23935586 11.01317515], Loss = 0.8708\n",
      "Iteration 2370: Weights = [55.30333333  3.17133459  7.59471651  0.12491308  0.2393404  11.01370381], Loss = 0.8707\n",
      "Iteration 2371: Weights = [55.30333333  3.17112968  7.5942258   0.12490501  0.23932493 11.01423243], Loss = 0.8706\n",
      "Iteration 2372: Weights = [55.30333333  3.17092479  7.59373512  0.12489694  0.23930947 11.01476101], Loss = 0.8705\n",
      "Iteration 2373: Weights = [55.30333333  3.17071991  7.59324447  0.12488887  0.23929401 11.01528957], Loss = 0.8704\n",
      "Iteration 2374: Weights = [55.30333333  3.17051504  7.59275386  0.1248808   0.23927855 11.01581808], Loss = 0.8703\n",
      "Iteration 2375: Weights = [55.30333333  3.17031019  7.59226328  0.12487273  0.23926309 11.01634657], Loss = 0.8702\n",
      "Iteration 2376: Weights = [55.30333333  3.17010535  7.59177272  0.12486466  0.23924763 11.01687502], Loss = 0.8701\n",
      "Iteration 2377: Weights = [55.30333333  3.16990052  7.5912822   0.12485659  0.23923217 11.01740343], Loss = 0.8699\n",
      "Iteration 2378: Weights = [55.30333333  3.16969571  7.59079172  0.12484853  0.23921671 11.01793181], Loss = 0.8698\n",
      "Iteration 2379: Weights = [55.30333333  3.16949091  7.59030126  0.12484046  0.23920125 11.01846016], Loss = 0.8697\n",
      "Iteration 2380: Weights = [55.30333333  3.16928612  7.58981084  0.12483239  0.2391858  11.01898847], Loss = 0.8696\n",
      "Iteration 2381: Weights = [55.30333333  3.16908135  7.58932044  0.12482433  0.23917034 11.01951675], Loss = 0.8695\n",
      "Iteration 2382: Weights = [55.30333333  3.16887659  7.58883008  0.12481626  0.23915489 11.020045  ], Loss = 0.8694\n",
      "Iteration 2383: Weights = [55.30333333  3.16867184  7.58833975  0.1248082   0.23913944 11.02057321], Loss = 0.8693\n",
      "Iteration 2384: Weights = [55.30333333  3.16846711  7.58784945  0.12480013  0.23912399 11.02110138], Loss = 0.8692\n",
      "Iteration 2385: Weights = [55.30333333  3.16826238  7.58735919  0.12479207  0.23910854 11.02162953], Loss = 0.8690\n",
      "Iteration 2386: Weights = [55.30333333  3.16805768  7.58686895  0.12478401  0.23909309 11.02215764], Loss = 0.8689\n",
      "Iteration 2387: Weights = [55.30333333  3.16785298  7.58637875  0.12477594  0.23907764 11.02268571], Loss = 0.8688\n",
      "Iteration 2388: Weights = [55.30333333  3.1676483   7.58588858  0.12476788  0.23906219 11.02321375], Loss = 0.8687\n",
      "Iteration 2389: Weights = [55.30333333  3.16744363  7.58539844  0.12475982  0.23904675 11.02374176], Loss = 0.8686\n",
      "Iteration 2390: Weights = [55.30333333  3.16723898  7.58490833  0.12475176  0.2390313  11.02426973], Loss = 0.8685\n",
      "Iteration 2391: Weights = [55.30333333  3.16703434  7.58441826  0.1247437   0.23901586 11.02479767], Loss = 0.8684\n",
      "Iteration 2392: Weights = [55.30333333  3.16682971  7.58392821  0.12473564  0.23900041 11.02532557], Loss = 0.8683\n",
      "Iteration 2393: Weights = [55.30333333  3.16662509  7.5834382   0.12472758  0.23898497 11.02585344], Loss = 0.8682\n",
      "Iteration 2394: Weights = [55.30333333  3.16642049  7.58294822  0.12471952  0.23896953 11.02638127], Loss = 0.8680\n",
      "Iteration 2395: Weights = [55.30333333  3.1662159   7.58245827  0.12471146  0.23895409 11.02690907], Loss = 0.8679\n",
      "Iteration 2396: Weights = [55.30333333  3.16601133  7.58196835  0.12470341  0.23893865 11.02743684], Loss = 0.8678\n",
      "Iteration 2397: Weights = [55.30333333  3.16580676  7.58147846  0.12469535  0.23892321 11.02796458], Loss = 0.8677\n",
      "Iteration 2398: Weights = [55.30333333  3.16560221  7.58098861  0.12468729  0.23890777 11.02849227], Loss = 0.8676\n",
      "Iteration 2399: Weights = [55.30333333  3.16539768  7.58049879  0.12467923  0.23889234 11.02901994], Loss = 0.8675\n",
      "Iteration 2400: Weights = [55.30333333  3.16519316  7.58000899  0.12467118  0.2388769  11.02954757], Loss = 0.8674\n",
      "Iteration 2401: Weights = [55.30333333  3.16498865  7.57951924  0.12466312  0.23886147 11.03007517], Loss = 0.8673\n",
      "Iteration 2402: Weights = [55.30333333  3.16478415  7.57902951  0.12465507  0.23884604 11.03060273], Loss = 0.8671\n",
      "Iteration 2403: Weights = [55.30333333  3.16457967  7.57853981  0.12464701  0.2388306  11.03113026], Loss = 0.8670\n",
      "Iteration 2404: Weights = [55.30333333  3.1643752   7.57805015  0.12463896  0.23881517 11.03165775], Loss = 0.8669\n",
      "Iteration 2405: Weights = [55.30333333  3.16417074  7.57756051  0.12463091  0.23879974 11.03218521], Loss = 0.8668\n",
      "Iteration 2406: Weights = [55.30333333  3.1639663   7.57707091  0.12462286  0.23878431 11.03271264], Loss = 0.8667\n",
      "Iteration 2407: Weights = [55.30333333  3.16376187  7.57658134  0.1246148   0.23876888 11.03324003], Loss = 0.8666\n",
      "Iteration 2408: Weights = [55.30333333  3.16355745  7.5760918   0.12460675  0.23875346 11.03376739], Loss = 0.8665\n",
      "Iteration 2409: Weights = [55.30333333  3.16335305  7.5756023   0.1245987   0.23873803 11.03429471], Loss = 0.8664\n",
      "Iteration 2410: Weights = [55.30333333  3.16314866  7.57511282  0.12459065  0.2387226  11.034822  ], Loss = 0.8662\n",
      "Iteration 2411: Weights = [55.30333333  3.16294428  7.57462338  0.1245826   0.23870718 11.03534926], Loss = 0.8661\n",
      "Iteration 2412: Weights = [55.30333333  3.16273991  7.57413397  0.12457455  0.23869176 11.03587648], Loss = 0.8660\n",
      "Iteration 2413: Weights = [55.30333333  3.16253556  7.57364459  0.1245665   0.23867633 11.03640367], Loss = 0.8659\n",
      "Iteration 2414: Weights = [55.30333333  3.16233122  7.57315524  0.12455845  0.23866091 11.03693082], Loss = 0.8658\n",
      "Iteration 2415: Weights = [55.30333333  3.1621269   7.57266592  0.1245504   0.23864549 11.03745794], Loss = 0.8657\n",
      "Iteration 2416: Weights = [55.30333333  3.16192259  7.57217664  0.12454236  0.23863007 11.03798503], Loss = 0.8656\n",
      "Iteration 2417: Weights = [55.30333333  3.16171829  7.57168738  0.12453431  0.23861466 11.03851208], Loss = 0.8655\n",
      "Iteration 2418: Weights = [55.30333333  3.16151401  7.57119816  0.12452626  0.23859924 11.0390391 ], Loss = 0.8654\n",
      "Iteration 2419: Weights = [55.30333333  3.16130973  7.57070897  0.12451822  0.23858382 11.03956608], Loss = 0.8652\n",
      "Iteration 2420: Weights = [55.30333333  3.16110548  7.57021981  0.12451017  0.23856841 11.04009303], Loss = 0.8651\n",
      "Iteration 2421: Weights = [55.30333333  3.16090123  7.56973069  0.12450213  0.23855299 11.04061994], Loss = 0.8650\n",
      "Iteration 2422: Weights = [55.30333333  3.160697    7.56924159  0.12449408  0.23853758 11.04114683], Loss = 0.8649\n",
      "Iteration 2423: Weights = [55.30333333  3.16049278  7.56875253  0.12448604  0.23852217 11.04167367], Loss = 0.8648\n",
      "Iteration 2424: Weights = [55.30333333  3.16028857  7.56826349  0.124478    0.23850675 11.04220049], Loss = 0.8647\n",
      "Iteration 2425: Weights = [55.30333333  3.16008438  7.56777449  0.12446995  0.23849134 11.04272727], Loss = 0.8646\n",
      "Iteration 2426: Weights = [55.30333333  3.1598802   7.56728552  0.12446191  0.23847593 11.04325401], Loss = 0.8645\n",
      "Iteration 2427: Weights = [55.30333333  3.15967603  7.56679659  0.12445387  0.23846053 11.04378072], Loss = 0.8643\n",
      "Iteration 2428: Weights = [55.30333333  3.15947188  7.56630768  0.12444583  0.23844512 11.0443074 ], Loss = 0.8642\n",
      "Iteration 2429: Weights = [55.30333333  3.15926774  7.56581881  0.12443779  0.23842971 11.04483404], Loss = 0.8641\n",
      "Iteration 2430: Weights = [55.30333333  3.15906362  7.56532996  0.12442975  0.23841431 11.04536065], Loss = 0.8640\n",
      "Iteration 2431: Weights = [55.30333333  3.1588595   7.56484115  0.12442171  0.2383989  11.04588723], Loss = 0.8639\n",
      "Iteration 2432: Weights = [55.30333333  3.1586554   7.56435237  0.12441367  0.2383835  11.04641377], Loss = 0.8638\n",
      "Iteration 2433: Weights = [55.30333333  3.15845131  7.56386362  0.12440563  0.2383681  11.04694027], Loss = 0.8637\n",
      "Iteration 2434: Weights = [55.30333333  3.15824724  7.56337491  0.12439759  0.2383527  11.04746675], Loss = 0.8636\n",
      "Iteration 2435: Weights = [55.30333333  3.15804318  7.56288622  0.12438955  0.23833729 11.04799319], Loss = 0.8635\n",
      "Iteration 2436: Weights = [55.30333333  3.15783913  7.56239757  0.12438152  0.2383219  11.04851959], Loss = 0.8633\n",
      "Iteration 2437: Weights = [55.30333333  3.1576351   7.56190895  0.12437348  0.2383065  11.04904596], Loss = 0.8632\n",
      "Iteration 2438: Weights = [55.30333333  3.15743108  7.56142036  0.12436544  0.2382911  11.0495723 ], Loss = 0.8631\n",
      "Iteration 2439: Weights = [55.30333333  3.15722707  7.5609318   0.12435741  0.2382757  11.0500986 ], Loss = 0.8630\n",
      "Iteration 2440: Weights = [55.30333333  3.15702307  7.56044327  0.12434937  0.23826031 11.05062487], Loss = 0.8629\n",
      "Iteration 2441: Weights = [55.30333333  3.15681909  7.55995478  0.12434134  0.23824491 11.05115111], Loss = 0.8628\n",
      "Iteration 2442: Weights = [55.30333333  3.15661512  7.55946631  0.12433331  0.23822952 11.05167731], Loss = 0.8627\n",
      "Iteration 2443: Weights = [55.30333333  3.15641117  7.55897788  0.12432527  0.23821413 11.05220347], Loss = 0.8626\n",
      "Iteration 2444: Weights = [55.30333333  3.15620723  7.55848948  0.12431724  0.23819874 11.05272961], Loss = 0.8624\n",
      "Iteration 2445: Weights = [55.30333333  3.1560033   7.55800111  0.12430921  0.23818335 11.0532557 ], Loss = 0.8623\n",
      "Iteration 2446: Weights = [55.30333333  3.15579938  7.55751277  0.12430118  0.23816796 11.05378177], Loss = 0.8622\n",
      "Iteration 2447: Weights = [55.30333333  3.15559548  7.55702447  0.12429314  0.23815257 11.0543078 ], Loss = 0.8621\n",
      "Iteration 2448: Weights = [55.30333333  3.15539159  7.55653619  0.12428511  0.23813718 11.0548338 ], Loss = 0.8620\n",
      "Iteration 2449: Weights = [55.30333333  3.15518771  7.55604795  0.12427708  0.23812179 11.05535976], Loss = 0.8619\n",
      "Iteration 2450: Weights = [55.30333333  3.15498385  7.55555974  0.12426905  0.23810641 11.05588569], Loss = 0.8618\n",
      "Iteration 2451: Weights = [55.30333333  3.15478     7.55507156  0.12426102  0.23809102 11.05641158], Loss = 0.8617\n",
      "Iteration 2452: Weights = [55.30333333  3.15457616  7.55458341  0.12425299  0.23807564 11.05693744], Loss = 0.8616\n",
      "Iteration 2453: Weights = [55.30333333  3.15437234  7.55409529  0.12424497  0.23806026 11.05746327], Loss = 0.8614\n",
      "Iteration 2454: Weights = [55.30333333  3.15416853  7.55360721  0.12423694  0.23804488 11.05798906], Loss = 0.8613\n",
      "Iteration 2455: Weights = [55.30333333  3.15396473  7.55311915  0.12422891  0.23802949 11.05851482], Loss = 0.8612\n",
      "Iteration 2456: Weights = [55.30333333  3.15376095  7.55263113  0.12422089  0.23801412 11.05904055], Loss = 0.8611\n",
      "Iteration 2457: Weights = [55.30333333  3.15355718  7.55214314  0.12421286  0.23799874 11.05956624], Loss = 0.8610\n",
      "Iteration 2458: Weights = [55.30333333  3.15335342  7.55165518  0.12420483  0.23798336 11.0600919 ], Loss = 0.8609\n",
      "Iteration 2459: Weights = [55.30333333  3.15314968  7.55116725  0.12419681  0.23796798 11.06061752], Loss = 0.8608\n",
      "Iteration 2460: Weights = [55.30333333  3.15294595  7.55067936  0.12418878  0.23795261 11.06114311], Loss = 0.8607\n",
      "Iteration 2461: Weights = [55.30333333  3.15274223  7.55019149  0.12418076  0.23793723 11.06166866], Loss = 0.8606\n",
      "Iteration 2462: Weights = [55.30333333  3.15253852  7.54970366  0.12417274  0.23792186 11.06219419], Loss = 0.8604\n",
      "Iteration 2463: Weights = [55.30333333  3.15233483  7.54921586  0.12416471  0.23790649 11.06271967], Loss = 0.8603\n",
      "Iteration 2464: Weights = [55.30333333  3.15213115  7.54872809  0.12415669  0.23789111 11.06324513], Loss = 0.8602\n",
      "Iteration 2465: Weights = [55.30333333  3.15192749  7.54824035  0.12414867  0.23787574 11.06377055], Loss = 0.8601\n",
      "Iteration 2466: Weights = [55.30333333  3.15172383  7.54775264  0.12414065  0.23786037 11.06429593], Loss = 0.8600\n",
      "Iteration 2467: Weights = [55.30333333  3.15152019  7.54726497  0.12413263  0.23784501 11.06482128], Loss = 0.8599\n",
      "Iteration 2468: Weights = [55.30333333  3.15131657  7.54677732  0.12412461  0.23782964 11.0653466 ], Loss = 0.8598\n",
      "Iteration 2469: Weights = [55.30333333  3.15111296  7.54628971  0.12411659  0.23781427 11.06587188], Loss = 0.8597\n",
      "Iteration 2470: Weights = [55.30333333  3.15090936  7.54580213  0.12410857  0.23779891 11.06639713], Loss = 0.8596\n",
      "Iteration 2471: Weights = [55.30333333  3.15070577  7.54531458  0.12410055  0.23778354 11.06692235], Loss = 0.8594\n",
      "Iteration 2472: Weights = [55.30333333  3.1505022   7.54482706  0.12409253  0.23776818 11.06744753], Loss = 0.8593\n",
      "Iteration 2473: Weights = [55.30333333  3.15029864  7.54433958  0.12408451  0.23775281 11.06797268], Loss = 0.8592\n",
      "Iteration 2474: Weights = [55.30333333  3.15009509  7.54385212  0.12407649  0.23773745 11.06849779], Loss = 0.8591\n",
      "Iteration 2475: Weights = [55.30333333  3.14989155  7.5433647   0.12406848  0.23772209 11.06902287], Loss = 0.8590\n",
      "Iteration 2476: Weights = [55.30333333  3.14968803  7.54287731  0.12406046  0.23770673 11.06954792], Loss = 0.8589\n",
      "Iteration 2477: Weights = [55.30333333  3.14948453  7.54238995  0.12405244  0.23769137 11.07007293], Loss = 0.8588\n",
      "Iteration 2478: Weights = [55.30333333  3.14928103  7.54190262  0.12404443  0.23767602 11.07059791], Loss = 0.8587\n",
      "Iteration 2479: Weights = [55.30333333  3.14907755  7.54141532  0.12403641  0.23766066 11.07112285], Loss = 0.8586\n",
      "Iteration 2480: Weights = [55.30333333  3.14887408  7.54092805  0.1240284   0.2376453  11.07164777], Loss = 0.8584\n",
      "Iteration 2481: Weights = [55.30333333  3.14867063  7.54044082  0.12402039  0.23762995 11.07217264], Loss = 0.8583\n",
      "Iteration 2482: Weights = [55.30333333  3.14846718  7.53995362  0.12401237  0.2376146  11.07269748], Loss = 0.8582\n",
      "Iteration 2483: Weights = [55.30333333  3.14826376  7.53946645  0.12400436  0.23759924 11.07322229], Loss = 0.8581\n",
      "Iteration 2484: Weights = [55.30333333  3.14806034  7.53897931  0.12399635  0.23758389 11.07374707], Loss = 0.8580\n",
      "Iteration 2485: Weights = [55.30333333  3.14785694  7.5384922   0.12398834  0.23756854 11.07427181], Loss = 0.8579\n",
      "Iteration 2486: Weights = [55.30333333  3.14765355  7.53800512  0.12398033  0.23755319 11.07479652], Loss = 0.8578\n",
      "Iteration 2487: Weights = [55.30333333  3.14745017  7.53751807  0.12397232  0.23753784 11.07532119], Loss = 0.8577\n",
      "Iteration 2488: Weights = [55.30333333  3.14724681  7.53703106  0.12396431  0.23752249 11.07584583], Loss = 0.8576\n",
      "Iteration 2489: Weights = [55.30333333  3.14704346  7.53654408  0.1239563   0.23750715 11.07637043], Loss = 0.8574\n",
      "Iteration 2490: Weights = [55.30333333  3.14684012  7.53605713  0.12394829  0.2374918  11.076895  ], Loss = 0.8573\n",
      "Iteration 2491: Weights = [55.30333333  3.1466368   7.53557021  0.12394028  0.23747646 11.07741954], Loss = 0.8572\n",
      "Iteration 2492: Weights = [55.30333333  3.14643349  7.53508332  0.12393227  0.23746111 11.07794405], Loss = 0.8571\n",
      "Iteration 2493: Weights = [55.30333333  3.14623019  7.53459646  0.12392426  0.23744577 11.07846852], Loss = 0.8570\n",
      "Iteration 2494: Weights = [55.30333333  3.14602691  7.53410964  0.12391626  0.23743043 11.07899295], Loss = 0.8569\n",
      "Iteration 2495: Weights = [55.30333333  3.14582364  7.53362284  0.12390825  0.23741509 11.07951735], Loss = 0.8568\n",
      "Iteration 2496: Weights = [55.30333333  3.14562038  7.53313608  0.12390024  0.23739975 11.08004172], Loss = 0.8567\n",
      "Iteration 2497: Weights = [55.30333333  3.14541713  7.53264935  0.12389224  0.23738441 11.08056606], Loss = 0.8566\n",
      "Iteration 2498: Weights = [55.30333333  3.1452139   7.53216265  0.12388423  0.23736907 11.08109036], Loss = 0.8565\n",
      "Iteration 2499: Weights = [55.30333333  3.14501068  7.53167598  0.12387623  0.23735373 11.08161462], Loss = 0.8563\n",
      "Iteration 2500: Weights = [55.30333333  3.14480748  7.53118934  0.12386822  0.2373384  11.08213885], Loss = 0.8562\n",
      "Iteration 2501: Weights = [55.30333333  3.14460428  7.53070274  0.12386022  0.23732306 11.08266305], Loss = 0.8561\n",
      "Iteration 2502: Weights = [55.30333333  3.14440111  7.53021616  0.12385222  0.23730773 11.08318722], Loss = 0.8560\n",
      "Iteration 2503: Weights = [55.30333333  3.14419794  7.52972962  0.12384422  0.2372924  11.08371135], Loss = 0.8559\n",
      "Iteration 2504: Weights = [55.30333333  3.14399479  7.52924311  0.12383621  0.23727706 11.08423545], Loss = 0.8558\n",
      "Iteration 2505: Weights = [55.30333333  3.14379165  7.52875663  0.12382821  0.23726173 11.08475951], Loss = 0.8557\n",
      "Iteration 2506: Weights = [55.30333333  3.14358852  7.52827018  0.12382021  0.2372464  11.08528354], Loss = 0.8556\n",
      "Iteration 2507: Weights = [55.30333333  3.14338541  7.52778377  0.12381221  0.23723107 11.08580753], Loss = 0.8555\n",
      "Iteration 2508: Weights = [55.30333333  3.14318231  7.52729738  0.12380421  0.23721575 11.0863315 ], Loss = 0.8553\n",
      "Iteration 2509: Weights = [55.30333333  3.14297922  7.52681103  0.12379621  0.23720042 11.08685542], Loss = 0.8552\n",
      "Iteration 2510: Weights = [55.30333333  3.14277614  7.5263247   0.12378821  0.23718509 11.08737932], Loss = 0.8551\n",
      "Iteration 2511: Weights = [55.30333333  3.14257308  7.52583841  0.12378022  0.23716977 11.08790318], Loss = 0.8550\n",
      "Iteration 2512: Weights = [55.30333333  3.14237004  7.52535215  0.12377222  0.23715444 11.088427  ], Loss = 0.8549\n",
      "Iteration 2513: Weights = [55.30333333  3.142167    7.52486593  0.12376422  0.23713912 11.0889508 ], Loss = 0.8548\n",
      "Iteration 2514: Weights = [55.30333333  3.14196398  7.52437973  0.12375622  0.2371238  11.08947455], Loss = 0.8547\n",
      "Iteration 2515: Weights = [55.30333333  3.14176097  7.52389356  0.12374823  0.23710848 11.08999828], Loss = 0.8546\n",
      "Iteration 2516: Weights = [55.30333333  3.14155797  7.52340743  0.12374023  0.23709316 11.09052197], Loss = 0.8545\n",
      "Iteration 2517: Weights = [55.30333333  3.14135499  7.52292133  0.12373224  0.23707784 11.09104563], Loss = 0.8544\n",
      "Iteration 2518: Weights = [55.30333333  3.14115202  7.52243526  0.12372424  0.23706252 11.09156925], Loss = 0.8542\n",
      "Iteration 2519: Weights = [55.30333333  3.14094907  7.52194922  0.12371625  0.2370472  11.09209284], Loss = 0.8541\n",
      "Iteration 2520: Weights = [55.30333333  3.14074612  7.52146321  0.12370825  0.23703189 11.0926164 ], Loss = 0.8540\n",
      "Iteration 2521: Weights = [55.30333333  3.14054319  7.52097723  0.12370026  0.23701657 11.09313992], Loss = 0.8539\n",
      "Iteration 2522: Weights = [55.30333333  3.14034028  7.52049129  0.12369227  0.23700126 11.0936634 ], Loss = 0.8538\n",
      "Iteration 2523: Weights = [55.30333333  3.14013737  7.52000537  0.12368428  0.23698595 11.09418686], Loss = 0.8537\n",
      "Iteration 2524: Weights = [55.30333333  3.13993448  7.51951949  0.12367629  0.23697063 11.09471028], Loss = 0.8536\n",
      "Iteration 2525: Weights = [55.30333333  3.13973161  7.51903364  0.12366829  0.23695532 11.09523367], Loss = 0.8535\n",
      "Iteration 2526: Weights = [55.30333333  3.13952874  7.51854782  0.1236603   0.23694001 11.09575702], Loss = 0.8534\n",
      "Iteration 2527: Weights = [55.30333333  3.13932589  7.51806203  0.12365231  0.2369247  11.09628034], Loss = 0.8532\n",
      "Iteration 2528: Weights = [55.30333333  3.13912305  7.51757627  0.12364433  0.23690939 11.09680362], Loss = 0.8531\n",
      "Iteration 2529: Weights = [55.30333333  3.13892023  7.51709055  0.12363634  0.23689409 11.09732687], Loss = 0.8530\n",
      "Iteration 2530: Weights = [55.30333333  3.13871741  7.51660485  0.12362835  0.23687878 11.09785009], Loss = 0.8529\n",
      "Iteration 2531: Weights = [55.30333333  3.13851462  7.51611919  0.12362036  0.23686348 11.09837327], Loss = 0.8528\n",
      "Iteration 2532: Weights = [55.30333333  3.13831183  7.51563356  0.12361237  0.23684817 11.09889642], Loss = 0.8527\n",
      "Iteration 2533: Weights = [55.30333333  3.13810906  7.51514796  0.12360439  0.23683287 11.09941954], Loss = 0.8526\n",
      "Iteration 2534: Weights = [55.30333333  3.1379063   7.51466239  0.1235964   0.23681757 11.09994262], Loss = 0.8525\n",
      "Iteration 2535: Weights = [55.30333333  3.13770355  7.51417685  0.12358841  0.23680226 11.10046567], Loss = 0.8524\n",
      "Iteration 2536: Weights = [55.30333333  3.13750082  7.51369135  0.12358043  0.23678696 11.10098868], Loss = 0.8523\n",
      "Iteration 2537: Weights = [55.30333333  3.1372981   7.51320587  0.12357244  0.23677167 11.10151167], Loss = 0.8521\n",
      "Iteration 2538: Weights = [55.30333333  3.13709539  7.51272043  0.12356446  0.23675637 11.10203461], Loss = 0.8520\n",
      "Iteration 2539: Weights = [55.30333333  3.1368927   7.51223502  0.12355648  0.23674107 11.10255753], Loss = 0.8519\n",
      "Iteration 2540: Weights = [55.30333333  3.13669002  7.51174964  0.12354849  0.23672577 11.1030804 ], Loss = 0.8518\n",
      "Iteration 2541: Weights = [55.30333333  3.13648735  7.51126429  0.12354051  0.23671048 11.10360325], Loss = 0.8517\n",
      "Iteration 2542: Weights = [55.30333333  3.13628469  7.51077897  0.12353253  0.23669518 11.10412606], Loss = 0.8516\n",
      "Iteration 2543: Weights = [55.30333333  3.13608205  7.51029368  0.12352455  0.23667989 11.10464884], Loss = 0.8515\n",
      "Iteration 2544: Weights = [55.30333333  3.13587942  7.50980843  0.12351656  0.2366646  11.10517158], Loss = 0.8514\n",
      "Iteration 2545: Weights = [55.30333333  3.13567681  7.5093232   0.12350858  0.23664931 11.1056943 ], Loss = 0.8513\n",
      "Iteration 2546: Weights = [55.30333333  3.13547421  7.50883801  0.1235006   0.23663402 11.10621697], Loss = 0.8512\n",
      "Iteration 2547: Weights = [55.30333333  3.13527162  7.50835285  0.12349262  0.23661873 11.10673961], Loss = 0.8510\n",
      "Iteration 2548: Weights = [55.30333333  3.13506904  7.50786772  0.12348464  0.23660344 11.10726222], Loss = 0.8509\n",
      "Iteration 2549: Weights = [55.30333333  3.13486648  7.50738262  0.12347667  0.23658815 11.1077848 ], Loss = 0.8508\n",
      "Iteration 2550: Weights = [55.30333333  3.13466393  7.50689755  0.12346869  0.23657286 11.10830734], Loss = 0.8507\n",
      "Iteration 2551: Weights = [55.30333333  3.13446139  7.50641252  0.12346071  0.23655758 11.10882985], Loss = 0.8506\n",
      "Iteration 2552: Weights = [55.30333333  3.13425887  7.50592751  0.12345273  0.23654229 11.10935232], Loss = 0.8505\n",
      "Iteration 2553: Weights = [55.30333333  3.13405635  7.50544254  0.12344476  0.23652701 11.10987476], Loss = 0.8504\n",
      "Iteration 2554: Weights = [55.30333333  3.13385386  7.5049576   0.12343678  0.23651173 11.11039717], Loss = 0.8503\n",
      "Iteration 2555: Weights = [55.30333333  3.13365137  7.50447269  0.12342881  0.23649645 11.11091954], Loss = 0.8502\n",
      "Iteration 2556: Weights = [55.30333333  3.1334489   7.50398781  0.12342083  0.23648117 11.11144188], Loss = 0.8501\n",
      "Iteration 2557: Weights = [55.30333333  3.13324644  7.50350296  0.12341286  0.23646589 11.11196419], Loss = 0.8499\n",
      "Iteration 2558: Weights = [55.30333333  3.133044    7.50301815  0.12340488  0.23645061 11.11248646], Loss = 0.8498\n",
      "Iteration 2559: Weights = [55.30333333  3.13284157  7.50253336  0.12339691  0.23643533 11.1130087 ], Loss = 0.8497\n",
      "Iteration 2560: Weights = [55.30333333  3.13263915  7.50204861  0.12338894  0.23642005 11.1135309 ], Loss = 0.8496\n",
      "Iteration 2561: Weights = [55.30333333  3.13243674  7.50156388  0.12338096  0.23640478 11.11405307], Loss = 0.8495\n",
      "Iteration 2562: Weights = [55.30333333  3.13223435  7.50107919  0.12337299  0.2363895  11.11457521], Loss = 0.8494\n",
      "Iteration 2563: Weights = [55.30333333  3.13203197  7.50059453  0.12336502  0.23637423 11.11509731], Loss = 0.8493\n",
      "Iteration 2564: Weights = [55.30333333  3.1318296   7.5001099   0.12335705  0.23635896 11.11561938], Loss = 0.8492\n",
      "Iteration 2565: Weights = [55.30333333  3.13162725  7.49962531  0.12334908  0.23634369 11.11614142], Loss = 0.8491\n",
      "Iteration 2566: Weights = [55.30333333  3.13142491  7.49914074  0.12334111  0.23632842 11.11666342], Loss = 0.8490\n",
      "Iteration 2567: Weights = [55.30333333  3.13122258  7.49865621  0.12333314  0.23631315 11.11718538], Loss = 0.8488\n",
      "Iteration 2568: Weights = [55.30333333  3.13102026  7.4981717   0.12332517  0.23629788 11.11770732], Loss = 0.8487\n",
      "Iteration 2569: Weights = [55.30333333  3.13081796  7.49768723  0.1233172   0.23628261 11.11822922], Loss = 0.8486\n",
      "Iteration 2570: Weights = [55.30333333  3.13061567  7.49720279  0.12330923  0.23626734 11.11875109], Loss = 0.8485\n",
      "Iteration 2571: Weights = [55.30333333  3.1304134   7.49671838  0.12330127  0.23625208 11.11927292], Loss = 0.8484\n",
      "Iteration 2572: Weights = [55.30333333  3.13021114  7.496234    0.1232933   0.23623681 11.11979472], Loss = 0.8483\n",
      "Iteration 2573: Weights = [55.30333333  3.13000889  7.49574966  0.12328533  0.23622155 11.12031648], Loss = 0.8482\n",
      "Iteration 2574: Weights = [55.30333333  3.12980665  7.49526534  0.12327737  0.23620629 11.12083822], Loss = 0.8481\n",
      "Iteration 2575: Weights = [55.30333333  3.12960443  7.49478106  0.1232694   0.23619102 11.12135991], Loss = 0.8480\n",
      "Iteration 2576: Weights = [55.30333333  3.12940222  7.4942968   0.12326144  0.23617576 11.12188158], Loss = 0.8479\n",
      "Iteration 2577: Weights = [55.30333333  3.12920002  7.49381258  0.12325347  0.2361605  11.12240321], Loss = 0.8478\n",
      "Iteration 2578: Weights = [55.30333333  3.12899784  7.49332839  0.12324551  0.23614524 11.12292481], Loss = 0.8476\n",
      "Iteration 2579: Weights = [55.30333333  3.12879567  7.49284423  0.12323755  0.23612999 11.12344637], Loss = 0.8475\n",
      "Iteration 2580: Weights = [55.30333333  3.12859351  7.49236011  0.12322959  0.23611473 11.1239679 ], Loss = 0.8474\n",
      "Iteration 2581: Weights = [55.30333333  3.12839136  7.49187601  0.12322162  0.23609947 11.1244894 ], Loss = 0.8473\n",
      "Iteration 2582: Weights = [55.30333333  3.12818923  7.49139194  0.12321366  0.23608422 11.12501086], Loss = 0.8472\n",
      "Iteration 2583: Weights = [55.30333333  3.12798711  7.49090791  0.1232057   0.23606897 11.12553229], Loss = 0.8471\n",
      "Iteration 2584: Weights = [55.30333333  3.12778501  7.49042391  0.12319774  0.23605371 11.12605368], Loss = 0.8470\n",
      "Iteration 2585: Weights = [55.30333333  3.12758291  7.48993994  0.12318978  0.23603846 11.12657504], Loss = 0.8469\n",
      "Iteration 2586: Weights = [55.30333333  3.12738084  7.489456    0.12318182  0.23602321 11.12709637], Loss = 0.8468\n",
      "Iteration 2587: Weights = [55.30333333  3.12717877  7.48897209  0.12317386  0.23600796 11.12761766], Loss = 0.8467\n",
      "Iteration 2588: Weights = [55.30333333  3.12697672  7.48848821  0.1231659   0.23599271 11.12813893], Loss = 0.8465\n",
      "Iteration 2589: Weights = [55.30333333  3.12677468  7.48800436  0.12315795  0.23597746 11.12866015], Loss = 0.8464\n",
      "Iteration 2590: Weights = [55.30333333  3.12657265  7.48752055  0.12314999  0.23596222 11.12918134], Loss = 0.8463\n",
      "Iteration 2591: Weights = [55.30333333  3.12637063  7.48703677  0.12314203  0.23594697 11.1297025 ], Loss = 0.8462\n",
      "Iteration 2592: Weights = [55.30333333  3.12616863  7.48655301  0.12313407  0.23593173 11.13022363], Loss = 0.8461\n",
      "Iteration 2593: Weights = [55.30333333  3.12596664  7.48606929  0.12312612  0.23591648 11.13074472], Loss = 0.8460\n",
      "Iteration 2594: Weights = [55.30333333  3.12576467  7.4855856   0.12311816  0.23590124 11.13126578], Loss = 0.8459\n",
      "Iteration 2595: Weights = [55.30333333  3.12556271  7.48510194  0.12311021  0.235886   11.1317868 ], Loss = 0.8458\n",
      "Iteration 2596: Weights = [55.30333333  3.12536076  7.48461832  0.12310225  0.23587075 11.13230779], Loss = 0.8457\n",
      "Iteration 2597: Weights = [55.30333333  3.12515882  7.48413472  0.1230943   0.23585551 11.13282875], Loss = 0.8456\n",
      "Iteration 2598: Weights = [55.30333333  3.1249569   7.48365115  0.12308635  0.23584028 11.13334968], Loss = 0.8455\n",
      "Iteration 2599: Weights = [55.30333333  3.12475499  7.48316762  0.12307839  0.23582504 11.13387056], Loss = 0.8453\n",
      "Iteration 2600: Weights = [55.30333333  3.12455309  7.48268412  0.12307044  0.2358098  11.13439142], Loss = 0.8452\n",
      "Iteration 2601: Weights = [55.30333333  3.12435121  7.48220065  0.12306249  0.23579456 11.13491224], Loss = 0.8451\n",
      "Iteration 2602: Weights = [55.30333333  3.12414934  7.48171721  0.12305454  0.23577933 11.13543303], Loss = 0.8450\n",
      "Iteration 2603: Weights = [55.30333333  3.12394748  7.4812338   0.12304659  0.2357641  11.13595379], Loss = 0.8449\n",
      "Iteration 2604: Weights = [55.30333333  3.12374564  7.48075042  0.12303864  0.23574886 11.13647451], Loss = 0.8448\n",
      "Iteration 2605: Weights = [55.30333333  3.12354381  7.48026707  0.12303069  0.23573363 11.1369952 ], Loss = 0.8447\n",
      "Iteration 2606: Weights = [55.30333333  3.12334199  7.47978376  0.12302274  0.2357184  11.13751585], Loss = 0.8446\n",
      "Iteration 2607: Weights = [55.30333333  3.12314018  7.47930048  0.12301479  0.23570317 11.13803647], Loss = 0.8445\n",
      "Iteration 2608: Weights = [55.30333333  3.12293839  7.47881722  0.12300684  0.23568794 11.13855706], Loss = 0.8444\n",
      "Iteration 2609: Weights = [55.30333333  3.12273661  7.478334    0.12299889  0.23567271 11.13907761], Loss = 0.8443\n",
      "Iteration 2610: Weights = [55.30333333  3.12253484  7.47785081  0.12299095  0.23565748 11.13959813], Loss = 0.8441\n",
      "Iteration 2611: Weights = [55.30333333  3.12233309  7.47736765  0.122983    0.23564226 11.14011862], Loss = 0.8440\n",
      "Iteration 2612: Weights = [55.30333333  3.12213135  7.47688452  0.12297505  0.23562703 11.14063907], Loss = 0.8439\n",
      "Iteration 2613: Weights = [55.30333333  3.12192962  7.47640143  0.12296711  0.23561181 11.14115949], Loss = 0.8438\n",
      "Iteration 2614: Weights = [55.30333333  3.12172791  7.47591836  0.12295916  0.23559658 11.14167987], Loss = 0.8437\n",
      "Iteration 2615: Weights = [55.30333333  3.12152621  7.47543533  0.12295122  0.23558136 11.14220023], Loss = 0.8436\n",
      "Iteration 2616: Weights = [55.30333333  3.12132452  7.47495233  0.12294327  0.23556614 11.14272054], Loss = 0.8435\n",
      "Iteration 2617: Weights = [55.30333333  3.12112285  7.47446935  0.12293533  0.23555092 11.14324083], Loss = 0.8434\n",
      "Iteration 2618: Weights = [55.30333333  3.12092118  7.47398641  0.12292739  0.2355357  11.14376108], Loss = 0.8433\n",
      "Iteration 2619: Weights = [55.30333333  3.12071953  7.4735035   0.12291944  0.23552048 11.1442813 ], Loss = 0.8432\n",
      "Iteration 2620: Weights = [55.30333333  3.1205179   7.47302063  0.1229115   0.23550526 11.14480148], Loss = 0.8431\n",
      "Iteration 2621: Weights = [55.30333333  3.12031628  7.47253778  0.12290356  0.23549005 11.14532163], Loss = 0.8429\n",
      "Iteration 2622: Weights = [55.30333333  3.12011467  7.47205496  0.12289562  0.23547483 11.14584175], Loss = 0.8428\n",
      "Iteration 2623: Weights = [55.30333333  3.11991307  7.47157218  0.12288768  0.23545962 11.14636183], Loss = 0.8427\n",
      "Iteration 2624: Weights = [55.30333333  3.11971148  7.47108943  0.12287974  0.2354444  11.14688188], Loss = 0.8426\n",
      "Iteration 2625: Weights = [55.30333333  3.11950991  7.4706067   0.1228718   0.23542919 11.14740189], Loss = 0.8425\n",
      "Iteration 2626: Weights = [55.30333333  3.11930836  7.47012401  0.12286386  0.23541398 11.14792188], Loss = 0.8424\n",
      "Iteration 2627: Weights = [55.30333333  3.11910681  7.46964135  0.12285592  0.23539877 11.14844182], Loss = 0.8423\n",
      "Iteration 2628: Weights = [55.30333333  3.11890528  7.46915872  0.12284798  0.23538356 11.14896174], Loss = 0.8422\n",
      "Iteration 2629: Weights = [55.30333333  3.11870376  7.46867613  0.12284005  0.23536835 11.14948162], Loss = 0.8421\n",
      "Iteration 2630: Weights = [55.30333333  3.11850225  7.46819356  0.12283211  0.23535314 11.15000147], Loss = 0.8420\n",
      "Iteration 2631: Weights = [55.30333333  3.11830076  7.46771103  0.12282417  0.23533794 11.15052128], Loss = 0.8419\n",
      "Iteration 2632: Weights = [55.30333333  3.11809928  7.46722852  0.12281624  0.23532273 11.15104106], Loss = 0.8417\n",
      "Iteration 2633: Weights = [55.30333333  3.11789782  7.46674605  0.1228083   0.23530753 11.15156081], Loss = 0.8416\n",
      "Iteration 2634: Weights = [55.30333333  3.11769636  7.46626361  0.12280037  0.23529232 11.15208052], Loss = 0.8415\n",
      "Iteration 2635: Weights = [55.30333333  3.11749492  7.4657812   0.12279243  0.23527712 11.1526002 ], Loss = 0.8414\n",
      "Iteration 2636: Weights = [55.30333333  3.11729349  7.46529882  0.1227845   0.23526192 11.15311985], Loss = 0.8413\n",
      "Iteration 2637: Weights = [55.30333333  3.11709208  7.46481647  0.12277657  0.23524672 11.15363946], Loss = 0.8412\n",
      "Iteration 2638: Weights = [55.30333333  3.11689068  7.46433415  0.12276863  0.23523152 11.15415904], Loss = 0.8411\n",
      "Iteration 2639: Weights = [55.30333333  3.11668929  7.46385187  0.1227607   0.23521632 11.15467858], Loss = 0.8410\n",
      "Iteration 2640: Weights = [55.30333333  3.11648791  7.46336961  0.12275277  0.23520112 11.1551981 ], Loss = 0.8409\n",
      "Iteration 2641: Weights = [55.30333333  3.11628655  7.46288739  0.12274484  0.23518593 11.15571757], Loss = 0.8408\n",
      "Iteration 2642: Weights = [55.30333333  3.1160852   7.4624052   0.12273691  0.23517073 11.15623702], Loss = 0.8407\n",
      "Iteration 2643: Weights = [55.30333333  3.11588387  7.46192304  0.12272898  0.23515553 11.15675643], Loss = 0.8406\n",
      "Iteration 2644: Weights = [55.30333333  3.11568254  7.46144091  0.12272105  0.23514034 11.15727581], Loss = 0.8404\n",
      "Iteration 2645: Weights = [55.30333333  3.11548123  7.46095881  0.12271312  0.23512515 11.15779515], Loss = 0.8403\n",
      "Iteration 2646: Weights = [55.30333333  3.11527993  7.46047674  0.12270519  0.23510996 11.15831446], Loss = 0.8402\n",
      "Iteration 2647: Weights = [55.30333333  3.11507865  7.4599947   0.12269726  0.23509476 11.15883374], Loss = 0.8401\n",
      "Iteration 2648: Weights = [55.30333333  3.11487738  7.4595127   0.12268933  0.23507957 11.15935298], Loss = 0.8400\n",
      "Iteration 2649: Weights = [55.30333333  3.11467612  7.45903073  0.12268141  0.23506439 11.15987219], Loss = 0.8399\n",
      "Iteration 2650: Weights = [55.30333333  3.11447487  7.45854878  0.12267348  0.2350492  11.16039137], Loss = 0.8398\n",
      "Iteration 2651: Weights = [55.30333333  3.11427364  7.45806687  0.12266555  0.23503401 11.16091051], Loss = 0.8397\n",
      "Iteration 2652: Weights = [55.30333333  3.11407242  7.45758499  0.12265763  0.23501882 11.16142962], Loss = 0.8396\n",
      "Iteration 2653: Weights = [55.30333333  3.11387122  7.45710314  0.1226497   0.23500364 11.16194869], Loss = 0.8395\n",
      "Iteration 2654: Weights = [55.30333333  3.11367002  7.45662132  0.12264178  0.23498846 11.16246774], Loss = 0.8394\n",
      "Iteration 2655: Weights = [55.30333333  3.11346884  7.45613953  0.12263385  0.23497327 11.16298674], Loss = 0.8392\n",
      "Iteration 2656: Weights = [55.30333333  3.11326767  7.45565778  0.12262593  0.23495809 11.16350572], Loss = 0.8391\n",
      "Iteration 2657: Weights = [55.30333333  3.11306652  7.45517605  0.12261801  0.23494291 11.16402466], Loss = 0.8390\n",
      "Iteration 2658: Weights = [55.30333333  3.11286538  7.45469436  0.12261008  0.23492773 11.16454357], Loss = 0.8389\n",
      "Iteration 2659: Weights = [55.30333333  3.11266425  7.4542127   0.12260216  0.23491255 11.16506244], Loss = 0.8388\n",
      "Iteration 2660: Weights = [55.30333333  3.11246313  7.45373107  0.12259424  0.23489737 11.16558128], Loss = 0.8387\n",
      "Iteration 2661: Weights = [55.30333333  3.11226203  7.45324946  0.12258632  0.23488219 11.16610009], Loss = 0.8386\n",
      "Iteration 2662: Weights = [55.30333333  3.11206094  7.4527679   0.1225784   0.23486702 11.16661887], Loss = 0.8385\n",
      "Iteration 2663: Weights = [55.30333333  3.11185987  7.45228636  0.12257048  0.23485184 11.16713761], Loss = 0.8384\n",
      "Iteration 2664: Weights = [55.30333333  3.1116588   7.45180485  0.12256256  0.23483667 11.16765631], Loss = 0.8383\n",
      "Iteration 2665: Weights = [55.30333333  3.11145775  7.45132337  0.12255464  0.2348215  11.16817499], Loss = 0.8382\n",
      "Iteration 2666: Weights = [55.30333333  3.11125671  7.45084193  0.12254672  0.23480632 11.16869363], Loss = 0.8381\n",
      "Iteration 2667: Weights = [55.30333333  3.11105569  7.45036051  0.1225388   0.23479115 11.16921223], Loss = 0.8379\n",
      "Iteration 2668: Weights = [55.30333333  3.11085468  7.44987913  0.12253089  0.23477598 11.1697308 ], Loss = 0.8378\n",
      "Iteration 2669: Weights = [55.30333333  3.11065368  7.44939778  0.12252297  0.23476081 11.17024934], Loss = 0.8377\n",
      "Iteration 2670: Weights = [55.30333333  3.11045269  7.44891646  0.12251505  0.23474564 11.17076785], Loss = 0.8376\n",
      "Iteration 2671: Weights = [55.30333333  3.11025172  7.44843517  0.12250714  0.23473048 11.17128632], Loss = 0.8375\n",
      "Iteration 2672: Weights = [55.30333333  3.11005076  7.44795391  0.12249922  0.23471531 11.17180476], Loss = 0.8374\n",
      "Iteration 2673: Weights = [55.30333333  3.10984981  7.44747268  0.12249131  0.23470015 11.17232316], Loss = 0.8373\n",
      "Iteration 2674: Weights = [55.30333333  3.10964888  7.44699149  0.12248339  0.23468498 11.17284154], Loss = 0.8372\n",
      "Iteration 2675: Weights = [55.30333333  3.10944796  7.44651032  0.12247548  0.23466982 11.17335987], Loss = 0.8371\n",
      "Iteration 2676: Weights = [55.30333333  3.10924705  7.44602919  0.12246756  0.23465465 11.17387818], Loss = 0.8370\n",
      "Iteration 2677: Weights = [55.30333333  3.10904616  7.44554809  0.12245965  0.23463949 11.17439645], Loss = 0.8369\n",
      "Iteration 2678: Weights = [55.30333333  3.10884528  7.44506701  0.12245174  0.23462433 11.17491469], Loss = 0.8368\n",
      "Iteration 2679: Weights = [55.30333333  3.10864441  7.44458597  0.12244383  0.23460917 11.17543289], Loss = 0.8367\n",
      "Iteration 2680: Weights = [55.30333333  3.10844355  7.44410496  0.12243592  0.23459401 11.17595106], Loss = 0.8365\n",
      "Iteration 2681: Weights = [55.30333333  3.10824271  7.44362399  0.122428    0.23457886 11.1764692 ], Loss = 0.8364\n",
      "Iteration 2682: Weights = [55.30333333  3.10804188  7.44314304  0.12242009  0.2345637  11.1769873 ], Loss = 0.8363\n",
      "Iteration 2683: Weights = [55.30333333  3.10784106  7.44266212  0.12241218  0.23454854 11.17750537], Loss = 0.8362\n",
      "Iteration 2684: Weights = [55.30333333  3.10764026  7.44218124  0.12240428  0.23453339 11.17802341], Loss = 0.8361\n",
      "Iteration 2685: Weights = [55.30333333  3.10743947  7.44170038  0.12239637  0.23451824 11.17854142], Loss = 0.8360\n",
      "Iteration 2686: Weights = [55.30333333  3.10723869  7.44121956  0.12238846  0.23450308 11.17905938], Loss = 0.8359\n",
      "Iteration 2687: Weights = [55.30333333  3.10703792  7.44073877  0.12238055  0.23448793 11.17957732], Loss = 0.8358\n",
      "Iteration 2688: Weights = [55.30333333  3.10683717  7.44025801  0.12237264  0.23447278 11.18009522], Loss = 0.8357\n",
      "Iteration 2689: Weights = [55.30333333  3.10663643  7.43977728  0.12236474  0.23445763 11.18061309], Loss = 0.8356\n",
      "Iteration 2690: Weights = [55.30333333  3.10643571  7.43929658  0.12235683  0.23444248 11.18113093], Loss = 0.8355\n",
      "Iteration 2691: Weights = [55.30333333  3.10623499  7.43881591  0.12234892  0.23442733 11.18164873], Loss = 0.8354\n",
      "Iteration 2692: Weights = [55.30333333  3.10603429  7.43833527  0.12234102  0.23441219 11.1821665 ], Loss = 0.8352\n",
      "Iteration 2693: Weights = [55.30333333  3.10583361  7.43785467  0.12233311  0.23439704 11.18268424], Loss = 0.8351\n",
      "Iteration 2694: Weights = [55.30333333  3.10563293  7.43737409  0.12232521  0.2343819  11.18320194], Loss = 0.8350\n",
      "Iteration 2695: Weights = [55.30333333  3.10543227  7.43689355  0.12231731  0.23436675 11.18371961], Loss = 0.8349\n",
      "Iteration 2696: Weights = [55.30333333  3.10523162  7.43641303  0.1223094   0.23435161 11.18423724], Loss = 0.8348\n",
      "Iteration 2697: Weights = [55.30333333  3.10503099  7.43593255  0.1223015   0.23433647 11.18475485], Loss = 0.8347\n",
      "Iteration 2698: Weights = [55.30333333  3.10483037  7.4354521   0.1222936   0.23432133 11.18527241], Loss = 0.8346\n",
      "Iteration 2699: Weights = [55.30333333  3.10462976  7.43497168  0.1222857   0.23430619 11.18578995], Loss = 0.8345\n",
      "Iteration 2700: Weights = [55.30333333  3.10442916  7.43449129  0.1222778   0.23429105 11.18630745], Loss = 0.8344\n",
      "Iteration 2701: Weights = [55.30333333  3.10422858  7.43401094  0.1222699   0.23427591 11.18682492], Loss = 0.8343\n",
      "Iteration 2702: Weights = [55.30333333  3.10402801  7.43353061  0.122262    0.23426077 11.18734235], Loss = 0.8342\n",
      "Iteration 2703: Weights = [55.30333333  3.10382745  7.43305032  0.1222541   0.23424564 11.18785975], Loss = 0.8341\n",
      "Iteration 2704: Weights = [55.30333333  3.1036269   7.43257005  0.1222462   0.2342305  11.18837712], Loss = 0.8340\n",
      "Iteration 2705: Weights = [55.30333333  3.10342637  7.43208982  0.1222383   0.23421537 11.18889446], Loss = 0.8338\n",
      "Iteration 2706: Weights = [55.30333333  3.10322585  7.43160962  0.1222304   0.23420024 11.18941176], Loss = 0.8337\n",
      "Iteration 2707: Weights = [55.30333333  3.10302535  7.43112944  0.1222225   0.2341851  11.18992903], Loss = 0.8336\n",
      "Iteration 2708: Weights = [55.30333333  3.10282486  7.4306493   0.12221461  0.23416997 11.19044626], Loss = 0.8335\n",
      "Iteration 2709: Weights = [55.30333333  3.10262438  7.4301692   0.12220671  0.23415484 11.19096346], Loss = 0.8334\n",
      "Iteration 2710: Weights = [55.30333333  3.10242391  7.42968912  0.12219881  0.23413971 11.19148063], Loss = 0.8333\n",
      "Iteration 2711: Weights = [55.30333333  3.10222346  7.42920907  0.12219092  0.23412458 11.19199776], Loss = 0.8332\n",
      "Iteration 2712: Weights = [55.30333333  3.10202302  7.42872905  0.12218302  0.23410946 11.19251486], Loss = 0.8331\n",
      "Iteration 2713: Weights = [55.30333333  3.10182259  7.42824907  0.12217513  0.23409433 11.19303193], Loss = 0.8330\n",
      "Iteration 2714: Weights = [55.30333333  3.10162217  7.42776911  0.12216723  0.23407921 11.19354896], Loss = 0.8329\n",
      "Iteration 2715: Weights = [55.30333333  3.10142177  7.42728919  0.12215934  0.23406408 11.19406596], Loss = 0.8328\n",
      "Iteration 2716: Weights = [55.30333333  3.10122138  7.4268093   0.12215145  0.23404896 11.19458293], Loss = 0.8327\n",
      "Iteration 2717: Weights = [55.30333333  3.101021    7.42632944  0.12214356  0.23403384 11.19509986], Loss = 0.8326\n",
      "Iteration 2718: Weights = [55.30333333  3.10082064  7.42584961  0.12213566  0.23401871 11.19561676], Loss = 0.8324\n",
      "Iteration 2719: Weights = [55.30333333  3.10062029  7.42536981  0.12212777  0.23400359 11.19613363], Loss = 0.8323\n",
      "Iteration 2720: Weights = [55.30333333  3.10041995  7.42489004  0.12211988  0.23398847 11.19665046], Loss = 0.8322\n",
      "Iteration 2721: Weights = [55.30333333  3.10021963  7.4244103   0.12211199  0.23397336 11.19716726], Loss = 0.8321\n",
      "Iteration 2722: Weights = [55.30333333  3.10001932  7.4239306   0.1221041   0.23395824 11.19768403], Loss = 0.8320\n",
      "Iteration 2723: Weights = [55.30333333  3.09981902  7.42345092  0.12209621  0.23394312 11.19820076], Loss = 0.8319\n",
      "Iteration 2724: Weights = [55.30333333  3.09961873  7.42297128  0.12208832  0.23392801 11.19871746], Loss = 0.8318\n",
      "Iteration 2725: Weights = [55.30333333  3.09941846  7.42249167  0.12208043  0.23391289 11.19923413], Loss = 0.8317\n",
      "Iteration 2726: Weights = [55.30333333  3.0992182   7.42201208  0.12207255  0.23389778 11.19975076], Loss = 0.8316\n",
      "Iteration 2727: Weights = [55.30333333  3.09901796  7.42153253  0.12206466  0.23388267 11.20026736], Loss = 0.8315\n",
      "Iteration 2728: Weights = [55.30333333  3.09881772  7.42105301  0.12205677  0.23386755 11.20078393], Loss = 0.8314\n",
      "Iteration 2729: Weights = [55.30333333  3.0986175   7.42057352  0.12204889  0.23385244 11.20130046], Loss = 0.8313\n",
      "Iteration 2730: Weights = [55.30333333  3.09841729  7.42009407  0.122041    0.23383733 11.20181696], Loss = 0.8312\n",
      "Iteration 2731: Weights = [55.30333333  3.0982171   7.41961464  0.12203311  0.23382222 11.20233343], Loss = 0.8310\n",
      "Iteration 2732: Weights = [55.30333333  3.09801692  7.41913524  0.12202523  0.23380712 11.20284986], Loss = 0.8309\n",
      "Iteration 2733: Weights = [55.30333333  3.09781675  7.41865588  0.12201735  0.23379201 11.20336626], Loss = 0.8308\n",
      "Iteration 2734: Weights = [55.30333333  3.09761659  7.41817654  0.12200946  0.2337769  11.20388262], Loss = 0.8307\n",
      "Iteration 2735: Weights = [55.30333333  3.09741645  7.41769724  0.12200158  0.2337618  11.20439896], Loss = 0.8306\n",
      "Iteration 2736: Weights = [55.30333333  3.09721632  7.41721797  0.1219937   0.2337467  11.20491525], Loss = 0.8305\n",
      "Iteration 2737: Weights = [55.30333333  3.0970162   7.41673873  0.12198581  0.23373159 11.20543152], Loss = 0.8304\n",
      "Iteration 2738: Weights = [55.30333333  3.0968161   7.41625951  0.12197793  0.23371649 11.20594775], Loss = 0.8303\n",
      "Iteration 2739: Weights = [55.30333333  3.096616    7.41578034  0.12197005  0.23370139 11.20646395], Loss = 0.8302\n",
      "Iteration 2740: Weights = [55.30333333  3.09641593  7.41530119  0.12196217  0.23368629 11.20698012], Loss = 0.8301\n",
      "Iteration 2741: Weights = [55.30333333  3.09621586  7.41482207  0.12195429  0.23367119 11.20749625], Loss = 0.8300\n",
      "Iteration 2742: Weights = [55.30333333  3.09601581  7.41434298  0.12194641  0.23365609 11.20801235], Loss = 0.8299\n",
      "Iteration 2743: Weights = [55.30333333  3.09581577  7.41386393  0.12193853  0.233641   11.20852842], Loss = 0.8298\n",
      "Iteration 2744: Weights = [55.30333333  3.09561574  7.4133849   0.12193065  0.2336259  11.20904445], Loss = 0.8297\n",
      "Iteration 2745: Weights = [55.30333333  3.09541573  7.41290591  0.12192277  0.23361081 11.20956045], Loss = 0.8295\n",
      "Iteration 2746: Weights = [55.30333333  3.09521573  7.41242695  0.1219149   0.23359571 11.21007641], Loss = 0.8294\n",
      "Iteration 2747: Weights = [55.30333333  3.09501574  7.41194801  0.12190702  0.23358062 11.21059234], Loss = 0.8293\n",
      "Iteration 2748: Weights = [55.30333333  3.09481576  7.41146911  0.12189914  0.23356553 11.21110824], Loss = 0.8292\n",
      "Iteration 2749: Weights = [55.30333333  3.0946158   7.41099024  0.12189127  0.23355044 11.21162411], Loss = 0.8291\n",
      "Iteration 2750: Weights = [55.30333333  3.09441585  7.4105114   0.12188339  0.23353534 11.21213994], Loss = 0.8290\n",
      "Iteration 2751: Weights = [55.30333333  3.09421591  7.4100326   0.12187551  0.23352026 11.21265574], Loss = 0.8289\n",
      "Iteration 2752: Weights = [55.30333333  3.09401599  7.40955382  0.12186764  0.23350517 11.21317151], Loss = 0.8288\n",
      "Iteration 2753: Weights = [55.30333333  3.09381608  7.40907507  0.12185977  0.23349008 11.21368724], Loss = 0.8287\n",
      "Iteration 2754: Weights = [55.30333333  3.09361618  7.40859636  0.12185189  0.23347499 11.21420294], Loss = 0.8286\n",
      "Iteration 2755: Weights = [55.30333333  3.0934163   7.40811767  0.12184402  0.23345991 11.2147186 ], Loss = 0.8285\n",
      "Iteration 2756: Weights = [55.30333333  3.09321643  7.40763902  0.12183615  0.23344482 11.21523424], Loss = 0.8284\n",
      "Iteration 2757: Weights = [55.30333333  3.09301657  7.4071604   0.12182827  0.23342974 11.21574984], Loss = 0.8283\n",
      "Iteration 2758: Weights = [55.30333333  3.09281672  7.40668181  0.1218204   0.23341466 11.2162654 ], Loss = 0.8282\n",
      "Iteration 2759: Weights = [55.30333333  3.09261689  7.40620325  0.12181253  0.23339958 11.21678093], Loss = 0.8280\n",
      "Iteration 2760: Weights = [55.30333333  3.09241707  7.40572472  0.12180466  0.2333845  11.21729643], Loss = 0.8279\n",
      "Iteration 2761: Weights = [55.30333333  3.09221726  7.40524622  0.12179679  0.23336942 11.2178119 ], Loss = 0.8278\n",
      "Iteration 2762: Weights = [55.30333333  3.09201747  7.40476775  0.12178892  0.23335434 11.21832733], Loss = 0.8277\n",
      "Iteration 2763: Weights = [55.30333333  3.09181768  7.40428931  0.12178105  0.23333926 11.21884273], Loss = 0.8276\n",
      "Iteration 2764: Weights = [55.30333333  3.09161792  7.40381091  0.12177318  0.23332418 11.2193581 ], Loss = 0.8275\n",
      "Iteration 2765: Weights = [55.30333333  3.09141816  7.40333253  0.12176532  0.23330911 11.21987343], Loss = 0.8274\n",
      "Iteration 2766: Weights = [55.30333333  3.09121842  7.40285419  0.12175745  0.23329403 11.22038873], Loss = 0.8273\n",
      "Iteration 2767: Weights = [55.30333333  3.09101869  7.40237587  0.12174958  0.23327896 11.22090399], Loss = 0.8272\n",
      "Iteration 2768: Weights = [55.30333333  3.09081897  7.40189759  0.12174172  0.23326389 11.22141923], Loss = 0.8271\n",
      "Iteration 2769: Weights = [55.30333333  3.09061927  7.40141934  0.12173385  0.23324882 11.22193443], Loss = 0.8270\n",
      "Iteration 2770: Weights = [55.30333333  3.09041958  7.40094112  0.12172598  0.23323375 11.22244959], Loss = 0.8269\n",
      "Iteration 2771: Weights = [55.30333333  3.0902199   7.40046293  0.12171812  0.23321868 11.22296473], Loss = 0.8268\n",
      "Iteration 2772: Weights = [55.30333333  3.09002023  7.39998477  0.12171025  0.23320361 11.22347983], Loss = 0.8267\n",
      "Iteration 2773: Weights = [55.30333333  3.08982058  7.39950664  0.12170239  0.23318854 11.22399489], Loss = 0.8265\n",
      "Iteration 2774: Weights = [55.30333333  3.08962094  7.39902855  0.12169453  0.23317347 11.22450993], Loss = 0.8264\n",
      "Iteration 2775: Weights = [55.30333333  3.08942131  7.39855048  0.12168666  0.23315841 11.22502492], Loss = 0.8263\n",
      "Iteration 2776: Weights = [55.30333333  3.0892217   7.39807245  0.1216788   0.23314334 11.22553989], Loss = 0.8262\n",
      "Iteration 2777: Weights = [55.30333333  3.0890221   7.39759444  0.12167094  0.23312828 11.22605482], Loss = 0.8261\n",
      "Iteration 2778: Weights = [55.30333333  3.08882251  7.39711647  0.12166308  0.23311322 11.22656972], Loss = 0.8260\n",
      "Iteration 2779: Weights = [55.30333333  3.08862294  7.39663852  0.12165522  0.23309815 11.22708459], Loss = 0.8259\n",
      "Iteration 2780: Weights = [55.30333333  3.08842337  7.39616061  0.12164736  0.23308309 11.22759942], Loss = 0.8258\n",
      "Iteration 2781: Weights = [55.30333333  3.08822382  7.39568273  0.1216395   0.23306803 11.22811422], Loss = 0.8257\n",
      "Iteration 2782: Weights = [55.30333333  3.08802429  7.39520488  0.12163164  0.23305297 11.22862899], Loss = 0.8256\n",
      "Iteration 2783: Weights = [55.30333333  3.08782476  7.39472706  0.12162378  0.23303792 11.22914373], Loss = 0.8255\n",
      "Iteration 2784: Weights = [55.30333333  3.08762525  7.39424928  0.12161592  0.23302286 11.22965843], Loss = 0.8254\n",
      "Iteration 2785: Weights = [55.30333333  3.08742576  7.39377152  0.12160806  0.2330078  11.23017309], Loss = 0.8253\n",
      "Iteration 2786: Weights = [55.30333333  3.08722627  7.39329379  0.12160021  0.23299275 11.23068773], Loss = 0.8252\n",
      "Iteration 2787: Weights = [55.30333333  3.0870268   7.3928161   0.12159235  0.23297769 11.23120233], Loss = 0.8251\n",
      "Iteration 2788: Weights = [55.30333333  3.08682734  7.39233843  0.12158449  0.23296264 11.23171689], Loss = 0.8249\n",
      "Iteration 2789: Weights = [55.30333333  3.08662789  7.3918608   0.12157664  0.23294759 11.23223143], Loss = 0.8248\n",
      "Iteration 2790: Weights = [55.30333333  3.08642846  7.39138319  0.12156878  0.23293254 11.23274593], Loss = 0.8247\n",
      "Iteration 2791: Weights = [55.30333333  3.08622904  7.39090562  0.12156093  0.23291749 11.2332604 ], Loss = 0.8246\n",
      "Iteration 2792: Weights = [55.30333333  3.08602963  7.39042808  0.12155307  0.23290244 11.23377483], Loss = 0.8245\n",
      "Iteration 2793: Weights = [55.30333333  3.08583024  7.38995057  0.12154522  0.23288739 11.23428923], Loss = 0.8244\n",
      "Iteration 2794: Weights = [55.30333333  3.08563086  7.38947309  0.12153737  0.23287234 11.2348036 ], Loss = 0.8243\n",
      "Iteration 2795: Weights = [55.30333333  3.08543149  7.38899564  0.12152951  0.2328573  11.23531794], Loss = 0.8242\n",
      "Iteration 2796: Weights = [55.30333333  3.08523213  7.38851822  0.12152166  0.23284225 11.23583224], Loss = 0.8241\n",
      "Iteration 2797: Weights = [55.30333333  3.08503279  7.38804084  0.12151381  0.23282721 11.23634651], Loss = 0.8240\n",
      "Iteration 2798: Weights = [55.30333333  3.08483346  7.38756348  0.12150596  0.23281216 11.23686074], Loss = 0.8239\n",
      "Iteration 2799: Weights = [55.30333333  3.08463414  7.38708616  0.12149811  0.23279712 11.23737494], Loss = 0.8238\n",
      "Iteration 2800: Weights = [55.30333333  3.08443484  7.38660886  0.12149026  0.23278208 11.23788911], Loss = 0.8237\n",
      "Iteration 2801: Weights = [55.30333333  3.08423555  7.3861316   0.12148241  0.23276704 11.23840325], Loss = 0.8236\n",
      "Iteration 2802: Weights = [55.30333333  3.08403627  7.38565436  0.12147456  0.232752   11.23891735], Loss = 0.8235\n",
      "Iteration 2803: Weights = [55.30333333  3.083837    7.38517716  0.12146671  0.23273696 11.23943142], Loss = 0.8234\n",
      "Iteration 2804: Weights = [55.30333333  3.08363775  7.38469999  0.12145886  0.23272192 11.23994545], Loss = 0.8232\n",
      "Iteration 2805: Weights = [55.30333333  3.08343851  7.38422285  0.12145101  0.23270689 11.24045946], Loss = 0.8231\n",
      "Iteration 2806: Weights = [55.30333333  3.08323928  7.38374574  0.12144317  0.23269185 11.24097342], Loss = 0.8230\n",
      "Iteration 2807: Weights = [55.30333333  3.08304007  7.38326866  0.12143532  0.23267682 11.24148736], Loss = 0.8229\n",
      "Iteration 2808: Weights = [55.30333333  3.08284087  7.38279162  0.12142747  0.23266178 11.24200126], Loss = 0.8228\n",
      "Iteration 2809: Weights = [55.30333333  3.08264168  7.3823146   0.12141963  0.23264675 11.24251513], Loss = 0.8227\n",
      "Iteration 2810: Weights = [55.30333333  3.0824425   7.38183761  0.12141178  0.23263172 11.24302897], Loss = 0.8226\n",
      "Iteration 2811: Weights = [55.30333333  3.08224334  7.38136066  0.12140394  0.23261669 11.24354277], Loss = 0.8225\n",
      "Iteration 2812: Weights = [55.30333333  3.08204419  7.38088373  0.12139609  0.23260166 11.24405654], Loss = 0.8224\n",
      "Iteration 2813: Weights = [55.30333333  3.08184505  7.38040684  0.12138825  0.23258663 11.24457028], Loss = 0.8223\n",
      "Iteration 2814: Weights = [55.30333333  3.08164593  7.37992998  0.12138041  0.2325716  11.24508398], Loss = 0.8222\n",
      "Iteration 2815: Weights = [55.30333333  3.08144682  7.37945314  0.12137256  0.23255657 11.24559765], Loss = 0.8221\n",
      "Iteration 2816: Weights = [55.30333333  3.08124772  7.37897634  0.12136472  0.23254155 11.24611129], Loss = 0.8220\n",
      "Iteration 2817: Weights = [55.30333333  3.08104863  7.37849957  0.12135688  0.23252652 11.2466249 ], Loss = 0.8219\n",
      "Iteration 2818: Weights = [55.30333333  3.08084956  7.37802283  0.12134904  0.2325115  11.24713847], Loss = 0.8218\n",
      "Iteration 2819: Weights = [55.30333333  3.0806505   7.37754612  0.1213412   0.23249647 11.247652  ], Loss = 0.8216\n",
      "Iteration 2820: Weights = [55.30333333  3.08045145  7.37706945  0.12133336  0.23248145 11.24816551], Loss = 0.8215\n",
      "Iteration 2821: Weights = [55.30333333  3.08025242  7.3765928   0.12132552  0.23246643 11.24867898], Loss = 0.8214\n",
      "Iteration 2822: Weights = [55.30333333  3.0800534   7.37611618  0.12131768  0.23245141 11.24919242], Loss = 0.8213\n",
      "Iteration 2823: Weights = [55.30333333  3.07985439  7.3756396   0.12130984  0.23243639 11.24970582], Loss = 0.8212\n",
      "Iteration 2824: Weights = [55.30333333  3.07965539  7.37516304  0.121302    0.23242137 11.25021919], Loss = 0.8211\n",
      "Iteration 2825: Weights = [55.30333333  3.07945641  7.37468652  0.12129417  0.23240636 11.25073253], Loss = 0.8210\n",
      "Iteration 2826: Weights = [55.30333333  3.07925744  7.37421002  0.12128633  0.23239134 11.25124584], Loss = 0.8209\n",
      "Iteration 2827: Weights = [55.30333333  3.07905848  7.37373356  0.12127849  0.23237633 11.25175911], Loss = 0.8208\n",
      "Iteration 2828: Weights = [55.30333333  3.07885954  7.37325713  0.12127066  0.23236131 11.25227235], Loss = 0.8207\n",
      "Iteration 2829: Weights = [55.30333333  3.07866061  7.37278073  0.12126282  0.2323463  11.25278556], Loss = 0.8206\n",
      "Iteration 2830: Weights = [55.30333333  3.07846169  7.37230436  0.12125498  0.23233129 11.25329873], Loss = 0.8205\n",
      "Iteration 2831: Weights = [55.30333333  3.07826278  7.37182802  0.12124715  0.23231627 11.25381187], Loss = 0.8204\n",
      "Iteration 2832: Weights = [55.30333333  3.07806389  7.37135171  0.12123932  0.23230126 11.25432498], Loss = 0.8203\n",
      "Iteration 2833: Weights = [55.30333333  3.07786501  7.37087543  0.12123148  0.23228625 11.25483805], Loss = 0.8202\n",
      "Iteration 2834: Weights = [55.30333333  3.07766614  7.37039919  0.12122365  0.23227125 11.25535109], Loss = 0.8201\n",
      "Iteration 2835: Weights = [55.30333333  3.07746729  7.36992297  0.12121582  0.23225624 11.2558641 ], Loss = 0.8200\n",
      "Iteration 2836: Weights = [55.30333333  3.07726845  7.36944678  0.12120799  0.23224123 11.25637707], Loss = 0.8198\n",
      "Iteration 2837: Weights = [55.30333333  3.07706962  7.36897063  0.12120015  0.23222623 11.25689001], Loss = 0.8197\n",
      "Iteration 2838: Weights = [55.30333333  3.0768708   7.3684945   0.12119232  0.23221122 11.25740292], Loss = 0.8196\n",
      "Iteration 2839: Weights = [55.30333333  3.076672    7.36801841  0.12118449  0.23219622 11.25791579], Loss = 0.8195\n",
      "Iteration 2840: Weights = [55.30333333  3.07647321  7.36754235  0.12117666  0.23218122 11.25842863], Loss = 0.8194\n",
      "Iteration 2841: Weights = [55.30333333  3.07627443  7.36706632  0.12116883  0.23216621 11.25894144], Loss = 0.8193\n",
      "Iteration 2842: Weights = [55.30333333  3.07607567  7.36659032  0.121161    0.23215121 11.25945422], Loss = 0.8192\n",
      "Iteration 2843: Weights = [55.30333333  3.07587692  7.36611435  0.12115318  0.23213621 11.25996696], Loss = 0.8191\n",
      "Iteration 2844: Weights = [55.30333333  3.07567818  7.36563841  0.12114535  0.23212121 11.26047967], Loss = 0.8190\n",
      "Iteration 2845: Weights = [55.30333333  3.07547945  7.3651625   0.12113752  0.23210622 11.26099234], Loss = 0.8189\n",
      "Iteration 2846: Weights = [55.30333333  3.07528074  7.36468662  0.12112969  0.23209122 11.26150499], Loss = 0.8188\n",
      "Iteration 2847: Weights = [55.30333333  3.07508204  7.36421077  0.12112187  0.23207622 11.2620176 ], Loss = 0.8187\n",
      "Iteration 2848: Weights = [55.30333333  3.07488335  7.36373496  0.12111404  0.23206123 11.26253017], Loss = 0.8186\n",
      "Iteration 2849: Weights = [55.30333333  3.07468468  7.36325917  0.12110622  0.23204623 11.26304272], Loss = 0.8185\n",
      "Iteration 2850: Weights = [55.30333333  3.07448602  7.36278342  0.12109839  0.23203124 11.26355523], Loss = 0.8184\n",
      "Iteration 2851: Weights = [55.30333333  3.07428737  7.36230769  0.12109057  0.23201625 11.2640677 ], Loss = 0.8183\n",
      "Iteration 2852: Weights = [55.30333333  3.07408873  7.361832    0.12108274  0.23200126 11.26458015], Loss = 0.8182\n",
      "Iteration 2853: Weights = [55.30333333  3.07389011  7.36135634  0.12107492  0.23198627 11.26509256], Loss = 0.8180\n",
      "Iteration 2854: Weights = [55.30333333  3.0736915   7.3608807   0.1210671   0.23197128 11.26560494], Loss = 0.8179\n",
      "Iteration 2855: Weights = [55.30333333  3.0734929   7.3604051   0.12105927  0.23195629 11.26611728], Loss = 0.8178\n",
      "Iteration 2856: Weights = [55.30333333  3.07329432  7.35992953  0.12105145  0.2319413  11.26662959], Loss = 0.8177\n",
      "Iteration 2857: Weights = [55.30333333  3.07309575  7.35945399  0.12104363  0.23192632 11.26714187], Loss = 0.8176\n",
      "Iteration 2858: Weights = [55.30333333  3.07289719  7.35897848  0.12103581  0.23191133 11.26765411], Loss = 0.8175\n",
      "Iteration 2859: Weights = [55.30333333  3.07269864  7.358503    0.12102799  0.23189635 11.26816633], Loss = 0.8174\n",
      "Iteration 2860: Weights = [55.30333333  3.07250011  7.35802756  0.12102017  0.23188137 11.26867851], Loss = 0.8173\n",
      "Iteration 2861: Weights = [55.30333333  3.07230159  7.35755214  0.12101235  0.23186638 11.26919065], Loss = 0.8172\n",
      "Iteration 2862: Weights = [55.30333333  3.07210308  7.35707675  0.12100453  0.2318514  11.26970277], Loss = 0.8171\n",
      "Iteration 2863: Weights = [55.30333333  3.07190459  7.3566014   0.12099671  0.23183642 11.27021484], Loss = 0.8170\n",
      "Iteration 2864: Weights = [55.30333333  3.0717061   7.35612607  0.12098889  0.23182144 11.27072689], Loss = 0.8169\n",
      "Iteration 2865: Weights = [55.30333333  3.07150763  7.35565078  0.12098108  0.23180646 11.27123891], Loss = 0.8168\n",
      "Iteration 2866: Weights = [55.30333333  3.07130918  7.35517552  0.12097326  0.23179149 11.27175089], Loss = 0.8167\n",
      "Iteration 2867: Weights = [55.30333333  3.07111074  7.35470028  0.12096544  0.23177651 11.27226283], Loss = 0.8166\n",
      "Iteration 2868: Weights = [55.30333333  3.0709123   7.35422508  0.12095763  0.23176153 11.27277475], Loss = 0.8165\n",
      "Iteration 2869: Weights = [55.30333333  3.07071389  7.35374991  0.12094981  0.23174656 11.27328663], Loss = 0.8164\n",
      "Iteration 2870: Weights = [55.30333333  3.07051548  7.35327477  0.120942    0.23173159 11.27379848], Loss = 0.8163\n",
      "Iteration 2871: Weights = [55.30333333  3.07031709  7.35279966  0.12093418  0.23171661 11.27431029], Loss = 0.8161\n",
      "Iteration 2872: Weights = [55.30333333  3.07011871  7.35232458  0.12092637  0.23170164 11.27482207], Loss = 0.8160\n",
      "Iteration 2873: Weights = [55.30333333  3.06992034  7.35184953  0.12091856  0.23168667 11.27533382], Loss = 0.8159\n",
      "Iteration 2874: Weights = [55.30333333  3.06972199  7.35137451  0.12091074  0.2316717  11.27584554], Loss = 0.8158\n",
      "Iteration 2875: Weights = [55.30333333  3.06952365  7.35089953  0.12090293  0.23165673 11.27635722], Loss = 0.8157\n",
      "Iteration 2876: Weights = [55.30333333  3.06932532  7.35042457  0.12089512  0.23164176 11.27686887], Loss = 0.8156\n",
      "Iteration 2877: Weights = [55.30333333  3.06912701  7.34994964  0.12088731  0.2316268  11.27738049], Loss = 0.8155\n",
      "Iteration 2878: Weights = [55.30333333  3.0689287   7.34947475  0.1208795   0.23161183 11.27789207], Loss = 0.8154\n",
      "Iteration 2879: Weights = [55.30333333  3.06873041  7.34899989  0.12087169  0.23159687 11.27840362], Loss = 0.8153\n",
      "Iteration 2880: Weights = [55.30333333  3.06853214  7.34852505  0.12086388  0.2315819  11.27891514], Loss = 0.8152\n",
      "Iteration 2881: Weights = [55.30333333  3.06833387  7.34805025  0.12085607  0.23156694 11.27942663], Loss = 0.8151\n",
      "Iteration 2882: Weights = [55.30333333  3.06813562  7.34757548  0.12084826  0.23155198 11.27993808], Loss = 0.8150\n",
      "Iteration 2883: Weights = [55.30333333  3.06793738  7.34710073  0.12084045  0.23153702 11.2804495 ], Loss = 0.8149\n",
      "Iteration 2884: Weights = [55.30333333  3.06773916  7.34662602  0.12083264  0.23152206 11.28096088], Loss = 0.8148\n",
      "Iteration 2885: Weights = [55.30333333  3.06754094  7.34615134  0.12082484  0.2315071  11.28147223], Loss = 0.8147\n",
      "Iteration 2886: Weights = [55.30333333  3.06734274  7.34567669  0.12081703  0.23149214 11.28198355], Loss = 0.8146\n",
      "Iteration 2887: Weights = [55.30333333  3.06714456  7.34520207  0.12080922  0.23147718 11.28249484], Loss = 0.8145\n",
      "Iteration 2888: Weights = [55.30333333  3.06694638  7.34472749  0.12080142  0.23146223 11.28300609], Loss = 0.8144\n",
      "Iteration 2889: Weights = [55.30333333  3.06674822  7.34425293  0.12079361  0.23144727 11.28351731], Loss = 0.8143\n",
      "Iteration 2890: Weights = [55.30333333  3.06655007  7.3437784   0.12078581  0.23143232 11.2840285 ], Loss = 0.8141\n",
      "Iteration 2891: Weights = [55.30333333  3.06635194  7.34330391  0.120778    0.23141736 11.28453965], Loss = 0.8140\n",
      "Iteration 2892: Weights = [55.30333333  3.06615381  7.34282944  0.1207702   0.23140241 11.28505077], Loss = 0.8139\n",
      "Iteration 2893: Weights = [55.30333333  3.0659557   7.34235501  0.1207624   0.23138746 11.28556186], Loss = 0.8138\n",
      "Iteration 2894: Weights = [55.30333333  3.06575761  7.3418806   0.12075459  0.23137251 11.28607292], Loss = 0.8137\n",
      "Iteration 2895: Weights = [55.30333333  3.06555952  7.34140623  0.12074679  0.23135756 11.28658394], Loss = 0.8136\n",
      "Iteration 2896: Weights = [55.30333333  3.06536145  7.34093188  0.12073899  0.23134261 11.28709493], Loss = 0.8135\n",
      "Iteration 2897: Weights = [55.30333333  3.06516339  7.34045757  0.12073119  0.23132766 11.28760589], Loss = 0.8134\n",
      "Iteration 2898: Weights = [55.30333333  3.06496534  7.33998329  0.12072339  0.23131272 11.28811681], Loss = 0.8133\n",
      "Iteration 2899: Weights = [55.30333333  3.06476731  7.33950904  0.12071559  0.23129777 11.2886277 ], Loss = 0.8132\n",
      "Iteration 2900: Weights = [55.30333333  3.06456929  7.33903482  0.12070779  0.23128283 11.28913856], Loss = 0.8131\n",
      "Iteration 2901: Weights = [55.30333333  3.06437128  7.33856063  0.12069999  0.23126788 11.28964938], Loss = 0.8130\n",
      "Iteration 2902: Weights = [55.30333333  3.06417329  7.33808647  0.12069219  0.23125294 11.29016017], Loss = 0.8129\n",
      "Iteration 2903: Weights = [55.30333333  3.0639753   7.33761234  0.12068439  0.231238   11.29067093], Loss = 0.8128\n",
      "Iteration 2904: Weights = [55.30333333  3.06377733  7.33713824  0.12067659  0.23122306 11.29118165], Loss = 0.8127\n",
      "Iteration 2905: Weights = [55.30333333  3.06357938  7.33666418  0.1206688   0.23120812 11.29169235], Loss = 0.8126\n",
      "Iteration 2906: Weights = [55.30333333  3.06338143  7.33619014  0.120661    0.23119318 11.292203  ], Loss = 0.8125\n",
      "Iteration 2907: Weights = [55.30333333  3.0631835   7.33571613  0.1206532   0.23117824 11.29271363], Loss = 0.8124\n",
      "Iteration 2908: Weights = [55.30333333  3.06298558  7.33524216  0.12064541  0.2311633  11.29322422], Loss = 0.8123\n",
      "Iteration 2909: Weights = [55.30333333  3.06278768  7.33476821  0.12063761  0.23114837 11.29373478], Loss = 0.8121\n",
      "Iteration 2910: Weights = [55.30333333  3.06258978  7.3342943   0.12062982  0.23113343 11.29424531], Loss = 0.8120\n",
      "Iteration 2911: Weights = [55.30333333  3.0623919   7.33382042  0.12062203  0.2311185  11.2947558 ], Loss = 0.8119\n",
      "Iteration 2912: Weights = [55.30333333  3.06219404  7.33334656  0.12061423  0.23110357 11.29526627], Loss = 0.8118\n",
      "Iteration 2913: Weights = [55.30333333  3.06199618  7.33287274  0.12060644  0.23108863 11.29577669], Loss = 0.8117\n",
      "Iteration 2914: Weights = [55.30333333  3.06179834  7.33239895  0.12059865  0.2310737  11.29628709], Loss = 0.8116\n",
      "Iteration 2915: Weights = [55.30333333  3.06160051  7.33192519  0.12059085  0.23105877 11.29679745], Loss = 0.8115\n",
      "Iteration 2916: Weights = [55.30333333  3.0614027   7.33145146  0.12058306  0.23104384 11.29730778], Loss = 0.8114\n",
      "Iteration 2917: Weights = [55.30333333  3.06120489  7.33097776  0.12057527  0.23102892 11.29781808], Loss = 0.8113\n",
      "Iteration 2918: Weights = [55.30333333  3.0610071   7.33050409  0.12056748  0.23101399 11.29832834], Loss = 0.8112\n",
      "Iteration 2919: Weights = [55.30333333  3.06080932  7.33003045  0.12055969  0.23099906 11.29883857], Loss = 0.8111\n",
      "Iteration 2920: Weights = [55.30333333  3.06061156  7.32955684  0.1205519   0.23098414 11.29934877], Loss = 0.8110\n",
      "Iteration 2921: Weights = [55.30333333  3.06041381  7.32908326  0.12054411  0.23096921 11.29985893], Loss = 0.8109\n",
      "Iteration 2922: Weights = [55.30333333  3.06021607  7.32860972  0.12053632  0.23095429 11.30036906], Loss = 0.8108\n",
      "Iteration 2923: Weights = [55.30333333  3.06001834  7.3281362   0.12052853  0.23093937 11.30087916], Loss = 0.8107\n",
      "Iteration 2924: Weights = [55.30333333  3.05982063  7.32766272  0.12052075  0.23092445 11.30138923], Loss = 0.8106\n",
      "Iteration 2925: Weights = [55.30333333  3.05962293  7.32718926  0.12051296  0.23090953 11.30189926], Loss = 0.8105\n",
      "Iteration 2926: Weights = [55.30333333  3.05942524  7.32671584  0.12050517  0.23089461 11.30240926], Loss = 0.8104\n",
      "Iteration 2927: Weights = [55.30333333  3.05922756  7.32624244  0.12049739  0.23087969 11.30291922], Loss = 0.8103\n",
      "Iteration 2928: Weights = [55.30333333  3.0590299   7.32576908  0.1204896   0.23086477 11.30342916], Loss = 0.8102\n",
      "Iteration 2929: Weights = [55.30333333  3.05883225  7.32529575  0.12048182  0.23084985 11.30393906], Loss = 0.8101\n",
      "Iteration 2930: Weights = [55.30333333  3.05863461  7.32482244  0.12047403  0.23083494 11.30444893], Loss = 0.8099\n",
      "Iteration 2931: Weights = [55.30333333  3.05843699  7.32434917  0.12046625  0.23082002 11.30495876], Loss = 0.8098\n",
      "Iteration 2932: Weights = [55.30333333  3.05823938  7.32387593  0.12045846  0.23080511 11.30546856], Loss = 0.8097\n",
      "Iteration 2933: Weights = [55.30333333  3.05804178  7.32340272  0.12045068  0.2307902  11.30597833], Loss = 0.8096\n",
      "Iteration 2934: Weights = [55.30333333  3.05784419  7.32292954  0.1204429   0.23077528 11.30648807], Loss = 0.8095\n",
      "Iteration 2935: Weights = [55.30333333  3.05764662  7.32245639  0.12043512  0.23076037 11.30699777], Loss = 0.8094\n",
      "Iteration 2936: Weights = [55.30333333  3.05744906  7.32198327  0.12042734  0.23074546 11.30750744], Loss = 0.8093\n",
      "Iteration 2937: Weights = [55.30333333  3.05725151  7.32151019  0.12041955  0.23073055 11.30801708], Loss = 0.8092\n",
      "Iteration 2938: Weights = [55.30333333  3.05705397  7.32103713  0.12041177  0.23071565 11.30852668], Loss = 0.8091\n",
      "Iteration 2939: Weights = [55.30333333  3.05685645  7.3205641   0.12040399  0.23070074 11.30903625], Loss = 0.8090\n",
      "Iteration 2940: Weights = [55.30333333  3.05665894  7.32009111  0.12039621  0.23068583 11.30954579], Loss = 0.8089\n",
      "Iteration 2941: Weights = [55.30333333  3.05646144  7.31961814  0.12038844  0.23067093 11.3100553 ], Loss = 0.8088\n",
      "Iteration 2942: Weights = [55.30333333  3.05626396  7.3191452   0.12038066  0.23065602 11.31056477], Loss = 0.8087\n",
      "Iteration 2943: Weights = [55.30333333  3.05606649  7.3186723   0.12037288  0.23064112 11.31107421], Loss = 0.8086\n",
      "Iteration 2944: Weights = [55.30333333  3.05586903  7.31819943  0.1203651   0.23062622 11.31158362], Loss = 0.8085\n",
      "Iteration 2945: Weights = [55.30333333  3.05567158  7.31772658  0.12035732  0.23061132 11.31209299], Loss = 0.8084\n",
      "Iteration 2946: Weights = [55.30333333  3.05547415  7.31725377  0.12034955  0.23059642 11.31260233], Loss = 0.8083\n",
      "Iteration 2947: Weights = [55.30333333  3.05527673  7.31678099  0.12034177  0.23058152 11.31311164], Loss = 0.8082\n",
      "Iteration 2948: Weights = [55.30333333  3.05507932  7.31630823  0.120334    0.23056662 11.31362091], Loss = 0.8081\n",
      "Iteration 2949: Weights = [55.30333333  3.05488193  7.31583551  0.12032622  0.23055172 11.31413016], Loss = 0.8080\n",
      "Iteration 2950: Weights = [55.30333333  3.05468455  7.31536282  0.12031845  0.23053683 11.31463937], Loss = 0.8079\n",
      "Iteration 2951: Weights = [55.30333333  3.05448718  7.31489016  0.12031067  0.23052193 11.31514854], Loss = 0.8078\n",
      "Iteration 2952: Weights = [55.30333333  3.05428982  7.31441753  0.1203029   0.23050704 11.31565769], Loss = 0.8076\n",
      "Iteration 2953: Weights = [55.30333333  3.05409248  7.31394493  0.12029513  0.23049214 11.3161668 ], Loss = 0.8075\n",
      "Iteration 2954: Weights = [55.30333333  3.05389515  7.31347236  0.12028735  0.23047725 11.31667587], Loss = 0.8074\n",
      "Iteration 2955: Weights = [55.30333333  3.05369783  7.31299983  0.12027958  0.23046236 11.31718492], Loss = 0.8073\n",
      "Iteration 2956: Weights = [55.30333333  3.05350052  7.31252732  0.12027181  0.23044747 11.31769393], Loss = 0.8072\n",
      "Iteration 2957: Weights = [55.30333333  3.05330323  7.31205484  0.12026404  0.23043258 11.31820291], Loss = 0.8071\n",
      "Iteration 2958: Weights = [55.30333333  3.05310595  7.3115824   0.12025627  0.23041769 11.31871186], Loss = 0.8070\n",
      "Iteration 2959: Weights = [55.30333333  3.05290868  7.31110998  0.1202485   0.2304028  11.31922077], Loss = 0.8069\n",
      "Iteration 2960: Weights = [55.30333333  3.05271143  7.31063759  0.12024073  0.23038792 11.31972965], Loss = 0.8068\n",
      "Iteration 2961: Weights = [55.30333333  3.05251418  7.31016524  0.12023296  0.23037303 11.3202385 ], Loss = 0.8067\n",
      "Iteration 2962: Weights = [55.30333333  3.05231696  7.30969291  0.12022519  0.23035814 11.32074731], Loss = 0.8066\n",
      "Iteration 2963: Weights = [55.30333333  3.05211974  7.30922062  0.12021742  0.23034326 11.32125609], Loss = 0.8065\n",
      "Iteration 2964: Weights = [55.30333333  3.05192254  7.30874836  0.12020966  0.23032838 11.32176484], Loss = 0.8064\n",
      "Iteration 2965: Weights = [55.30333333  3.05172535  7.30827612  0.12020189  0.2303135  11.32227356], Loss = 0.8063\n",
      "Iteration 2966: Weights = [55.30333333  3.05152817  7.30780392  0.12019412  0.23029861 11.32278224], Loss = 0.8062\n",
      "Iteration 2967: Weights = [55.30333333  3.051331    7.30733175  0.12018636  0.23028373 11.32329089], Loss = 0.8061\n",
      "Iteration 2968: Weights = [55.30333333  3.05113385  7.30685961  0.12017859  0.23026886 11.32379951], Loss = 0.8060\n",
      "Iteration 2969: Weights = [55.30333333  3.05093671  7.3063875   0.12017083  0.23025398 11.32430809], Loss = 0.8059\n",
      "Iteration 2970: Weights = [55.30333333  3.05073958  7.30591542  0.12016306  0.2302391  11.32481664], Loss = 0.8058\n",
      "Iteration 2971: Weights = [55.30333333  3.05054247  7.30544337  0.1201553   0.23022422 11.32532516], Loss = 0.8057\n",
      "Iteration 2972: Weights = [55.30333333  3.05034537  7.30497135  0.12014753  0.23020935 11.32583365], Loss = 0.8056\n",
      "Iteration 2973: Weights = [55.30333333  3.05014828  7.30449936  0.12013977  0.23019447 11.3263421 ], Loss = 0.8055\n",
      "Iteration 2974: Weights = [55.30333333  3.0499512   7.3040274   0.12013201  0.2301796  11.32685052], Loss = 0.8054\n",
      "Iteration 2975: Weights = [55.30333333  3.04975414  7.30355547  0.12012425  0.23016473 11.32735891], Loss = 0.8053\n",
      "Iteration 2976: Weights = [55.30333333  3.04955709  7.30308357  0.12011649  0.23014986 11.32786726], Loss = 0.8051\n",
      "Iteration 2977: Weights = [55.30333333  3.04936005  7.30261171  0.12010872  0.23013499 11.32837559], Loss = 0.8050\n",
      "Iteration 2978: Weights = [55.30333333  3.04916302  7.30213987  0.12010096  0.23012012 11.32888387], Loss = 0.8049\n",
      "Iteration 2979: Weights = [55.30333333  3.04896601  7.30166807  0.1200932   0.23010525 11.32939213], Loss = 0.8048\n",
      "Iteration 2980: Weights = [55.30333333  3.04876901  7.30119629  0.12008544  0.23009038 11.32990035], Loss = 0.8047\n",
      "Iteration 2981: Weights = [55.30333333  3.04857202  7.30072455  0.12007769  0.23007551 11.33040854], Loss = 0.8046\n",
      "Iteration 2982: Weights = [55.30333333  3.04837505  7.30025283  0.12006993  0.23006065 11.3309167 ], Loss = 0.8045\n",
      "Iteration 2983: Weights = [55.30333333  3.04817809  7.29978115  0.12006217  0.23004578 11.33142483], Loss = 0.8044\n",
      "Iteration 2984: Weights = [55.30333333  3.04798114  7.29930949  0.12005441  0.23003092 11.33193292], Loss = 0.8043\n",
      "Iteration 2985: Weights = [55.30333333  3.0477842   7.29883787  0.12004665  0.23001606 11.33244098], Loss = 0.8042\n",
      "Iteration 2986: Weights = [55.30333333  3.04758728  7.29836628  0.1200389   0.2300012  11.332949  ], Loss = 0.8041\n",
      "Iteration 2987: Weights = [55.30333333  3.04739037  7.29789472  0.12003114  0.22998634 11.333457  ], Loss = 0.8040\n",
      "Iteration 2988: Weights = [55.30333333  3.04719347  7.29742318  0.12002339  0.22997148 11.33396496], Loss = 0.8039\n",
      "Iteration 2989: Weights = [55.30333333  3.04699659  7.29695168  0.12001563  0.22995662 11.33447288], Loss = 0.8038\n",
      "Iteration 2990: Weights = [55.30333333  3.04679971  7.29648021  0.12000788  0.22994176 11.33498078], Loss = 0.8037\n",
      "Iteration 2991: Weights = [55.30333333  3.04660285  7.29600877  0.12000012  0.2299269  11.33548864], Loss = 0.8036\n",
      "Iteration 2992: Weights = [55.30333333  3.04640601  7.29553736  0.11999237  0.22991205 11.33599647], Loss = 0.8035\n",
      "Iteration 2993: Weights = [55.30333333  3.04620917  7.29506598  0.11998462  0.22989719 11.33650427], Loss = 0.8034\n",
      "Iteration 2994: Weights = [55.30333333  3.04601235  7.29459463  0.11997686  0.22988234 11.33701203], Loss = 0.8033\n",
      "Iteration 2995: Weights = [55.30333333  3.04581554  7.29412332  0.11996911  0.22986748 11.33751976], Loss = 0.8032\n",
      "Iteration 2996: Weights = [55.30333333  3.04561874  7.29365203  0.11996136  0.22985263 11.33802746], Loss = 0.8031\n",
      "Iteration 2997: Weights = [55.30333333  3.04542196  7.29318077  0.11995361  0.22983778 11.33853512], Loss = 0.8030\n",
      "Iteration 2998: Weights = [55.30333333  3.04522519  7.29270954  0.11994586  0.22982293 11.33904276], Loss = 0.8029\n",
      "Iteration 2999: Weights = [55.30333333  3.04502843  7.29223835  0.11993811  0.22980808 11.33955036], Loss = 0.8028\n",
      "Iteration 3000: Weights = [55.30333333  3.04483169  7.29176718  0.11993036  0.22979323 11.34005792], Loss = 0.8027\n",
      "Iteration 3001: Weights = [55.30333333  3.04463495  7.29129604  0.11992261  0.22977838 11.34056546], Loss = 0.8026\n",
      "Iteration 3002: Weights = [55.30333333  3.04443823  7.29082494  0.11991486  0.22976354 11.34107296], Loss = 0.8024\n",
      "Iteration 3003: Weights = [55.30333333  3.04424153  7.29035386  0.11990712  0.22974869 11.34158043], Loss = 0.8023\n",
      "Iteration 3004: Weights = [55.30333333  3.04404483  7.28988282  0.11989937  0.22973385 11.34208786], Loss = 0.8022\n",
      "Iteration 3005: Weights = [55.30333333  3.04384815  7.28941181  0.11989162  0.229719   11.34259527], Loss = 0.8021\n",
      "Iteration 3006: Weights = [55.30333333  3.04365148  7.28894082  0.11988387  0.22970416 11.34310264], Loss = 0.8020\n",
      "Iteration 3007: Weights = [55.30333333  3.04345482  7.28846987  0.11987613  0.22968932 11.34360997], Loss = 0.8019\n",
      "Iteration 3008: Weights = [55.30333333  3.04325818  7.28799895  0.11986838  0.22967448 11.34411728], Loss = 0.8018\n",
      "Iteration 3009: Weights = [55.30333333  3.04306155  7.28752805  0.11986064  0.22965964 11.34462455], Loss = 0.8017\n",
      "Iteration 3010: Weights = [55.30333333  3.04286493  7.28705719  0.11985289  0.2296448  11.34513179], Loss = 0.8016\n",
      "Iteration 3011: Weights = [55.30333333  3.04266833  7.28658636  0.11984515  0.22962996 11.34563899], Loss = 0.8015\n",
      "Iteration 3012: Weights = [55.30333333  3.04247173  7.28611556  0.11983741  0.22961513 11.34614617], Loss = 0.8014\n",
      "Iteration 3013: Weights = [55.30333333  3.04227515  7.28564479  0.11982966  0.22960029 11.34665331], Loss = 0.8013\n",
      "Iteration 3014: Weights = [55.30333333  3.04207859  7.28517405  0.11982192  0.22958546 11.34716042], Loss = 0.8012\n",
      "Iteration 3015: Weights = [55.30333333  3.04188203  7.28470334  0.11981418  0.22957062 11.34766749], Loss = 0.8011\n",
      "Iteration 3016: Weights = [55.30333333  3.04168549  7.28423266  0.11980644  0.22955579 11.34817453], Loss = 0.8010\n",
      "Iteration 3017: Weights = [55.30333333  3.04148896  7.28376201  0.1197987   0.22954096 11.34868154], Loss = 0.8009\n",
      "Iteration 3018: Weights = [55.30333333  3.04129244  7.28329139  0.11979096  0.22952613 11.34918852], Loss = 0.8008\n",
      "Iteration 3019: Weights = [55.30333333  3.04109594  7.2828208   0.11978322  0.22951129 11.34969546], Loss = 0.8007\n",
      "Iteration 3020: Weights = [55.30333333  3.04089945  7.28235025  0.11977548  0.22949647 11.35020238], Loss = 0.8006\n",
      "Iteration 3021: Weights = [55.30333333  3.04070297  7.28187972  0.11976774  0.22948164 11.35070925], Loss = 0.8005\n",
      "Iteration 3022: Weights = [55.30333333  3.0405065   7.28140922  0.11976     0.22946681 11.3512161 ], Loss = 0.8004\n",
      "Iteration 3023: Weights = [55.30333333  3.04031005  7.28093876  0.11975226  0.22945198 11.35172291], Loss = 0.8003\n",
      "Iteration 3024: Weights = [55.30333333  3.04011361  7.28046832  0.11974452  0.22943716 11.35222969], Loss = 0.8002\n",
      "Iteration 3025: Weights = [55.30333333  3.03991718  7.27999791  0.11973679  0.22942233 11.35273644], Loss = 0.8001\n",
      "Iteration 3026: Weights = [55.30333333  3.03972077  7.27952754  0.11972905  0.22940751 11.35324316], Loss = 0.8000\n",
      "Iteration 3027: Weights = [55.30333333  3.03952436  7.27905719  0.11972131  0.22939269 11.35374984], Loss = 0.7999\n",
      "Iteration 3028: Weights = [55.30333333  3.03932797  7.27858688  0.11971358  0.22937787 11.35425649], Loss = 0.7998\n",
      "Iteration 3029: Weights = [55.30333333  3.0391316   7.2781166   0.11970584  0.22936305 11.3547631 ], Loss = 0.7997\n",
      "Iteration 3030: Weights = [55.30333333  3.03893523  7.27764634  0.11969811  0.22934823 11.35526969], Loss = 0.7995\n",
      "Iteration 3031: Weights = [55.30333333  3.03873888  7.27717612  0.11969038  0.22933341 11.35577624], Loss = 0.7994\n",
      "Iteration 3032: Weights = [55.30333333  3.03854254  7.27670593  0.11968264  0.22931859 11.35628276], Loss = 0.7993\n",
      "Iteration 3033: Weights = [55.30333333  3.03834621  7.27623576  0.11967491  0.22930377 11.35678924], Loss = 0.7992\n",
      "Iteration 3034: Weights = [55.30333333  3.0381499   7.27576563  0.11966718  0.22928896 11.3572957 ], Loss = 0.7991\n",
      "Iteration 3035: Weights = [55.30333333  3.0379536   7.27529553  0.11965945  0.22927414 11.35780212], Loss = 0.7990\n",
      "Iteration 3036: Weights = [55.30333333  3.03775731  7.27482546  0.11965171  0.22925933 11.3583085 ], Loss = 0.7989\n",
      "Iteration 3037: Weights = [55.30333333  3.03756104  7.27435542  0.11964398  0.22924452 11.35881486], Loss = 0.7988\n",
      "Iteration 3038: Weights = [55.30333333  3.03736477  7.27388541  0.11963625  0.2292297  11.35932118], Loss = 0.7987\n",
      "Iteration 3039: Weights = [55.30333333  3.03716852  7.27341543  0.11962852  0.22921489 11.35982747], Loss = 0.7986\n",
      "Iteration 3040: Weights = [55.30333333  3.03697229  7.27294548  0.11962079  0.22920008 11.36033373], Loss = 0.7985\n",
      "Iteration 3041: Weights = [55.30333333  3.03677606  7.27247556  0.11961306  0.22918527 11.36083995], Loss = 0.7984\n",
      "Iteration 3042: Weights = [55.30333333  3.03657985  7.27200567  0.11960534  0.22917047 11.36134614], Loss = 0.7983\n",
      "Iteration 3043: Weights = [55.30333333  3.03638365  7.27153581  0.11959761  0.22915566 11.3618523 ], Loss = 0.7982\n",
      "Iteration 3044: Weights = [55.30333333  3.03618746  7.27106598  0.11958988  0.22914085 11.36235843], Loss = 0.7981\n",
      "Iteration 3045: Weights = [55.30333333  3.03599129  7.27059618  0.11958215  0.22912605 11.36286452], Loss = 0.7980\n",
      "Iteration 3046: Weights = [55.30333333  3.03579513  7.27012641  0.11957443  0.22911124 11.36337058], Loss = 0.7979\n",
      "Iteration 3047: Weights = [55.30333333  3.03559898  7.26965668  0.1195667   0.22909644 11.36387661], Loss = 0.7978\n",
      "Iteration 3048: Weights = [55.30333333  3.03540284  7.26918697  0.11955898  0.22908164 11.3643826 ], Loss = 0.7977\n",
      "Iteration 3049: Weights = [55.30333333  3.03520672  7.26871729  0.11955125  0.22906684 11.36488856], Loss = 0.7976\n",
      "Iteration 3050: Weights = [55.30333333  3.03501061  7.26824765  0.11954353  0.22905204 11.36539449], Loss = 0.7975\n",
      "Iteration 3051: Weights = [55.30333333  3.03481451  7.26777803  0.1195358   0.22903724 11.36590039], Loss = 0.7974\n",
      "Iteration 3052: Weights = [55.30333333  3.03461842  7.26730844  0.11952808  0.22902244 11.36640626], Loss = 0.7973\n",
      "Iteration 3053: Weights = [55.30333333  3.03442235  7.26683889  0.11952036  0.22900764 11.36691209], Loss = 0.7972\n",
      "Iteration 3054: Weights = [55.30333333  3.03422629  7.26636936  0.11951263  0.22899284 11.36741789], Loss = 0.7971\n",
      "Iteration 3055: Weights = [55.30333333  3.03403024  7.26589987  0.11950491  0.22897805 11.36792365], Loss = 0.7970\n",
      "Iteration 3056: Weights = [55.30333333  3.03383421  7.2654304   0.11949719  0.22896325 11.36842939], Loss = 0.7969\n",
      "Iteration 3057: Weights = [55.30333333  3.03363819  7.26496097  0.11948947  0.22894846 11.36893509], Loss = 0.7968\n",
      "Iteration 3058: Weights = [55.30333333  3.03344218  7.26449157  0.11948175  0.22893367 11.36944075], Loss = 0.7967\n",
      "Iteration 3059: Weights = [55.30333333  3.03324618  7.26402219  0.11947403  0.22891887 11.36994639], Loss = 0.7966\n",
      "Iteration 3060: Weights = [55.30333333  3.0330502   7.26355285  0.11946631  0.22890408 11.37045199], Loss = 0.7965\n",
      "Iteration 3061: Weights = [55.30333333  3.03285422  7.26308354  0.11945859  0.22888929 11.37095756], Loss = 0.7964\n",
      "Iteration 3062: Weights = [55.30333333  3.03265827  7.26261425  0.11945087  0.2288745  11.3714631 ], Loss = 0.7962\n",
      "Iteration 3063: Weights = [55.30333333  3.03246232  7.262145    0.11944315  0.22885972 11.37196861], Loss = 0.7961\n",
      "Iteration 3064: Weights = [55.30333333  3.03226639  7.26167578  0.11943544  0.22884493 11.37247408], Loss = 0.7960\n",
      "Iteration 3065: Weights = [55.30333333  3.03207046  7.26120659  0.11942772  0.22883014 11.37297952], Loss = 0.7959\n",
      "Iteration 3066: Weights = [55.30333333  3.03187456  7.26073743  0.11942     0.22881536 11.37348492], Loss = 0.7958\n",
      "Iteration 3067: Weights = [55.30333333  3.03167866  7.2602683   0.11941229  0.22880057 11.3739903 ], Loss = 0.7957\n",
      "Iteration 3068: Weights = [55.30333333  3.03148278  7.2597992   0.11940457  0.22878579 11.37449564], Loss = 0.7956\n",
      "Iteration 3069: Weights = [55.30333333  3.03128691  7.25933013  0.11939686  0.22877101 11.37500095], Loss = 0.7955\n",
      "Iteration 3070: Weights = [55.30333333  3.03109105  7.25886109  0.11938914  0.22875623 11.37550623], Loss = 0.7954\n",
      "Iteration 3071: Weights = [55.30333333  3.03089521  7.25839208  0.11938143  0.22874145 11.37601147], Loss = 0.7953\n",
      "Iteration 3072: Weights = [55.30333333  3.03069937  7.2579231   0.11937371  0.22872667 11.37651668], Loss = 0.7952\n",
      "Iteration 3073: Weights = [55.30333333  3.03050355  7.25745415  0.119366    0.22871189 11.37702186], Loss = 0.7951\n",
      "Iteration 3074: Weights = [55.30333333  3.03030775  7.25698523  0.11935829  0.22869711 11.377527  ], Loss = 0.7950\n",
      "Iteration 3075: Weights = [55.30333333  3.03011195  7.25651634  0.11935058  0.22868233 11.37803212], Loss = 0.7949\n",
      "Iteration 3076: Weights = [55.30333333  3.02991617  7.25604748  0.11934287  0.22866756 11.3785372 ], Loss = 0.7948\n",
      "Iteration 3077: Weights = [55.30333333  3.0297204   7.25557866  0.11933515  0.22865278 11.37904225], Loss = 0.7947\n",
      "Iteration 3078: Weights = [55.30333333  3.02952465  7.25510986  0.11932744  0.22863801 11.37954726], Loss = 0.7946\n",
      "Iteration 3079: Weights = [55.30333333  3.0293289   7.25464109  0.11931973  0.22862324 11.38005224], Loss = 0.7945\n",
      "Iteration 3080: Weights = [55.30333333  3.02913317  7.25417235  0.11931202  0.22860847 11.38055719], Loss = 0.7944\n",
      "Iteration 3081: Weights = [55.30333333  3.02893745  7.25370365  0.11930432  0.22859369 11.38106211], Loss = 0.7943\n",
      "Iteration 3082: Weights = [55.30333333  3.02874175  7.25323497  0.11929661  0.22857892 11.38156699], Loss = 0.7942\n",
      "Iteration 3083: Weights = [55.30333333  3.02854605  7.25276632  0.1192889   0.22856416 11.38207185], Loss = 0.7941\n",
      "Iteration 3084: Weights = [55.30333333  3.02835037  7.25229771  0.11928119  0.22854939 11.38257667], Loss = 0.7940\n",
      "Iteration 3085: Weights = [55.30333333  3.02815471  7.25182912  0.11927348  0.22853462 11.38308145], Loss = 0.7939\n",
      "Iteration 3086: Weights = [55.30333333  3.02795905  7.25136057  0.11926578  0.22851985 11.38358621], Loss = 0.7938\n",
      "Iteration 3087: Weights = [55.30333333  3.02776341  7.25089204  0.11925807  0.22850509 11.38409093], Loss = 0.7937\n",
      "Iteration 3088: Weights = [55.30333333  3.02756778  7.25042355  0.11925037  0.22849033 11.38459562], Loss = 0.7936\n",
      "Iteration 3089: Weights = [55.30333333  3.02737216  7.24995508  0.11924266  0.22847556 11.38510027], Loss = 0.7935\n",
      "Iteration 3090: Weights = [55.30333333  3.02717656  7.24948665  0.11923496  0.2284608  11.3856049 ], Loss = 0.7934\n",
      "Iteration 3091: Weights = [55.30333333  3.02698096  7.24901825  0.11922725  0.22844604 11.38610949], Loss = 0.7933\n",
      "Iteration 3092: Weights = [55.30333333  3.02678539  7.24854987  0.11921955  0.22843128 11.38661405], Loss = 0.7932\n",
      "Iteration 3093: Weights = [55.30333333  3.02658982  7.24808153  0.11921185  0.22841652 11.38711857], Loss = 0.7931\n",
      "Iteration 3094: Weights = [55.30333333  3.02639426  7.24761322  0.11920414  0.22840176 11.38762307], Loss = 0.7930\n",
      "Iteration 3095: Weights = [55.30333333  3.02619872  7.24714493  0.11919644  0.228387   11.38812753], Loss = 0.7929\n",
      "Iteration 3096: Weights = [55.30333333  3.02600319  7.24667668  0.11918874  0.22837225 11.38863196], Loss = 0.7928\n",
      "Iteration 3097: Weights = [55.30333333  3.02580768  7.24620846  0.11918104  0.22835749 11.38913635], Loss = 0.7927\n",
      "Iteration 3098: Weights = [55.30333333  3.02561217  7.24574027  0.11917334  0.22834274 11.38964071], Loss = 0.7926\n",
      "Iteration 3099: Weights = [55.30333333  3.02541668  7.2452721   0.11916564  0.22832798 11.39014504], Loss = 0.7925\n",
      "Iteration 3100: Weights = [55.30333333  3.02522121  7.24480397  0.11915794  0.22831323 11.39064934], Loss = 0.7923\n",
      "Iteration 3101: Weights = [55.30333333  3.02502574  7.24433587  0.11915024  0.22829848 11.39115361], Loss = 0.7922\n",
      "Iteration 3102: Weights = [55.30333333  3.02483029  7.2438678   0.11914254  0.22828373 11.39165784], Loss = 0.7921\n",
      "Iteration 3103: Weights = [55.30333333  3.02463485  7.24339976  0.11913484  0.22826898 11.39216204], Loss = 0.7920\n",
      "Iteration 3104: Weights = [55.30333333  3.02443942  7.24293175  0.11912715  0.22825423 11.39266621], Loss = 0.7919\n",
      "Iteration 3105: Weights = [55.30333333  3.024244    7.24246377  0.11911945  0.22823948 11.39317034], Loss = 0.7918\n",
      "Iteration 3106: Weights = [55.30333333  3.0240486   7.24199582  0.11911175  0.22822473 11.39367444], Loss = 0.7917\n",
      "Iteration 3107: Weights = [55.30333333  3.02385321  7.2415279   0.11910406  0.22820999 11.39417851], Loss = 0.7916\n",
      "Iteration 3108: Weights = [55.30333333  3.02365783  7.24106001  0.11909636  0.22819524 11.39468255], Loss = 0.7915\n",
      "Iteration 3109: Weights = [55.30333333  3.02346247  7.24059215  0.11908867  0.2281805  11.39518656], Loss = 0.7914\n",
      "Iteration 3110: Weights = [55.30333333  3.02326712  7.24012432  0.11908097  0.22816576 11.39569053], Loss = 0.7913\n",
      "Iteration 3111: Weights = [55.30333333  3.02307178  7.23965652  0.11907328  0.22815101 11.39619447], Loss = 0.7912\n",
      "Iteration 3112: Weights = [55.30333333  3.02287645  7.23918875  0.11906558  0.22813627 11.39669837], Loss = 0.7911\n",
      "Iteration 3113: Weights = [55.30333333  3.02268114  7.23872101  0.11905789  0.22812153 11.39720225], Loss = 0.7910\n",
      "Iteration 3114: Weights = [55.30333333  3.02248584  7.23825331  0.1190502   0.22810679 11.39770609], Loss = 0.7909\n",
      "Iteration 3115: Weights = [55.30333333  3.02229055  7.23778563  0.11904251  0.22809205 11.3982099 ], Loss = 0.7908\n",
      "Iteration 3116: Weights = [55.30333333  3.02209527  7.23731798  0.11903481  0.22807732 11.39871368], Loss = 0.7907\n",
      "Iteration 3117: Weights = [55.30333333  3.02190001  7.23685036  0.11902712  0.22806258 11.39921742], Loss = 0.7906\n",
      "Iteration 3118: Weights = [55.30333333  3.02170476  7.23638277  0.11901943  0.22804784 11.39972113], Loss = 0.7905\n",
      "Iteration 3119: Weights = [55.30333333  3.02150952  7.23591522  0.11901174  0.22803311 11.40022481], Loss = 0.7904\n",
      "Iteration 3120: Weights = [55.30333333  3.02131429  7.23544769  0.11900405  0.22801838 11.40072846], Loss = 0.7903\n",
      "Iteration 3121: Weights = [55.30333333  3.02111908  7.23498019  0.11899636  0.22800364 11.40123207], Loss = 0.7902\n",
      "Iteration 3122: Weights = [55.30333333  3.02092388  7.23451273  0.11898868  0.22798891 11.40173565], Loss = 0.7901\n",
      "Iteration 3123: Weights = [55.30333333  3.02072869  7.23404529  0.11898099  0.22797418 11.4022392 ], Loss = 0.7900\n",
      "Iteration 3124: Weights = [55.30333333  3.02053351  7.23357788  0.1189733   0.22795945 11.40274272], Loss = 0.7899\n",
      "Iteration 3125: Weights = [55.30333333  3.02033835  7.23311051  0.11896561  0.22794472 11.4032462 ], Loss = 0.7898\n",
      "Iteration 3126: Weights = [55.30333333  3.0201432   7.23264316  0.11895793  0.22792999 11.40374965], Loss = 0.7897\n",
      "Iteration 3127: Weights = [55.30333333  3.01994806  7.23217585  0.11895024  0.22791527 11.40425307], Loss = 0.7896\n",
      "Iteration 3128: Weights = [55.30333333  3.01975294  7.23170856  0.11894255  0.22790054 11.40475646], Loss = 0.7895\n",
      "Iteration 3129: Weights = [55.30333333  3.01955783  7.23124131  0.11893487  0.22788582 11.40525981], Loss = 0.7894\n",
      "Iteration 3130: Weights = [55.30333333  3.01936273  7.23077408  0.11892718  0.22787109 11.40576313], Loss = 0.7893\n",
      "Iteration 3131: Weights = [55.30333333  3.01916764  7.23030689  0.1189195   0.22785637 11.40626642], Loss = 0.7892\n",
      "Iteration 3132: Weights = [55.30333333  3.01897257  7.22983972  0.11891182  0.22784165 11.40676968], Loss = 0.7891\n",
      "Iteration 3133: Weights = [55.30333333  3.0187775   7.22937259  0.11890413  0.22782692 11.4072729 ], Loss = 0.7890\n",
      "Iteration 3134: Weights = [55.30333333  3.01858245  7.22890548  0.11889645  0.2278122  11.40777609], Loss = 0.7889\n",
      "Iteration 3135: Weights = [55.30333333  3.01838742  7.22843841  0.11888877  0.22779748 11.40827925], Loss = 0.7888\n",
      "Iteration 3136: Weights = [55.30333333  3.01819239  7.22797136  0.11888109  0.22778277 11.40878238], Loss = 0.7887\n",
      "Iteration 3137: Weights = [55.30333333  3.01799738  7.22750435  0.11887341  0.22776805 11.40928547], Loss = 0.7886\n",
      "Iteration 3138: Weights = [55.30333333  3.01780238  7.22703737  0.11886573  0.22775333 11.40978853], Loss = 0.7885\n",
      "Iteration 3139: Weights = [55.30333333  3.0176074   7.22657041  0.11885805  0.22773862 11.41029156], Loss = 0.7884\n",
      "Iteration 3140: Weights = [55.30333333  3.01741242  7.22610349  0.11885037  0.2277239  11.41079456], Loss = 0.7883\n",
      "Iteration 3141: Weights = [55.30333333  3.01721746  7.2256366   0.11884269  0.22770919 11.41129752], Loss = 0.7882\n",
      "Iteration 3142: Weights = [55.30333333  3.01702251  7.22516973  0.11883501  0.22769448 11.41180045], Loss = 0.7881\n",
      "Iteration 3143: Weights = [55.30333333  3.01682758  7.2247029   0.11882733  0.22767976 11.41230335], Loss = 0.7880\n",
      "Iteration 3144: Weights = [55.30333333  3.01663265  7.2242361   0.11881965  0.22766505 11.41280622], Loss = 0.7879\n",
      "Iteration 3145: Weights = [55.30333333  3.01643774  7.22376933  0.11881197  0.22765034 11.41330905], Loss = 0.7878\n",
      "Iteration 3146: Weights = [55.30333333  3.01624285  7.22330258  0.1188043   0.22763563 11.41381185], Loss = 0.7877\n",
      "Iteration 3147: Weights = [55.30333333  3.01604796  7.22283587  0.11879662  0.22762093 11.41431462], Loss = 0.7876\n",
      "Iteration 3148: Weights = [55.30333333  3.01585309  7.22236919  0.11878895  0.22760622 11.41481736], Loss = 0.7874\n",
      "Iteration 3149: Weights = [55.30333333  3.01565823  7.22190254  0.11878127  0.22759151 11.41532006], Loss = 0.7873\n",
      "Iteration 3150: Weights = [55.30333333  3.01546338  7.22143592  0.1187736   0.22757681 11.41582273], Loss = 0.7872\n",
      "Iteration 3151: Weights = [55.30333333  3.01526854  7.22096933  0.11876592  0.2275621  11.41632537], Loss = 0.7871\n",
      "Iteration 3152: Weights = [55.30333333  3.01507372  7.22050276  0.11875825  0.2275474  11.41682797], Loss = 0.7870\n",
      "Iteration 3153: Weights = [55.30333333  3.01487891  7.22003623  0.11875058  0.2275327  11.41733055], Loss = 0.7869\n",
      "Iteration 3154: Weights = [55.30333333  3.01468411  7.21956973  0.1187429   0.227518   11.41783309], Loss = 0.7868\n",
      "Iteration 3155: Weights = [55.30333333  3.01448933  7.21910326  0.11873523  0.2275033  11.4183356 ], Loss = 0.7867\n",
      "Iteration 3156: Weights = [55.30333333  3.01429456  7.21863682  0.11872756  0.2274886  11.41883808], Loss = 0.7866\n",
      "Iteration 3157: Weights = [55.30333333  3.0140998   7.21817041  0.11871989  0.2274739  11.41934052], Loss = 0.7865\n",
      "Iteration 3158: Weights = [55.30333333  3.01390505  7.21770403  0.11871222  0.2274592  11.41984293], Loss = 0.7864\n",
      "Iteration 3159: Weights = [55.30333333  3.01371032  7.21723768  0.11870455  0.2274445  11.42034531], Loss = 0.7863\n",
      "Iteration 3160: Weights = [55.30333333  3.01351559  7.21677136  0.11869688  0.22742981 11.42084766], Loss = 0.7862\n",
      "Iteration 3161: Weights = [55.30333333  3.01332088  7.21630507  0.11868921  0.22741511 11.42134997], Loss = 0.7861\n",
      "Iteration 3162: Weights = [55.30333333  3.01312619  7.21583881  0.11868154  0.22740042 11.42185225], Loss = 0.7860\n",
      "Iteration 3163: Weights = [55.30333333  3.0129315   7.21537258  0.11867387  0.22738573 11.4223545 ], Loss = 0.7859\n",
      "Iteration 3164: Weights = [55.30333333  3.01273683  7.21490638  0.1186662   0.22737104 11.42285672], Loss = 0.7858\n",
      "Iteration 3165: Weights = [55.30333333  3.01254217  7.21444021  0.11865854  0.22735634 11.4233589 ], Loss = 0.7857\n",
      "Iteration 3166: Weights = [55.30333333  3.01234753  7.21397407  0.11865087  0.22734165 11.42386105], Loss = 0.7856\n",
      "Iteration 3167: Weights = [55.30333333  3.01215289  7.21350796  0.1186432   0.22732697 11.42436317], Loss = 0.7855\n",
      "Iteration 3168: Weights = [55.30333333  3.01195827  7.21304188  0.11863554  0.22731228 11.42486526], Loss = 0.7854\n",
      "Iteration 3169: Weights = [55.30333333  3.01176366  7.21257583  0.11862787  0.22729759 11.42536731], Loss = 0.7853\n",
      "Iteration 3170: Weights = [55.30333333  3.01156907  7.21210981  0.11862021  0.2272829  11.42586934], Loss = 0.7852\n",
      "Iteration 3171: Weights = [55.30333333  3.01137448  7.21164382  0.11861254  0.22726822 11.42637133], Loss = 0.7851\n",
      "Iteration 3172: Weights = [55.30333333  3.01117991  7.21117787  0.11860488  0.22725353 11.42687328], Loss = 0.7850\n",
      "Iteration 3173: Weights = [55.30333333  3.01098535  7.21071194  0.11859722  0.22723885 11.42737521], Loss = 0.7849\n",
      "Iteration 3174: Weights = [55.30333333  3.01079081  7.21024604  0.11858955  0.22722417 11.4278771 ], Loss = 0.7848\n",
      "Iteration 3175: Weights = [55.30333333  3.01059628  7.20978017  0.11858189  0.22720949 11.42837896], Loss = 0.7847\n",
      "Iteration 3176: Weights = [55.30333333  3.01040175  7.20931433  0.11857423  0.22719481 11.42888079], Loss = 0.7846\n",
      "Iteration 3177: Weights = [55.30333333  3.01020725  7.20884852  0.11856657  0.22718013 11.42938258], Loss = 0.7845\n",
      "Iteration 3178: Weights = [55.30333333  3.01001275  7.20838274  0.11855891  0.22716545 11.42988434], Loss = 0.7844\n",
      "Iteration 3179: Weights = [55.30333333  3.00981827  7.207917    0.11855125  0.22715077 11.43038607], Loss = 0.7843\n",
      "Iteration 3180: Weights = [55.30333333  3.0096238   7.20745128  0.11854359  0.22713609 11.43088777], Loss = 0.7842\n",
      "Iteration 3181: Weights = [55.30333333  3.00942934  7.20698559  0.11853593  0.22712142 11.43138944], Loss = 0.7841\n",
      "Iteration 3182: Weights = [55.30333333  3.00923489  7.20651993  0.11852827  0.22710674 11.43189107], Loss = 0.7840\n",
      "Iteration 3183: Weights = [55.30333333  3.00904046  7.20605431  0.11852061  0.22709207 11.43239267], Loss = 0.7839\n",
      "Iteration 3184: Weights = [55.30333333  3.00884604  7.20558871  0.11851295  0.2270774  11.43289424], Loss = 0.7838\n",
      "Iteration 3185: Weights = [55.30333333  3.00865163  7.20512314  0.11850529  0.22706273 11.43339577], Loss = 0.7837\n",
      "Iteration 3186: Weights = [55.30333333  3.00845724  7.2046576   0.11849764  0.22704805 11.43389728], Loss = 0.7836\n",
      "Iteration 3187: Weights = [55.30333333  3.00826286  7.2041921   0.11848998  0.22703338 11.43439875], Loss = 0.7835\n",
      "Iteration 3188: Weights = [55.30333333  3.00806849  7.20372662  0.11848233  0.22701872 11.43490019], Loss = 0.7834\n",
      "Iteration 3189: Weights = [55.30333333  3.00787413  7.20326117  0.11847467  0.22700405 11.43540159], Loss = 0.7833\n",
      "Iteration 3190: Weights = [55.30333333  3.00767978  7.20279575  0.11846701  0.22698938 11.43590297], Loss = 0.7832\n",
      "Iteration 3191: Weights = [55.30333333  3.00748545  7.20233037  0.11845936  0.22697471 11.43640431], Loss = 0.7831\n",
      "Iteration 3192: Weights = [55.30333333  3.00729113  7.20186501  0.11845171  0.22696005 11.43690562], Loss = 0.7830\n",
      "Iteration 3193: Weights = [55.30333333  3.00709683  7.20139968  0.11844405  0.22694538 11.43740689], Loss = 0.7829\n",
      "Iteration 3194: Weights = [55.30333333  3.00690253  7.20093439  0.1184364   0.22693072 11.43790814], Loss = 0.7828\n",
      "Iteration 3195: Weights = [55.30333333  3.00670825  7.20046912  0.11842875  0.22691606 11.43840935], Loss = 0.7827\n",
      "Iteration 3196: Weights = [55.30333333  3.00651398  7.20000388  0.1184211   0.2269014  11.43891053], Loss = 0.7826\n",
      "Iteration 3197: Weights = [55.30333333  3.00631972  7.19953868  0.11841344  0.22688674 11.43941168], Loss = 0.7825\n",
      "Iteration 3198: Weights = [55.30333333  3.00612548  7.1990735   0.11840579  0.22687208 11.43991279], Loss = 0.7824\n",
      "Iteration 3199: Weights = [55.30333333  3.00593125  7.19860835  0.11839814  0.22685742 11.44041387], Loss = 0.7823\n",
      "Iteration 3200: Weights = [55.30333333  3.00573703  7.19814324  0.11839049  0.22684276 11.44091492], Loss = 0.7822\n",
      "Iteration 3201: Weights = [55.30333333  3.00554282  7.19767815  0.11838284  0.2268281  11.44141594], Loss = 0.7821\n",
      "Iteration 3202: Weights = [55.30333333  3.00534863  7.19721309  0.11837519  0.22681345 11.44191692], Loss = 0.7820\n",
      "Iteration 3203: Weights = [55.30333333  3.00515444  7.19674807  0.11836755  0.22679879 11.44241788], Loss = 0.7819\n",
      "Iteration 3204: Weights = [55.30333333  3.00496028  7.19628307  0.1183599   0.22678414 11.4429188 ], Loss = 0.7818\n",
      "Iteration 3205: Weights = [55.30333333  3.00476612  7.1958181   0.11835225  0.22676949 11.44341969], Loss = 0.7817\n",
      "Iteration 3206: Weights = [55.30333333  3.00457198  7.19535317  0.1183446   0.22675483 11.44392054], Loss = 0.7816\n",
      "Iteration 3207: Weights = [55.30333333  3.00437784  7.19488826  0.11833696  0.22674018 11.44442136], Loss = 0.7815\n",
      "Iteration 3208: Weights = [55.30333333  3.00418372  7.19442338  0.11832931  0.22672553 11.44492215], Loss = 0.7814\n",
      "Iteration 3209: Weights = [55.30333333  3.00398962  7.19395854  0.11832167  0.22671088 11.44542291], Loss = 0.7813\n",
      "Iteration 3210: Weights = [55.30333333  3.00379552  7.19349372  0.11831402  0.22669624 11.44592364], Loss = 0.7812\n",
      "Iteration 3211: Weights = [55.30333333  3.00360144  7.19302894  0.11830638  0.22668159 11.44642433], Loss = 0.7811\n",
      "Iteration 3212: Weights = [55.30333333  3.00340737  7.19256418  0.11829873  0.22666694 11.44692499], Loss = 0.7810\n",
      "Iteration 3213: Weights = [55.30333333  3.00321332  7.19209945  0.11829109  0.2266523  11.44742562], Loss = 0.7809\n",
      "Iteration 3214: Weights = [55.30333333  3.00301927  7.19163476  0.11828345  0.22663765 11.44792622], Loss = 0.7808\n",
      "Iteration 3215: Weights = [55.30333333  3.00282524  7.19117009  0.1182758   0.22662301 11.44842679], Loss = 0.7807\n",
      "Iteration 3216: Weights = [55.30333333  3.00263123  7.19070546  0.11826816  0.22660837 11.44892732], Loss = 0.7806\n",
      "Iteration 3217: Weights = [55.30333333  3.00243722  7.19024085  0.11826052  0.22659372 11.44942782], Loss = 0.7805\n",
      "Iteration 3218: Weights = [55.30333333  3.00224323  7.18977627  0.11825288  0.22657908 11.44992828], Loss = 0.7804\n",
      "Iteration 3219: Weights = [55.30333333  3.00204924  7.18931173  0.11824524  0.22656444 11.45042872], Loss = 0.7803\n",
      "Iteration 3220: Weights = [55.30333333  3.00185528  7.18884721  0.1182376   0.2265498  11.45092912], Loss = 0.7802\n",
      "Iteration 3221: Weights = [55.30333333  3.00166132  7.18838273  0.11822996  0.22653517 11.45142949], Loss = 0.7801\n",
      "Iteration 3222: Weights = [55.30333333  3.00146738  7.18791827  0.11822232  0.22652053 11.45192983], Loss = 0.7800\n",
      "Iteration 3223: Weights = [55.30333333  3.00127345  7.18745385  0.11821468  0.22650589 11.45243014], Loss = 0.7799\n",
      "Iteration 3224: Weights = [55.30333333  3.00107953  7.18698945  0.11820704  0.22649126 11.45293041], Loss = 0.7798\n",
      "Iteration 3225: Weights = [55.30333333  3.00088562  7.18652508  0.11819941  0.22647663 11.45343065], Loss = 0.7797\n",
      "Iteration 3226: Weights = [55.30333333  3.00069173  7.18606075  0.11819177  0.22646199 11.45393086], Loss = 0.7796\n",
      "Iteration 3227: Weights = [55.30333333  3.00049785  7.18559644  0.11818413  0.22644736 11.45443104], Loss = 0.7795\n",
      "Iteration 3228: Weights = [55.30333333  3.00030398  7.18513217  0.1181765   0.22643273 11.45493118], Loss = 0.7793\n",
      "Iteration 3229: Weights = [55.30333333  3.00011012  7.18466792  0.11816886  0.2264181  11.45543129], Loss = 0.7792\n",
      "Iteration 3230: Weights = [55.30333333  2.99991628  7.1842037   0.11816122  0.22640347 11.45593137], Loss = 0.7791\n",
      "Iteration 3231: Weights = [55.30333333  2.99972245  7.18373952  0.11815359  0.22638884 11.45643142], Loss = 0.7790\n",
      "Iteration 3232: Weights = [55.30333333  2.99952863  7.18327536  0.11814596  0.22637421 11.45693143], Loss = 0.7789\n",
      "Iteration 3233: Weights = [55.30333333  2.99933483  7.18281124  0.11813832  0.22635959 11.45743142], Loss = 0.7788\n",
      "Iteration 3234: Weights = [55.30333333  2.99914103  7.18234714  0.11813069  0.22634496 11.45793137], Loss = 0.7787\n",
      "Iteration 3235: Weights = [55.30333333  2.99894725  7.18188308  0.11812306  0.22633034 11.45843128], Loss = 0.7786\n",
      "Iteration 3236: Weights = [55.30333333  2.99875349  7.18141904  0.11811542  0.22631571 11.45893117], Loss = 0.7785\n",
      "Iteration 3237: Weights = [55.30333333  2.99855973  7.18095503  0.11810779  0.22630109 11.45943102], Loss = 0.7784\n",
      "Iteration 3238: Weights = [55.30333333  2.99836599  7.18049106  0.11810016  0.22628647 11.45993084], Loss = 0.7783\n",
      "Iteration 3239: Weights = [55.30333333  2.99817226  7.18002711  0.11809253  0.22627185 11.46043063], Loss = 0.7782\n",
      "Iteration 3240: Weights = [55.30333333  2.99797854  7.1795632   0.1180849   0.22625723 11.46093039], Loss = 0.7781\n",
      "Iteration 3241: Weights = [55.30333333  2.99778483  7.17909931  0.11807727  0.22624261 11.46143011], Loss = 0.7780\n",
      "Iteration 3242: Weights = [55.30333333  2.99759114  7.17863545  0.11806964  0.22622799 11.46192981], Loss = 0.7779\n",
      "Iteration 3243: Weights = [55.30333333  2.99739746  7.17817163  0.11806201  0.22621337 11.46242947], Loss = 0.7778\n",
      "Iteration 3244: Weights = [55.30333333  2.99720379  7.17770783  0.11805438  0.22619876 11.46292909], Loss = 0.7777\n",
      "Iteration 3245: Weights = [55.30333333  2.99701014  7.17724407  0.11804676  0.22618414 11.46342869], Loss = 0.7776\n",
      "Iteration 3246: Weights = [55.30333333  2.99681649  7.17678033  0.11803913  0.22616953 11.46392825], Loss = 0.7775\n",
      "Iteration 3247: Weights = [55.30333333  2.99662286  7.17631662  0.1180315   0.22615492 11.46442778], Loss = 0.7774\n",
      "Iteration 3248: Weights = [55.30333333  2.99642925  7.17585295  0.11802388  0.2261403  11.46492728], Loss = 0.7773\n",
      "Iteration 3249: Weights = [55.30333333  2.99623564  7.1753893   0.11801625  0.22612569 11.46542675], Loss = 0.7772\n",
      "Iteration 3250: Weights = [55.30333333  2.99604205  7.17492568  0.11800863  0.22611108 11.46592618], Loss = 0.7771\n",
      "Iteration 3251: Weights = [55.30333333  2.99584847  7.1744621   0.118001    0.22609647 11.46642558], Loss = 0.7770\n",
      "Iteration 3252: Weights = [55.30333333  2.9956549   7.17399854  0.11799338  0.22608186 11.46692495], Loss = 0.7769\n",
      "Iteration 3253: Weights = [55.30333333  2.99546134  7.17353501  0.11798575  0.22606726 11.46742429], Loss = 0.7768\n",
      "Iteration 3254: Weights = [55.30333333  2.9952678   7.17307152  0.11797813  0.22605265 11.46792359], Loss = 0.7767\n",
      "Iteration 3255: Weights = [55.30333333  2.99507427  7.17260805  0.11797051  0.22603804 11.46842286], Loss = 0.7766\n",
      "Iteration 3256: Weights = [55.30333333  2.99488075  7.17214461  0.11796288  0.22602344 11.4689221 ], Loss = 0.7765\n",
      "Iteration 3257: Weights = [55.30333333  2.99468725  7.17168121  0.11795526  0.22600883 11.46942131], Loss = 0.7764\n",
      "Iteration 3258: Weights = [55.30333333  2.99449375  7.17121783  0.11794764  0.22599423 11.46992049], Loss = 0.7763\n",
      "Iteration 3259: Weights = [55.30333333  2.99430027  7.17075448  0.11794002  0.22597963 11.47041963], Loss = 0.7762\n",
      "Iteration 3260: Weights = [55.30333333  2.99410681  7.17029117  0.1179324   0.22596503 11.47091874], Loss = 0.7761\n",
      "Iteration 3261: Weights = [55.30333333  2.99391335  7.16982788  0.11792478  0.22595043 11.47141782], Loss = 0.7760\n",
      "Iteration 3262: Weights = [55.30333333  2.99371991  7.16936462  0.11791716  0.22593583 11.47191687], Loss = 0.7759\n",
      "Iteration 3263: Weights = [55.30333333  2.99352648  7.1689014   0.11790954  0.22592123 11.47241588], Loss = 0.7758\n",
      "Iteration 3264: Weights = [55.30333333  2.99333306  7.1684382   0.11790192  0.22590663 11.47291486], Loss = 0.7757\n",
      "Iteration 3265: Weights = [55.30333333  2.99313966  7.16797503  0.11789431  0.22589204 11.47341381], Loss = 0.7756\n",
      "Iteration 3266: Weights = [55.30333333  2.99294626  7.1675119   0.11788669  0.22587744 11.47391273], Loss = 0.7755\n",
      "Iteration 3267: Weights = [55.30333333  2.99275288  7.16704879  0.11787907  0.22586285 11.47441162], Loss = 0.7754\n",
      "Iteration 3268: Weights = [55.30333333  2.99255951  7.16658571  0.11787145  0.22584825 11.47491047], Loss = 0.7753\n",
      "Iteration 3269: Weights = [55.30333333  2.99236616  7.16612266  0.11786384  0.22583366 11.47540929], Loss = 0.7752\n",
      "Iteration 3270: Weights = [55.30333333  2.99217282  7.16565965  0.11785622  0.22581907 11.47590808], Loss = 0.7751\n",
      "Iteration 3271: Weights = [55.30333333  2.99197949  7.16519666  0.11784861  0.22580448 11.47640683], Loss = 0.7750\n",
      "Iteration 3272: Weights = [55.30333333  2.99178617  7.1647337   0.11784099  0.22578989 11.47690556], Loss = 0.7749\n",
      "Iteration 3273: Weights = [55.30333333  2.99159286  7.16427077  0.11783338  0.2257753  11.47740425], Loss = 0.7748\n",
      "Iteration 3274: Weights = [55.30333333  2.99139957  7.16380787  0.11782577  0.22576071 11.47790291], Loss = 0.7747\n",
      "Iteration 3275: Weights = [55.30333333  2.99120629  7.16334501  0.11781815  0.22574613 11.47840154], Loss = 0.7746\n",
      "Iteration 3276: Weights = [55.30333333  2.99101302  7.16288217  0.11781054  0.22573154 11.47890013], Loss = 0.7745\n",
      "Iteration 3277: Weights = [55.30333333  2.99081977  7.16241936  0.11780293  0.22571696 11.4793987 ], Loss = 0.7744\n",
      "Iteration 3278: Weights = [55.30333333  2.99062652  7.16195658  0.11779532  0.22570237 11.47989723], Loss = 0.7743\n",
      "Iteration 3279: Weights = [55.30333333  2.99043329  7.16149383  0.11778771  0.22568779 11.48039573], Loss = 0.7742\n",
      "Iteration 3280: Weights = [55.30333333  2.99024008  7.16103111  0.1177801   0.22567321 11.48089419], Loss = 0.7741\n",
      "Iteration 3281: Weights = [55.30333333  2.99004687  7.16056843  0.11777249  0.22565863 11.48139263], Loss = 0.7740\n",
      "Iteration 3282: Weights = [55.30333333  2.98985368  7.16010577  0.11776488  0.22564405 11.48189103], Loss = 0.7739\n",
      "Iteration 3283: Weights = [55.30333333  2.9896605   7.15964314  0.11775727  0.22562947 11.4823894 ], Loss = 0.7738\n",
      "Iteration 3284: Weights = [55.30333333  2.98946733  7.15918054  0.11774966  0.22561489 11.48288774], Loss = 0.7737\n",
      "Iteration 3285: Weights = [55.30333333  2.98927417  7.15871797  0.11774205  0.22560031 11.48338604], Loss = 0.7736\n",
      "Iteration 3286: Weights = [55.30333333  2.98908103  7.15825543  0.11773444  0.22558573 11.48388432], Loss = 0.7735\n",
      "Iteration 3287: Weights = [55.30333333  2.9888879   7.15779292  0.11772684  0.22557116 11.48438256], Loss = 0.7734\n",
      "Iteration 3288: Weights = [55.30333333  2.98869478  7.15733044  0.11771923  0.22555658 11.48488077], Loss = 0.7733\n",
      "Iteration 3289: Weights = [55.30333333  2.98850168  7.15686799  0.11771162  0.22554201 11.48537894], Loss = 0.7732\n",
      "Iteration 3290: Weights = [55.30333333  2.98830858  7.15640557  0.11770402  0.22552744 11.48587709], Loss = 0.7731\n",
      "Iteration 3291: Weights = [55.30333333  2.9881155   7.15594318  0.11769641  0.22551287 11.4863752 ], Loss = 0.7730\n",
      "Iteration 3292: Weights = [55.30333333  2.98792243  7.15548082  0.11768881  0.22549829 11.48687328], Loss = 0.7729\n",
      "Iteration 3293: Weights = [55.30333333  2.98772938  7.15501849  0.1176812   0.22548372 11.48737133], Loss = 0.7728\n",
      "Iteration 3294: Weights = [55.30333333  2.98753634  7.15455619  0.1176736   0.22546916 11.48786934], Loss = 0.7727\n",
      "Iteration 3295: Weights = [55.30333333  2.9873433   7.15409392  0.117666    0.22545459 11.48836733], Loss = 0.7726\n",
      "Iteration 3296: Weights = [55.30333333  2.98715029  7.15363168  0.1176584   0.22544002 11.48886528], Loss = 0.7725\n",
      "Iteration 3297: Weights = [55.30333333  2.98695728  7.15316947  0.11765079  0.22542545 11.4893632 ], Loss = 0.7724\n",
      "Iteration 3298: Weights = [55.30333333  2.98676429  7.15270729  0.11764319  0.22541089 11.48986108], Loss = 0.7723\n",
      "Iteration 3299: Weights = [55.30333333  2.98657131  7.15224514  0.11763559  0.22539632 11.49035894], Loss = 0.7722\n",
      "Iteration 3300: Weights = [55.30333333  2.98637834  7.15178302  0.11762799  0.22538176 11.49085676], Loss = 0.7721\n",
      "Iteration 3301: Weights = [55.30333333  2.98618538  7.15132093  0.11762039  0.2253672  11.49135455], Loss = 0.7720\n",
      "Iteration 3302: Weights = [55.30333333  2.98599244  7.15085887  0.11761279  0.22535264 11.49185231], Loss = 0.7719\n",
      "Iteration 3303: Weights = [55.30333333  2.98579951  7.15039684  0.11760519  0.22533808 11.49235004], Loss = 0.7718\n",
      "Iteration 3304: Weights = [55.30333333  2.98560659  7.14993484  0.11759759  0.22532352 11.49284773], Loss = 0.7717\n",
      "Iteration 3305: Weights = [55.30333333  2.98541368  7.14947286  0.11758999  0.22530896 11.49334539], Loss = 0.7716\n",
      "Iteration 3306: Weights = [55.30333333  2.98522079  7.14901092  0.1175824   0.2252944  11.49384302], Loss = 0.7715\n",
      "Iteration 3307: Weights = [55.30333333  2.98502791  7.14854901  0.1175748   0.22527984 11.49434062], Loss = 0.7714\n",
      "Iteration 3308: Weights = [55.30333333  2.98483504  7.14808713  0.1175672   0.22526529 11.49483819], Loss = 0.7713\n",
      "Iteration 3309: Weights = [55.30333333  2.98464218  7.14762528  0.11755961  0.22525073 11.49533572], Loss = 0.7712\n",
      "Iteration 3310: Weights = [55.30333333  2.98444934  7.14716345  0.11755201  0.22523618 11.49583322], Loss = 0.7711\n",
      "Iteration 3311: Weights = [55.30333333  2.98425651  7.14670166  0.11754441  0.22522163 11.49633069], Loss = 0.7710\n",
      "Iteration 3312: Weights = [55.30333333  2.98406369  7.1462399   0.11753682  0.22520708 11.49682813], Loss = 0.7709\n",
      "Iteration 3313: Weights = [55.30333333  2.98387088  7.14577816  0.11752923  0.22519252 11.49732553], Loss = 0.7708\n",
      "Iteration 3314: Weights = [55.30333333  2.98367809  7.14531646  0.11752163  0.22517797 11.4978229 ], Loss = 0.7707\n",
      "Iteration 3315: Weights = [55.30333333  2.98348531  7.14485479  0.11751404  0.22516342 11.49832024], Loss = 0.7706\n",
      "Iteration 3316: Weights = [55.30333333  2.98329254  7.14439314  0.11750645  0.22514888 11.49881755], Loss = 0.7705\n",
      "Iteration 3317: Weights = [55.30333333  2.98309978  7.14393153  0.11749885  0.22513433 11.49931483], Loss = 0.7704\n",
      "Iteration 3318: Weights = [55.30333333  2.98290704  7.14346995  0.11749126  0.22511978 11.49981207], Loss = 0.7703\n",
      "Iteration 3319: Weights = [55.30333333  2.98271431  7.14300839  0.11748367  0.22510524 11.50030929], Loss = 0.7702\n",
      "Iteration 3320: Weights = [55.30333333  2.98252159  7.14254687  0.11747608  0.22509069 11.50080647], Loss = 0.7701\n",
      "Iteration 3321: Weights = [55.30333333  2.98232888  7.14208538  0.11746849  0.22507615 11.50130361], Loss = 0.7700\n",
      "Iteration 3322: Weights = [55.30333333  2.98213619  7.14162391  0.1174609   0.22506161 11.50180073], Loss = 0.7699\n",
      "Iteration 3323: Weights = [55.30333333  2.98194351  7.14116248  0.11745331  0.22504707 11.50229781], Loss = 0.7698\n",
      "Iteration 3324: Weights = [55.30333333  2.98175084  7.14070107  0.11744572  0.22503252 11.50279486], Loss = 0.7697\n",
      "Iteration 3325: Weights = [55.30333333  2.98155818  7.1402397   0.11743813  0.22501798 11.50329188], Loss = 0.7696\n",
      "Iteration 3326: Weights = [55.30333333  2.98136553  7.13977835  0.11743054  0.22500345 11.50378887], Loss = 0.7695\n",
      "Iteration 3327: Weights = [55.30333333  2.9811729   7.13931704  0.11742296  0.22498891 11.50428582], Loss = 0.7694\n",
      "Iteration 3328: Weights = [55.30333333  2.98098028  7.13885575  0.11741537  0.22497437 11.50478275], Loss = 0.7693\n",
      "Iteration 3329: Weights = [55.30333333  2.98078768  7.13839449  0.11740778  0.22495983 11.50527964], Loss = 0.7692\n",
      "Iteration 3330: Weights = [55.30333333  2.98059508  7.13793327  0.1174002   0.2249453  11.5057765 ], Loss = 0.7691\n",
      "Iteration 3331: Weights = [55.30333333  2.9804025   7.13747207  0.11739261  0.22493077 11.50627332], Loss = 0.7690\n",
      "Iteration 3332: Weights = [55.30333333  2.98020993  7.13701091  0.11738503  0.22491623 11.50677012], Loss = 0.7689\n",
      "Iteration 3333: Weights = [55.30333333  2.98001737  7.13654977  0.11737744  0.2249017  11.50726688], Loss = 0.7688\n",
      "Iteration 3334: Weights = [55.30333333  2.97982483  7.13608866  0.11736986  0.22488717 11.50776361], Loss = 0.7687\n",
      "Iteration 3335: Weights = [55.30333333  2.97963229  7.13562758  0.11736228  0.22487264 11.50826031], Loss = 0.7686\n",
      "Iteration 3336: Weights = [55.30333333  2.97943977  7.13516654  0.11735469  0.22485811 11.50875698], Loss = 0.7685\n",
      "Iteration 3337: Weights = [55.30333333  2.97924727  7.13470552  0.11734711  0.22484358 11.50925361], Loss = 0.7684\n",
      "Iteration 3338: Weights = [55.30333333  2.97905477  7.13424453  0.11733953  0.22482905 11.50975021], Loss = 0.7683\n",
      "Iteration 3339: Weights = [55.30333333  2.97886229  7.13378357  0.11733195  0.22481453 11.51024678], Loss = 0.7683\n",
      "Iteration 3340: Weights = [55.30333333  2.97866982  7.13332265  0.11732437  0.2248     11.51074332], Loss = 0.7682\n",
      "Iteration 3341: Weights = [55.30333333  2.97847736  7.13286175  0.11731678  0.22478548 11.51123982], Loss = 0.7681\n",
      "Iteration 3342: Weights = [55.30333333  2.97828492  7.13240088  0.1173092   0.22477095 11.5117363 ], Loss = 0.7680\n",
      "Iteration 3343: Weights = [55.30333333  2.97809248  7.13194004  0.11730163  0.22475643 11.51223274], Loss = 0.7679\n",
      "Iteration 3344: Weights = [55.30333333  2.97790006  7.13147923  0.11729405  0.22474191 11.51272915], Loss = 0.7678\n",
      "Iteration 3345: Weights = [55.30333333  2.97770765  7.13101845  0.11728647  0.22472739 11.51322553], Loss = 0.7677\n",
      "Iteration 3346: Weights = [55.30333333  2.97751526  7.1305577   0.11727889  0.22471287 11.51372187], Loss = 0.7676\n",
      "Iteration 3347: Weights = [55.30333333  2.97732287  7.13009698  0.11727131  0.22469835 11.51421819], Loss = 0.7675\n",
      "Iteration 3348: Weights = [55.30333333  2.9771305   7.12963629  0.11726373  0.22468383 11.51471447], Loss = 0.7674\n",
      "Iteration 3349: Weights = [55.30333333  2.97693815  7.12917563  0.11725616  0.22466931 11.51521072], Loss = 0.7673\n",
      "Iteration 3350: Weights = [55.30333333  2.9767458   7.128715    0.11724858  0.22465479 11.51570693], Loss = 0.7672\n",
      "Iteration 3351: Weights = [55.30333333  2.97655347  7.1282544   0.11724101  0.22464028 11.51620312], Loss = 0.7671\n",
      "Iteration 3352: Weights = [55.30333333  2.97636115  7.12779383  0.11723343  0.22462576 11.51669927], Loss = 0.7670\n",
      "Iteration 3353: Weights = [55.30333333  2.97616884  7.12733329  0.11722586  0.22461125 11.51719539], Loss = 0.7669\n",
      "Iteration 3354: Weights = [55.30333333  2.97597654  7.12687278  0.11721828  0.22459674 11.51769148], Loss = 0.7668\n",
      "Iteration 3355: Weights = [55.30333333  2.97578426  7.1264123   0.11721071  0.22458223 11.51818754], Loss = 0.7667\n",
      "Iteration 3356: Weights = [55.30333333  2.97559198  7.12595185  0.11720313  0.22456772 11.51868356], Loss = 0.7666\n",
      "Iteration 3357: Weights = [55.30333333  2.97539973  7.12549142  0.11719556  0.22455321 11.51917955], Loss = 0.7665\n",
      "Iteration 3358: Weights = [55.30333333  2.97520748  7.12503103  0.11718799  0.2245387  11.51967551], Loss = 0.7664\n",
      "Iteration 3359: Weights = [55.30333333  2.97501525  7.12457067  0.11718042  0.22452419 11.52017144], Loss = 0.7663\n",
      "Iteration 3360: Weights = [55.30333333  2.97482302  7.12411034  0.11717285  0.22450968 11.52066734], Loss = 0.7662\n",
      "Iteration 3361: Weights = [55.30333333  2.97463081  7.12365003  0.11716528  0.22449518 11.5211632 ], Loss = 0.7661\n",
      "Iteration 3362: Weights = [55.30333333  2.97443862  7.12318976  0.11715771  0.22448067 11.52165904], Loss = 0.7660\n",
      "Iteration 3363: Weights = [55.30333333  2.97424643  7.12272952  0.11715014  0.22446617 11.52215484], Loss = 0.7659\n",
      "Iteration 3364: Weights = [55.30333333  2.97405426  7.1222693   0.11714257  0.22445166 11.52265061], Loss = 0.7658\n",
      "Iteration 3365: Weights = [55.30333333  2.9738621   7.12180912  0.117135    0.22443716 11.52314634], Loss = 0.7657\n",
      "Iteration 3366: Weights = [55.30333333  2.97366995  7.12134896  0.11712743  0.22442266 11.52364205], Loss = 0.7656\n",
      "Iteration 3367: Weights = [55.30333333  2.97347782  7.12088884  0.11711986  0.22440816 11.52413772], Loss = 0.7655\n",
      "Iteration 3368: Weights = [55.30333333  2.9732857   7.12042874  0.11711229  0.22439366 11.52463336], Loss = 0.7654\n",
      "Iteration 3369: Weights = [55.30333333  2.97309359  7.11996868  0.11710473  0.22437916 11.52512897], Loss = 0.7653\n",
      "Iteration 3370: Weights = [55.30333333  2.97290149  7.11950864  0.11709716  0.22436466 11.52562454], Loss = 0.7652\n",
      "Iteration 3371: Weights = [55.30333333  2.9727094   7.11904864  0.1170896   0.22435017 11.52612009], Loss = 0.7651\n",
      "Iteration 3372: Weights = [55.30333333  2.97251733  7.11858866  0.11708203  0.22433567 11.5266156 ], Loss = 0.7650\n",
      "Iteration 3373: Weights = [55.30333333  2.97232527  7.11812872  0.11707447  0.22432118 11.52711108], Loss = 0.7649\n",
      "Iteration 3374: Weights = [55.30333333  2.97213322  7.1176688   0.1170669   0.22430668 11.52760653], Loss = 0.7648\n",
      "Iteration 3375: Weights = [55.30333333  2.97194119  7.11720891  0.11705934  0.22429219 11.52810194], Loss = 0.7647\n",
      "Iteration 3376: Weights = [55.30333333  2.97174916  7.11674905  0.11705177  0.2242777  11.52859733], Loss = 0.7646\n",
      "Iteration 3377: Weights = [55.30333333  2.97155715  7.11628923  0.11704421  0.22426321 11.52909268], Loss = 0.7645\n",
      "Iteration 3378: Weights = [55.30333333  2.97136516  7.11582943  0.11703665  0.22424872 11.529588  ], Loss = 0.7644\n",
      "Iteration 3379: Weights = [55.30333333  2.97117317  7.11536966  0.11702909  0.22423423 11.53008329], Loss = 0.7643\n",
      "Iteration 3380: Weights = [55.30333333  2.9709812   7.11490992  0.11702152  0.22421974 11.53057854], Loss = 0.7642\n",
      "Iteration 3381: Weights = [55.30333333  2.97078924  7.11445021  0.11701396  0.22420525 11.53107377], Loss = 0.7641\n",
      "Iteration 3382: Weights = [55.30333333  2.97059729  7.11399054  0.1170064   0.22419077 11.53156896], Loss = 0.7640\n",
      "Iteration 3383: Weights = [55.30333333  2.97040535  7.11353089  0.11699884  0.22417628 11.53206412], Loss = 0.7639\n",
      "Iteration 3384: Weights = [55.30333333  2.97021343  7.11307127  0.11699128  0.2241618  11.53255925], Loss = 0.7638\n",
      "Iteration 3385: Weights = [55.30333333  2.97002152  7.11261168  0.11698372  0.22414731 11.53305434], Loss = 0.7637\n",
      "Iteration 3386: Weights = [55.30333333  2.96982962  7.11215212  0.11697617  0.22413283 11.53354941], Loss = 0.7636\n",
      "Iteration 3387: Weights = [55.30333333  2.96963773  7.11169259  0.11696861  0.22411835 11.53404444], Loss = 0.7635\n",
      "Iteration 3388: Weights = [55.30333333  2.96944586  7.11123309  0.11696105  0.22410387 11.53453944], Loss = 0.7634\n",
      "Iteration 3389: Weights = [55.30333333  2.96925399  7.11077361  0.11695349  0.22408939 11.53503441], Loss = 0.7633\n",
      "Iteration 3390: Weights = [55.30333333  2.96906214  7.11031417  0.11694594  0.22407491 11.53552935], Loss = 0.7632\n",
      "Iteration 3391: Weights = [55.30333333  2.96887031  7.10985476  0.11693838  0.22406043 11.53602425], Loss = 0.7631\n",
      "Iteration 3392: Weights = [55.30333333  2.96867848  7.10939538  0.11693082  0.22404595 11.53651912], Loss = 0.7630\n",
      "Iteration 3393: Weights = [55.30333333  2.96848667  7.10893603  0.11692327  0.22403148 11.53701396], Loss = 0.7629\n",
      "Iteration 3394: Weights = [55.30333333  2.96829487  7.1084767   0.11691571  0.224017   11.53750877], Loss = 0.7628\n",
      "Iteration 3395: Weights = [55.30333333  2.96810308  7.10801741  0.11690816  0.22400253 11.53800355], Loss = 0.7627\n",
      "Iteration 3396: Weights = [55.30333333  2.96791131  7.10755815  0.11690061  0.22398806 11.53849829], Loss = 0.7626\n",
      "Iteration 3397: Weights = [55.30333333  2.96771954  7.10709891  0.11689305  0.22397358 11.538993  ], Loss = 0.7625\n",
      "Iteration 3398: Weights = [55.30333333  2.96752779  7.10663971  0.1168855   0.22395911 11.53948768], Loss = 0.7624\n",
      "Iteration 3399: Weights = [55.30333333  2.96733606  7.10618054  0.11687795  0.22394464 11.53998233], Loss = 0.7623\n",
      "Iteration 3400: Weights = [55.30333333  2.96714433  7.10572139  0.1168704   0.22393017 11.54047695], Loss = 0.7622\n",
      "Iteration 3401: Weights = [55.30333333  2.96695262  7.10526228  0.11686285  0.2239157  11.54097153], Loss = 0.7621\n",
      "Iteration 3402: Weights = [55.30333333  2.96676092  7.10480319  0.1168553   0.22390124 11.54146609], Loss = 0.7620\n",
      "Iteration 3403: Weights = [55.30333333  2.96656923  7.10434414  0.11684775  0.22388677 11.54196061], Loss = 0.7619\n",
      "Iteration 3404: Weights = [55.30333333  2.96637755  7.10388511  0.1168402   0.2238723  11.54245509], Loss = 0.7618\n",
      "Iteration 3405: Weights = [55.30333333  2.96618589  7.10342611  0.11683265  0.22385784 11.54294955], Loss = 0.7617\n",
      "Iteration 3406: Weights = [55.30333333  2.96599424  7.10296715  0.1168251   0.22384337 11.54344398], Loss = 0.7616\n",
      "Iteration 3407: Weights = [55.30333333  2.9658026   7.10250821  0.11681755  0.22382891 11.54393837], Loss = 0.7615\n",
      "Iteration 3408: Weights = [55.30333333  2.96561097  7.1020493   0.11681     0.22381445 11.54443273], Loss = 0.7614\n",
      "Iteration 3409: Weights = [55.30333333  2.96541936  7.10159043  0.11680245  0.22379999 11.54492706], Loss = 0.7613\n",
      "Iteration 3410: Weights = [55.30333333  2.96522776  7.10113158  0.11679491  0.22378553 11.54542136], Loss = 0.7612\n",
      "Iteration 3411: Weights = [55.30333333  2.96503617  7.10067276  0.11678736  0.22377107 11.54591562], Loss = 0.7611\n",
      "Iteration 3412: Weights = [55.30333333  2.96484459  7.10021397  0.11677981  0.22375661 11.54640985], Loss = 0.7610\n",
      "Iteration 3413: Weights = [55.30333333  2.96465303  7.09975521  0.11677227  0.22374215 11.54690406], Loss = 0.7609\n",
      "Iteration 3414: Weights = [55.30333333  2.96446147  7.09929648  0.11676472  0.2237277  11.54739822], Loss = 0.7608\n",
      "Iteration 3415: Weights = [55.30333333  2.96426993  7.09883778  0.11675718  0.22371324 11.54789236], Loss = 0.7607\n",
      "Iteration 3416: Weights = [55.30333333  2.96407841  7.09837911  0.11674964  0.22369879 11.54838647], Loss = 0.7606\n",
      "Iteration 3417: Weights = [55.30333333  2.96388689  7.09792047  0.11674209  0.22368433 11.54888054], Loss = 0.7605\n",
      "Iteration 3418: Weights = [55.30333333  2.96369539  7.09746186  0.11673455  0.22366988 11.54937458], Loss = 0.7604\n",
      "Iteration 3419: Weights = [55.30333333  2.9635039   7.09700328  0.11672701  0.22365543 11.54986859], Loss = 0.7603\n",
      "Iteration 3420: Weights = [55.30333333  2.96331242  7.09654473  0.11671947  0.22364098 11.55036257], Loss = 0.7603\n",
      "Iteration 3421: Weights = [55.30333333  2.96312095  7.09608621  0.11671192  0.22362653 11.55085652], Loss = 0.7602\n",
      "Iteration 3422: Weights = [55.30333333  2.9629295   7.09562771  0.11670438  0.22361208 11.55135043], Loss = 0.7601\n",
      "Iteration 3423: Weights = [55.30333333  2.96273806  7.09516925  0.11669684  0.22359763 11.55184431], Loss = 0.7600\n",
      "Iteration 3424: Weights = [55.30333333  2.96254663  7.09471082  0.1166893   0.22358318 11.55233816], Loss = 0.7599\n",
      "Iteration 3425: Weights = [55.30333333  2.96235522  7.09425241  0.11668176  0.22356874 11.55283198], Loss = 0.7598\n",
      "Iteration 3426: Weights = [55.30333333  2.96216381  7.09379404  0.11667422  0.22355429 11.55332577], Loss = 0.7597\n",
      "Iteration 3427: Weights = [55.30333333  2.96197242  7.0933357   0.11666669  0.22353985 11.55381952], Loss = 0.7596\n",
      "Iteration 3428: Weights = [55.30333333  2.96178104  7.09287738  0.11665915  0.22352541 11.55431324], Loss = 0.7595\n",
      "Iteration 3429: Weights = [55.30333333  2.96158968  7.0924191   0.11665161  0.22351096 11.55480693], Loss = 0.7594\n",
      "Iteration 3430: Weights = [55.30333333  2.96139832  7.09196084  0.11664407  0.22349652 11.55530059], Loss = 0.7593\n",
      "Iteration 3431: Weights = [55.30333333  2.96120698  7.09150261  0.11663654  0.22348208 11.55579422], Loss = 0.7592\n",
      "Iteration 3432: Weights = [55.30333333  2.96101565  7.09104442  0.116629    0.22346764 11.55628781], Loss = 0.7591\n",
      "Iteration 3433: Weights = [55.30333333  2.96082433  7.09058625  0.11662146  0.2234532  11.55678138], Loss = 0.7590\n",
      "Iteration 3434: Weights = [55.30333333  2.96063303  7.09012811  0.11661393  0.22343876 11.55727491], Loss = 0.7589\n",
      "Iteration 3435: Weights = [55.30333333  2.96044174  7.08967001  0.11660639  0.22342433 11.55776841], Loss = 0.7588\n",
      "Iteration 3436: Weights = [55.30333333  2.96025046  7.08921193  0.11659886  0.22340989 11.55826187], Loss = 0.7587\n",
      "Iteration 3437: Weights = [55.30333333  2.96005919  7.08875388  0.11659133  0.22339546 11.55875531], Loss = 0.7586\n",
      "Iteration 3438: Weights = [55.30333333  2.95986793  7.08829586  0.11658379  0.22338102 11.55924871], Loss = 0.7585\n",
      "Iteration 3439: Weights = [55.30333333  2.95967669  7.08783787  0.11657626  0.22336659 11.55974208], Loss = 0.7584\n",
      "Iteration 3440: Weights = [55.30333333  2.95948546  7.08737991  0.11656873  0.22335216 11.56023542], Loss = 0.7583\n",
      "Iteration 3441: Weights = [55.30333333  2.95929424  7.08692198  0.1165612   0.22333773 11.56072873], Loss = 0.7582\n",
      "Iteration 3442: Weights = [55.30333333  2.95910303  7.08646408  0.11655367  0.2233233  11.56122201], Loss = 0.7581\n",
      "Iteration 3443: Weights = [55.30333333  2.95891184  7.08600621  0.11654613  0.22330887 11.56171525], Loss = 0.7580\n",
      "Iteration 3444: Weights = [55.30333333  2.95872066  7.08554837  0.1165386   0.22329444 11.56220846], Loss = 0.7579\n",
      "Iteration 3445: Weights = [55.30333333  2.95852949  7.08509056  0.11653107  0.22328001 11.56270164], Loss = 0.7578\n",
      "Iteration 3446: Weights = [55.30333333  2.95833833  7.08463278  0.11652355  0.22326558 11.56319479], Loss = 0.7577\n",
      "Iteration 3447: Weights = [55.30333333  2.95814719  7.08417503  0.11651602  0.22325116 11.56368791], Loss = 0.7576\n",
      "Iteration 3448: Weights = [55.30333333  2.95795606  7.0837173   0.11650849  0.22323673 11.564181  ], Loss = 0.7575\n",
      "Iteration 3449: Weights = [55.30333333  2.95776494  7.08325961  0.11650096  0.22322231 11.56467405], Loss = 0.7574\n",
      "Iteration 3450: Weights = [55.30333333  2.95757383  7.08280195  0.11649343  0.22320789 11.56516707], Loss = 0.7573\n",
      "Iteration 3451: Weights = [55.30333333  2.95738274  7.08234431  0.11648591  0.22319347 11.56566006], Loss = 0.7572\n",
      "Iteration 3452: Weights = [55.30333333  2.95719165  7.08188671  0.11647838  0.22317904 11.56615302], Loss = 0.7571\n",
      "Iteration 3453: Weights = [55.30333333  2.95700058  7.08142913  0.11647085  0.22316462 11.56664594], Loss = 0.7570\n",
      "Iteration 3454: Weights = [55.30333333  2.95680953  7.08097159  0.11646333  0.22315021 11.56713883], Loss = 0.7569\n",
      "Iteration 3455: Weights = [55.30333333  2.95661848  7.08051407  0.1164558   0.22313579 11.5676317 ], Loss = 0.7568\n",
      "Iteration 3456: Weights = [55.30333333  2.95642745  7.08005659  0.11644828  0.22312137 11.56812453], Loss = 0.7567\n",
      "Iteration 3457: Weights = [55.30333333  2.95623643  7.07959913  0.11644075  0.22310695 11.56861733], Loss = 0.7566\n",
      "Iteration 3458: Weights = [55.30333333  2.95604542  7.0791417   0.11643323  0.22309254 11.56911009], Loss = 0.7565\n",
      "Iteration 3459: Weights = [55.30333333  2.95585442  7.0786843   0.11642571  0.22307812 11.56960283], Loss = 0.7564\n",
      "Iteration 3460: Weights = [55.30333333  2.95566344  7.07822694  0.11641819  0.22306371 11.57009553], Loss = 0.7563\n",
      "Iteration 3461: Weights = [55.30333333  2.95547247  7.0777696   0.11641066  0.2230493  11.5705882 ], Loss = 0.7562\n",
      "Iteration 3462: Weights = [55.30333333  2.95528151  7.07731229  0.11640314  0.22303489 11.57108084], Loss = 0.7561\n",
      "Iteration 3463: Weights = [55.30333333  2.95509056  7.07685501  0.11639562  0.22302048 11.57157345], Loss = 0.7560\n",
      "Iteration 3464: Weights = [55.30333333  2.95489963  7.07639776  0.1163881   0.22300607 11.57206602], Loss = 0.7559\n",
      "Iteration 3465: Weights = [55.30333333  2.95470871  7.07594054  0.11638058  0.22299166 11.57255856], Loss = 0.7558\n",
      "Iteration 3466: Weights = [55.30333333  2.9545178   7.07548335  0.11637306  0.22297725 11.57305108], Loss = 0.7557\n",
      "Iteration 3467: Weights = [55.30333333  2.9543269   7.07502619  0.11636554  0.22296284 11.57354356], Loss = 0.7556\n",
      "Iteration 3468: Weights = [55.30333333  2.95413601  7.07456906  0.11635802  0.22294844 11.574036  ], Loss = 0.7555\n",
      "Iteration 3469: Weights = [55.30333333  2.95394514  7.07411195  0.11635051  0.22293403 11.57452842], Loss = 0.7555\n",
      "Iteration 3470: Weights = [55.30333333  2.95375428  7.07365488  0.11634299  0.22291963 11.5750208 ], Loss = 0.7554\n",
      "Iteration 3471: Weights = [55.30333333  2.95356343  7.07319784  0.11633547  0.22290522 11.57551316], Loss = 0.7553\n",
      "Iteration 3472: Weights = [55.30333333  2.9533726   7.07274082  0.11632795  0.22289082 11.57600548], Loss = 0.7552\n",
      "Iteration 3473: Weights = [55.30333333  2.95318177  7.07228384  0.11632044  0.22287642 11.57649777], Loss = 0.7551\n",
      "Iteration 3474: Weights = [55.30333333  2.95299096  7.07182689  0.11631292  0.22286202 11.57699002], Loss = 0.7550\n",
      "Iteration 3475: Weights = [55.30333333  2.95280016  7.07136996  0.11630541  0.22284762 11.57748225], Loss = 0.7549\n",
      "Iteration 3476: Weights = [55.30333333  2.95260938  7.07091307  0.11629789  0.22283322 11.57797444], Loss = 0.7548\n",
      "Iteration 3477: Weights = [55.30333333  2.9524186   7.0704562   0.11629038  0.22281882 11.5784666 ], Loss = 0.7547\n",
      "Iteration 3478: Weights = [55.30333333  2.95222784  7.06999936  0.11628286  0.22280443 11.57895873], Loss = 0.7546\n",
      "Iteration 3479: Weights = [55.30333333  2.95203709  7.06954256  0.11627535  0.22279003 11.57945083], Loss = 0.7545\n",
      "Iteration 3480: Weights = [55.30333333  2.95184636  7.06908578  0.11626784  0.22277564 11.5799429 ], Loss = 0.7544\n",
      "Iteration 3481: Weights = [55.30333333  2.95165563  7.06862903  0.11626033  0.22276124 11.58043493], Loss = 0.7543\n",
      "Iteration 3482: Weights = [55.30333333  2.95146492  7.06817231  0.11625281  0.22274685 11.58092694], Loss = 0.7542\n",
      "Iteration 3483: Weights = [55.30333333  2.95127422  7.06771562  0.1162453   0.22273246 11.58141891], Loss = 0.7541\n",
      "Iteration 3484: Weights = [55.30333333  2.95108353  7.06725896  0.11623779  0.22271806 11.58191085], Loss = 0.7540\n",
      "Iteration 3485: Weights = [55.30333333  2.95089285  7.06680233  0.11623028  0.22270367 11.58240275], Loss = 0.7539\n",
      "Iteration 3486: Weights = [55.30333333  2.95070219  7.06634573  0.11622277  0.22268929 11.58289463], Loss = 0.7538\n",
      "Iteration 3487: Weights = [55.30333333  2.95051154  7.06588916  0.11621526  0.2226749  11.58338647], Loss = 0.7537\n",
      "Iteration 3488: Weights = [55.30333333  2.9503209   7.06543262  0.11620775  0.22266051 11.58387829], Loss = 0.7536\n",
      "Iteration 3489: Weights = [55.30333333  2.95013028  7.06497611  0.11620024  0.22264612 11.58437007], Loss = 0.7535\n",
      "Iteration 3490: Weights = [55.30333333  2.94993966  7.06451963  0.11619274  0.22263174 11.58486181], Loss = 0.7534\n",
      "Iteration 3491: Weights = [55.30333333  2.94974906  7.06406318  0.11618523  0.22261735 11.58535353], Loss = 0.7533\n",
      "Iteration 3492: Weights = [55.30333333  2.94955847  7.06360675  0.11617772  0.22260297 11.58584522], Loss = 0.7532\n",
      "Iteration 3493: Weights = [55.30333333  2.94936789  7.06315036  0.11617022  0.22258859 11.58633687], Loss = 0.7531\n",
      "Iteration 3494: Weights = [55.30333333  2.94917733  7.06269399  0.11616271  0.2225742  11.58682849], Loss = 0.7530\n",
      "Iteration 3495: Weights = [55.30333333  2.94898678  7.06223766  0.1161552   0.22255982 11.58732008], Loss = 0.7529\n",
      "Iteration 3496: Weights = [55.30333333  2.94879624  7.06178135  0.1161477   0.22254544 11.58781164], Loss = 0.7528\n",
      "Iteration 3497: Weights = [55.30333333  2.94860571  7.06132508  0.11614019  0.22253106 11.58830316], Loss = 0.7527\n",
      "Iteration 3498: Weights = [55.30333333  2.94841519  7.06086883  0.11613269  0.22251669 11.58879466], Loss = 0.7526\n",
      "Iteration 3499: Weights = [55.30333333  2.94822469  7.06041261  0.11612519  0.22250231 11.58928612], Loss = 0.7525\n",
      "Iteration 3500: Weights = [55.30333333  2.9480342   7.05995643  0.11611768  0.22248793 11.58977755], Loss = 0.7524\n",
      "Iteration 3501: Weights = [55.30333333  2.94784372  7.05950027  0.11611018  0.22247356 11.59026895], Loss = 0.7523\n",
      "Iteration 3502: Weights = [55.30333333  2.94765326  7.05904414  0.11610268  0.22245918 11.59076032], Loss = 0.7522\n",
      "Iteration 3503: Weights = [55.30333333  2.9474628   7.05858804  0.11609518  0.22244481 11.59125165], Loss = 0.7521\n",
      "Iteration 3504: Weights = [55.30333333  2.94727236  7.05813197  0.11608768  0.22243044 11.59174296], Loss = 0.7520\n",
      "Iteration 3505: Weights = [55.30333333  2.94708193  7.05767593  0.11608018  0.22241606 11.59223423], Loss = 0.7519\n",
      "Iteration 3506: Weights = [55.30333333  2.94689152  7.05721992  0.11607268  0.22240169 11.59272547], Loss = 0.7518\n",
      "Iteration 3507: Weights = [55.30333333  2.94670111  7.05676394  0.11606518  0.22238732 11.59321668], Loss = 0.7518\n",
      "Iteration 3508: Weights = [55.30333333  2.94651072  7.05630799  0.11605768  0.22237296 11.59370786], Loss = 0.7517\n",
      "Iteration 3509: Weights = [55.30333333  2.94632034  7.05585207  0.11605018  0.22235859 11.594199  ], Loss = 0.7516\n",
      "Iteration 3510: Weights = [55.30333333  2.94612997  7.05539617  0.11604268  0.22234422 11.59469011], Loss = 0.7515\n",
      "Iteration 3511: Weights = [55.30333333  2.94593962  7.05494031  0.11603518  0.22232985 11.5951812 ], Loss = 0.7514\n",
      "Iteration 3512: Weights = [55.30333333  2.94574927  7.05448448  0.11602769  0.22231549 11.59567225], Loss = 0.7513\n",
      "Iteration 3513: Weights = [55.30333333  2.94555894  7.05402867  0.11602019  0.22230112 11.59616326], Loss = 0.7512\n",
      "Iteration 3514: Weights = [55.30333333  2.94536862  7.0535729   0.11601269  0.22228676 11.59665425], Loss = 0.7511\n",
      "Iteration 3515: Weights = [55.30333333  2.94517832  7.05311715  0.1160052   0.2222724  11.59714521], Loss = 0.7510\n",
      "Iteration 3516: Weights = [55.30333333  2.94498802  7.05266144  0.1159977   0.22225804 11.59763613], Loss = 0.7509\n",
      "Iteration 3517: Weights = [55.30333333  2.94479774  7.05220575  0.11599021  0.22224368 11.59812702], Loss = 0.7508\n",
      "Iteration 3518: Weights = [55.30333333  2.94460747  7.05175009  0.11598271  0.22222932 11.59861788], Loss = 0.7507\n",
      "Iteration 3519: Weights = [55.30333333  2.94441722  7.05129446  0.11597522  0.22221496 11.59910871], Loss = 0.7506\n",
      "Iteration 3520: Weights = [55.30333333  2.94422697  7.05083887  0.11596772  0.2222006  11.5995995 ], Loss = 0.7505\n",
      "Iteration 3521: Weights = [55.30333333  2.94403674  7.0503833   0.11596023  0.22218624 11.60009027], Loss = 0.7504\n",
      "Iteration 3522: Weights = [55.30333333  2.94384652  7.04992776  0.11595274  0.22217189 11.600581  ], Loss = 0.7503\n",
      "Iteration 3523: Weights = [55.30333333  2.94365631  7.04947225  0.11594525  0.22215753 11.6010717 ], Loss = 0.7502\n",
      "Iteration 3524: Weights = [55.30333333  2.94346611  7.04901677  0.11593776  0.22214318 11.60156237], Loss = 0.7501\n",
      "Iteration 3525: Weights = [55.30333333  2.94327593  7.04856132  0.11593026  0.22212883 11.60205301], Loss = 0.7500\n",
      "Iteration 3526: Weights = [55.30333333  2.94308576  7.04810589  0.11592277  0.22211447 11.60254362], Loss = 0.7499\n",
      "Iteration 3527: Weights = [55.30333333  2.9428956   7.0476505   0.11591528  0.22210012 11.60303419], Loss = 0.7498\n",
      "Iteration 3528: Weights = [55.30333333  2.94270546  7.04719514  0.11590779  0.22208577 11.60352473], Loss = 0.7497\n",
      "Iteration 3529: Weights = [55.30333333  2.94251532  7.04673981  0.11590031  0.22207142 11.60401524], Loss = 0.7496\n",
      "Iteration 3530: Weights = [55.30333333  2.9423252   7.0462845   0.11589282  0.22205707 11.60450572], Loss = 0.7495\n",
      "Iteration 3531: Weights = [55.30333333  2.94213509  7.04582923  0.11588533  0.22204273 11.60499617], Loss = 0.7494\n",
      "Iteration 3532: Weights = [55.30333333  2.94194499  7.04537398  0.11587784  0.22202838 11.60548659], Loss = 0.7493\n",
      "Iteration 3533: Weights = [55.30333333  2.94175491  7.04491877  0.11587035  0.22201403 11.60597697], Loss = 0.7492\n",
      "Iteration 3534: Weights = [55.30333333  2.94156483  7.04446358  0.11586287  0.22199969 11.60646732], Loss = 0.7491\n",
      "Iteration 3535: Weights = [55.30333333  2.94137477  7.04400842  0.11585538  0.22198535 11.60695764], Loss = 0.7490\n",
      "Iteration 3536: Weights = [55.30333333  2.94118473  7.04355329  0.1158479   0.221971   11.60744793], Loss = 0.7489\n",
      "Iteration 3537: Weights = [55.30333333  2.94099469  7.0430982   0.11584041  0.22195666 11.60793819], Loss = 0.7488\n",
      "Iteration 3538: Weights = [55.30333333  2.94080467  7.04264313  0.11583293  0.22194232 11.60842842], Loss = 0.7487\n",
      "Iteration 3539: Weights = [55.30333333  2.94061466  7.04218809  0.11582544  0.22192798 11.60891861], Loss = 0.7486\n",
      "Iteration 3540: Weights = [55.30333333  2.94042466  7.04173308  0.11581796  0.22191364 11.60940877], Loss = 0.7486\n",
      "Iteration 3541: Weights = [55.30333333  2.94023467  7.0412781   0.11581047  0.2218993  11.6098989 ], Loss = 0.7485\n",
      "Iteration 3542: Weights = [55.30333333  2.9400447   7.04082315  0.11580299  0.22188496 11.610389  ], Loss = 0.7484\n",
      "Iteration 3543: Weights = [55.30333333  2.93985473  7.04036823  0.11579551  0.22187063 11.61087907], Loss = 0.7483\n",
      "Iteration 3544: Weights = [55.30333333  2.93966478  7.03991333  0.11578803  0.22185629 11.6113691 ], Loss = 0.7482\n",
      "Iteration 3545: Weights = [55.30333333  2.93947485  7.03945847  0.11578055  0.22184196 11.61185911], Loss = 0.7481\n",
      "Iteration 3546: Weights = [55.30333333  2.93928492  7.03900364  0.11577307  0.22182762 11.61234908], Loss = 0.7480\n",
      "Iteration 3547: Weights = [55.30333333  2.93909501  7.03854883  0.11576559  0.22181329 11.61283902], Loss = 0.7479\n",
      "Iteration 3548: Weights = [55.30333333  2.93890511  7.03809406  0.11575811  0.22179896 11.61332893], Loss = 0.7478\n",
      "Iteration 3549: Weights = [55.30333333  2.93871522  7.03763931  0.11575063  0.22178463 11.61381881], Loss = 0.7477\n",
      "Iteration 3550: Weights = [55.30333333  2.93852534  7.0371846   0.11574315  0.2217703  11.61430865], Loss = 0.7476\n",
      "Iteration 3551: Weights = [55.30333333  2.93833548  7.03672991  0.11573567  0.22175597 11.61479847], Loss = 0.7475\n",
      "Iteration 3552: Weights = [55.30333333  2.93814563  7.03627525  0.11572819  0.22174164 11.61528825], Loss = 0.7474\n",
      "Iteration 3553: Weights = [55.30333333  2.93795579  7.03582062  0.11572071  0.22172731 11.615778  ], Loss = 0.7473\n",
      "Iteration 3554: Weights = [55.30333333  2.93776596  7.03536603  0.11571324  0.22171299 11.61626772], Loss = 0.7472\n",
      "Iteration 3555: Weights = [55.30333333  2.93757614  7.03491146  0.11570576  0.22169866 11.61675741], Loss = 0.7471\n",
      "Iteration 3556: Weights = [55.30333333  2.93738634  7.03445692  0.11569828  0.22168434 11.61724706], Loss = 0.7470\n",
      "Iteration 3557: Weights = [55.30333333  2.93719655  7.03400241  0.11569081  0.22167002 11.61773669], Loss = 0.7469\n",
      "Iteration 3558: Weights = [55.30333333  2.93700677  7.03354793  0.11568333  0.22165569 11.61822628], Loss = 0.7468\n",
      "Iteration 3559: Weights = [55.30333333  2.93681701  7.03309347  0.11567586  0.22164137 11.61871584], Loss = 0.7467\n",
      "Iteration 3560: Weights = [55.30333333  2.93662725  7.03263905  0.11566839  0.22162705 11.61920537], Loss = 0.7466\n",
      "Iteration 3561: Weights = [55.30333333  2.93643751  7.03218466  0.11566091  0.22161273 11.61969487], Loss = 0.7465\n",
      "Iteration 3562: Weights = [55.30333333  2.93624778  7.0317303   0.11565344  0.22159841 11.62018433], Loss = 0.7464\n",
      "Iteration 3563: Weights = [55.30333333  2.93605807  7.03127596  0.11564597  0.22158409 11.62067377], Loss = 0.7463\n",
      "Iteration 3564: Weights = [55.30333333  2.93586836  7.03082166  0.11563849  0.22156978 11.62116317], Loss = 0.7462\n",
      "Iteration 3565: Weights = [55.30333333  2.93567867  7.03036738  0.11563102  0.22155546 11.62165254], Loss = 0.7461\n",
      "Iteration 3566: Weights = [55.30333333  2.93548899  7.02991313  0.11562355  0.22154115 11.62214188], Loss = 0.7460\n",
      "Iteration 3567: Weights = [55.30333333  2.93529932  7.02945892  0.11561608  0.22152683 11.62263119], Loss = 0.7459\n",
      "Iteration 3568: Weights = [55.30333333  2.93510967  7.02900473  0.11560861  0.22151252 11.62312047], Loss = 0.7458\n",
      "Iteration 3569: Weights = [55.30333333  2.93492002  7.02855057  0.11560114  0.22149821 11.62360971], Loss = 0.7458\n",
      "Iteration 3570: Weights = [55.30333333  2.93473039  7.02809644  0.11559367  0.22148389 11.62409893], Loss = 0.7457\n",
      "Iteration 3571: Weights = [55.30333333  2.93454077  7.02764234  0.1155862   0.22146958 11.62458811], Loss = 0.7456\n",
      "Iteration 3572: Weights = [55.30333333  2.93435117  7.02718827  0.11557873  0.22145527 11.62507726], Loss = 0.7455\n",
      "Iteration 3573: Weights = [55.30333333  2.93416157  7.02673423  0.11557127  0.22144097 11.62556638], Loss = 0.7454\n",
      "Iteration 3574: Weights = [55.30333333  2.93397199  7.02628022  0.1155638   0.22142666 11.62605546], Loss = 0.7453\n",
      "Iteration 3575: Weights = [55.30333333  2.93378242  7.02582624  0.11555633  0.22141235 11.62654452], Loss = 0.7452\n",
      "Iteration 3576: Weights = [55.30333333  2.93359286  7.02537229  0.11554887  0.22139805 11.62703354], Loss = 0.7451\n",
      "Iteration 3577: Weights = [55.30333333  2.93340332  7.02491836  0.1155414   0.22138374 11.62752253], Loss = 0.7450\n",
      "Iteration 3578: Weights = [55.30333333  2.93321378  7.02446447  0.11553393  0.22136944 11.62801149], Loss = 0.7449\n",
      "Iteration 3579: Weights = [55.30333333  2.93302426  7.0240106   0.11552647  0.22135513 11.62850042], Loss = 0.7448\n",
      "Iteration 3580: Weights = [55.30333333  2.93283475  7.02355677  0.11551901  0.22134083 11.62898932], Loss = 0.7447\n",
      "Iteration 3581: Weights = [55.30333333  2.93264526  7.02310296  0.11551154  0.22132653 11.62947818], Loss = 0.7446\n",
      "Iteration 3582: Weights = [55.30333333  2.93245577  7.02264919  0.11550408  0.22131223 11.62996702], Loss = 0.7445\n",
      "Iteration 3583: Weights = [55.30333333  2.9322663   7.02219544  0.11549662  0.22129793 11.63045582], Loss = 0.7444\n",
      "Iteration 3584: Weights = [55.30333333  2.93207684  7.02174172  0.11548915  0.22128363 11.63094459], Loss = 0.7443\n",
      "Iteration 3585: Weights = [55.30333333  2.93188739  7.02128803  0.11548169  0.22126933 11.63143333], Loss = 0.7442\n",
      "Iteration 3586: Weights = [55.30333333  2.93169796  7.02083437  0.11547423  0.22125504 11.63192204], Loss = 0.7441\n",
      "Iteration 3587: Weights = [55.30333333  2.93150854  7.02038074  0.11546677  0.22124074 11.63241071], Loss = 0.7440\n",
      "Iteration 3588: Weights = [55.30333333  2.93131913  7.01992714  0.11545931  0.22122645 11.63289936], Loss = 0.7439\n",
      "Iteration 3589: Weights = [55.30333333  2.93112973  7.01947357  0.11545185  0.22121215 11.63338797], Loss = 0.7438\n",
      "Iteration 3590: Weights = [55.30333333  2.93094034  7.01902003  0.11544439  0.22119786 11.63387655], Loss = 0.7437\n",
      "Iteration 3591: Weights = [55.30333333  2.93075097  7.01856651  0.11543693  0.22118357 11.6343651 ], Loss = 0.7436\n",
      "Iteration 3592: Weights = [55.30333333  2.93056161  7.01811303  0.11542947  0.22116928 11.63485362], Loss = 0.7435\n",
      "Iteration 3593: Weights = [55.30333333  2.93037226  7.01765958  0.11542201  0.22115499 11.63534211], Loss = 0.7434\n",
      "Iteration 3594: Weights = [55.30333333  2.93018292  7.01720615  0.11541455  0.2211407  11.63583056], Loss = 0.7433\n",
      "Iteration 3595: Weights = [55.30333333  2.92999359  7.01675275  0.1154071   0.22112641 11.63631899], Loss = 0.7433\n",
      "Iteration 3596: Weights = [55.30333333  2.92980428  7.01629939  0.11539964  0.22111212 11.63680738], Loss = 0.7432\n",
      "Iteration 3597: Weights = [55.30333333  2.92961498  7.01584605  0.11539218  0.22109783 11.63729574], Loss = 0.7431\n",
      "Iteration 3598: Weights = [55.30333333  2.92942569  7.01539274  0.11538473  0.22108355 11.63778407], Loss = 0.7430\n",
      "Iteration 3599: Weights = [55.30333333  2.92923642  7.01493946  0.11537727  0.22106926 11.63827237], Loss = 0.7429\n",
      "Iteration 3600: Weights = [55.30333333  2.92904715  7.01448622  0.11536982  0.22105498 11.63876063], Loss = 0.7428\n",
      "Iteration 3601: Weights = [55.30333333  2.9288579   7.014033    0.11536236  0.2210407  11.63924887], Loss = 0.7427\n",
      "Iteration 3602: Weights = [55.30333333  2.92866866  7.0135798   0.11535491  0.22102642 11.63973707], Loss = 0.7426\n",
      "Iteration 3603: Weights = [55.30333333  2.92847943  7.01312664  0.11534746  0.22101214 11.64022524], Loss = 0.7425\n",
      "Iteration 3604: Weights = [55.30333333  2.92829022  7.01267351  0.11534     0.22099786 11.64071338], Loss = 0.7424\n",
      "Iteration 3605: Weights = [55.30333333  2.92810102  7.01222041  0.11533255  0.22098358 11.64120149], Loss = 0.7423\n",
      "Iteration 3606: Weights = [55.30333333  2.92791183  7.01176733  0.1153251   0.2209693  11.64168956], Loss = 0.7422\n",
      "Iteration 3607: Weights = [55.30333333  2.92772265  7.01131429  0.11531765  0.22095502 11.64217761], Loss = 0.7421\n",
      "Iteration 3608: Weights = [55.30333333  2.92753348  7.01086127  0.1153102   0.22094074 11.64266562], Loss = 0.7420\n",
      "Iteration 3609: Weights = [55.30333333  2.92734433  7.01040829  0.11530275  0.22092647 11.6431536 ], Loss = 0.7419\n",
      "Iteration 3610: Weights = [55.30333333  2.92715519  7.00995533  0.1152953   0.22091219 11.64364155], Loss = 0.7418\n",
      "Iteration 3611: Weights = [55.30333333  2.92696606  7.00950241  0.11528785  0.22089792 11.64412947], Loss = 0.7417\n",
      "Iteration 3612: Weights = [55.30333333  2.92677694  7.00904951  0.1152804   0.22088365 11.64461736], Loss = 0.7416\n",
      "Iteration 3613: Weights = [55.30333333  2.92658783  7.00859664  0.11527295  0.22086938 11.64510522], Loss = 0.7415\n",
      "Iteration 3614: Weights = [55.30333333  2.92639874  7.0081438   0.1152655   0.22085511 11.64559304], Loss = 0.7414\n",
      "Iteration 3615: Weights = [55.30333333  2.92620966  7.00769099  0.11525806  0.22084084 11.64608083], Loss = 0.7413\n",
      "Iteration 3616: Weights = [55.30333333  2.92602059  7.00723821  0.11525061  0.22082657 11.64656859], Loss = 0.7412\n",
      "Iteration 3617: Weights = [55.30333333  2.92583154  7.00678546  0.11524316  0.2208123  11.64705632], Loss = 0.7411\n",
      "Iteration 3618: Weights = [55.30333333  2.92564249  7.00633273  0.11523572  0.22079803 11.64754402], Loss = 0.7410\n",
      "Iteration 3619: Weights = [55.30333333  2.92545346  7.00588004  0.11522827  0.22078377 11.64803169], Loss = 0.7409\n",
      "Iteration 3620: Weights = [55.30333333  2.92526444  7.00542738  0.11522082  0.2207695  11.64851932], Loss = 0.7409\n",
      "Iteration 3621: Weights = [55.30333333  2.92507543  7.00497474  0.11521338  0.22075524 11.64900693], Loss = 0.7408\n",
      "Iteration 3622: Weights = [55.30333333  2.92488644  7.00452214  0.11520594  0.22074097 11.6494945 ], Loss = 0.7407\n",
      "Iteration 3623: Weights = [55.30333333  2.92469746  7.00406956  0.11519849  0.22072671 11.64998204], Loss = 0.7406\n",
      "Iteration 3624: Weights = [55.30333333  2.92450849  7.00361701  0.11519105  0.22071245 11.65046955], Loss = 0.7405\n",
      "Iteration 3625: Weights = [55.30333333  2.92431953  7.00316449  0.11518361  0.22069819 11.65095703], Loss = 0.7404\n",
      "Iteration 3626: Weights = [55.30333333  2.92413058  7.00271201  0.11517616  0.22068393 11.65144447], Loss = 0.7403\n",
      "Iteration 3627: Weights = [55.30333333  2.92394165  7.00225955  0.11516872  0.22066967 11.65193189], Loss = 0.7402\n",
      "Iteration 3628: Weights = [55.30333333  2.92375273  7.00180712  0.11516128  0.22065541 11.65241927], Loss = 0.7401\n",
      "Iteration 3629: Weights = [55.30333333  2.92356382  7.00135472  0.11515384  0.22064115 11.65290662], Loss = 0.7400\n",
      "Iteration 3630: Weights = [55.30333333  2.92337492  7.00090234  0.1151464   0.2206269  11.65339394], Loss = 0.7399\n",
      "Iteration 3631: Weights = [55.30333333  2.92318603  7.00045     0.11513896  0.22061264 11.65388123], Loss = 0.7398\n",
      "Iteration 3632: Weights = [55.30333333  2.92299716  6.99999769  0.11513152  0.22059839 11.65436849], Loss = 0.7397\n",
      "Iteration 3633: Weights = [55.30333333  2.9228083   6.99954541  0.11512408  0.22058414 11.65485572], Loss = 0.7396\n",
      "Iteration 3634: Weights = [55.30333333  2.92261945  6.99909315  0.11511664  0.22056988 11.65534291], Loss = 0.7395\n",
      "Iteration 3635: Weights = [55.30333333  2.92243062  6.99864092  0.11510921  0.22055563 11.65583007], Loss = 0.7394\n",
      "Iteration 3636: Weights = [55.30333333  2.92224179  6.99818873  0.11510177  0.22054138 11.6563172 ], Loss = 0.7393\n",
      "Iteration 3637: Weights = [55.30333333  2.92205298  6.99773656  0.11509433  0.22052713 11.6568043 ], Loss = 0.7392\n",
      "Iteration 3638: Weights = [55.30333333  2.92186418  6.99728442  0.11508689  0.22051288 11.65729137], Loss = 0.7391\n",
      "Iteration 3639: Weights = [55.30333333  2.92167539  6.99683232  0.11507946  0.22049863 11.65777841], Loss = 0.7390\n",
      "Iteration 3640: Weights = [55.30333333  2.92148662  6.99638024  0.11507202  0.22048439 11.65826541], Loss = 0.7389\n",
      "Iteration 3641: Weights = [55.30333333  2.92129785  6.99592819  0.11506459  0.22047014 11.65875239], Loss = 0.7388\n",
      "Iteration 3642: Weights = [55.30333333  2.9211091   6.99547616  0.11505715  0.2204559  11.65923933], Loss = 0.7387\n",
      "Iteration 3643: Weights = [55.30333333  2.92092036  6.99502417  0.11504972  0.22044165 11.65972624], Loss = 0.7387\n",
      "Iteration 3644: Weights = [55.30333333  2.92073164  6.99457221  0.11504229  0.22042741 11.66021312], Loss = 0.7386\n",
      "Iteration 3645: Weights = [55.30333333  2.92054292  6.99412028  0.11503485  0.22041317 11.66069997], Loss = 0.7385\n",
      "Iteration 3646: Weights = [55.30333333  2.92035422  6.99366837  0.11502742  0.22039893 11.66118679], Loss = 0.7384\n",
      "Iteration 3647: Weights = [55.30333333  2.92016553  6.9932165   0.11501999  0.22038469 11.66167357], Loss = 0.7383\n",
      "Iteration 3648: Weights = [55.30333333  2.91997685  6.99276465  0.11501256  0.22037045 11.66216033], Loss = 0.7382\n",
      "Iteration 3649: Weights = [55.30333333  2.91978819  6.99231284  0.11500513  0.22035621 11.66264705], Loss = 0.7381\n",
      "Iteration 3650: Weights = [55.30333333  2.91959953  6.99186105  0.11499769  0.22034197 11.66313374], Loss = 0.7380\n",
      "Iteration 3651: Weights = [55.30333333  2.91941089  6.99140929  0.11499026  0.22032773 11.6636204 ], Loss = 0.7379\n",
      "Iteration 3652: Weights = [55.30333333  2.91922226  6.99095756  0.11498283  0.2203135  11.66410703], Loss = 0.7378\n",
      "Iteration 3653: Weights = [55.30333333  2.91903365  6.99050586  0.11497541  0.22029926 11.66459362], Loss = 0.7377\n",
      "Iteration 3654: Weights = [55.30333333  2.91884504  6.99005419  0.11496798  0.22028503 11.66508019], Loss = 0.7376\n",
      "Iteration 3655: Weights = [55.30333333  2.91865645  6.98960255  0.11496055  0.2202708  11.66556672], Loss = 0.7375\n",
      "Iteration 3656: Weights = [55.30333333  2.91846787  6.98915094  0.11495312  0.22025656 11.66605322], Loss = 0.7374\n",
      "Iteration 3657: Weights = [55.30333333  2.9182793   6.98869935  0.11494569  0.22024233 11.66653969], Loss = 0.7373\n",
      "Iteration 3658: Weights = [55.30333333  2.91809075  6.9882478   0.11493827  0.2202281  11.66702613], Loss = 0.7372\n",
      "Iteration 3659: Weights = [55.30333333  2.9179022   6.98779627  0.11493084  0.22021387 11.66751254], Loss = 0.7371\n",
      "Iteration 3660: Weights = [55.30333333  2.91771367  6.98734478  0.11492341  0.22019964 11.66799892], Loss = 0.7370\n",
      "Iteration 3661: Weights = [55.30333333  2.91752515  6.98689331  0.11491599  0.22018542 11.66848526], Loss = 0.7369\n",
      "Iteration 3662: Weights = [55.30333333  2.91733664  6.98644188  0.11490856  0.22017119 11.66897158], Loss = 0.7368\n",
      "Iteration 3663: Weights = [55.30333333  2.91714815  6.98599047  0.11490114  0.22015696 11.66945786], Loss = 0.7367\n",
      "Iteration 3664: Weights = [55.30333333  2.91695967  6.98553909  0.11489372  0.22014274 11.66994411], Loss = 0.7367\n",
      "Iteration 3665: Weights = [55.30333333  2.9167712   6.98508774  0.11488629  0.22012852 11.67043033], Loss = 0.7366\n",
      "Iteration 3666: Weights = [55.30333333  2.91658274  6.98463642  0.11487887  0.22011429 11.67091652], Loss = 0.7365\n",
      "Iteration 3667: Weights = [55.30333333  2.91639429  6.98418513  0.11487145  0.22010007 11.67140267], Loss = 0.7364\n",
      "Iteration 3668: Weights = [55.30333333  2.91620586  6.98373386  0.11486402  0.22008585 11.6718888 ], Loss = 0.7363\n",
      "Iteration 3669: Weights = [55.30333333  2.91601744  6.98328263  0.1148566   0.22007163 11.67237489], Loss = 0.7362\n",
      "Iteration 3670: Weights = [55.30333333  2.91582903  6.98283143  0.11484918  0.22005741 11.67286095], Loss = 0.7361\n",
      "Iteration 3671: Weights = [55.30333333  2.91564063  6.98238025  0.11484176  0.22004319 11.67334699], Loss = 0.7360\n",
      "Iteration 3672: Weights = [55.30333333  2.91545224  6.98192911  0.11483434  0.22002897 11.67383299], Loss = 0.7359\n",
      "Iteration 3673: Weights = [55.30333333  2.91526387  6.98147799  0.11482692  0.22001476 11.67431895], Loss = 0.7358\n",
      "Iteration 3674: Weights = [55.30333333  2.91507551  6.9810269   0.1148195   0.22000054 11.67480489], Loss = 0.7357\n",
      "Iteration 3675: Weights = [55.30333333  2.91488716  6.98057585  0.11481208  0.21998633 11.6752908 ], Loss = 0.7356\n",
      "Iteration 3676: Weights = [55.30333333  2.91469882  6.98012482  0.11480466  0.21997211 11.67577667], Loss = 0.7355\n",
      "Iteration 3677: Weights = [55.30333333  2.9145105   6.97967382  0.11479725  0.2199579  11.67626251], Loss = 0.7354\n",
      "Iteration 3678: Weights = [55.30333333  2.91432219  6.97922285  0.11478983  0.21994369 11.67674832], Loss = 0.7353\n",
      "Iteration 3679: Weights = [55.30333333  2.91413389  6.9787719   0.11478241  0.21992948 11.6772341 ], Loss = 0.7352\n",
      "Iteration 3680: Weights = [55.30333333  2.9139456   6.97832099  0.114775    0.21991527 11.67771985], Loss = 0.7351\n",
      "Iteration 3681: Weights = [55.30333333  2.91375732  6.97787011  0.11476758  0.21990106 11.67820557], Loss = 0.7350\n",
      "Iteration 3682: Weights = [55.30333333  2.91356906  6.97741925  0.11476017  0.21988685 11.67869125], Loss = 0.7349\n",
      "Iteration 3683: Weights = [55.30333333  2.91338081  6.97696843  0.11475275  0.21987264 11.67917691], Loss = 0.7348\n",
      "Iteration 3684: Weights = [55.30333333  2.91319257  6.97651763  0.11474534  0.21985844 11.67966253], Loss = 0.7348\n",
      "Iteration 3685: Weights = [55.30333333  2.91300434  6.97606687  0.11473792  0.21984423 11.68014812], Loss = 0.7347\n",
      "Iteration 3686: Weights = [55.30333333  2.91281612  6.97561613  0.11473051  0.21983003 11.68063368], Loss = 0.7346\n",
      "Iteration 3687: Weights = [55.30333333  2.91262792  6.97516542  0.1147231   0.21981582 11.68111921], Loss = 0.7345\n",
      "Iteration 3688: Weights = [55.30333333  2.91243973  6.97471474  0.11471568  0.21980162 11.68160471], Loss = 0.7344\n",
      "Iteration 3689: Weights = [55.30333333  2.91225155  6.97426409  0.11470827  0.21978742 11.68209017], Loss = 0.7343\n",
      "Iteration 3690: Weights = [55.30333333  2.91206339  6.97381347  0.11470086  0.21977322 11.68257561], Loss = 0.7342\n",
      "Iteration 3691: Weights = [55.30333333  2.91187523  6.97336288  0.11469345  0.21975902 11.68306101], Loss = 0.7341\n",
      "Iteration 3692: Weights = [55.30333333  2.91168709  6.97291231  0.11468604  0.21974482 11.68354638], Loss = 0.7340\n",
      "Iteration 3693: Weights = [55.30333333  2.91149896  6.97246178  0.11467863  0.21973062 11.68403172], Loss = 0.7339\n",
      "Iteration 3694: Weights = [55.30333333  2.91131084  6.97201128  0.11467122  0.21971642 11.68451703], Loss = 0.7338\n",
      "Iteration 3695: Weights = [55.30333333  2.91112274  6.9715608   0.11466381  0.21970223 11.68500231], Loss = 0.7337\n",
      "Iteration 3696: Weights = [55.30333333  2.91093464  6.97111035  0.1146564   0.21968803 11.68548756], Loss = 0.7336\n",
      "Iteration 3697: Weights = [55.30333333  2.91074656  6.97065994  0.11464899  0.21967384 11.68597277], Loss = 0.7335\n",
      "Iteration 3698: Weights = [55.30333333  2.91055849  6.97020955  0.11464158  0.21965964 11.68645796], Loss = 0.7334\n",
      "Iteration 3699: Weights = [55.30333333  2.91037044  6.96975919  0.11463418  0.21964545 11.68694311], Loss = 0.7333\n",
      "Iteration 3700: Weights = [55.30333333  2.91018239  6.96930886  0.11462677  0.21963126 11.68742823], Loss = 0.7332\n",
      "Iteration 3701: Weights = [55.30333333  2.90999436  6.96885856  0.11461936  0.21961707 11.68791332], Loss = 0.7331\n",
      "Iteration 3702: Weights = [55.30333333  2.90980634  6.96840829  0.11461196  0.21960288 11.68839838], Loss = 0.7330\n",
      "Iteration 3703: Weights = [55.30333333  2.90961833  6.96795804  0.11460455  0.21958869 11.68888341], Loss = 0.7329\n",
      "Iteration 3704: Weights = [55.30333333  2.90943033  6.96750783  0.11459715  0.2195745  11.6893684 ], Loss = 0.7329\n",
      "Iteration 3705: Weights = [55.30333333  2.90924235  6.96705764  0.11458974  0.21956031 11.68985337], Loss = 0.7328\n",
      "Iteration 3706: Weights = [55.30333333  2.90905438  6.96660749  0.11458234  0.21954613 11.6903383 ], Loss = 0.7327\n",
      "Iteration 3707: Weights = [55.30333333  2.90886642  6.96615736  0.11457494  0.21953194 11.6908232 ], Loss = 0.7326\n",
      "Iteration 3708: Weights = [55.30333333  2.90867847  6.96570726  0.11456753  0.21951776 11.69130807], Loss = 0.7325\n",
      "Iteration 3709: Weights = [55.30333333  2.90849053  6.9652572   0.11456013  0.21950357 11.69179291], Loss = 0.7324\n",
      "Iteration 3710: Weights = [55.30333333  2.90830261  6.96480716  0.11455273  0.21948939 11.69227772], Loss = 0.7323\n",
      "Iteration 3711: Weights = [55.30333333  2.9081147   6.96435715  0.11454533  0.21947521 11.69276249], Loss = 0.7322\n",
      "Iteration 3712: Weights = [55.30333333  2.9079268   6.96390717  0.11453793  0.21946103 11.69324724], Loss = 0.7321\n",
      "Iteration 3713: Weights = [55.30333333  2.90773891  6.96345721  0.11453053  0.21944685 11.69373195], Loss = 0.7320\n",
      "Iteration 3714: Weights = [55.30333333  2.90755104  6.96300729  0.11452313  0.21943267 11.69421663], Loss = 0.7319\n",
      "Iteration 3715: Weights = [55.30333333  2.90736317  6.9625574   0.11451573  0.21941849 11.69470129], Loss = 0.7318\n",
      "Iteration 3716: Weights = [55.30333333  2.90717532  6.96210753  0.11450833  0.21940432 11.69518591], Loss = 0.7317\n",
      "Iteration 3717: Weights = [55.30333333  2.90698749  6.9616577   0.11450093  0.21939014 11.69567049], Loss = 0.7316\n",
      "Iteration 3718: Weights = [55.30333333  2.90679966  6.96120789  0.11449353  0.21937596 11.69615505], Loss = 0.7315\n",
      "Iteration 3719: Weights = [55.30333333  2.90661185  6.96075811  0.11448613  0.21936179 11.69663958], Loss = 0.7314\n",
      "Iteration 3720: Weights = [55.30333333  2.90642404  6.96030836  0.11447874  0.21934762 11.69712407], Loss = 0.7313\n",
      "Iteration 3721: Weights = [55.30333333  2.90623625  6.95985864  0.11447134  0.21933344 11.69760853], Loss = 0.7312\n",
      "Iteration 3722: Weights = [55.30333333  2.90604848  6.95940895  0.11446394  0.21931927 11.69809297], Loss = 0.7312\n",
      "Iteration 3723: Weights = [55.30333333  2.90586071  6.95895929  0.11445655  0.2193051  11.69857737], Loss = 0.7311\n",
      "Iteration 3724: Weights = [55.30333333  2.90567296  6.95850966  0.11444915  0.21929093 11.69906174], Loss = 0.7310\n",
      "Iteration 3725: Weights = [55.30333333  2.90548522  6.95806006  0.11444176  0.21927676 11.69954607], Loss = 0.7309\n",
      "Iteration 3726: Weights = [55.30333333  2.90529749  6.95761048  0.11443436  0.2192626  11.70003038], Loss = 0.7308\n",
      "Iteration 3727: Weights = [55.30333333  2.90510977  6.95716094  0.11442697  0.21924843 11.70051466], Loss = 0.7307\n",
      "Iteration 3728: Weights = [55.30333333  2.90492206  6.95671142  0.11441958  0.21923426 11.7009989 ], Loss = 0.7306\n",
      "Iteration 3729: Weights = [55.30333333  2.90473437  6.95626193  0.11441218  0.2192201  11.70148311], Loss = 0.7305\n",
      "Iteration 3730: Weights = [55.30333333  2.90454669  6.95581248  0.11440479  0.21920593 11.7019673 ], Loss = 0.7304\n",
      "Iteration 3731: Weights = [55.30333333  2.90435902  6.95536305  0.1143974   0.21919177 11.70245145], Loss = 0.7303\n",
      "Iteration 3732: Weights = [55.30333333  2.90417137  6.95491365  0.11439001  0.21917761 11.70293557], Loss = 0.7302\n",
      "Iteration 3733: Weights = [55.30333333  2.90398372  6.95446428  0.11438262  0.21916345 11.70341965], Loss = 0.7301\n",
      "Iteration 3734: Weights = [55.30333333  2.90379609  6.95401493  0.11437523  0.21914929 11.70390371], Loss = 0.7300\n",
      "Iteration 3735: Weights = [55.30333333  2.90360847  6.95356562  0.11436784  0.21913513 11.70438773], Loss = 0.7299\n",
      "Iteration 3736: Weights = [55.30333333  2.90342086  6.95311634  0.11436045  0.21912097 11.70487173], Loss = 0.7298\n",
      "Iteration 3737: Weights = [55.30333333  2.90323326  6.95266708  0.11435306  0.21910681 11.70535569], Loss = 0.7297\n",
      "Iteration 3738: Weights = [55.30333333  2.90304568  6.95221786  0.11434567  0.21909265 11.70583962], Loss = 0.7296\n",
      "Iteration 3739: Weights = [55.30333333  2.90285811  6.95176866  0.11433828  0.2190785  11.70632352], Loss = 0.7295\n",
      "Iteration 3740: Weights = [55.30333333  2.90267055  6.95131949  0.11433089  0.21906434 11.70680739], Loss = 0.7295\n",
      "Iteration 3741: Weights = [55.30333333  2.902483    6.95087035  0.11432351  0.21905019 11.70729123], Loss = 0.7294\n",
      "Iteration 3742: Weights = [55.30333333  2.90229547  6.95042124  0.11431612  0.21903603 11.70777504], Loss = 0.7293\n",
      "Iteration 3743: Weights = [55.30333333  2.90210794  6.94997216  0.11430873  0.21902188 11.70825881], Loss = 0.7292\n",
      "Iteration 3744: Weights = [55.30333333  2.90192043  6.94952311  0.11430135  0.21900773 11.70874255], Loss = 0.7291\n",
      "Iteration 3745: Weights = [55.30333333  2.90173293  6.94907409  0.11429396  0.21899358 11.70922627], Loss = 0.7290\n",
      "Iteration 3746: Weights = [55.30333333  2.90154545  6.94862509  0.11428658  0.21897943 11.70970995], Loss = 0.7289\n",
      "Iteration 3747: Weights = [55.30333333  2.90135797  6.94817613  0.11427919  0.21896528 11.7101936 ], Loss = 0.7288\n",
      "Iteration 3748: Weights = [55.30333333  2.90117051  6.94772719  0.11427181  0.21895113 11.71067722], Loss = 0.7287\n",
      "Iteration 3749: Weights = [55.30333333  2.90098306  6.94727829  0.11426443  0.21893699 11.7111608 ], Loss = 0.7286\n",
      "Iteration 3750: Weights = [55.30333333  2.90079562  6.94682941  0.11425704  0.21892284 11.71164436], Loss = 0.7285\n",
      "Iteration 3751: Weights = [55.30333333  2.9006082   6.94638056  0.11424966  0.21890869 11.71212789], Loss = 0.7284\n",
      "Iteration 3752: Weights = [55.30333333  2.90042078  6.94593174  0.11424228  0.21889455 11.71261138], Loss = 0.7283\n",
      "Iteration 3753: Weights = [55.30333333  2.90023338  6.94548295  0.1142349   0.21888041 11.71309484], Loss = 0.7282\n",
      "Iteration 3754: Weights = [55.30333333  2.90004599  6.94503419  0.11422752  0.21886627 11.71357827], Loss = 0.7281\n",
      "Iteration 3755: Weights = [55.30333333  2.89985861  6.94458546  0.11422014  0.21885212 11.71406167], Loss = 0.7280\n",
      "Iteration 3756: Weights = [55.30333333  2.89967125  6.94413675  0.11421276  0.21883798 11.71454504], Loss = 0.7279\n",
      "Iteration 3757: Weights = [55.30333333  2.89948389  6.94368808  0.11420538  0.21882384 11.71502838], Loss = 0.7279\n",
      "Iteration 3758: Weights = [55.30333333  2.89929655  6.94323943  0.114198    0.21880971 11.71551169], Loss = 0.7278\n",
      "Iteration 3759: Weights = [55.30333333  2.89910922  6.94279082  0.11419062  0.21879557 11.71599496], Loss = 0.7277\n",
      "Iteration 3760: Weights = [55.30333333  2.8989219   6.94234223  0.11418324  0.21878143 11.71647821], Loss = 0.7276\n",
      "Iteration 3761: Weights = [55.30333333  2.8987346   6.94189367  0.11417586  0.21876729 11.71696142], Loss = 0.7275\n",
      "Iteration 3762: Weights = [55.30333333  2.89854731  6.94144514  0.11416849  0.21875316 11.7174446 ], Loss = 0.7274\n",
      "Iteration 3763: Weights = [55.30333333  2.89836002  6.94099664  0.11416111  0.21873903 11.71792775], Loss = 0.7273\n",
      "Iteration 3764: Weights = [55.30333333  2.89817276  6.94054817  0.11415373  0.21872489 11.71841087], Loss = 0.7272\n",
      "Iteration 3765: Weights = [55.30333333  2.8979855   6.94009973  0.11414636  0.21871076 11.71889396], Loss = 0.7271\n",
      "Iteration 3766: Weights = [55.30333333  2.89779825  6.93965131  0.11413898  0.21869663 11.71937701], Loss = 0.7270\n",
      "Iteration 3767: Weights = [55.30333333  2.89761102  6.93920293  0.11413161  0.2186825  11.71986004], Loss = 0.7269\n",
      "Iteration 3768: Weights = [55.30333333  2.8974238   6.93875457  0.11412423  0.21866837 11.72034303], Loss = 0.7268\n",
      "Iteration 3769: Weights = [55.30333333  2.89723659  6.93830624  0.11411686  0.21865424 11.720826  ], Loss = 0.7267\n",
      "Iteration 3770: Weights = [55.30333333  2.8970494   6.93785795  0.11410949  0.21864011 11.72130893], Loss = 0.7266\n",
      "Iteration 3771: Weights = [55.30333333  2.89686221  6.93740968  0.11410211  0.21862599 11.72179183], Loss = 0.7265\n",
      "Iteration 3772: Weights = [55.30333333  2.89667504  6.93696144  0.11409474  0.21861186 11.7222747 ], Loss = 0.7264\n",
      "Iteration 3773: Weights = [55.30333333  2.89648788  6.93651323  0.11408737  0.21859774 11.72275754], Loss = 0.7263\n",
      "Iteration 3774: Weights = [55.30333333  2.89630073  6.93606504  0.11408     0.21858361 11.72324034], Loss = 0.7263\n",
      "Iteration 3775: Weights = [55.30333333  2.8961136   6.93561689  0.11407263  0.21856949 11.72372312], Loss = 0.7262\n",
      "Iteration 3776: Weights = [55.30333333  2.89592647  6.93516877  0.11406526  0.21855537 11.72420586], Loss = 0.7261\n",
      "Iteration 3777: Weights = [55.30333333  2.89573936  6.93472067  0.11405789  0.21854124 11.72468858], Loss = 0.7260\n",
      "Iteration 3778: Weights = [55.30333333  2.89555226  6.93427261  0.11405052  0.21852712 11.72517126], Loss = 0.7259\n",
      "Iteration 3779: Weights = [55.30333333  2.89536517  6.93382457  0.11404315  0.218513   11.72565391], Loss = 0.7258\n",
      "Iteration 3780: Weights = [55.30333333  2.8951781   6.93337656  0.11403578  0.21849889 11.72613653], Loss = 0.7257\n",
      "Iteration 3781: Weights = [55.30333333  2.89499104  6.93292858  0.11402841  0.21848477 11.72661912], Loss = 0.7256\n",
      "Iteration 3782: Weights = [55.30333333  2.89480398  6.93248063  0.11402104  0.21847065 11.72710168], Loss = 0.7255\n",
      "Iteration 3783: Weights = [55.30333333  2.89461695  6.93203271  0.11401368  0.21845654 11.7275842 ], Loss = 0.7254\n",
      "Iteration 3784: Weights = [55.30333333  2.89442992  6.93158482  0.11400631  0.21844242 11.7280667 ], Loss = 0.7253\n",
      "Iteration 3785: Weights = [55.30333333  2.8942429   6.93113695  0.11399894  0.21842831 11.72854916], Loss = 0.7252\n",
      "Iteration 3786: Weights = [55.30333333  2.8940559   6.93068912  0.11399158  0.21841419 11.7290316 ], Loss = 0.7251\n",
      "Iteration 3787: Weights = [55.30333333  2.89386891  6.93024131  0.11398421  0.21840008 11.729514  ], Loss = 0.7250\n",
      "Iteration 3788: Weights = [55.30333333  2.89368193  6.92979353  0.11397685  0.21838597 11.72999637], Loss = 0.7249\n",
      "Iteration 3789: Weights = [55.30333333  2.89349496  6.92934579  0.11396948  0.21837186 11.73047871], Loss = 0.7248\n",
      "Iteration 3790: Weights = [55.30333333  2.89330801  6.92889807  0.11396212  0.21835775 11.73096101], Loss = 0.7248\n",
      "Iteration 3791: Weights = [55.30333333  2.89312107  6.92845038  0.11395476  0.21834364 11.73144329], Loss = 0.7247\n",
      "Iteration 3792: Weights = [55.30333333  2.89293414  6.92800272  0.11394739  0.21832953 11.73192554], Loss = 0.7246\n",
      "Iteration 3793: Weights = [55.30333333  2.89274722  6.92755508  0.11394003  0.21831543 11.73240775], Loss = 0.7245\n",
      "Iteration 3794: Weights = [55.30333333  2.89256031  6.92710748  0.11393267  0.21830132 11.73288994], Loss = 0.7244\n",
      "Iteration 3795: Weights = [55.30333333  2.89237342  6.92665991  0.11392531  0.21828722 11.73337209], Loss = 0.7243\n",
      "Iteration 3796: Weights = [55.30333333  2.89218654  6.92621236  0.11391795  0.21827311 11.73385421], Loss = 0.7242\n",
      "Iteration 3797: Weights = [55.30333333  2.89199967  6.92576485  0.11391059  0.21825901 11.7343363 ], Loss = 0.7241\n",
      "Iteration 3798: Weights = [55.30333333  2.89181281  6.92531736  0.11390323  0.21824491 11.73481836], Loss = 0.7240\n",
      "Iteration 3799: Weights = [55.30333333  2.89162596  6.9248699   0.11389587  0.21823081 11.73530039], Loss = 0.7239\n",
      "Iteration 3800: Weights = [55.30333333  2.89143913  6.92442247  0.11388851  0.21821671 11.73578238], Loss = 0.7238\n",
      "Iteration 3801: Weights = [55.30333333  2.89125231  6.92397507  0.11388115  0.21820261 11.73626435], Loss = 0.7237\n",
      "Iteration 3802: Weights = [55.30333333  2.8910655   6.9235277   0.11387379  0.21818851 11.73674628], Loss = 0.7236\n",
      "Iteration 3803: Weights = [55.30333333  2.8908787   6.92308035  0.11386643  0.21817441 11.73722819], Loss = 0.7235\n",
      "Iteration 3804: Weights = [55.30333333  2.89069191  6.92263304  0.11385908  0.21816031 11.73771006], Loss = 0.7234\n",
      "Iteration 3805: Weights = [55.30333333  2.89050514  6.92218575  0.11385172  0.21814622 11.7381919 ], Loss = 0.7234\n",
      "Iteration 3806: Weights = [55.30333333  2.89031838  6.9217385   0.11384436  0.21813212 11.73867371], Loss = 0.7233\n",
      "Iteration 3807: Weights = [55.30333333  2.89013163  6.92129127  0.11383701  0.21811803 11.73915549], Loss = 0.7232\n",
      "Iteration 3808: Weights = [55.30333333  2.88994489  6.92084407  0.11382965  0.21810394 11.73963723], Loss = 0.7231\n",
      "Iteration 3809: Weights = [55.30333333  2.88975817  6.9203969   0.1138223   0.21808984 11.74011895], Loss = 0.7230\n",
      "Iteration 3810: Weights = [55.30333333  2.88957146  6.91994976  0.11381494  0.21807575 11.74060064], Loss = 0.7229\n",
      "Iteration 3811: Weights = [55.30333333  2.88938475  6.91950265  0.11380759  0.21806166 11.74108229], Loss = 0.7228\n",
      "Iteration 3812: Weights = [55.30333333  2.88919807  6.91905557  0.11380024  0.21804757 11.74156391], Loss = 0.7227\n",
      "Iteration 3813: Weights = [55.30333333  2.88901139  6.91860851  0.11379288  0.21803348 11.7420455 ], Loss = 0.7226\n",
      "Iteration 3814: Weights = [55.30333333  2.88882472  6.91816149  0.11378553  0.2180194  11.74252707], Loss = 0.7225\n",
      "Iteration 3815: Weights = [55.30333333  2.88863807  6.91771449  0.11377818  0.21800531 11.7430086 ], Loss = 0.7224\n",
      "Iteration 3816: Weights = [55.30333333  2.88845143  6.91726752  0.11377083  0.21799122 11.74349009], Loss = 0.7223\n",
      "Iteration 3817: Weights = [55.30333333  2.8882648   6.91682058  0.11376348  0.21797714 11.74397156], Loss = 0.7222\n",
      "Iteration 3818: Weights = [55.30333333  2.88807818  6.91637368  0.11375613  0.21796306 11.744453  ], Loss = 0.7221\n",
      "Iteration 3819: Weights = [55.30333333  2.88789158  6.91592679  0.11374878  0.21794897 11.7449344 ], Loss = 0.7220\n",
      "Iteration 3820: Weights = [55.30333333  2.88770499  6.91547994  0.11374143  0.21793489 11.74541578], Loss = 0.7220\n",
      "Iteration 3821: Weights = [55.30333333  2.88751841  6.91503312  0.11373408  0.21792081 11.74589712], Loss = 0.7219\n",
      "Iteration 3822: Weights = [55.30333333  2.88733184  6.91458633  0.11372673  0.21790673 11.74637843], Loss = 0.7218\n",
      "Iteration 3823: Weights = [55.30333333  2.88714528  6.91413956  0.11371938  0.21789265 11.74685971], Loss = 0.7217\n",
      "Iteration 3824: Weights = [55.30333333  2.88695874  6.91369282  0.11371203  0.21787857 11.74734096], Loss = 0.7216\n",
      "Iteration 3825: Weights = [55.30333333  2.88677221  6.91324612  0.11370469  0.21786449 11.74782218], Loss = 0.7215\n",
      "Iteration 3826: Weights = [55.30333333  2.88658569  6.91279944  0.11369734  0.21785042 11.74830337], Loss = 0.7214\n",
      "Iteration 3827: Weights = [55.30333333  2.88639918  6.91235279  0.11368999  0.21783634 11.74878452], Loss = 0.7213\n",
      "Iteration 3828: Weights = [55.30333333  2.88621268  6.91190617  0.11368265  0.21782227 11.74926565], Loss = 0.7212\n",
      "Iteration 3829: Weights = [55.30333333  2.8860262   6.91145957  0.1136753   0.21780819 11.74974674], Loss = 0.7211\n",
      "Iteration 3830: Weights = [55.30333333  2.88583973  6.91101301  0.11366796  0.21779412 11.75022781], Loss = 0.7210\n",
      "Iteration 3831: Weights = [55.30333333  2.88565327  6.91056648  0.11366061  0.21778005 11.75070884], Loss = 0.7209\n",
      "Iteration 3832: Weights = [55.30333333  2.88546682  6.91011997  0.11365327  0.21776598 11.75118984], Loss = 0.7208\n",
      "Iteration 3833: Weights = [55.30333333  2.88528038  6.90967349  0.11364593  0.21775191 11.75167081], Loss = 0.7207\n",
      "Iteration 3834: Weights = [55.30333333  2.88509396  6.90922705  0.11363858  0.21773784 11.75215175], Loss = 0.7206\n",
      "Iteration 3835: Weights = [55.30333333  2.88490755  6.90878063  0.11363124  0.21772377 11.75263266], Loss = 0.7206\n",
      "Iteration 3836: Weights = [55.30333333  2.88472115  6.90833424  0.1136239   0.2177097  11.75311353], Loss = 0.7205\n",
      "Iteration 3837: Weights = [55.30333333  2.88453476  6.90788788  0.11361656  0.21769563 11.75359438], Loss = 0.7204\n",
      "Iteration 3838: Weights = [55.30333333  2.88434838  6.90744154  0.11360922  0.21768157 11.75407519], Loss = 0.7203\n",
      "Iteration 3839: Weights = [55.30333333  2.88416202  6.90699524  0.11360188  0.2176675  11.75455598], Loss = 0.7202\n",
      "Iteration 3840: Weights = [55.30333333  2.88397567  6.90654896  0.11359454  0.21765344 11.75503673], Loss = 0.7201\n",
      "Iteration 3841: Weights = [55.30333333  2.88378933  6.90610272  0.1135872   0.21763938 11.75551745], Loss = 0.7200\n",
      "Iteration 3842: Weights = [55.30333333  2.883603    6.9056565   0.11357986  0.21762531 11.75599814], Loss = 0.7199\n",
      "Iteration 3843: Weights = [55.30333333  2.88341669  6.90521031  0.11357252  0.21761125 11.7564788 ], Loss = 0.7198\n",
      "Iteration 3844: Weights = [55.30333333  2.88323038  6.90476415  0.11356518  0.21759719 11.75695943], Loss = 0.7197\n",
      "Iteration 3845: Weights = [55.30333333  2.88304409  6.90431802  0.11355784  0.21758313 11.75744003], Loss = 0.7196\n",
      "Iteration 3846: Weights = [55.30333333  2.88285781  6.90387192  0.11355051  0.21756907 11.75792059], Loss = 0.7195\n",
      "Iteration 3847: Weights = [55.30333333  2.88267155  6.90342585  0.11354317  0.21755502 11.75840113], Loss = 0.7194\n",
      "Iteration 3848: Weights = [55.30333333  2.88248529  6.9029798   0.11353583  0.21754096 11.75888163], Loss = 0.7193\n",
      "Iteration 3849: Weights = [55.30333333  2.88229905  6.90253379  0.1135285   0.2175269  11.7593621 ], Loss = 0.7193\n",
      "Iteration 3850: Weights = [55.30333333  2.88211282  6.9020878   0.11352116  0.21751285 11.75984255], Loss = 0.7192\n",
      "Iteration 3851: Weights = [55.30333333  2.8819266   6.90164184  0.11351383  0.2174988  11.76032296], Loss = 0.7191\n",
      "Iteration 3852: Weights = [55.30333333  2.88174039  6.90119591  0.11350649  0.21748474 11.76080334], Loss = 0.7190\n",
      "Iteration 3853: Weights = [55.30333333  2.88155419  6.90075001  0.11349916  0.21747069 11.76128369], Loss = 0.7189\n",
      "Iteration 3854: Weights = [55.30333333  2.88136801  6.90030414  0.11349182  0.21745664 11.761764  ], Loss = 0.7188\n",
      "Iteration 3855: Weights = [55.30333333  2.88118184  6.8998583   0.11348449  0.21744259 11.76224429], Loss = 0.7187\n",
      "Iteration 3856: Weights = [55.30333333  2.88099568  6.89941249  0.11347716  0.21742854 11.76272455], Loss = 0.7186\n",
      "Iteration 3857: Weights = [55.30333333  2.88080953  6.8989667   0.11346983  0.21741449 11.76320477], Loss = 0.7185\n",
      "Iteration 3858: Weights = [55.30333333  2.8806234   6.89852095  0.1134625   0.21740044 11.76368496], Loss = 0.7184\n",
      "Iteration 3859: Weights = [55.30333333  2.88043728  6.89807522  0.11345516  0.2173864  11.76416513], Loss = 0.7183\n",
      "Iteration 3860: Weights = [55.30333333  2.88025117  6.89762952  0.11344783  0.21737235 11.76464526], Loss = 0.7182\n",
      "Iteration 3861: Weights = [55.30333333  2.88006507  6.89718385  0.1134405   0.21735831 11.76512536], Loss = 0.7181\n",
      "Iteration 3862: Weights = [55.30333333  2.87987898  6.89673821  0.11343317  0.21734426 11.76560543], Loss = 0.7180\n",
      "Iteration 3863: Weights = [55.30333333  2.87969291  6.8962926   0.11342585  0.21733022 11.76608547], Loss = 0.7179\n",
      "Iteration 3864: Weights = [55.30333333  2.87950684  6.89584701  0.11341852  0.21731618 11.76656548], Loss = 0.7179\n",
      "Iteration 3865: Weights = [55.30333333  2.87932079  6.89540146  0.11341119  0.21730214 11.76704545], Loss = 0.7178\n",
      "Iteration 3866: Weights = [55.30333333  2.87913475  6.89495593  0.11340386  0.2172881  11.7675254 ], Loss = 0.7177\n",
      "Iteration 3867: Weights = [55.30333333  2.87894873  6.89451044  0.11339653  0.21727406 11.76800531], Loss = 0.7176\n",
      "Iteration 3868: Weights = [55.30333333  2.87876271  6.89406497  0.11338921  0.21726002 11.7684852 ], Loss = 0.7175\n",
      "Iteration 3869: Weights = [55.30333333  2.87857671  6.89361953  0.11338188  0.21724598 11.76896505], Loss = 0.7174\n",
      "Iteration 3870: Weights = [55.30333333  2.87839072  6.89317412  0.11337455  0.21723194 11.76944487], Loss = 0.7173\n",
      "Iteration 3871: Weights = [55.30333333  2.87820474  6.89272874  0.11336723  0.21721791 11.76992466], Loss = 0.7172\n",
      "Iteration 3872: Weights = [55.30333333  2.87801877  6.89228338  0.1133599   0.21720387 11.77040442], Loss = 0.7171\n",
      "Iteration 3873: Weights = [55.30333333  2.87783282  6.89183806  0.11335258  0.21718984 11.77088415], Loss = 0.7170\n",
      "Iteration 3874: Weights = [55.30333333  2.87764688  6.89139276  0.11334526  0.21717581 11.77136384], Loss = 0.7169\n",
      "Iteration 3875: Weights = [55.30333333  2.87746095  6.8909475   0.11333793  0.21716177 11.77184351], Loss = 0.7168\n",
      "Iteration 3876: Weights = [55.30333333  2.87727503  6.89050226  0.11333061  0.21714774 11.77232315], Loss = 0.7167\n",
      "Iteration 3877: Weights = [55.30333333  2.87708912  6.89005705  0.11332329  0.21713371 11.77280275], Loss = 0.7167\n",
      "Iteration 3878: Weights = [55.30333333  2.87690323  6.88961187  0.11331596  0.21711968 11.77328232], Loss = 0.7166\n",
      "Iteration 3879: Weights = [55.30333333  2.87671734  6.88916672  0.11330864  0.21710565 11.77376187], Loss = 0.7165\n",
      "Iteration 3880: Weights = [55.30333333  2.87653147  6.8887216   0.11330132  0.21709163 11.77424138], Loss = 0.7164\n",
      "Iteration 3881: Weights = [55.30333333  2.87634562  6.8882765   0.113294    0.2170776  11.77472086], Loss = 0.7163\n",
      "Iteration 3882: Weights = [55.30333333  2.87615977  6.88783144  0.11328668  0.21706357 11.77520031], Loss = 0.7162\n",
      "Iteration 3883: Weights = [55.30333333  2.87597393  6.8873864   0.11327936  0.21704955 11.77567973], Loss = 0.7161\n",
      "Iteration 3884: Weights = [55.30333333  2.87578811  6.88694139  0.11327204  0.21703552 11.77615911], Loss = 0.7160\n",
      "Iteration 3885: Weights = [55.30333333  2.8756023   6.88649641  0.11326472  0.2170215  11.77663847], Loss = 0.7159\n",
      "Iteration 3886: Weights = [55.30333333  2.8754165   6.88605146  0.11325741  0.21700748 11.7771178 ], Loss = 0.7158\n",
      "Iteration 3887: Weights = [55.30333333  2.87523072  6.88560654  0.11325009  0.21699346 11.77759709], Loss = 0.7157\n",
      "Iteration 3888: Weights = [55.30333333  2.87504494  6.88516165  0.11324277  0.21697944 11.77807636], Loss = 0.7156\n",
      "Iteration 3889: Weights = [55.30333333  2.87485918  6.88471678  0.11323545  0.21696542 11.77855559], Loss = 0.7155\n",
      "Iteration 3890: Weights = [55.30333333  2.87467343  6.88427195  0.11322814  0.2169514  11.77903479], Loss = 0.7154\n",
      "Iteration 3891: Weights = [55.30333333  2.87448769  6.88382714  0.11322082  0.21693738 11.77951396], Loss = 0.7154\n",
      "Iteration 3892: Weights = [55.30333333  2.87430196  6.88338236  0.11321351  0.21692337 11.7799931 ], Loss = 0.7153\n",
      "Iteration 3893: Weights = [55.30333333  2.87411625  6.88293761  0.11320619  0.21690935 11.78047221], Loss = 0.7152\n",
      "Iteration 3894: Weights = [55.30333333  2.87393055  6.88249289  0.11319888  0.21689533 11.78095129], Loss = 0.7151\n",
      "Iteration 3895: Weights = [55.30333333  2.87374486  6.8820482   0.11319156  0.21688132 11.78143033], Loss = 0.7150\n",
      "Iteration 3896: Weights = [55.30333333  2.87355918  6.88160354  0.11318425  0.21686731 11.78190935], Loss = 0.7149\n",
      "Iteration 3897: Weights = [55.30333333  2.87337351  6.8811589   0.11317694  0.2168533  11.78238834], Loss = 0.7148\n",
      "Iteration 3898: Weights = [55.30333333  2.87318786  6.8807143   0.11316962  0.21683928 11.78286729], Loss = 0.7147\n",
      "Iteration 3899: Weights = [55.30333333  2.87300222  6.88026972  0.11316231  0.21682527 11.78334621], Loss = 0.7146\n",
      "Iteration 3900: Weights = [55.30333333  2.87281658  6.87982517  0.113155    0.21681126 11.78382511], Loss = 0.7145\n",
      "Iteration 3901: Weights = [55.30333333  2.87263097  6.87938066  0.11314769  0.21679726 11.78430397], Loss = 0.7144\n",
      "Iteration 3902: Weights = [55.30333333  2.87244536  6.87893616  0.11314038  0.21678325 11.7847828 ], Loss = 0.7143\n",
      "Iteration 3903: Weights = [55.30333333  2.87225977  6.8784917   0.11313307  0.21676924 11.7852616 ], Loss = 0.7142\n",
      "Iteration 3904: Weights = [55.30333333  2.87207418  6.87804727  0.11312576  0.21675524 11.78574037], Loss = 0.7142\n",
      "Iteration 3905: Weights = [55.30333333  2.87188861  6.87760287  0.11311845  0.21674123 11.7862191 ], Loss = 0.7141\n",
      "Iteration 3906: Weights = [55.30333333  2.87170305  6.87715849  0.11311114  0.21672723 11.78669781], Loss = 0.7140\n",
      "Iteration 3907: Weights = [55.30333333  2.87151751  6.87671414  0.11310383  0.21671322 11.78717649], Loss = 0.7139\n",
      "Iteration 3908: Weights = [55.30333333  2.87133197  6.87626982  0.11309652  0.21669922 11.78765513], Loss = 0.7138\n",
      "Iteration 3909: Weights = [55.30333333  2.87114645  6.87582553  0.11308922  0.21668522 11.78813374], Loss = 0.7137\n",
      "Iteration 3910: Weights = [55.30333333  2.87096094  6.87538127  0.11308191  0.21667122 11.78861233], Loss = 0.7136\n",
      "Iteration 3911: Weights = [55.30333333  2.87077544  6.87493704  0.1130746   0.21665722 11.78909088], Loss = 0.7135\n",
      "Iteration 3912: Weights = [55.30333333  2.87058995  6.87449284  0.1130673   0.21664322 11.7895694 ], Loss = 0.7134\n",
      "Iteration 3913: Weights = [55.30333333  2.87040448  6.87404866  0.11305999  0.21662922 11.79004789], Loss = 0.7133\n",
      "Iteration 3914: Weights = [55.30333333  2.87021902  6.87360452  0.11305269  0.21661523 11.79052635], Loss = 0.7132\n",
      "Iteration 3915: Weights = [55.30333333  2.87003357  6.8731604   0.11304538  0.21660123 11.79100478], Loss = 0.7131\n",
      "Iteration 3916: Weights = [55.30333333  2.86984813  6.87271631  0.11303808  0.21658723 11.79148317], Loss = 0.7130\n",
      "Iteration 3917: Weights = [55.30333333  2.8696627   6.87227225  0.11303077  0.21657324 11.79196154], Loss = 0.7130\n",
      "Iteration 3918: Weights = [55.30333333  2.86947729  6.87182822  0.11302347  0.21655925 11.79243988], Loss = 0.7129\n",
      "Iteration 3919: Weights = [55.30333333  2.86929188  6.87138422  0.11301617  0.21654526 11.79291818], Loss = 0.7128\n",
      "Iteration 3920: Weights = [55.30333333  2.86910649  6.87094024  0.11300887  0.21653126 11.79339646], Loss = 0.7127\n",
      "Iteration 3921: Weights = [55.30333333  2.86892111  6.8704963   0.11300156  0.21651727 11.7938747 ], Loss = 0.7126\n",
      "Iteration 3922: Weights = [55.30333333  2.86873575  6.87005238  0.11299426  0.21650328 11.79435291], Loss = 0.7125\n",
      "Iteration 3923: Weights = [55.30333333  2.86855039  6.86960849  0.11298696  0.21648929 11.79483109], Loss = 0.7124\n",
      "Iteration 3924: Weights = [55.30333333  2.86836505  6.86916463  0.11297966  0.21647531 11.79530924], Loss = 0.7123\n",
      "Iteration 3925: Weights = [55.30333333  2.86817972  6.8687208   0.11297236  0.21646132 11.79578736], Loss = 0.7122\n",
      "Iteration 3926: Weights = [55.30333333  2.8679944   6.868277    0.11296506  0.21644733 11.79626545], Loss = 0.7121\n",
      "Iteration 3927: Weights = [55.30333333  2.86780909  6.86783323  0.11295776  0.21643335 11.79674351], Loss = 0.7120\n",
      "Iteration 3928: Weights = [55.30333333  2.8676238   6.86738948  0.11295047  0.21641936 11.79722153], Loss = 0.7119\n",
      "Iteration 3929: Weights = [55.30333333  2.86743852  6.86694577  0.11294317  0.21640538 11.79769953], Loss = 0.7119\n",
      "Iteration 3930: Weights = [55.30333333  2.86725325  6.86650208  0.11293587  0.2163914  11.79817749], Loss = 0.7118\n",
      "Iteration 3931: Weights = [55.30333333  2.86706799  6.86605842  0.11292857  0.21637742 11.79865543], Loss = 0.7117\n",
      "Iteration 3932: Weights = [55.30333333  2.86688274  6.86561479  0.11292128  0.21636344 11.79913333], Loss = 0.7116\n",
      "Iteration 3933: Weights = [55.30333333  2.8666975   6.86517119  0.11291398  0.21634946 11.7996112 ], Loss = 0.7115\n",
      "Iteration 3934: Weights = [55.30333333  2.86651228  6.86472762  0.11290668  0.21633548 11.80008904], Loss = 0.7114\n",
      "Iteration 3935: Weights = [55.30333333  2.86632707  6.86428407  0.11289939  0.2163215  11.80056686], Loss = 0.7113\n",
      "Iteration 3936: Weights = [55.30333333  2.86614187  6.86384056  0.11289209  0.21630752 11.80104463], Loss = 0.7112\n",
      "Iteration 3937: Weights = [55.30333333  2.86595668  6.86339707  0.1128848   0.21629355 11.80152238], Loss = 0.7111\n",
      "Iteration 3938: Weights = [55.30333333  2.86577151  6.86295362  0.11287751  0.21627957 11.8020001 ], Loss = 0.7110\n",
      "Iteration 3939: Weights = [55.30333333  2.86558635  6.86251019  0.11287021  0.2162656  11.80247779], Loss = 0.7109\n",
      "Iteration 3940: Weights = [55.30333333  2.86540119  6.86206679  0.11286292  0.21625163 11.80295544], Loss = 0.7108\n",
      "Iteration 3941: Weights = [55.30333333  2.86521605  6.86162341  0.11285563  0.21623765 11.80343307], Loss = 0.7107\n",
      "Iteration 3942: Weights = [55.30333333  2.86503093  6.86118007  0.11284834  0.21622368 11.80391066], Loss = 0.7107\n",
      "Iteration 3943: Weights = [55.30333333  2.86484581  6.86073676  0.11284105  0.21620971 11.80438823], Loss = 0.7106\n",
      "Iteration 3944: Weights = [55.30333333  2.86466071  6.86029347  0.11283375  0.21619574 11.80486576], Loss = 0.7105\n",
      "Iteration 3945: Weights = [55.30333333  2.86447562  6.85985021  0.11282646  0.21618177 11.80534326], Loss = 0.7104\n",
      "Iteration 3946: Weights = [55.30333333  2.86429054  6.85940698  0.11281917  0.2161678  11.80582073], Loss = 0.7103\n",
      "Iteration 3947: Weights = [55.30333333  2.86410547  6.85896378  0.11281188  0.21615384 11.80629817], Loss = 0.7102\n",
      "Iteration 3948: Weights = [55.30333333  2.86392041  6.85852061  0.1128046   0.21613987 11.80677558], Loss = 0.7101\n",
      "Iteration 3949: Weights = [55.30333333  2.86373537  6.85807747  0.11279731  0.21612591 11.80725296], Loss = 0.7100\n",
      "Iteration 3950: Weights = [55.30333333  2.86355034  6.85763436  0.11279002  0.21611194 11.80773031], Loss = 0.7099\n",
      "Iteration 3951: Weights = [55.30333333  2.86336532  6.85719127  0.11278273  0.21609798 11.80820763], Loss = 0.7098\n",
      "Iteration 3952: Weights = [55.30333333  2.86318031  6.85674821  0.11277544  0.21608402 11.80868491], Loss = 0.7097\n",
      "Iteration 3953: Weights = [55.30333333  2.86299532  6.85630518  0.11276816  0.21607005 11.80916217], Loss = 0.7096\n",
      "Iteration 3954: Weights = [55.30333333  2.86281033  6.85586218  0.11276087  0.21605609 11.80963939], Loss = 0.7096\n",
      "Iteration 3955: Weights = [55.30333333  2.86262536  6.85541921  0.11275359  0.21604213 11.81011658], Loss = 0.7095\n",
      "Iteration 3956: Weights = [55.30333333  2.8624404   6.85497627  0.1127463   0.21602817 11.81059375], Loss = 0.7094\n",
      "Iteration 3957: Weights = [55.30333333  2.86225545  6.85453336  0.11273902  0.21601422 11.81107088], Loss = 0.7093\n",
      "Iteration 3958: Weights = [55.30333333  2.86207052  6.85409047  0.11273173  0.21600026 11.81154798], Loss = 0.7092\n",
      "Iteration 3959: Weights = [55.30333333  2.86188559  6.85364762  0.11272445  0.2159863  11.81202505], Loss = 0.7091\n",
      "Iteration 3960: Weights = [55.30333333  2.86170068  6.85320479  0.11271716  0.21597235 11.81250209], Loss = 0.7090\n",
      "Iteration 3961: Weights = [55.30333333  2.86151578  6.85276199  0.11270988  0.21595839 11.8129791 ], Loss = 0.7089\n",
      "Iteration 3962: Weights = [55.30333333  2.86133089  6.85231922  0.1127026   0.21594444 11.81345607], Loss = 0.7088\n",
      "Iteration 3963: Weights = [55.30333333  2.86114601  6.85187648  0.11269532  0.21593049 11.81393302], Loss = 0.7087\n",
      "Iteration 3964: Weights = [55.30333333  2.86096115  6.85143376  0.11268804  0.21591654 11.81440994], Loss = 0.7086\n",
      "Iteration 3965: Weights = [55.30333333  2.8607763   6.85099108  0.11268075  0.21590258 11.81488682], Loss = 0.7085\n",
      "Iteration 3966: Weights = [55.30333333  2.86059146  6.85054842  0.11267347  0.21588863 11.81536368], Loss = 0.7085\n",
      "Iteration 3967: Weights = [55.30333333  2.86040663  6.85010579  0.11266619  0.21587469 11.8158405 ], Loss = 0.7084\n",
      "Iteration 3968: Weights = [55.30333333  2.86022181  6.84966319  0.11265891  0.21586074 11.81631729], Loss = 0.7083\n",
      "Iteration 3969: Weights = [55.30333333  2.86003701  6.84922062  0.11265164  0.21584679 11.81679406], Loss = 0.7082\n",
      "Iteration 3970: Weights = [55.30333333  2.85985221  6.84877808  0.11264436  0.21583284 11.81727079], Loss = 0.7081\n",
      "Iteration 3971: Weights = [55.30333333  2.85966743  6.84833557  0.11263708  0.2158189  11.81774749], Loss = 0.7080\n",
      "Iteration 3972: Weights = [55.30333333  2.85948266  6.84789308  0.1126298   0.21580495 11.81822416], Loss = 0.7079\n",
      "Iteration 3973: Weights = [55.30333333  2.85929791  6.84745063  0.11262252  0.21579101 11.81870079], Loss = 0.7078\n",
      "Iteration 3974: Weights = [55.30333333  2.85911316  6.8470082   0.11261525  0.21577707 11.8191774 ], Loss = 0.7077\n",
      "Iteration 3975: Weights = [55.30333333  2.85892843  6.8465658   0.11260797  0.21576313 11.81965398], Loss = 0.7076\n",
      "Iteration 3976: Weights = [55.30333333  2.85874371  6.84612343  0.11260069  0.21574919 11.82013053], Loss = 0.7075\n",
      "Iteration 3977: Weights = [55.30333333  2.858559    6.84568109  0.11259342  0.21573525 11.82060704], Loss = 0.7075\n",
      "Iteration 3978: Weights = [55.30333333  2.8583743   6.84523878  0.11258614  0.21572131 11.82108353], Loss = 0.7074\n",
      "Iteration 3979: Weights = [55.30333333  2.85818962  6.84479649  0.11257887  0.21570737 11.82155998], Loss = 0.7073\n",
      "Iteration 3980: Weights = [55.30333333  2.85800494  6.84435424  0.1125716   0.21569343 11.8220364 ], Loss = 0.7072\n",
      "Iteration 3981: Weights = [55.30333333  2.85782028  6.84391201  0.11256432  0.21567949 11.8225128 ], Loss = 0.7071\n",
      "Iteration 3982: Weights = [55.30333333  2.85763563  6.84346981  0.11255705  0.21566556 11.82298916], Loss = 0.7070\n",
      "Iteration 3983: Weights = [55.30333333  2.857451    6.84302764  0.11254978  0.21565162 11.82346549], Loss = 0.7069\n",
      "Iteration 3984: Weights = [55.30333333  2.85726637  6.8425855   0.11254251  0.21563769 11.82394179], Loss = 0.7068\n",
      "Iteration 3985: Weights = [55.30333333  2.85708176  6.84214338  0.11253523  0.21562376 11.82441806], Loss = 0.7067\n",
      "Iteration 3986: Weights = [55.30333333  2.85689715  6.8417013   0.11252796  0.21560983 11.8248943 ], Loss = 0.7066\n",
      "Iteration 3987: Weights = [55.30333333  2.85671256  6.84125924  0.11252069  0.21559589 11.8253705 ], Loss = 0.7065\n",
      "Iteration 3988: Weights = [55.30333333  2.85652799  6.84081722  0.11251342  0.21558196 11.82584668], Loss = 0.7064\n",
      "Iteration 3989: Weights = [55.30333333  2.85634342  6.84037522  0.11250615  0.21556804 11.82632283], Loss = 0.7064\n",
      "Iteration 3990: Weights = [55.30333333  2.85615887  6.83993325  0.11249888  0.21555411 11.82679894], Loss = 0.7063\n",
      "Iteration 3991: Weights = [55.30333333  2.85597432  6.8394913   0.11249161  0.21554018 11.82727503], Loss = 0.7062\n",
      "Iteration 3992: Weights = [55.30333333  2.85578979  6.83904939  0.11248435  0.21552625 11.82775108], Loss = 0.7061\n",
      "Iteration 3993: Weights = [55.30333333  2.85560528  6.83860751  0.11247708  0.21551233 11.8282271 ], Loss = 0.7060\n",
      "Iteration 3994: Weights = [55.30333333  2.85542077  6.83816565  0.11246981  0.2154984  11.8287031 ], Loss = 0.7059\n",
      "Iteration 3995: Weights = [55.30333333  2.85523627  6.83772382  0.11246254  0.21548448 11.82917906], Loss = 0.7058\n",
      "Iteration 3996: Weights = [55.30333333  2.85505179  6.83728202  0.11245528  0.21547056 11.82965499], Loss = 0.7057\n",
      "Iteration 3997: Weights = [55.30333333  2.85486732  6.83684025  0.11244801  0.21545663 11.83013089], Loss = 0.7056\n",
      "Iteration 3998: Weights = [55.30333333  2.85468286  6.83639851  0.11244075  0.21544271 11.83060676], Loss = 0.7055\n",
      "Iteration 3999: Weights = [55.30333333  2.85449842  6.8359568   0.11243348  0.21542879 11.8310826 ], Loss = 0.7054\n",
      "Iteration 4000: Weights = [55.30333333  2.85431398  6.83551511  0.11242622  0.21541487 11.83155841], Loss = 0.7054\n",
      "Iteration 4001: Weights = [55.30333333  2.85412956  6.83507346  0.11241895  0.21540096 11.83203418], Loss = 0.7053\n",
      "Iteration 4002: Weights = [55.30333333  2.85394515  6.83463183  0.11241169  0.21538704 11.83250993], Loss = 0.7052\n",
      "Iteration 4003: Weights = [55.30333333  2.85376075  6.83419023  0.11240442  0.21537312 11.83298564], Loss = 0.7051\n",
      "Iteration 4004: Weights = [55.30333333  2.85357636  6.83374866  0.11239716  0.21535921 11.83346133], Loss = 0.7050\n",
      "Iteration 4005: Weights = [55.30333333  2.85339199  6.83330712  0.1123899   0.21534529 11.83393698], Loss = 0.7049\n",
      "Iteration 4006: Weights = [55.30333333  2.85320762  6.8328656   0.11238264  0.21533138 11.83441261], Loss = 0.7048\n",
      "Iteration 4007: Weights = [55.30333333  2.85302327  6.83242412  0.11237538  0.21531746 11.8348882 ], Loss = 0.7047\n",
      "Iteration 4008: Weights = [55.30333333  2.85283893  6.83198266  0.11236812  0.21530355 11.83536376], Loss = 0.7046\n",
      "Iteration 4009: Weights = [55.30333333  2.8526546   6.83154123  0.11236086  0.21528964 11.83583929], Loss = 0.7045\n",
      "Iteration 4010: Weights = [55.30333333  2.85247029  6.83109983  0.1123536   0.21527573 11.83631479], Loss = 0.7044\n",
      "Iteration 4011: Weights = [55.30333333  2.85228598  6.83065846  0.11234634  0.21526182 11.83679026], Loss = 0.7043\n",
      "Iteration 4012: Weights = [55.30333333  2.85210169  6.83021712  0.11233908  0.21524791 11.8372657 ], Loss = 0.7043\n",
      "Iteration 4013: Weights = [55.30333333  2.85191741  6.8297758   0.11233182  0.215234   11.83774111], Loss = 0.7042\n",
      "Iteration 4014: Weights = [55.30333333  2.85173314  6.82933452  0.11232456  0.2152201  11.83821649], Loss = 0.7041\n",
      "Iteration 4015: Weights = [55.30333333  2.85154889  6.82889326  0.1123173   0.21520619 11.83869184], Loss = 0.7040\n",
      "Iteration 4016: Weights = [55.30333333  2.85136464  6.82845203  0.11231005  0.21519229 11.83916715], Loss = 0.7039\n",
      "Iteration 4017: Weights = [55.30333333  2.85118041  6.82801083  0.11230279  0.21517838 11.83964244], Loss = 0.7038\n",
      "Iteration 4018: Weights = [55.30333333  2.85099619  6.82756966  0.11229553  0.21516448 11.84011769], Loss = 0.7037\n",
      "Iteration 4019: Weights = [55.30333333  2.85081198  6.82712852  0.11228828  0.21515058 11.84059292], Loss = 0.7036\n",
      "Iteration 4020: Weights = [55.30333333  2.85062778  6.8266874   0.11228102  0.21513668 11.84106811], Loss = 0.7035\n",
      "Iteration 4021: Weights = [55.30333333  2.8504436   6.82624632  0.11227377  0.21512278 11.84154327], Loss = 0.7034\n",
      "Iteration 4022: Weights = [55.30333333  2.85025943  6.82580526  0.11226651  0.21510888 11.84201841], Loss = 0.7033\n",
      "Iteration 4023: Weights = [55.30333333  2.85007527  6.82536423  0.11225926  0.21509498 11.84249351], Loss = 0.7033\n",
      "Iteration 4024: Weights = [55.30333333  2.84989112  6.82492323  0.11225201  0.21508108 11.84296858], Loss = 0.7032\n",
      "Iteration 4025: Weights = [55.30333333  2.84970698  6.82448226  0.11224475  0.21506718 11.84344362], Loss = 0.7031\n",
      "Iteration 4026: Weights = [55.30333333  2.84952285  6.82404132  0.1122375   0.21505329 11.84391863], Loss = 0.7030\n",
      "Iteration 4027: Weights = [55.30333333  2.84933874  6.8236004   0.11223025  0.21503939 11.8443936 ], Loss = 0.7029\n",
      "Iteration 4028: Weights = [55.30333333  2.84915464  6.82315951  0.112223    0.2150255  11.84486855], Loss = 0.7028\n",
      "Iteration 4029: Weights = [55.30333333  2.84897055  6.82271866  0.11221575  0.21501161 11.84534347], Loss = 0.7027\n",
      "Iteration 4030: Weights = [55.30333333  2.84878647  6.82227783  0.1122085   0.21499771 11.84581836], Loss = 0.7026\n",
      "Iteration 4031: Weights = [55.30333333  2.84860241  6.82183703  0.11220125  0.21498382 11.84629321], Loss = 0.7025\n",
      "Iteration 4032: Weights = [55.30333333  2.84841835  6.82139625  0.112194    0.21496993 11.84676804], Loss = 0.7024\n",
      "Iteration 4033: Weights = [55.30333333  2.84823431  6.82095551  0.11218675  0.21495604 11.84724283], Loss = 0.7023\n",
      "Iteration 4034: Weights = [55.30333333  2.84805028  6.82051479  0.1121795   0.21494215 11.8477176 ], Loss = 0.7023\n",
      "Iteration 4035: Weights = [55.30333333  2.84786626  6.82007411  0.11217225  0.21492826 11.84819233], Loss = 0.7022\n",
      "Iteration 4036: Weights = [55.30333333  2.84768226  6.81963345  0.112165    0.21491438 11.84866703], Loss = 0.7021\n",
      "Iteration 4037: Weights = [55.30333333  2.84749826  6.81919282  0.11215776  0.21490049 11.8491417 ], Loss = 0.7020\n",
      "Iteration 4038: Weights = [55.30333333  2.84731428  6.81875222  0.11215051  0.21488661 11.84961634], Loss = 0.7019\n",
      "Iteration 4039: Weights = [55.30333333  2.84713031  6.81831164  0.11214326  0.21487272 11.85009095], Loss = 0.7018\n",
      "Iteration 4040: Weights = [55.30333333  2.84694635  6.8178711   0.11213602  0.21485884 11.85056553], Loss = 0.7017\n",
      "Iteration 4041: Weights = [55.30333333  2.8467624   6.81743058  0.11212877  0.21484496 11.85104008], Loss = 0.7016\n",
      "Iteration 4042: Weights = [55.30333333  2.84657847  6.81699009  0.11212153  0.21483108 11.8515146 ], Loss = 0.7015\n",
      "Iteration 4043: Weights = [55.30333333  2.84639454  6.81654963  0.11211428  0.21481719 11.85198909], Loss = 0.7014\n",
      "Iteration 4044: Weights = [55.30333333  2.84621063  6.8161092   0.11210704  0.21480331 11.85246355], Loss = 0.7014\n",
      "Iteration 4045: Weights = [55.30333333  2.84602673  6.8156688   0.1120998   0.21478944 11.85293797], Loss = 0.7013\n",
      "Iteration 4046: Weights = [55.30333333  2.84584285  6.81522843  0.11209255  0.21477556 11.85341237], Loss = 0.7012\n",
      "Iteration 4047: Weights = [55.30333333  2.84565897  6.81478808  0.11208531  0.21476168 11.85388673], Loss = 0.7011\n",
      "Iteration 4048: Weights = [55.30333333  2.84547511  6.81434776  0.11207807  0.2147478  11.85436107], Loss = 0.7010\n",
      "Iteration 4049: Weights = [55.30333333  2.84529125  6.81390748  0.11207083  0.21473393 11.85483537], Loss = 0.7009\n",
      "Iteration 4050: Weights = [55.30333333  2.84510741  6.81346721  0.11206359  0.21472005 11.85530965], Loss = 0.7008\n",
      "Iteration 4051: Weights = [55.30333333  2.84492359  6.81302698  0.11205635  0.21470618 11.85578389], Loss = 0.7007\n",
      "Iteration 4052: Weights = [55.30333333  2.84473977  6.81258678  0.11204911  0.21469231 11.8562581 ], Loss = 0.7006\n",
      "Iteration 4053: Weights = [55.30333333  2.84455597  6.8121466   0.11204187  0.21467844 11.85673228], Loss = 0.7005\n",
      "Iteration 4054: Weights = [55.30333333  2.84437217  6.81170646  0.11203463  0.21466457 11.85720643], Loss = 0.7004\n",
      "Iteration 4055: Weights = [55.30333333  2.84418839  6.81126634  0.11202739  0.2146507  11.85768055], Loss = 0.7004\n",
      "Iteration 4056: Weights = [55.30333333  2.84400462  6.81082625  0.11202015  0.21463683 11.85815464], Loss = 0.7003\n",
      "Iteration 4057: Weights = [55.30333333  2.84382087  6.81038619  0.11201291  0.21462296 11.8586287 ], Loss = 0.7002\n",
      "Iteration 4058: Weights = [55.30333333  2.84363712  6.80994616  0.11200567  0.21460909 11.85910273], Loss = 0.7001\n",
      "Iteration 4059: Weights = [55.30333333  2.84345339  6.80950615  0.11199844  0.21459523 11.85957673], Loss = 0.7000\n",
      "Iteration 4060: Weights = [55.30333333  2.84326967  6.80906618  0.1119912   0.21458136 11.86005069], Loss = 0.6999\n",
      "Iteration 4061: Weights = [55.30333333  2.84308596  6.80862623  0.11198396  0.2145675  11.86052463], Loss = 0.6998\n",
      "Iteration 4062: Weights = [55.30333333  2.84290226  6.80818631  0.11197673  0.21455363 11.86099853], Loss = 0.6997\n",
      "Iteration 4063: Weights = [55.30333333  2.84271858  6.80774642  0.11196949  0.21453977 11.86147241], Loss = 0.6996\n",
      "Iteration 4064: Weights = [55.30333333  2.8425349   6.80730656  0.11196226  0.21452591 11.86194625], Loss = 0.6995\n",
      "Iteration 4065: Weights = [55.30333333  2.84235124  6.80686672  0.11195503  0.21451205 11.86242007], Loss = 0.6995\n",
      "Iteration 4066: Weights = [55.30333333  2.84216759  6.80642692  0.11194779  0.21449819 11.86289385], Loss = 0.6994\n",
      "Iteration 4067: Weights = [55.30333333  2.84198395  6.80598714  0.11194056  0.21448433 11.8633676 ], Loss = 0.6993\n",
      "Iteration 4068: Weights = [55.30333333  2.84180033  6.80554739  0.11193333  0.21447047 11.86384132], Loss = 0.6992\n",
      "Iteration 4069: Weights = [55.30333333  2.84161671  6.80510767  0.11192609  0.21445661 11.86431502], Loss = 0.6991\n",
      "Iteration 4070: Weights = [55.30333333  2.84143311  6.80466798  0.11191886  0.21444276 11.86478868], Loss = 0.6990\n",
      "Iteration 4071: Weights = [55.30333333  2.84124952  6.80422832  0.11191163  0.2144289  11.86526231], Loss = 0.6989\n",
      "Iteration 4072: Weights = [55.30333333  2.84106594  6.80378868  0.1119044   0.21441505 11.86573591], Loss = 0.6988\n",
      "Iteration 4073: Weights = [55.30333333  2.84088237  6.80334908  0.11189717  0.21440119 11.86620948], Loss = 0.6987\n",
      "Iteration 4074: Weights = [55.30333333  2.84069882  6.8029095   0.11188994  0.21438734 11.86668301], Loss = 0.6986\n",
      "Iteration 4075: Weights = [55.30333333  2.84051527  6.80246995  0.11188271  0.21437349 11.86715652], Loss = 0.6985\n",
      "Iteration 4076: Weights = [55.30333333  2.84033174  6.80203043  0.11187548  0.21435964 11.86763   ], Loss = 0.6985\n",
      "Iteration 4077: Weights = [55.30333333  2.84014822  6.80159093  0.11186825  0.21434579 11.86810344], Loss = 0.6984\n",
      "Iteration 4078: Weights = [55.30333333  2.83996471  6.80115147  0.11186102  0.21433194 11.86857686], Loss = 0.6983\n",
      "Iteration 4079: Weights = [55.30333333  2.83978122  6.80071203  0.1118538   0.21431809 11.86905025], Loss = 0.6982\n",
      "Iteration 4080: Weights = [55.30333333  2.83959774  6.80027263  0.11184657  0.21430424 11.8695236 ], Loss = 0.6981\n",
      "Iteration 4081: Weights = [55.30333333  2.83941426  6.79983325  0.11183934  0.21429039 11.86999693], Loss = 0.6980\n",
      "Iteration 4082: Weights = [55.30333333  2.8392308   6.7993939   0.11183212  0.21427655 11.87047022], Loss = 0.6979\n",
      "Iteration 4083: Weights = [55.30333333  2.83904735  6.79895457  0.11182489  0.2142627  11.87094348], Loss = 0.6978\n",
      "Iteration 4084: Weights = [55.30333333  2.83886392  6.79851528  0.11181767  0.21424886 11.87141671], Loss = 0.6977\n",
      "Iteration 4085: Weights = [55.30333333  2.83868049  6.79807601  0.11181044  0.21423502 11.87188992], Loss = 0.6976\n",
      "Iteration 4086: Weights = [55.30333333  2.83849708  6.79763678  0.11180322  0.21422117 11.87236309], Loss = 0.6976\n",
      "Iteration 4087: Weights = [55.30333333  2.83831368  6.79719757  0.11179599  0.21420733 11.87283623], Loss = 0.6975\n",
      "Iteration 4088: Weights = [55.30333333  2.83813029  6.79675839  0.11178877  0.21419349 11.87330934], Loss = 0.6974\n",
      "Iteration 4089: Weights = [55.30333333  2.83794691  6.79631923  0.11178155  0.21417965 11.87378242], Loss = 0.6973\n",
      "Iteration 4090: Weights = [55.30333333  2.83776355  6.79588011  0.11177432  0.21416581 11.87425547], Loss = 0.6972\n",
      "Iteration 4091: Weights = [55.30333333  2.83758019  6.79544102  0.1117671   0.21415198 11.87472848], Loss = 0.6971\n",
      "Iteration 4092: Weights = [55.30333333  2.83739685  6.79500195  0.11175988  0.21413814 11.87520147], Loss = 0.6970\n",
      "Iteration 4093: Weights = [55.30333333  2.83721352  6.79456291  0.11175266  0.2141243  11.87567443], Loss = 0.6969\n",
      "Iteration 4094: Weights = [55.30333333  2.8370302   6.7941239   0.11174544  0.21411047 11.87614736], Loss = 0.6968\n",
      "Iteration 4095: Weights = [55.30333333  2.8368469   6.79368492  0.11173822  0.21409663 11.87662025], Loss = 0.6967\n",
      "Iteration 4096: Weights = [55.30333333  2.8366636   6.79324596  0.111731    0.2140828  11.87709312], Loss = 0.6967\n",
      "Iteration 4097: Weights = [55.30333333  2.83648032  6.79280704  0.11172378  0.21406897 11.87756595], Loss = 0.6966\n",
      "Iteration 4098: Weights = [55.30333333  2.83629705  6.79236814  0.11171656  0.21405514 11.87803876], Loss = 0.6965\n",
      "Iteration 4099: Weights = [55.30333333  2.83611379  6.79192927  0.11170934  0.21404131 11.87851153], Loss = 0.6964\n",
      "Iteration 4100: Weights = [55.30333333  2.83593054  6.79149043  0.11170213  0.21402748 11.87898428], Loss = 0.6963\n",
      "Iteration 4101: Weights = [55.30333333  2.83574731  6.79105162  0.11169491  0.21401365 11.87945699], Loss = 0.6962\n",
      "Iteration 4102: Weights = [55.30333333  2.83556409  6.79061284  0.11168769  0.21399982 11.87992967], Loss = 0.6961\n",
      "Iteration 4103: Weights = [55.30333333  2.83538087  6.79017408  0.11168048  0.21398599 11.88040232], Loss = 0.6960\n",
      "Iteration 4104: Weights = [55.30333333  2.83519767  6.78973536  0.11167326  0.21397217 11.88087494], Loss = 0.6959\n",
      "Iteration 4105: Weights = [55.30333333  2.83501449  6.78929666  0.11166604  0.21395834 11.88134753], Loss = 0.6958\n",
      "Iteration 4106: Weights = [55.30333333  2.83483131  6.78885799  0.11165883  0.21394452 11.88182009], Loss = 0.6958\n",
      "Iteration 4107: Weights = [55.30333333  2.83464815  6.78841935  0.11165161  0.21393069 11.88229262], Loss = 0.6957\n",
      "Iteration 4108: Weights = [55.30333333  2.83446499  6.78798073  0.1116444   0.21391687 11.88276512], Loss = 0.6956\n",
      "Iteration 4109: Weights = [55.30333333  2.83428185  6.78754215  0.11163719  0.21390305 11.88323759], Loss = 0.6955\n",
      "Iteration 4110: Weights = [55.30333333  2.83409873  6.78710359  0.11162997  0.21388923 11.88371003], Loss = 0.6954\n",
      "Iteration 4111: Weights = [55.30333333  2.83391561  6.78666506  0.11162276  0.21387541 11.88418244], Loss = 0.6953\n",
      "Iteration 4112: Weights = [55.30333333  2.8337325   6.78622656  0.11161555  0.21386159 11.88465482], Loss = 0.6952\n",
      "Iteration 4113: Weights = [55.30333333  2.83354941  6.78578809  0.11160834  0.21384777 11.88512716], Loss = 0.6951\n",
      "Iteration 4114: Weights = [55.30333333  2.83336633  6.78534965  0.11160113  0.21383396 11.88559948], Loss = 0.6950\n",
      "Iteration 4115: Weights = [55.30333333  2.83318326  6.78491123  0.11159392  0.21382014 11.88607176], Loss = 0.6949\n",
      "Iteration 4116: Weights = [55.30333333  2.8330002   6.78447284  0.1115867   0.21380632 11.88654402], Loss = 0.6949\n",
      "Iteration 4117: Weights = [55.30333333  2.83281716  6.78403449  0.1115795   0.21379251 11.88701624], Loss = 0.6948\n",
      "Iteration 4118: Weights = [55.30333333  2.83263412  6.78359616  0.11157229  0.2137787  11.88748844], Loss = 0.6947\n",
      "Iteration 4119: Weights = [55.30333333  2.8324511   6.78315785  0.11156508  0.21376488 11.8879606 ], Loss = 0.6946\n",
      "Iteration 4120: Weights = [55.30333333  2.83226809  6.78271958  0.11155787  0.21375107 11.88843273], Loss = 0.6945\n",
      "Iteration 4121: Weights = [55.30333333  2.83208509  6.78228134  0.11155066  0.21373726 11.88890484], Loss = 0.6944\n",
      "Iteration 4122: Weights = [55.30333333  2.8319021   6.78184312  0.11154345  0.21372345 11.88937691], Loss = 0.6943\n",
      "Iteration 4123: Weights = [55.30333333  2.83171913  6.78140493  0.11153625  0.21370964 11.88984895], Loss = 0.6942\n",
      "Iteration 4124: Weights = [55.30333333  2.83153617  6.78096677  0.11152904  0.21369583 11.89032096], Loss = 0.6941\n",
      "Iteration 4125: Weights = [55.30333333  2.83135321  6.78052864  0.11152183  0.21368203 11.89079294], Loss = 0.6940\n",
      "Iteration 4126: Weights = [55.30333333  2.83117028  6.78009053  0.11151463  0.21366822 11.89126489], Loss = 0.6940\n",
      "Iteration 4127: Weights = [55.30333333  2.83098735  6.77965246  0.11150742  0.21365441 11.89173681], Loss = 0.6939\n",
      "Iteration 4128: Weights = [55.30333333  2.83080443  6.77921441  0.11150022  0.21364061 11.8922087 ], Loss = 0.6938\n",
      "Iteration 4129: Weights = [55.30333333  2.83062153  6.77877639  0.11149301  0.21362681 11.89268056], Loss = 0.6937\n",
      "Iteration 4130: Weights = [55.30333333  2.83043864  6.7783384   0.11148581  0.213613   11.89315238], Loss = 0.6936\n",
      "Iteration 4131: Weights = [55.30333333  2.83025576  6.77790044  0.11147861  0.2135992  11.89362418], Loss = 0.6935\n",
      "Iteration 4132: Weights = [55.30333333  2.83007289  6.77746251  0.1114714   0.2135854  11.89409595], Loss = 0.6934\n",
      "Iteration 4133: Weights = [55.30333333  2.82989003  6.7770246   0.1114642   0.2135716  11.89456769], Loss = 0.6933\n",
      "Iteration 4134: Weights = [55.30333333  2.82970719  6.77658672  0.111457    0.2135578  11.89503939], Loss = 0.6932\n",
      "Iteration 4135: Weights = [55.30333333  2.82952435  6.77614888  0.1114498   0.213544   11.89551107], Loss = 0.6932\n",
      "Iteration 4136: Weights = [55.30333333  2.82934153  6.77571106  0.1114426   0.2135302  11.89598271], Loss = 0.6931\n",
      "Iteration 4137: Weights = [55.30333333  2.82915872  6.77527326  0.1114354   0.21351641 11.89645433], Loss = 0.6930\n",
      "Iteration 4138: Weights = [55.30333333  2.82897592  6.7748355   0.1114282   0.21350261 11.89692591], Loss = 0.6929\n",
      "Iteration 4139: Weights = [55.30333333  2.82879314  6.77439776  0.111421    0.21348882 11.89739746], Loss = 0.6928\n",
      "Iteration 4140: Weights = [55.30333333  2.82861036  6.77396006  0.1114138   0.21347502 11.89786899], Loss = 0.6927\n",
      "Iteration 4141: Weights = [55.30333333  2.8284276   6.77352238  0.1114066   0.21346123 11.89834048], Loss = 0.6926\n",
      "Iteration 4142: Weights = [55.30333333  2.82824485  6.77308473  0.1113994   0.21344744 11.89881194], Loss = 0.6925\n",
      "Iteration 4143: Weights = [55.30333333  2.82806211  6.7726471   0.1113922   0.21343365 11.89928337], Loss = 0.6924\n",
      "Iteration 4144: Weights = [55.30333333  2.82787939  6.77220951  0.11138501  0.21341986 11.89975478], Loss = 0.6923\n",
      "Iteration 4145: Weights = [55.30333333  2.82769667  6.77177194  0.11137781  0.21340607 11.90022615], Loss = 0.6923\n",
      "Iteration 4146: Weights = [55.30333333  2.82751397  6.7713344   0.11137061  0.21339228 11.90069749], Loss = 0.6922\n",
      "Iteration 4147: Weights = [55.30333333  2.82733128  6.7708969   0.11136342  0.21337849 11.9011688 ], Loss = 0.6921\n",
      "Iteration 4148: Weights = [55.30333333  2.8271486   6.77045941  0.11135622  0.2133647  11.90164008], Loss = 0.6920\n",
      "Iteration 4149: Weights = [55.30333333  2.82696593  6.77002196  0.11134903  0.21335092 11.90211133], Loss = 0.6919\n",
      "Iteration 4150: Weights = [55.30333333  2.82678327  6.76958454  0.11134183  0.21333713 11.90258254], Loss = 0.6918\n",
      "Iteration 4151: Weights = [55.30333333  2.82660063  6.76914714  0.11133464  0.21332335 11.90305373], Loss = 0.6917\n",
      "Iteration 4152: Weights = [55.30333333  2.826418    6.76870977  0.11132744  0.21330957 11.90352489], Loss = 0.6916\n",
      "Iteration 4153: Weights = [55.30333333  2.82623538  6.76827243  0.11132025  0.21329578 11.90399602], Loss = 0.6915\n",
      "Iteration 4154: Weights = [55.30333333  2.82605277  6.76783512  0.11131306  0.213282   11.90446711], Loss = 0.6915\n",
      "Iteration 4155: Weights = [55.30333333  2.82587017  6.76739784  0.11130587  0.21326822 11.90493818], Loss = 0.6914\n",
      "Iteration 4156: Weights = [55.30333333  2.82568759  6.76696058  0.11129867  0.21325444 11.90540922], Loss = 0.6913\n",
      "Iteration 4157: Weights = [55.30333333  2.82550501  6.76652335  0.11129148  0.21324066 11.90588022], Loss = 0.6912\n",
      "Iteration 4158: Weights = [55.30333333  2.82532245  6.76608616  0.11128429  0.21322689 11.9063512 ], Loss = 0.6911\n",
      "Iteration 4159: Weights = [55.30333333  2.8251399   6.76564899  0.1112771   0.21321311 11.90682214], Loss = 0.6910\n",
      "Iteration 4160: Weights = [55.30333333  2.82495736  6.76521184  0.11126991  0.21319933 11.90729306], Loss = 0.6909\n",
      "Iteration 4161: Weights = [55.30333333  2.82477484  6.76477473  0.11126272  0.21318556 11.90776394], Loss = 0.6908\n",
      "Iteration 4162: Weights = [55.30333333  2.82459232  6.76433764  0.11125553  0.21317178 11.90823479], Loss = 0.6907\n",
      "Iteration 4163: Weights = [55.30333333  2.82440982  6.76390059  0.11124835  0.21315801 11.90870562], Loss = 0.6906\n",
      "Iteration 4164: Weights = [55.30333333  2.82422733  6.76346356  0.11124116  0.21314424 11.90917641], Loss = 0.6906\n",
      "Iteration 4165: Weights = [55.30333333  2.82404485  6.76302656  0.11123397  0.21313046 11.90964717], Loss = 0.6905\n",
      "Iteration 4166: Weights = [55.30333333  2.82386238  6.76258958  0.11122678  0.21311669 11.9101179 ], Loss = 0.6904\n",
      "Iteration 4167: Weights = [55.30333333  2.82367993  6.76215264  0.1112196   0.21310292 11.9105886 ], Loss = 0.6903\n",
      "Iteration 4168: Weights = [55.30333333  2.82349748  6.76171572  0.11121241  0.21308916 11.91105927], Loss = 0.6902\n",
      "Iteration 4169: Weights = [55.30333333  2.82331505  6.76127883  0.11120522  0.21307539 11.91152991], Loss = 0.6901\n",
      "Iteration 4170: Weights = [55.30333333  2.82313263  6.76084197  0.11119804  0.21306162 11.91200052], Loss = 0.6900\n",
      "Iteration 4171: Weights = [55.30333333  2.82295022  6.76040514  0.11119085  0.21304785 11.9124711 ], Loss = 0.6899\n",
      "Iteration 4172: Weights = [55.30333333  2.82276783  6.75996834  0.11118367  0.21303409 11.91294165], Loss = 0.6898\n",
      "Iteration 4173: Weights = [55.30333333  2.82258544  6.75953156  0.11117649  0.21302032 11.91341217], Loss = 0.6898\n",
      "Iteration 4174: Weights = [55.30333333  2.82240307  6.75909482  0.1111693   0.21300656 11.91388266], Loss = 0.6897\n",
      "Iteration 4175: Weights = [55.30333333  2.82222071  6.7586581   0.11116212  0.2129928  11.91435312], Loss = 0.6896\n",
      "Iteration 4176: Weights = [55.30333333  2.82203836  6.75822141  0.11115494  0.21297903 11.91482355], Loss = 0.6895\n",
      "Iteration 4177: Weights = [55.30333333  2.82185602  6.75778475  0.11114776  0.21296527 11.91529394], Loss = 0.6894\n",
      "Iteration 4178: Weights = [55.30333333  2.8216737   6.75734811  0.11114057  0.21295151 11.91576431], Loss = 0.6893\n",
      "Iteration 4179: Weights = [55.30333333  2.82149138  6.75691151  0.11113339  0.21293775 11.91623465], Loss = 0.6892\n",
      "Iteration 4180: Weights = [55.30333333  2.82130908  6.75647493  0.11112621  0.212924   11.91670495], Loss = 0.6891\n",
      "Iteration 4181: Weights = [55.30333333  2.82112679  6.75603838  0.11111903  0.21291024 11.91717523], Loss = 0.6890\n",
      "Iteration 4182: Weights = [55.30333333  2.82094451  6.75560186  0.11111185  0.21289648 11.91764547], Loss = 0.6890\n",
      "Iteration 4183: Weights = [55.30333333  2.82076224  6.75516537  0.11110467  0.21288273 11.91811569], Loss = 0.6889\n",
      "Iteration 4184: Weights = [55.30333333  2.82057999  6.7547289   0.1110975   0.21286897 11.91858587], Loss = 0.6888\n",
      "Iteration 4185: Weights = [55.30333333  2.82039775  6.75429246  0.11109032  0.21285522 11.91905603], Loss = 0.6887\n",
      "Iteration 4186: Weights = [55.30333333  2.82021551  6.75385606  0.11108314  0.21284146 11.91952615], Loss = 0.6886\n",
      "Iteration 4187: Weights = [55.30333333  2.82003329  6.75341968  0.11107596  0.21282771 11.91999624], Loss = 0.6885\n",
      "Iteration 4188: Weights = [55.30333333  2.81985109  6.75298332  0.11106879  0.21281396 11.92046631], Loss = 0.6884\n",
      "Iteration 4189: Weights = [55.30333333  2.81966889  6.752547    0.11106161  0.21280021 11.92093634], Loss = 0.6883\n",
      "Iteration 4190: Weights = [55.30333333  2.81948671  6.7521107   0.11105443  0.21278646 11.92140634], Loss = 0.6882\n",
      "Iteration 4191: Weights = [55.30333333  2.81930453  6.75167444  0.11104726  0.21277271 11.92187631], Loss = 0.6882\n",
      "Iteration 4192: Weights = [55.30333333  2.81912237  6.7512382   0.11104008  0.21275897 11.92234626], Loss = 0.6881\n",
      "Iteration 4193: Weights = [55.30333333  2.81894022  6.75080199  0.11103291  0.21274522 11.92281617], Loss = 0.6880\n",
      "Iteration 4194: Weights = [55.30333333  2.81875808  6.7503658   0.11102573  0.21273147 11.92328605], Loss = 0.6879\n",
      "Iteration 4195: Weights = [55.30333333  2.81857596  6.74992965  0.11101856  0.21271773 11.9237559 ], Loss = 0.6878\n",
      "Iteration 4196: Weights = [55.30333333  2.81839385  6.74949352  0.11101139  0.21270398 11.92422572], Loss = 0.6877\n",
      "Iteration 4197: Weights = [55.30333333  2.81821174  6.74905742  0.11100421  0.21269024 11.92469551], Loss = 0.6876\n",
      "Iteration 4198: Weights = [55.30333333  2.81802965  6.74862135  0.11099704  0.2126765  11.92516527], Loss = 0.6875\n",
      "Iteration 4199: Weights = [55.30333333  2.81784757  6.74818531  0.11098987  0.21266276 11.925635  ], Loss = 0.6874\n",
      "Iteration 4200: Weights = [55.30333333  2.81766551  6.7477493   0.1109827   0.21264902 11.9261047 ], Loss = 0.6874\n",
      "Iteration 4201: Weights = [55.30333333  2.81748345  6.74731331  0.11097553  0.21263528 11.92657436], Loss = 0.6873\n",
      "Iteration 4202: Weights = [55.30333333  2.81730141  6.74687735  0.11096836  0.21262154 11.927044  ], Loss = 0.6872\n",
      "Iteration 4203: Weights = [55.30333333  2.81711938  6.74644142  0.11096119  0.2126078  11.92751361], Loss = 0.6871\n",
      "Iteration 4204: Weights = [55.30333333  2.81693736  6.74600552  0.11095402  0.21259406 11.92798319], Loss = 0.6870\n",
      "Iteration 4205: Weights = [55.30333333  2.81675535  6.74556965  0.11094685  0.21258033 11.92845273], Loss = 0.6869\n",
      "Iteration 4206: Weights = [55.30333333  2.81657335  6.74513381  0.11093968  0.21256659 11.92892225], Loss = 0.6868\n",
      "Iteration 4207: Weights = [55.30333333  2.81639137  6.74469799  0.11093251  0.21255286 11.92939174], Loss = 0.6867\n",
      "Iteration 4208: Weights = [55.30333333  2.8162094   6.7442622   0.11092535  0.21253912 11.92986119], Loss = 0.6866\n",
      "Iteration 4209: Weights = [55.30333333  2.81602744  6.74382644  0.11091818  0.21252539 11.93033062], Loss = 0.6866\n",
      "Iteration 4210: Weights = [55.30333333  2.81584549  6.74339071  0.11091101  0.21251166 11.93080001], Loss = 0.6865\n",
      "Iteration 4211: Weights = [55.30333333  2.81566355  6.742955    0.11090385  0.21249793 11.93126938], Loss = 0.6864\n",
      "Iteration 4212: Weights = [55.30333333  2.81548162  6.74251933  0.11089668  0.2124842  11.93173871], Loss = 0.6863\n",
      "Iteration 4213: Weights = [55.30333333  2.81529971  6.74208368  0.11088951  0.21247047 11.93220802], Loss = 0.6862\n",
      "Iteration 4214: Weights = [55.30333333  2.81511781  6.74164806  0.11088235  0.21245674 11.93267729], Loss = 0.6861\n",
      "Iteration 4215: Weights = [55.30333333  2.81493592  6.74121247  0.11087519  0.21244301 11.93314654], Loss = 0.6860\n",
      "Iteration 4216: Weights = [55.30333333  2.81475404  6.74077691  0.11086802  0.21242929 11.93361575], Loss = 0.6859\n",
      "Iteration 4217: Weights = [55.30333333  2.81457217  6.74034137  0.11086086  0.21241556 11.93408493], Loss = 0.6858\n",
      "Iteration 4218: Weights = [55.30333333  2.81439032  6.73990586  0.1108537   0.21240184 11.93455408], Loss = 0.6858\n",
      "Iteration 4219: Weights = [55.30333333  2.81420847  6.73947039  0.11084653  0.21238811 11.93502321], Loss = 0.6857\n",
      "Iteration 4220: Weights = [55.30333333  2.81402664  6.73903493  0.11083937  0.21237439 11.9354923 ], Loss = 0.6856\n",
      "Iteration 4221: Weights = [55.30333333  2.81384482  6.73859951  0.11083221  0.21236067 11.93596136], Loss = 0.6855\n",
      "Iteration 4222: Weights = [55.30333333  2.81366301  6.73816412  0.11082505  0.21234695 11.93643039], Loss = 0.6854\n",
      "Iteration 4223: Weights = [55.30333333  2.81348122  6.73772875  0.11081789  0.21233323 11.93689939], Loss = 0.6853\n",
      "Iteration 4224: Weights = [55.30333333  2.81329943  6.73729341  0.11081073  0.21231951 11.93736836], Loss = 0.6852\n",
      "Iteration 4225: Weights = [55.30333333  2.81311766  6.7368581   0.11080357  0.21230579 11.9378373 ], Loss = 0.6851\n",
      "Iteration 4226: Weights = [55.30333333  2.8129359   6.73642282  0.11079641  0.21229207 11.93830622], Loss = 0.6850\n",
      "Iteration 4227: Weights = [55.30333333  2.81275415  6.73598757  0.11078925  0.21227836 11.9387751 ], Loss = 0.6850\n",
      "Iteration 4228: Weights = [55.30333333  2.81257241  6.73555234  0.11078209  0.21226464 11.93924394], Loss = 0.6849\n",
      "Iteration 4229: Weights = [55.30333333  2.81239068  6.73511714  0.11077493  0.21225093 11.93971276], Loss = 0.6848\n",
      "Iteration 4230: Weights = [55.30333333  2.81220897  6.73468197  0.11076778  0.21223721 11.94018155], Loss = 0.6847\n",
      "Iteration 4231: Weights = [55.30333333  2.81202727  6.73424683  0.11076062  0.2122235  11.94065031], Loss = 0.6846\n",
      "Iteration 4232: Weights = [55.30333333  2.81184558  6.73381172  0.11075346  0.21220979 11.94111904], Loss = 0.6845\n",
      "Iteration 4233: Weights = [55.30333333  2.8116639   6.73337664  0.11074631  0.21219608 11.94158774], Loss = 0.6844\n",
      "Iteration 4234: Weights = [55.30333333  2.81148223  6.73294158  0.11073915  0.21218236 11.94205641], Loss = 0.6843\n",
      "Iteration 4235: Weights = [55.30333333  2.81130058  6.73250655  0.110732    0.21216866 11.94252504], Loss = 0.6843\n",
      "Iteration 4236: Weights = [55.30333333  2.81111893  6.73207155  0.11072484  0.21215495 11.94299365], Loss = 0.6842\n",
      "Iteration 4237: Weights = [55.30333333  2.8109373   6.73163658  0.11071769  0.21214124 11.94346223], Loss = 0.6841\n",
      "Iteration 4238: Weights = [55.30333333  2.81075568  6.73120163  0.11071053  0.21212753 11.94393078], Loss = 0.6840\n",
      "Iteration 4239: Weights = [55.30333333  2.81057407  6.73076671  0.11070338  0.21211383 11.94439929], Loss = 0.6839\n",
      "Iteration 4240: Weights = [55.30333333  2.81039247  6.73033183  0.11069623  0.21210012 11.94486778], Loss = 0.6838\n",
      "Iteration 4241: Weights = [55.30333333  2.81021089  6.72989697  0.11068908  0.21208642 11.94533623], Loss = 0.6837\n",
      "Iteration 4242: Weights = [55.30333333  2.81002932  6.72946213  0.11068192  0.21207271 11.94580466], Loss = 0.6836\n",
      "Iteration 4243: Weights = [55.30333333  2.80984775  6.72902733  0.11067477  0.21205901 11.94627306], Loss = 0.6835\n",
      "Iteration 4244: Weights = [55.30333333  2.8096662   6.72859255  0.11066762  0.21204531 11.94674142], Loss = 0.6835\n",
      "Iteration 4245: Weights = [55.30333333  2.80948467  6.72815781  0.11066047  0.21203161 11.94720976], Loss = 0.6834\n",
      "Iteration 4246: Weights = [55.30333333  2.80930314  6.72772309  0.11065332  0.21201791 11.94767806], Loss = 0.6833\n",
      "Iteration 4247: Weights = [55.30333333  2.80912162  6.72728839  0.11064617  0.21200421 11.94814634], Loss = 0.6832\n",
      "Iteration 4248: Weights = [55.30333333  2.80894012  6.72685373  0.11063902  0.21199051 11.94861458], Loss = 0.6831\n",
      "Iteration 4249: Weights = [55.30333333  2.80875863  6.7264191   0.11063187  0.21197681 11.94908279], Loss = 0.6830\n",
      "Iteration 4250: Weights = [55.30333333  2.80857715  6.72598449  0.11062473  0.21196312 11.94955098], Loss = 0.6829\n",
      "Iteration 4251: Weights = [55.30333333  2.80839568  6.72554991  0.11061758  0.21194942 11.95001913], Loss = 0.6828\n",
      "Iteration 4252: Weights = [55.30333333  2.80821423  6.72511536  0.11061043  0.21193573 11.95048725], Loss = 0.6828\n",
      "Iteration 4253: Weights = [55.30333333  2.80803278  6.72468083  0.11060328  0.21192204 11.95095535], Loss = 0.6827\n",
      "Iteration 4254: Weights = [55.30333333  2.80785135  6.72424634  0.11059614  0.21190834 11.95142341], Loss = 0.6826\n",
      "Iteration 4255: Weights = [55.30333333  2.80766993  6.72381187  0.11058899  0.21189465 11.95189144], Loss = 0.6825\n",
      "Iteration 4256: Weights = [55.30333333  2.80748852  6.72337743  0.11058185  0.21188096 11.95235944], Loss = 0.6824\n",
      "Iteration 4257: Weights = [55.30333333  2.80730712  6.72294302  0.1105747   0.21186727 11.95282742], Loss = 0.6823\n",
      "Iteration 4258: Weights = [55.30333333  2.80712574  6.72250864  0.11056756  0.21185358 11.95329536], Loss = 0.6822\n",
      "Iteration 4259: Weights = [55.30333333  2.80694436  6.72207428  0.11056041  0.21183989 11.95376327], Loss = 0.6821\n",
      "Iteration 4260: Weights = [55.30333333  2.806763    6.72163996  0.11055327  0.2118262  11.95423115], Loss = 0.6820\n",
      "Iteration 4261: Weights = [55.30333333  2.80658165  6.72120566  0.11054613  0.21181252 11.954699  ], Loss = 0.6820\n",
      "Iteration 4262: Weights = [55.30333333  2.80640031  6.72077139  0.11053898  0.21179883 11.95516682], Loss = 0.6819\n",
      "Iteration 4263: Weights = [55.30333333  2.80621898  6.72033715  0.11053184  0.21178515 11.95563461], Loss = 0.6818\n",
      "Iteration 4264: Weights = [55.30333333  2.80603767  6.71990293  0.1105247   0.21177146 11.95610237], Loss = 0.6817\n",
      "Iteration 4265: Weights = [55.30333333  2.80585636  6.71946874  0.11051756  0.21175778 11.9565701 ], Loss = 0.6816\n",
      "Iteration 4266: Weights = [55.30333333  2.80567507  6.71903459  0.11051042  0.2117441  11.9570378 ], Loss = 0.6815\n",
      "Iteration 4267: Weights = [55.30333333  2.80549379  6.71860046  0.11050328  0.21173042 11.95750547], Loss = 0.6814\n",
      "Iteration 4268: Weights = [55.30333333  2.80531252  6.71816635  0.11049614  0.21171674 11.95797311], Loss = 0.6813\n",
      "Iteration 4269: Weights = [55.30333333  2.80513127  6.71773228  0.110489    0.21170306 11.95844072], Loss = 0.6813\n",
      "Iteration 4270: Weights = [55.30333333  2.80495002  6.71729823  0.11048186  0.21168938 11.9589083 ], Loss = 0.6812\n",
      "Iteration 4271: Weights = [55.30333333  2.80476879  6.71686422  0.11047472  0.2116757  11.95937585], Loss = 0.6811\n",
      "Iteration 4272: Weights = [55.30333333  2.80458757  6.71643023  0.11046758  0.21166202 11.95984337], Loss = 0.6810\n",
      "Iteration 4273: Weights = [55.30333333  2.80440636  6.71599626  0.11046045  0.21164835 11.96031086], Loss = 0.6809\n",
      "Iteration 4274: Weights = [55.30333333  2.80422516  6.71556233  0.11045331  0.21163467 11.96077832], Loss = 0.6808\n",
      "Iteration 4275: Weights = [55.30333333  2.80404397  6.71512842  0.11044617  0.211621   11.96124574], Loss = 0.6807\n",
      "Iteration 4276: Weights = [55.30333333  2.8038628   6.71469455  0.11043904  0.21160733 11.96171314], Loss = 0.6806\n",
      "Iteration 4277: Weights = [55.30333333  2.80368163  6.7142607   0.1104319   0.21159365 11.96218051], Loss = 0.6805\n",
      "Iteration 4278: Weights = [55.30333333  2.80350048  6.71382687  0.11042476  0.21157998 11.96264785], Loss = 0.6805\n",
      "Iteration 4279: Weights = [55.30333333  2.80331934  6.71339308  0.11041763  0.21156631 11.96311515], Loss = 0.6804\n",
      "Iteration 4280: Weights = [55.30333333  2.80313821  6.71295931  0.1104105   0.21155264 11.96358243], Loss = 0.6803\n",
      "Iteration 4281: Weights = [55.30333333  2.8029571   6.71252558  0.11040336  0.21153897 11.96404968], Loss = 0.6802\n",
      "Iteration 4282: Weights = [55.30333333  2.80277599  6.71209187  0.11039623  0.21152531 11.96451689], Loss = 0.6801\n",
      "Iteration 4283: Weights = [55.30333333  2.8025949   6.71165818  0.1103891   0.21151164 11.96498408], Loss = 0.6800\n",
      "Iteration 4284: Weights = [55.30333333  2.80241382  6.71122453  0.11038196  0.21149797 11.96545124], Loss = 0.6799\n",
      "Iteration 4285: Weights = [55.30333333  2.80223275  6.7107909   0.11037483  0.21148431 11.96591836], Loss = 0.6798\n",
      "Iteration 4286: Weights = [55.30333333  2.80205169  6.71035731  0.1103677   0.21147064 11.96638546], Loss = 0.6798\n",
      "Iteration 4287: Weights = [55.30333333  2.80187064  6.70992374  0.11036057  0.21145698 11.96685252], Loss = 0.6797\n",
      "Iteration 4288: Weights = [55.30333333  2.80168961  6.7094902   0.11035344  0.21144332 11.96731956], Loss = 0.6796\n",
      "Iteration 4289: Weights = [55.30333333  2.80150859  6.70905668  0.11034631  0.21142965 11.96778656], Loss = 0.6795\n",
      "Iteration 4290: Weights = [55.30333333  2.80132758  6.7086232   0.11033918  0.21141599 11.96825354], Loss = 0.6794\n",
      "Iteration 4291: Weights = [55.30333333  2.80114658  6.70818974  0.11033205  0.21140233 11.96872049], Loss = 0.6793\n",
      "Iteration 4292: Weights = [55.30333333  2.80096559  6.70775631  0.11032492  0.21138867 11.9691874 ], Loss = 0.6792\n",
      "Iteration 4293: Weights = [55.30333333  2.80078461  6.70732291  0.11031779  0.21137502 11.96965428], Loss = 0.6791\n",
      "Iteration 4294: Weights = [55.30333333  2.80060365  6.70688953  0.11031066  0.21136136 11.97012114], Loss = 0.6791\n",
      "Iteration 4295: Weights = [55.30333333  2.8004227   6.70645619  0.11030354  0.2113477  11.97058796], Loss = 0.6790\n",
      "Iteration 4296: Weights = [55.30333333  2.80024176  6.70602287  0.11029641  0.21133405 11.97105476], Loss = 0.6789\n",
      "Iteration 4297: Weights = [55.30333333  2.80006083  6.70558958  0.11028928  0.21132039 11.97152152], Loss = 0.6788\n",
      "Iteration 4298: Weights = [55.30333333  2.79987991  6.70515632  0.11028216  0.21130674 11.97198826], Loss = 0.6787\n",
      "Iteration 4299: Weights = [55.30333333  2.799699    6.70472309  0.11027503  0.21129309 11.97245496], Loss = 0.6786\n",
      "Iteration 4300: Weights = [55.30333333  2.79951811  6.70428988  0.11026791  0.21127943 11.97292163], Loss = 0.6785\n",
      "Iteration 4301: Weights = [55.30333333  2.79933723  6.7038567   0.11026078  0.21126578 11.97338828], Loss = 0.6784\n",
      "Iteration 4302: Weights = [55.30333333  2.79915636  6.70342355  0.11025366  0.21125213 11.97385489], Loss = 0.6784\n",
      "Iteration 4303: Weights = [55.30333333  2.7989755   6.70299043  0.11024653  0.21123848 11.97432147], Loss = 0.6783\n",
      "Iteration 4304: Weights = [55.30333333  2.79879465  6.70255734  0.11023941  0.21122483 11.97478803], Loss = 0.6782\n",
      "Iteration 4305: Weights = [55.30333333  2.79861381  6.70212427  0.11023229  0.21121119 11.97525455], Loss = 0.6781\n",
      "Iteration 4306: Weights = [55.30333333  2.79843299  6.70169123  0.11022517  0.21119754 11.97572104], Loss = 0.6780\n",
      "Iteration 4307: Weights = [55.30333333  2.79825218  6.70125822  0.11021804  0.21118389 11.9761875 ], Loss = 0.6779\n",
      "Iteration 4308: Weights = [55.30333333  2.79807138  6.70082524  0.11021092  0.21117025 11.97665394], Loss = 0.6778\n",
      "Iteration 4309: Weights = [55.30333333  2.79789059  6.70039229  0.1102038   0.2111566  11.97712034], Loss = 0.6777\n",
      "Iteration 4310: Weights = [55.30333333  2.79770981  6.69995936  0.11019668  0.21114296 11.97758671], Loss = 0.6777\n",
      "Iteration 4311: Weights = [55.30333333  2.79752904  6.69952647  0.11018956  0.21112932 11.97805305], Loss = 0.6776\n",
      "Iteration 4312: Weights = [55.30333333  2.79734829  6.6990936   0.11018244  0.21111568 11.97851936], Loss = 0.6775\n",
      "Iteration 4313: Weights = [55.30333333  2.79716755  6.69866075  0.11017532  0.21110204 11.97898565], Loss = 0.6774\n",
      "Iteration 4314: Weights = [55.30333333  2.79698682  6.69822794  0.1101682   0.2110884  11.9794519 ], Loss = 0.6773\n",
      "Iteration 4315: Weights = [55.30333333  2.7968061   6.69779515  0.11016109  0.21107476 11.97991812], Loss = 0.6772\n",
      "Iteration 4316: Weights = [55.30333333  2.79662539  6.6973624   0.11015397  0.21106112 11.98038431], Loss = 0.6771\n",
      "Iteration 4317: Weights = [55.30333333  2.79644469  6.69692967  0.11014685  0.21104748 11.98085047], Loss = 0.6770\n",
      "Iteration 4318: Weights = [55.30333333  2.79626401  6.69649696  0.11013973  0.21103385 11.9813166 ], Loss = 0.6770\n",
      "Iteration 4319: Weights = [55.30333333  2.79608334  6.69606429  0.11013262  0.21102021 11.9817827 ], Loss = 0.6769\n",
      "Iteration 4320: Weights = [55.30333333  2.79590268  6.69563164  0.1101255   0.21100658 11.98224877], Loss = 0.6768\n",
      "Iteration 4321: Weights = [55.30333333  2.79572203  6.69519903  0.11011839  0.21099294 11.98271482], Loss = 0.6767\n",
      "Iteration 4322: Weights = [55.30333333  2.79554139  6.69476643  0.11011127  0.21097931 11.98318083], Loss = 0.6766\n",
      "Iteration 4323: Weights = [55.30333333  2.79536077  6.69433387  0.11010416  0.21096568 11.98364681], Loss = 0.6765\n",
      "Iteration 4324: Weights = [55.30333333  2.79518015  6.69390134  0.11009704  0.21095205 11.98411276], Loss = 0.6764\n",
      "Iteration 4325: Weights = [55.30333333  2.79499955  6.69346883  0.11008993  0.21093842 11.98457868], Loss = 0.6763\n",
      "Iteration 4326: Weights = [55.30333333  2.79481896  6.69303635  0.11008282  0.21092479 11.98504457], Loss = 0.6763\n",
      "Iteration 4327: Weights = [55.30333333  2.79463838  6.6926039   0.1100757   0.21091116 11.98551043], Loss = 0.6762\n",
      "Iteration 4328: Weights = [55.30333333  2.79445781  6.69217148  0.11006859  0.21089753 11.98597626], Loss = 0.6761\n",
      "Iteration 4329: Weights = [55.30333333  2.79427726  6.69173909  0.11006148  0.21088391 11.98644206], Loss = 0.6760\n",
      "Iteration 4330: Weights = [55.30333333  2.79409671  6.69130672  0.11005437  0.21087028 11.98690783], Loss = 0.6759\n",
      "Iteration 4331: Weights = [55.30333333  2.79391618  6.69087438  0.11004726  0.21085666 11.98737357], Loss = 0.6758\n",
      "Iteration 4332: Weights = [55.30333333  2.79373566  6.69044207  0.11004015  0.21084303 11.98783928], Loss = 0.6757\n",
      "Iteration 4333: Weights = [55.30333333  2.79355515  6.69000979  0.11003304  0.21082941 11.98830496], Loss = 0.6756\n",
      "Iteration 4334: Weights = [55.30333333  2.79337466  6.68957753  0.11002593  0.21081579 11.98877061], Loss = 0.6756\n",
      "Iteration 4335: Weights = [55.30333333  2.79319417  6.6891453   0.11001882  0.21080217 11.98923623], Loss = 0.6755\n",
      "Iteration 4336: Weights = [55.30333333  2.7930137   6.68871311  0.11001171  0.21078855 11.98970181], Loss = 0.6754\n",
      "Iteration 4337: Weights = [55.30333333  2.79283323  6.68828093  0.1100046   0.21077493 11.99016737], Loss = 0.6753\n",
      "Iteration 4338: Weights = [55.30333333  2.79265278  6.68784879  0.10999749  0.21076131 11.9906329 ], Loss = 0.6752\n",
      "Iteration 4339: Weights = [55.30333333  2.79247234  6.68741668  0.10999039  0.21074769 11.9910984 ], Loss = 0.6751\n",
      "Iteration 4340: Weights = [55.30333333  2.79229192  6.68698459  0.10998328  0.21073407 11.99156387], Loss = 0.6750\n",
      "Iteration 4341: Weights = [55.30333333  2.7921115   6.68655253  0.10997617  0.21072046 11.99202931], Loss = 0.6749\n",
      "Iteration 4342: Weights = [55.30333333  2.7919311   6.6861205   0.10996907  0.21070684 11.99249472], Loss = 0.6749\n",
      "Iteration 4343: Weights = [55.30333333  2.79175071  6.68568849  0.10996196  0.21069323 11.9929601 ], Loss = 0.6748\n",
      "Iteration 4344: Weights = [55.30333333  2.79157032  6.68525652  0.10995486  0.21067961 11.99342545], Loss = 0.6747\n",
      "Iteration 4345: Weights = [55.30333333  2.79138996  6.68482457  0.10994775  0.210666   11.99389076], Loss = 0.6746\n",
      "Iteration 4346: Weights = [55.30333333  2.7912096   6.68439265  0.10994065  0.21065239 11.99435605], Loss = 0.6745\n",
      "Iteration 4347: Weights = [55.30333333  2.79102925  6.68396076  0.10993355  0.21063878 11.99482131], Loss = 0.6744\n",
      "Iteration 4348: Weights = [55.30333333  2.79084892  6.68352889  0.10992644  0.21062517 11.99528654], Loss = 0.6743\n",
      "Iteration 4349: Weights = [55.30333333  2.7906686   6.68309706  0.10991934  0.21061156 11.99575174], Loss = 0.6742\n",
      "Iteration 4350: Weights = [55.30333333  2.79048829  6.68266525  0.10991224  0.21059795 11.99621691], Loss = 0.6742\n",
      "Iteration 4351: Weights = [55.30333333  2.79030799  6.68223347  0.10990514  0.21058435 11.99668204], Loss = 0.6741\n",
      "Iteration 4352: Weights = [55.30333333  2.7901277   6.68180172  0.10989804  0.21057074 11.99714715], Loss = 0.6740\n",
      "Iteration 4353: Weights = [55.30333333  2.78994742  6.68136999  0.10989093  0.21055713 11.99761223], Loss = 0.6739\n",
      "Iteration 4354: Weights = [55.30333333  2.78976716  6.68093829  0.10988383  0.21054353 11.99807728], Loss = 0.6738\n",
      "Iteration 4355: Weights = [55.30333333  2.78958691  6.68050663  0.10987673  0.21052993 11.9985423 ], Loss = 0.6737\n",
      "Iteration 4356: Weights = [55.30333333  2.78940667  6.68007498  0.10986964  0.21051632 11.99900729], Loss = 0.6736\n",
      "Iteration 4357: Weights = [55.30333333  2.78922644  6.67964337  0.10986254  0.21050272 11.99947224], Loss = 0.6735\n",
      "Iteration 4358: Weights = [55.30333333  2.78904622  6.67921179  0.10985544  0.21048912 11.99993717], Loss = 0.6735\n",
      "Iteration 4359: Weights = [55.30333333  2.78886601  6.67878023  0.10984834  0.21047552 12.00040207], Loss = 0.6734\n",
      "Iteration 4360: Weights = [55.30333333  2.78868582  6.6783487   0.10984124  0.21046192 12.00086694], Loss = 0.6733\n",
      "Iteration 4361: Weights = [55.30333333  2.78850564  6.6779172   0.10983415  0.21044832 12.00133177], Loss = 0.6732\n",
      "Iteration 4362: Weights = [55.30333333  2.78832547  6.67748572  0.10982705  0.21043473 12.00179658], Loss = 0.6731\n",
      "Iteration 4363: Weights = [55.30333333  2.78814531  6.67705428  0.10981995  0.21042113 12.00226136], Loss = 0.6730\n",
      "Iteration 4364: Weights = [55.30333333  2.78796516  6.67662286  0.10981286  0.21040753 12.00272611], Loss = 0.6729\n",
      "Iteration 4365: Weights = [55.30333333  2.78778502  6.67619147  0.10980576  0.21039394 12.00319083], Loss = 0.6729\n",
      "Iteration 4366: Weights = [55.30333333  2.7876049   6.67576011  0.10979867  0.21038034 12.00365551], Loss = 0.6728\n",
      "Iteration 4367: Weights = [55.30333333  2.78742479  6.67532877  0.10979157  0.21036675 12.00412017], Loss = 0.6727\n",
      "Iteration 4368: Weights = [55.30333333  2.78724468  6.67489747  0.10978448  0.21035316 12.0045848 ], Loss = 0.6726\n",
      "Iteration 4369: Weights = [55.30333333  2.7870646   6.67446619  0.10977739  0.21033957 12.0050494 ], Loss = 0.6725\n",
      "Iteration 4370: Weights = [55.30333333  2.78688452  6.67403494  0.10977029  0.21032598 12.00551396], Loss = 0.6724\n",
      "Iteration 4371: Weights = [55.30333333  2.78670445  6.67360372  0.1097632   0.21031239 12.0059785 ], Loss = 0.6723\n",
      "Iteration 4372: Weights = [55.30333333  2.7865244   6.67317252  0.10975611  0.2102988  12.00644301], Loss = 0.6722\n",
      "Iteration 4373: Weights = [55.30333333  2.78634435  6.67274135  0.10974902  0.21028521 12.00690749], Loss = 0.6722\n",
      "Iteration 4374: Weights = [55.30333333  2.78616432  6.67231021  0.10974193  0.21027162 12.00737193], Loss = 0.6721\n",
      "Iteration 4375: Weights = [55.30333333  2.7859843   6.6718791   0.10973483  0.21025804 12.00783635], Loss = 0.6720\n",
      "Iteration 4376: Weights = [55.30333333  2.78580429  6.67144802  0.10972774  0.21024445 12.00830074], Loss = 0.6719\n",
      "Iteration 4377: Weights = [55.30333333  2.7856243   6.67101696  0.10972065  0.21023087 12.0087651 ], Loss = 0.6718\n",
      "Iteration 4378: Weights = [55.30333333  2.78544431  6.67058594  0.10971357  0.21021728 12.00922942], Loss = 0.6717\n",
      "Iteration 4379: Weights = [55.30333333  2.78526434  6.67015494  0.10970648  0.2102037  12.00969372], Loss = 0.6716\n",
      "Iteration 4380: Weights = [55.30333333  2.78508438  6.66972396  0.10969939  0.21019012 12.01015799], Loss = 0.6716\n",
      "Iteration 4381: Weights = [55.30333333  2.78490443  6.66929302  0.1096923   0.21017654 12.01062223], Loss = 0.6715\n",
      "Iteration 4382: Weights = [55.30333333  2.78472449  6.6688621   0.10968521  0.21016296 12.01108643], Loss = 0.6714\n",
      "Iteration 4383: Weights = [55.30333333  2.78454456  6.66843122  0.10967813  0.21014938 12.01155061], Loss = 0.6713\n",
      "Iteration 4384: Weights = [55.30333333  2.78436465  6.66800035  0.10967104  0.2101358  12.01201476], Loss = 0.6712\n",
      "Iteration 4385: Weights = [55.30333333  2.78418475  6.66756952  0.10966395  0.21012223 12.01247888], Loss = 0.6711\n",
      "Iteration 4386: Weights = [55.30333333  2.78400485  6.66713872  0.10965687  0.21010865 12.01294296], Loss = 0.6710\n",
      "Iteration 4387: Weights = [55.30333333  2.78382497  6.66670794  0.10964978  0.21009507 12.01340702], Loss = 0.6709\n",
      "Iteration 4388: Weights = [55.30333333  2.7836451   6.66627719  0.1096427   0.2100815  12.01387105], Loss = 0.6709\n",
      "Iteration 4389: Weights = [55.30333333  2.78346525  6.66584647  0.10963561  0.21006793 12.01433505], Loss = 0.6708\n",
      "Iteration 4390: Weights = [55.30333333  2.7832854   6.66541577  0.10962853  0.21005435 12.01479901], Loss = 0.6707\n",
      "Iteration 4391: Weights = [55.30333333  2.78310557  6.66498511  0.10962145  0.21004078 12.01526295], Loss = 0.6706\n",
      "Iteration 4392: Weights = [55.30333333  2.78292575  6.66455447  0.10961436  0.21002721 12.01572686], Loss = 0.6705\n",
      "Iteration 4393: Weights = [55.30333333  2.78274594  6.66412386  0.10960728  0.21001364 12.01619074], Loss = 0.6704\n",
      "Iteration 4394: Weights = [55.30333333  2.78256614  6.66369328  0.1096002   0.21000007 12.01665458], Loss = 0.6703\n",
      "Iteration 4395: Weights = [55.30333333  2.78238635  6.66326272  0.10959312  0.2099865  12.0171184 ], Loss = 0.6702\n",
      "Iteration 4396: Weights = [55.30333333  2.78220658  6.6628322   0.10958604  0.20997293 12.01758219], Loss = 0.6702\n",
      "Iteration 4397: Weights = [55.30333333  2.78202681  6.6624017   0.10957896  0.20995937 12.01804595], Loss = 0.6701\n",
      "Iteration 4398: Weights = [55.30333333  2.78184706  6.66197123  0.10957188  0.2099458  12.01850968], Loss = 0.6700\n",
      "Iteration 4399: Weights = [55.30333333  2.78166732  6.66154078  0.1095648   0.20993224 12.01897337], Loss = 0.6699\n",
      "Iteration 4400: Weights = [55.30333333  2.78148759  6.66111037  0.10955772  0.20991867 12.01943704], Loss = 0.6698\n",
      "Iteration 4401: Weights = [55.30333333  2.78130787  6.66067998  0.10955064  0.20990511 12.01990068], Loss = 0.6697\n",
      "Iteration 4402: Weights = [55.30333333  2.78112817  6.66024962  0.10954356  0.20989155 12.02036429], Loss = 0.6696\n",
      "Iteration 4403: Weights = [55.30333333  2.78094847  6.65981929  0.10953648  0.20987798 12.02082787], Loss = 0.6696\n",
      "Iteration 4404: Weights = [55.30333333  2.78076879  6.65938898  0.10952941  0.20986442 12.02129141], Loss = 0.6695\n",
      "Iteration 4405: Weights = [55.30333333  2.78058912  6.65895871  0.10952233  0.20985086 12.02175493], Loss = 0.6694\n",
      "Iteration 4406: Weights = [55.30333333  2.78040946  6.65852846  0.10951525  0.2098373  12.02221842], Loss = 0.6693\n",
      "Iteration 4407: Weights = [55.30333333  2.78022981  6.65809824  0.10950818  0.20982375 12.02268188], Loss = 0.6692\n",
      "Iteration 4408: Weights = [55.30333333  2.78005017  6.65766804  0.1095011   0.20981019 12.02314531], Loss = 0.6691\n",
      "Iteration 4409: Weights = [55.30333333  2.77987055  6.65723788  0.10949403  0.20979663 12.02360871], Loss = 0.6690\n",
      "Iteration 4410: Weights = [55.30333333  2.77969094  6.65680774  0.10948695  0.20978308 12.02407207], Loss = 0.6690\n",
      "Iteration 4411: Weights = [55.30333333  2.77951133  6.65637763  0.10947988  0.20976952 12.02453541], Loss = 0.6689\n",
      "Iteration 4412: Weights = [55.30333333  2.77933175  6.65594755  0.1094728   0.20975597 12.02499872], Loss = 0.6688\n",
      "Iteration 4413: Weights = [55.30333333  2.77915217  6.6555175   0.10946573  0.20974242 12.025462  ], Loss = 0.6687\n",
      "Iteration 4414: Weights = [55.30333333  2.7789726   6.65508747  0.10945866  0.20972887 12.02592525], Loss = 0.6686\n",
      "Iteration 4415: Weights = [55.30333333  2.77879305  6.65465747  0.10945158  0.20971531 12.02638847], Loss = 0.6685\n",
      "Iteration 4416: Weights = [55.30333333  2.7786135   6.6542275   0.10944451  0.20970176 12.02685166], Loss = 0.6684\n",
      "Iteration 4417: Weights = [55.30333333  2.77843397  6.65379756  0.10943744  0.20968821 12.02731481], Loss = 0.6683\n",
      "Iteration 4418: Weights = [55.30333333  2.77825445  6.65336764  0.10943037  0.20967467 12.02777794], Loss = 0.6683\n",
      "Iteration 4419: Weights = [55.30333333  2.77807494  6.65293775  0.1094233   0.20966112 12.02824104], Loss = 0.6682\n",
      "Iteration 4420: Weights = [55.30333333  2.77789544  6.65250789  0.10941623  0.20964757 12.02870411], Loss = 0.6681\n",
      "Iteration 4421: Weights = [55.30333333  2.77771596  6.65207806  0.10940916  0.20963403 12.02916715], Loss = 0.6680\n",
      "Iteration 4422: Weights = [55.30333333  2.77753649  6.65164826  0.10940209  0.20962048 12.02963016], Loss = 0.6679\n",
      "Iteration 4423: Weights = [55.30333333  2.77735702  6.65121848  0.10939502  0.20960694 12.03009314], Loss = 0.6678\n",
      "Iteration 4424: Weights = [55.30333333  2.77717757  6.65078873  0.10938795  0.20959339 12.03055609], Loss = 0.6677\n",
      "Iteration 4425: Weights = [55.30333333  2.77699813  6.65035901  0.10938089  0.20957985 12.03101901], Loss = 0.6677\n",
      "Iteration 4426: Weights = [55.30333333  2.77681871  6.64992932  0.10937382  0.20956631 12.0314819 ], Loss = 0.6676\n",
      "Iteration 4427: Weights = [55.30333333  2.77663929  6.64949965  0.10936675  0.20955277 12.03194476], Loss = 0.6675\n",
      "Iteration 4428: Weights = [55.30333333  2.77645989  6.64907002  0.10935969  0.20953923 12.03240759], Loss = 0.6674\n",
      "Iteration 4429: Weights = [55.30333333  2.77628049  6.64864041  0.10935262  0.20952569 12.03287039], Loss = 0.6673\n",
      "Iteration 4430: Weights = [55.30333333  2.77610111  6.64821082  0.10934555  0.20951215 12.03333316], Loss = 0.6672\n",
      "Iteration 4431: Weights = [55.30333333  2.77592174  6.64778127  0.10933849  0.20949862 12.0337959 ], Loss = 0.6671\n",
      "Iteration 4432: Weights = [55.30333333  2.77574238  6.64735174  0.10933142  0.20948508 12.03425861], Loss = 0.6671\n",
      "Iteration 4433: Weights = [55.30333333  2.77556304  6.64692225  0.10932436  0.20947155 12.03472129], Loss = 0.6670\n",
      "Iteration 4434: Weights = [55.30333333  2.7753837   6.64649277  0.1093173   0.20945801 12.03518394], Loss = 0.6669\n",
      "Iteration 4435: Weights = [55.30333333  2.77520438  6.64606333  0.10931023  0.20944448 12.03564656], Loss = 0.6668\n",
      "Iteration 4436: Weights = [55.30333333  2.77502507  6.64563392  0.10930317  0.20943095 12.03610915], Loss = 0.6667\n",
      "Iteration 4437: Weights = [55.30333333  2.77484577  6.64520453  0.10929611  0.20941741 12.03657171], Loss = 0.6666\n",
      "Iteration 4438: Weights = [55.30333333  2.77466648  6.64477517  0.10928905  0.20940388 12.03703424], Loss = 0.6665\n",
      "Iteration 4439: Weights = [55.30333333  2.7744872   6.64434584  0.10928198  0.20939035 12.03749674], Loss = 0.6664\n",
      "Iteration 4440: Weights = [55.30333333  2.77430794  6.64391653  0.10927492  0.20937682 12.03795921], Loss = 0.6664\n",
      "Iteration 4441: Weights = [55.30333333  2.77412869  6.64348725  0.10926786  0.2093633  12.03842165], Loss = 0.6663\n",
      "Iteration 4442: Weights = [55.30333333  2.77394944  6.64305801  0.1092608   0.20934977 12.03888407], Loss = 0.6662\n",
      "Iteration 4443: Weights = [55.30333333  2.77377021  6.64262878  0.10925374  0.20933624 12.03934645], Loss = 0.6661\n",
      "Iteration 4444: Weights = [55.30333333  2.77359099  6.64219959  0.10924668  0.20932272 12.0398088 ], Loss = 0.6660\n",
      "Iteration 4445: Weights = [55.30333333  2.77341179  6.64177042  0.10923963  0.20930919 12.04027112], Loss = 0.6659\n",
      "Iteration 4446: Weights = [55.30333333  2.77323259  6.64134129  0.10923257  0.20929567 12.04073341], Loss = 0.6658\n",
      "Iteration 4447: Weights = [55.30333333  2.77305341  6.64091218  0.10922551  0.20928214 12.04119567], Loss = 0.6658\n",
      "Iteration 4448: Weights = [55.30333333  2.77287423  6.64048309  0.10921845  0.20926862 12.04165791], Loss = 0.6657\n",
      "Iteration 4449: Weights = [55.30333333  2.77269507  6.64005404  0.1092114   0.2092551  12.04212011], Loss = 0.6656\n",
      "Iteration 4450: Weights = [55.30333333  2.77251592  6.63962501  0.10920434  0.20924158 12.04258228], Loss = 0.6655\n",
      "Iteration 4451: Weights = [55.30333333  2.77233679  6.63919601  0.10919728  0.20922806 12.04304442], Loss = 0.6654\n",
      "Iteration 4452: Weights = [55.30333333  2.77215766  6.63876704  0.10919023  0.20921454 12.04350654], Loss = 0.6653\n",
      "Iteration 4453: Weights = [55.30333333  2.77197854  6.6383381   0.10918317  0.20920102 12.04396862], Loss = 0.6652\n",
      "Iteration 4454: Weights = [55.30333333  2.77179944  6.63790918  0.10917612  0.20918751 12.04443067], Loss = 0.6652\n",
      "Iteration 4455: Weights = [55.30333333  2.77162035  6.63748029  0.10916906  0.20917399 12.04489269], Loss = 0.6651\n",
      "Iteration 4456: Weights = [55.30333333  2.77144127  6.63705143  0.10916201  0.20916048 12.04535469], Loss = 0.6650\n",
      "Iteration 4457: Weights = [55.30333333  2.7712622   6.6366226   0.10915496  0.20914696 12.04581665], Loss = 0.6649\n",
      "Iteration 4458: Weights = [55.30333333  2.77108315  6.63619379  0.10914791  0.20913345 12.04627858], Loss = 0.6648\n",
      "Iteration 4459: Weights = [55.30333333  2.7709041   6.63576501  0.10914085  0.20911994 12.04674049], Loss = 0.6647\n",
      "Iteration 4460: Weights = [55.30333333  2.77072507  6.63533626  0.1091338   0.20910642 12.04720236], Loss = 0.6646\n",
      "Iteration 4461: Weights = [55.30333333  2.77054604  6.63490754  0.10912675  0.20909291 12.04766421], Loss = 0.6646\n",
      "Iteration 4462: Weights = [55.30333333  2.77036703  6.63447885  0.1091197   0.2090794  12.04812602], Loss = 0.6645\n",
      "Iteration 4463: Weights = [55.30333333  2.77018803  6.63405018  0.10911265  0.20906589 12.0485878 ], Loss = 0.6644\n",
      "Iteration 4464: Weights = [55.30333333  2.77000905  6.63362154  0.1091056   0.20905239 12.04904956], Loss = 0.6643\n",
      "Iteration 4465: Weights = [55.30333333  2.76983007  6.63319293  0.10909855  0.20903888 12.04951128], Loss = 0.6642\n",
      "Iteration 4466: Weights = [55.30333333  2.76965111  6.63276434  0.1090915   0.20902537 12.04997298], Loss = 0.6641\n",
      "Iteration 4467: Weights = [55.30333333  2.76947215  6.63233579  0.10908445  0.20901187 12.05043464], Loss = 0.6640\n",
      "Iteration 4468: Weights = [55.30333333  2.76929321  6.63190726  0.1090774   0.20899836 12.05089628], Loss = 0.6640\n",
      "Iteration 4469: Weights = [55.30333333  2.76911428  6.63147876  0.10907036  0.20898486 12.05135788], Loss = 0.6639\n",
      "Iteration 4470: Weights = [55.30333333  2.76893537  6.63105029  0.10906331  0.20897136 12.05181946], Loss = 0.6638\n",
      "Iteration 4471: Weights = [55.30333333  2.76875646  6.63062184  0.10905626  0.20895785 12.052281  ], Loss = 0.6637\n",
      "Iteration 4472: Weights = [55.30333333  2.76857756  6.63019342  0.10904921  0.20894435 12.05274252], Loss = 0.6636\n",
      "Iteration 4473: Weights = [55.30333333  2.76839868  6.62976503  0.10904217  0.20893085 12.05320401], Loss = 0.6635\n",
      "Iteration 4474: Weights = [55.30333333  2.76821981  6.62933667  0.10903512  0.20891735 12.05366546], Loss = 0.6634\n",
      "Iteration 4475: Weights = [55.30333333  2.76804095  6.62890833  0.10902808  0.20890385 12.05412689], Loss = 0.6634\n",
      "Iteration 4476: Weights = [55.30333333  2.7678621   6.62848003  0.10902103  0.20889036 12.05458829], Loss = 0.6633\n",
      "Iteration 4477: Weights = [55.30333333  2.76768326  6.62805175  0.10901399  0.20887686 12.05504965], Loss = 0.6632\n",
      "Iteration 4478: Weights = [55.30333333  2.76750444  6.6276235   0.10900695  0.20886336 12.05551099], Loss = 0.6631\n",
      "Iteration 4479: Weights = [55.30333333  2.76732562  6.62719527  0.1089999   0.20884987 12.0559723 ], Loss = 0.6630\n",
      "Iteration 4480: Weights = [55.30333333  2.76714682  6.62676708  0.10899286  0.20883637 12.05643357], Loss = 0.6629\n",
      "Iteration 4481: Weights = [55.30333333  2.76696803  6.62633891  0.10898582  0.20882288 12.05689482], Loss = 0.6628\n",
      "Iteration 4482: Weights = [55.30333333  2.76678925  6.62591077  0.10897878  0.20880939 12.05735604], Loss = 0.6628\n",
      "Iteration 4483: Weights = [55.30333333  2.76661048  6.62548265  0.10897174  0.2087959  12.05781723], Loss = 0.6627\n",
      "Iteration 4484: Weights = [55.30333333  2.76643173  6.62505457  0.10896469  0.20878241 12.05827839], Loss = 0.6626\n",
      "Iteration 4485: Weights = [55.30333333  2.76625298  6.62462651  0.10895765  0.20876892 12.05873951], Loss = 0.6625\n",
      "Iteration 4486: Weights = [55.30333333  2.76607425  6.62419848  0.10895061  0.20875543 12.05920061], Loss = 0.6624\n",
      "Iteration 4487: Weights = [55.30333333  2.76589553  6.62377048  0.10894357  0.20874194 12.05966168], Loss = 0.6623\n",
      "Iteration 4488: Weights = [55.30333333  2.76571682  6.6233425   0.10893654  0.20872845 12.06012272], Loss = 0.6622\n",
      "Iteration 4489: Weights = [55.30333333  2.76553812  6.62291455  0.1089295   0.20871497 12.06058373], Loss = 0.6622\n",
      "Iteration 4490: Weights = [55.30333333  2.76535943  6.62248663  0.10892246  0.20870148 12.06104471], Loss = 0.6621\n",
      "Iteration 4491: Weights = [55.30333333  2.76518076  6.62205874  0.10891542  0.208688   12.06150566], Loss = 0.6620\n",
      "Iteration 4492: Weights = [55.30333333  2.76500209  6.62163088  0.10890838  0.20867451 12.06196658], Loss = 0.6619\n",
      "Iteration 4493: Weights = [55.30333333  2.76482344  6.62120304  0.10890135  0.20866103 12.06242747], Loss = 0.6618\n",
      "Iteration 4494: Weights = [55.30333333  2.7646448   6.62077523  0.10889431  0.20864755 12.06288833], Loss = 0.6617\n",
      "Iteration 4495: Weights = [55.30333333  2.76446617  6.62034745  0.10888727  0.20863407 12.06334916], Loss = 0.6616\n",
      "Iteration 4496: Weights = [55.30333333  2.76428755  6.61991969  0.10888024  0.20862059 12.06380996], Loss = 0.6616\n",
      "Iteration 4497: Weights = [55.30333333  2.76410895  6.61949197  0.1088732   0.20860711 12.06427073], Loss = 0.6615\n",
      "Iteration 4498: Weights = [55.30333333  2.76393035  6.61906427  0.10886617  0.20859363 12.06473147], Loss = 0.6614\n",
      "Iteration 4499: Weights = [55.30333333  2.76375177  6.6186366   0.10885914  0.20858015 12.06519218], Loss = 0.6613\n",
      "Iteration 4500: Weights = [55.30333333  2.7635732   6.61820896  0.1088521   0.20856667 12.06565286], Loss = 0.6612\n",
      "Iteration 4501: Weights = [55.30333333  2.76339464  6.61778134  0.10884507  0.2085532  12.06611352], Loss = 0.6611\n",
      "Iteration 4502: Weights = [55.30333333  2.76321609  6.61735375  0.10883804  0.20853972 12.06657414], Loss = 0.6610\n",
      "Iteration 4503: Weights = [55.30333333  2.76303755  6.61692619  0.108831    0.20852625 12.06703473], Loss = 0.6610\n",
      "Iteration 4504: Weights = [55.30333333  2.76285903  6.61649866  0.10882397  0.20851277 12.06749529], Loss = 0.6609\n",
      "Iteration 4505: Weights = [55.30333333  2.76268051  6.61607115  0.10881694  0.2084993  12.06795583], Loss = 0.6608\n",
      "Iteration 4506: Weights = [55.30333333  2.76250201  6.61564368  0.10880991  0.20848583 12.06841633], Loss = 0.6607\n",
      "Iteration 4507: Weights = [55.30333333  2.76232352  6.61521622  0.10880288  0.20847236 12.0688768 ], Loss = 0.6606\n",
      "Iteration 4508: Weights = [55.30333333  2.76214504  6.6147888   0.10879585  0.20845889 12.06933725], Loss = 0.6605\n",
      "Iteration 4509: Weights = [55.30333333  2.76196657  6.61436141  0.10878882  0.20844542 12.06979766], Loss = 0.6604\n",
      "Iteration 4510: Weights = [55.30333333  2.76178812  6.61393404  0.10878179  0.20843195 12.07025804], Loss = 0.6604\n",
      "Iteration 4511: Weights = [55.30333333  2.76160967  6.6135067   0.10877476  0.20841849 12.0707184 ], Loss = 0.6603\n",
      "Iteration 4512: Weights = [55.30333333  2.76143124  6.61307939  0.10876773  0.20840502 12.07117872], Loss = 0.6602\n",
      "Iteration 4513: Weights = [55.30333333  2.76125282  6.6126521   0.10876071  0.20839155 12.07163902], Loss = 0.6601\n",
      "Iteration 4514: Weights = [55.30333333  2.76107441  6.61222485  0.10875368  0.20837809 12.07209928], Loss = 0.6600\n",
      "Iteration 4515: Weights = [55.30333333  2.76089601  6.61179762  0.10874665  0.20836463 12.07255952], Loss = 0.6599\n",
      "Iteration 4516: Weights = [55.30333333  2.76071762  6.61137042  0.10873963  0.20835116 12.07301972], Loss = 0.6599\n",
      "Iteration 4517: Weights = [55.30333333  2.76053924  6.61094324  0.1087326   0.2083377  12.0734799 ], Loss = 0.6598\n",
      "Iteration 4518: Weights = [55.30333333  2.76036088  6.6105161   0.10872557  0.20832424 12.07394005], Loss = 0.6597\n",
      "Iteration 4519: Weights = [55.30333333  2.76018253  6.61008898  0.10871855  0.20831078 12.07440016], Loss = 0.6596\n",
      "Iteration 4520: Weights = [55.30333333  2.76000419  6.60966189  0.10871153  0.20829732 12.07486025], Loss = 0.6595\n",
      "Iteration 4521: Weights = [55.30333333  2.75982586  6.60923482  0.1087045   0.20828386 12.07532031], Loss = 0.6594\n",
      "Iteration 4522: Weights = [55.30333333  2.75964754  6.60880779  0.10869748  0.2082704  12.07578033], Loss = 0.6593\n",
      "Iteration 4523: Weights = [55.30333333  2.75946923  6.60838078  0.10869045  0.20825695 12.07624033], Loss = 0.6593\n",
      "Iteration 4524: Weights = [55.30333333  2.75929094  6.6079538   0.10868343  0.20824349 12.0767003 ], Loss = 0.6592\n",
      "Iteration 4525: Weights = [55.30333333  2.75911266  6.60752685  0.10867641  0.20823004 12.07716024], Loss = 0.6591\n",
      "Iteration 4526: Weights = [55.30333333  2.75893438  6.60709992  0.10866939  0.20821658 12.07762014], Loss = 0.6590\n",
      "Iteration 4527: Weights = [55.30333333  2.75875612  6.60667302  0.10866237  0.20820313 12.07808002], Loss = 0.6589\n",
      "Iteration 4528: Weights = [55.30333333  2.75857787  6.60624615  0.10865535  0.20818968 12.07853987], Loss = 0.6588\n",
      "Iteration 4529: Weights = [55.30333333  2.75839964  6.60581931  0.10864832  0.20817623 12.07899969], Loss = 0.6587\n",
      "Iteration 4530: Weights = [55.30333333  2.75822141  6.60539249  0.10864131  0.20816277 12.07945948], Loss = 0.6587\n",
      "Iteration 4531: Weights = [55.30333333  2.7580432   6.60496571  0.10863429  0.20814932 12.07991924], Loss = 0.6586\n",
      "Iteration 4532: Weights = [55.30333333  2.75786499  6.60453895  0.10862727  0.20813588 12.08037897], Loss = 0.6585\n",
      "Iteration 4533: Weights = [55.30333333  2.7576868   6.60411221  0.10862025  0.20812243 12.08083867], Loss = 0.6584\n",
      "Iteration 4534: Weights = [55.30333333  2.75750862  6.60368551  0.10861323  0.20810898 12.08129834], Loss = 0.6583\n",
      "Iteration 4535: Weights = [55.30333333  2.75733046  6.60325883  0.10860621  0.20809553 12.08175798], Loss = 0.6582\n",
      "Iteration 4536: Weights = [55.30333333  2.7571523   6.60283218  0.10859919  0.20808209 12.08221759], Loss = 0.6581\n",
      "Iteration 4537: Weights = [55.30333333  2.75697415  6.60240556  0.10859218  0.20806864 12.08267718], Loss = 0.6581\n",
      "Iteration 4538: Weights = [55.30333333  2.75679602  6.60197896  0.10858516  0.2080552  12.08313673], Loss = 0.6580\n",
      "Iteration 4539: Weights = [55.30333333  2.7566179   6.6015524   0.10857815  0.20804176 12.08359625], Loss = 0.6579\n",
      "Iteration 4540: Weights = [55.30333333  2.75643979  6.60112586  0.10857113  0.20802832 12.08405574], Loss = 0.6578\n",
      "Iteration 4541: Weights = [55.30333333  2.75626169  6.60069935  0.10856412  0.20801487 12.0845152 ], Loss = 0.6577\n",
      "Iteration 4542: Weights = [55.30333333  2.7560836   6.60027286  0.1085571   0.20800143 12.08497464], Loss = 0.6576\n",
      "Iteration 4543: Weights = [55.30333333  2.75590552  6.5998464   0.10855009  0.20798799 12.08543404], Loss = 0.6576\n",
      "Iteration 4544: Weights = [55.30333333  2.75572746  6.59941997  0.10854307  0.20797456 12.08589342], Loss = 0.6575\n",
      "Iteration 4545: Weights = [55.30333333  2.75554941  6.59899357  0.10853606  0.20796112 12.08635276], Loss = 0.6574\n",
      "Iteration 4546: Weights = [55.30333333  2.75537137  6.5985672   0.10852905  0.20794768 12.08681207], Loss = 0.6573\n",
      "Iteration 4547: Weights = [55.30333333  2.75519334  6.59814085  0.10852203  0.20793425 12.08727136], Loss = 0.6572\n",
      "Iteration 4548: Weights = [55.30333333  2.75501532  6.59771453  0.10851502  0.20792081 12.08773061], Loss = 0.6571\n",
      "Iteration 4549: Weights = [55.30333333  2.75483731  6.59728824  0.10850801  0.20790738 12.08818984], Loss = 0.6570\n",
      "Iteration 4550: Weights = [55.30333333  2.75465931  6.59686198  0.108501    0.20789394 12.08864903], Loss = 0.6570\n",
      "Iteration 4551: Weights = [55.30333333  2.75448133  6.59643574  0.10849399  0.20788051 12.0891082 ], Loss = 0.6569\n",
      "Iteration 4552: Weights = [55.30333333  2.75430336  6.59600953  0.10848698  0.20786708 12.08956734], Loss = 0.6568\n",
      "Iteration 4553: Weights = [55.30333333  2.7541254   6.59558335  0.10847997  0.20785365 12.09002644], Loss = 0.6567\n",
      "Iteration 4554: Weights = [55.30333333  2.75394745  6.5951572   0.10847296  0.20784022 12.09048552], Loss = 0.6566\n",
      "Iteration 4555: Weights = [55.30333333  2.75376951  6.59473107  0.10846595  0.20782679 12.09094457], Loss = 0.6565\n",
      "Iteration 4556: Weights = [55.30333333  2.75359158  6.59430497  0.10845894  0.20781336 12.09140359], Loss = 0.6564\n",
      "Iteration 4557: Weights = [55.30333333  2.75341367  6.5938789   0.10845194  0.20779993 12.09186257], Loss = 0.6564\n",
      "Iteration 4558: Weights = [55.30333333  2.75323576  6.59345286  0.10844493  0.20778651 12.09232153], Loss = 0.6563\n",
      "Iteration 4559: Weights = [55.30333333  2.75305787  6.59302684  0.10843792  0.20777308 12.09278046], Loss = 0.6562\n",
      "Iteration 4560: Weights = [55.30333333  2.75287999  6.59260085  0.10843092  0.20775966 12.09323936], Loss = 0.6561\n",
      "Iteration 4561: Weights = [55.30333333  2.75270212  6.59217489  0.10842391  0.20774623 12.09369823], Loss = 0.6560\n",
      "Iteration 4562: Weights = [55.30333333  2.75252426  6.59174896  0.1084169   0.20773281 12.09415707], Loss = 0.6559\n",
      "Iteration 4563: Weights = [55.30333333  2.75234642  6.59132305  0.1084099   0.20771939 12.09461588], Loss = 0.6559\n",
      "Iteration 4564: Weights = [55.30333333  2.75216858  6.59089717  0.1084029   0.20770597 12.09507466], Loss = 0.6558\n",
      "Iteration 4565: Weights = [55.30333333  2.75199076  6.59047132  0.10839589  0.20769255 12.09553341], Loss = 0.6557\n",
      "Iteration 4566: Weights = [55.30333333  2.75181295  6.5900455   0.10838889  0.20767913 12.09599213], Loss = 0.6556\n",
      "Iteration 4567: Weights = [55.30333333  2.75163515  6.5896197   0.10838188  0.20766571 12.09645082], Loss = 0.6555\n",
      "Iteration 4568: Weights = [55.30333333  2.75145736  6.58919393  0.10837488  0.20765229 12.09690949], Loss = 0.6554\n",
      "Iteration 4569: Weights = [55.30333333  2.75127958  6.58876819  0.10836788  0.20763888 12.09736812], Loss = 0.6553\n",
      "Iteration 4570: Weights = [55.30333333  2.75110182  6.58834248  0.10836088  0.20762546 12.09782672], Loss = 0.6553\n",
      "Iteration 4571: Weights = [55.30333333  2.75092406  6.58791679  0.10835388  0.20761204 12.09828529], Loss = 0.6552\n",
      "Iteration 4572: Weights = [55.30333333  2.75074632  6.58749113  0.10834687  0.20759863 12.09874384], Loss = 0.6551\n",
      "Iteration 4573: Weights = [55.30333333  2.75056859  6.5870655   0.10833987  0.20758522 12.09920235], Loss = 0.6550\n",
      "Iteration 4574: Weights = [55.30333333  2.75039087  6.5866399   0.10833287  0.2075718  12.09966083], Loss = 0.6549\n",
      "Iteration 4575: Weights = [55.30333333  2.75021316  6.58621432  0.10832587  0.20755839 12.10011929], Loss = 0.6548\n",
      "Iteration 4576: Weights = [55.30333333  2.75003546  6.58578877  0.10831888  0.20754498 12.10057771], Loss = 0.6548\n",
      "Iteration 4577: Weights = [55.30333333  2.74985778  6.58536325  0.10831188  0.20753157 12.10103611], Loss = 0.6547\n",
      "Iteration 4578: Weights = [55.30333333  2.7496801   6.58493776  0.10830488  0.20751816 12.10149448], Loss = 0.6546\n",
      "Iteration 4579: Weights = [55.30333333  2.74950244  6.58451229  0.10829788  0.20750475 12.10195281], Loss = 0.6545\n",
      "Iteration 4580: Weights = [55.30333333  2.74932479  6.58408685  0.10829088  0.20749135 12.10241112], Loss = 0.6544\n",
      "Iteration 4581: Weights = [55.30333333  2.74914715  6.58366144  0.10828389  0.20747794 12.10286939], Loss = 0.6543\n",
      "Iteration 4582: Weights = [55.30333333  2.74896952  6.58323606  0.10827689  0.20746454 12.10332764], Loss = 0.6542\n",
      "Iteration 4583: Weights = [55.30333333  2.74879191  6.5828107   0.10826989  0.20745113 12.10378586], Loss = 0.6542\n",
      "Iteration 4584: Weights = [55.30333333  2.7486143   6.58238538  0.1082629   0.20743773 12.10424405], Loss = 0.6541\n",
      "Iteration 4585: Weights = [55.30333333  2.74843671  6.58196007  0.1082559   0.20742432 12.10470221], Loss = 0.6540\n",
      "Iteration 4586: Weights = [55.30333333  2.74825913  6.5815348   0.10824891  0.20741092 12.10516033], Loss = 0.6539\n",
      "Iteration 4587: Weights = [55.30333333  2.74808156  6.58110955  0.10824191  0.20739752 12.10561843], Loss = 0.6538\n",
      "Iteration 4588: Weights = [55.30333333  2.747904    6.58068434  0.10823492  0.20738412 12.1060765 ], Loss = 0.6537\n",
      "Iteration 4589: Weights = [55.30333333  2.74772645  6.58025914  0.10822793  0.20737072 12.10653454], Loss = 0.6537\n",
      "Iteration 4590: Weights = [55.30333333  2.74754891  6.57983398  0.10822093  0.20735732 12.10699255], Loss = 0.6536\n",
      "Iteration 4591: Weights = [55.30333333  2.74737139  6.57940884  0.10821394  0.20734392 12.10745053], Loss = 0.6535\n",
      "Iteration 4592: Weights = [55.30333333  2.74719388  6.57898374  0.10820695  0.20733053 12.10790849], Loss = 0.6534\n",
      "Iteration 4593: Weights = [55.30333333  2.74701637  6.57855865  0.10819996  0.20731713 12.10836641], Loss = 0.6533\n",
      "Iteration 4594: Weights = [55.30333333  2.74683888  6.5781336   0.10819297  0.20730374 12.1088243 ], Loss = 0.6532\n",
      "Iteration 4595: Weights = [55.30333333  2.74666141  6.57770857  0.10818598  0.20729034 12.10928216], Loss = 0.6531\n",
      "Iteration 4596: Weights = [55.30333333  2.74648394  6.57728357  0.10817899  0.20727695 12.10973999], Loss = 0.6531\n",
      "Iteration 4597: Weights = [55.30333333  2.74630648  6.5768586   0.108172    0.20726356 12.1101978 ], Loss = 0.6530\n",
      "Iteration 4598: Weights = [55.30333333  2.74612904  6.57643366  0.10816501  0.20725016 12.11065557], Loss = 0.6529\n",
      "Iteration 4599: Weights = [55.30333333  2.7459516   6.57600874  0.10815802  0.20723677 12.11111332], Loss = 0.6528\n",
      "Iteration 4600: Weights = [55.30333333  2.74577418  6.57558385  0.10815103  0.20722338 12.11157103], Loss = 0.6527\n",
      "Iteration 4601: Weights = [55.30333333  2.74559677  6.57515899  0.10814404  0.20720999 12.11202872], Loss = 0.6526\n",
      "Iteration 4602: Weights = [55.30333333  2.74541937  6.57473416  0.10813706  0.20719661 12.11248637], Loss = 0.6526\n",
      "Iteration 4603: Weights = [55.30333333  2.74524199  6.57430935  0.10813007  0.20718322 12.112944  ], Loss = 0.6525\n",
      "Iteration 4604: Weights = [55.30333333  2.74506461  6.57388457  0.10812308  0.20716983 12.11340159], Loss = 0.6524\n",
      "Iteration 4605: Weights = [55.30333333  2.74488725  6.57345982  0.1081161   0.20715645 12.11385916], Loss = 0.6523\n",
      "Iteration 4606: Weights = [55.30333333  2.7447099   6.57303509  0.10810911  0.20714306 12.1143167 ], Loss = 0.6522\n",
      "Iteration 4607: Weights = [55.30333333  2.74453255  6.5726104   0.10810213  0.20712968 12.1147742 ], Loss = 0.6521\n",
      "Iteration 4608: Weights = [55.30333333  2.74435522  6.57218573  0.10809514  0.20711629 12.11523168], Loss = 0.6521\n",
      "Iteration 4609: Weights = [55.30333333  2.74417791  6.57176109  0.10808816  0.20710291 12.11568913], Loss = 0.6520\n",
      "Iteration 4610: Weights = [55.30333333  2.7440006   6.57133647  0.10808117  0.20708953 12.11614655], Loss = 0.6519\n",
      "Iteration 4611: Weights = [55.30333333  2.7438233   6.57091188  0.10807419  0.20707615 12.11660394], Loss = 0.6518\n",
      "Iteration 4612: Weights = [55.30333333  2.74364602  6.57048732  0.10806721  0.20706277 12.1170613 ], Loss = 0.6517\n",
      "Iteration 4613: Weights = [55.30333333  2.74346875  6.57006279  0.10806022  0.20704939 12.11751863], Loss = 0.6516\n",
      "Iteration 4614: Weights = [55.30333333  2.74329149  6.56963829  0.10805324  0.20703601 12.11797593], Loss = 0.6515\n",
      "Iteration 4615: Weights = [55.30333333  2.74311424  6.56921381  0.10804626  0.20702264 12.1184332 ], Loss = 0.6515\n",
      "Iteration 4616: Weights = [55.30333333  2.742937    6.56878936  0.10803928  0.20700926 12.11889044], Loss = 0.6514\n",
      "Iteration 4617: Weights = [55.30333333  2.74275977  6.56836494  0.1080323   0.20699589 12.11934766], Loss = 0.6513\n",
      "Iteration 4618: Weights = [55.30333333  2.74258256  6.56794054  0.10802532  0.20698251 12.11980484], Loss = 0.6512\n",
      "Iteration 4619: Weights = [55.30333333  2.74240535  6.56751617  0.10801834  0.20696914 12.12026199], Loss = 0.6511\n",
      "Iteration 4620: Weights = [55.30333333  2.74222816  6.56709183  0.10801136  0.20695576 12.12071911], Loss = 0.6510\n",
      "Iteration 4621: Weights = [55.30333333  2.74205098  6.56666752  0.10800438  0.20694239 12.12117621], Loss = 0.6510\n",
      "Iteration 4622: Weights = [55.30333333  2.74187381  6.56624323  0.1079974   0.20692902 12.12163327], Loss = 0.6509\n",
      "Iteration 4623: Weights = [55.30333333  2.74169665  6.56581898  0.10799042  0.20691565 12.12209031], Loss = 0.6508\n",
      "Iteration 4624: Weights = [55.30333333  2.74151951  6.56539475  0.10798345  0.20690228 12.12254731], Loss = 0.6507\n",
      "Iteration 4625: Weights = [55.30333333  2.74134237  6.56497054  0.10797647  0.20688891 12.12300429], Loss = 0.6506\n",
      "Iteration 4626: Weights = [55.30333333  2.74116525  6.56454637  0.10796949  0.20687555 12.12346123], Loss = 0.6505\n",
      "Iteration 4627: Weights = [55.30333333  2.74098813  6.56412222  0.10796252  0.20686218 12.12391815], Loss = 0.6505\n",
      "Iteration 4628: Weights = [55.30333333  2.74081103  6.5636981   0.10795554  0.20684881 12.12437504], Loss = 0.6504\n",
      "Iteration 4629: Weights = [55.30333333  2.74063394  6.563274    0.10794857  0.20683545 12.1248319 ], Loss = 0.6503\n",
      "Iteration 4630: Weights = [55.30333333  2.74045687  6.56284994  0.10794159  0.20682209 12.12528872], Loss = 0.6502\n",
      "Iteration 4631: Weights = [55.30333333  2.7402798   6.5624259   0.10793462  0.20680872 12.12574552], Loss = 0.6501\n",
      "Iteration 4632: Weights = [55.30333333  2.74010275  6.56200189  0.10792764  0.20679536 12.12620229], Loss = 0.6500\n",
      "Iteration 4633: Weights = [55.30333333  2.7399257   6.5615779   0.10792067  0.206782   12.12665903], Loss = 0.6499\n",
      "Iteration 4634: Weights = [55.30333333  2.73974867  6.56115395  0.1079137   0.20676864 12.12711574], Loss = 0.6499\n",
      "Iteration 4635: Weights = [55.30333333  2.73957165  6.56073002  0.10790672  0.20675528 12.12757242], Loss = 0.6498\n",
      "Iteration 4636: Weights = [55.30333333  2.73939464  6.56030611  0.10789975  0.20674192 12.12802907], Loss = 0.6497\n",
      "Iteration 4637: Weights = [55.30333333  2.73921764  6.55988224  0.10789278  0.20672856 12.1284857 ], Loss = 0.6496\n",
      "Iteration 4638: Weights = [55.30333333  2.73904066  6.55945839  0.10788581  0.2067152  12.12894229], Loss = 0.6495\n",
      "Iteration 4639: Weights = [55.30333333  2.73886368  6.55903457  0.10787884  0.20670185 12.12939885], Loss = 0.6494\n",
      "Iteration 4640: Weights = [55.30333333  2.73868672  6.55861078  0.10787187  0.20668849 12.12985538], Loss = 0.6494\n",
      "Iteration 4641: Weights = [55.30333333  2.73850977  6.55818702  0.1078649   0.20667514 12.13031189], Loss = 0.6493\n",
      "Iteration 4642: Weights = [55.30333333  2.73833282  6.55776328  0.10785793  0.20666178 12.13076836], Loss = 0.6492\n",
      "Iteration 4643: Weights = [55.30333333  2.7381559   6.55733957  0.10785096  0.20664843 12.13122481], Loss = 0.6491\n",
      "Iteration 4644: Weights = [55.30333333  2.73797898  6.55691588  0.10784399  0.20663508 12.13168122], Loss = 0.6490\n",
      "Iteration 4645: Weights = [55.30333333  2.73780207  6.55649223  0.10783702  0.20662173 12.13213761], Loss = 0.6489\n",
      "Iteration 4646: Weights = [55.30333333  2.73762518  6.5560686   0.10783006  0.20660838 12.13259396], Loss = 0.6489\n",
      "Iteration 4647: Weights = [55.30333333  2.73744829  6.555645    0.10782309  0.20659503 12.13305029], Loss = 0.6488\n",
      "Iteration 4648: Weights = [55.30333333  2.73727142  6.55522143  0.10781612  0.20658168 12.13350659], Loss = 0.6487\n",
      "Iteration 4649: Weights = [55.30333333  2.73709456  6.55479788  0.10780916  0.20656833 12.13396285], Loss = 0.6486\n",
      "Iteration 4650: Weights = [55.30333333  2.73691771  6.55437436  0.10780219  0.20655499 12.13441909], Loss = 0.6485\n",
      "Iteration 4651: Weights = [55.30333333  2.73674087  6.55395087  0.10779523  0.20654164 12.1348753 ], Loss = 0.6484\n",
      "Iteration 4652: Weights = [55.30333333  2.73656405  6.55352741  0.10778826  0.20652829 12.13533148], Loss = 0.6484\n",
      "Iteration 4653: Weights = [55.30333333  2.73638723  6.55310397  0.1077813   0.20651495 12.13578763], Loss = 0.6483\n",
      "Iteration 4654: Weights = [55.30333333  2.73621043  6.55268056  0.10777433  0.20650161 12.13624375], Loss = 0.6482\n",
      "Iteration 4655: Weights = [55.30333333  2.73603364  6.55225718  0.10776737  0.20648826 12.13669984], Loss = 0.6481\n",
      "Iteration 4656: Weights = [55.30333333  2.73585686  6.55183382  0.10776041  0.20647492 12.1371559 ], Loss = 0.6480\n",
      "Iteration 4657: Weights = [55.30333333  2.73568009  6.5514105   0.10775344  0.20646158 12.13761193], Loss = 0.6479\n",
      "Iteration 4658: Weights = [55.30333333  2.73550333  6.5509872   0.10774648  0.20644824 12.13806794], Loss = 0.6479\n",
      "Iteration 4659: Weights = [55.30333333  2.73532658  6.55056393  0.10773952  0.2064349  12.13852391], Loss = 0.6478\n",
      "Iteration 4660: Weights = [55.30333333  2.73514985  6.55014068  0.10773256  0.20642157 12.13897985], Loss = 0.6477\n",
      "Iteration 4661: Weights = [55.30333333  2.73497312  6.54971746  0.1077256   0.20640823 12.13943577], Loss = 0.6476\n",
      "Iteration 4662: Weights = [55.30333333  2.73479641  6.54929427  0.10771864  0.20639489 12.13989165], Loss = 0.6475\n",
      "Iteration 4663: Weights = [55.30333333  2.73461971  6.54887111  0.10771168  0.20638156 12.14034751], Loss = 0.6474\n",
      "Iteration 4664: Weights = [55.30333333  2.73444302  6.54844797  0.10770472  0.20636822 12.14080333], Loss = 0.6474\n",
      "Iteration 4665: Weights = [55.30333333  2.73426634  6.54802486  0.10769776  0.20635489 12.14125913], Loss = 0.6473\n",
      "Iteration 4666: Weights = [55.30333333  2.73408968  6.54760178  0.1076908   0.20634155 12.1417149 ], Loss = 0.6472\n",
      "Iteration 4667: Weights = [55.30333333  2.73391302  6.54717873  0.10768384  0.20632822 12.14217063], Loss = 0.6471\n",
      "Iteration 4668: Weights = [55.30333333  2.73373638  6.5467557   0.10767688  0.20631489 12.14262634], Loss = 0.6470\n",
      "Iteration 4669: Weights = [55.30333333  2.73355975  6.5463327   0.10766993  0.20630156 12.14308202], Loss = 0.6469\n",
      "Iteration 4670: Weights = [55.30333333  2.73338313  6.54590973  0.10766297  0.20628823 12.14353767], Loss = 0.6468\n",
      "Iteration 4671: Weights = [55.30333333  2.73320652  6.54548679  0.10765601  0.2062749  12.14399329], Loss = 0.6468\n",
      "Iteration 4672: Weights = [55.30333333  2.73302992  6.54506387  0.10764906  0.20626157 12.14444888], Loss = 0.6467\n",
      "Iteration 4673: Weights = [55.30333333  2.73285333  6.54464098  0.1076421   0.20624825 12.14490444], Loss = 0.6466\n",
      "Iteration 4674: Weights = [55.30333333  2.73267676  6.54421812  0.10763515  0.20623492 12.14535997], Loss = 0.6465\n",
      "Iteration 4675: Weights = [55.30333333  2.73250019  6.54379528  0.10762819  0.2062216  12.14581547], Loss = 0.6464\n",
      "Iteration 4676: Weights = [55.30333333  2.73232364  6.54337248  0.10762124  0.20620827 12.14627094], Loss = 0.6463\n",
      "Iteration 4677: Weights = [55.30333333  2.7321471   6.5429497   0.10761429  0.20619495 12.14672639], Loss = 0.6463\n",
      "Iteration 4678: Weights = [55.30333333  2.73197057  6.54252694  0.10760733  0.20618163 12.1471818 ], Loss = 0.6462\n",
      "Iteration 4679: Weights = [55.30333333  2.73179405  6.54210422  0.10760038  0.2061683  12.14763718], Loss = 0.6461\n",
      "Iteration 4680: Weights = [55.30333333  2.73161755  6.54168152  0.10759343  0.20615498 12.14809254], Loss = 0.6460\n",
      "Iteration 4681: Weights = [55.30333333  2.73144105  6.54125885  0.10758648  0.20614166 12.14854786], Loss = 0.6459\n",
      "Iteration 4682: Weights = [55.30333333  2.73126457  6.5408362   0.10757952  0.20612834 12.14900316], Loss = 0.6458\n",
      "Iteration 4683: Weights = [55.30333333  2.73108809  6.54041359  0.10757257  0.20611502 12.14945843], Loss = 0.6458\n",
      "Iteration 4684: Weights = [55.30333333  2.73091163  6.539991    0.10756562  0.20610171 12.14991366], Loss = 0.6457\n",
      "Iteration 4685: Weights = [55.30333333  2.73073518  6.53956844  0.10755867  0.20608839 12.15036887], Loss = 0.6456\n",
      "Iteration 4686: Weights = [55.30333333  2.73055875  6.5391459   0.10755172  0.20607507 12.15082405], Loss = 0.6455\n",
      "Iteration 4687: Weights = [55.30333333  2.73038232  6.53872339  0.10754477  0.20606176 12.1512792 ], Loss = 0.6454\n",
      "Iteration 4688: Weights = [55.30333333  2.7302059   6.53830091  0.10753783  0.20604845 12.15173432], Loss = 0.6453\n",
      "Iteration 4689: Weights = [55.30333333  2.7300295   6.53787846  0.10753088  0.20603513 12.15218941], Loss = 0.6453\n",
      "Iteration 4690: Weights = [55.30333333  2.72985311  6.53745604  0.10752393  0.20602182 12.15264447], Loss = 0.6452\n",
      "Iteration 4691: Weights = [55.30333333  2.72967672  6.53703364  0.10751698  0.20600851 12.1530995 ], Loss = 0.6451\n",
      "Iteration 4692: Weights = [55.30333333  2.72950035  6.53661127  0.10751004  0.2059952  12.1535545 ], Loss = 0.6450\n",
      "Iteration 4693: Weights = [55.30333333  2.729324    6.53618892  0.10750309  0.20598189 12.15400947], Loss = 0.6449\n",
      "Iteration 4694: Weights = [55.30333333  2.72914765  6.53576661  0.10749614  0.20596858 12.15446442], Loss = 0.6448\n",
      "Iteration 4695: Weights = [55.30333333  2.72897131  6.53534432  0.1074892   0.20595527 12.15491933], Loss = 0.6448\n",
      "Iteration 4696: Weights = [55.30333333  2.72879499  6.53492206  0.10748225  0.20594196 12.15537421], Loss = 0.6447\n",
      "Iteration 4697: Weights = [55.30333333  2.72861868  6.53449982  0.10747531  0.20592866 12.15582907], Loss = 0.6446\n",
      "Iteration 4698: Weights = [55.30333333  2.72844237  6.53407761  0.10746836  0.20591535 12.15628389], Loss = 0.6445\n",
      "Iteration 4699: Weights = [55.30333333  2.72826608  6.53365543  0.10746142  0.20590205 12.15673869], Loss = 0.6444\n",
      "Iteration 4700: Weights = [55.30333333  2.72808981  6.53323328  0.10745448  0.20588874 12.15719346], Loss = 0.6443\n",
      "Iteration 4701: Weights = [55.30333333  2.72791354  6.53281116  0.10744753  0.20587544 12.15764819], Loss = 0.6443\n",
      "Iteration 4702: Weights = [55.30333333  2.72773728  6.53238906  0.10744059  0.20586214 12.1581029 ], Loss = 0.6442\n",
      "Iteration 4703: Weights = [55.30333333  2.72756104  6.53196699  0.10743365  0.20584884 12.15855758], Loss = 0.6441\n",
      "Iteration 4704: Weights = [55.30333333  2.7273848   6.53154494  0.10742671  0.20583554 12.15901223], Loss = 0.6440\n",
      "Iteration 4705: Weights = [55.30333333  2.72720858  6.53112293  0.10741977  0.20582224 12.15946685], Loss = 0.6439\n",
      "Iteration 4706: Weights = [55.30333333  2.72703237  6.53070094  0.10741283  0.20580894 12.15992144], Loss = 0.6438\n",
      "Iteration 4707: Weights = [55.30333333  2.72685617  6.53027898  0.10740589  0.20579564 12.160376  ], Loss = 0.6438\n",
      "Iteration 4708: Weights = [55.30333333  2.72667999  6.52985704  0.10739895  0.20578235 12.16083053], Loss = 0.6437\n",
      "Iteration 4709: Weights = [55.30333333  2.72650381  6.52943514  0.10739201  0.20576905 12.16128503], Loss = 0.6436\n",
      "Iteration 4710: Weights = [55.30333333  2.72632764  6.52901326  0.10738507  0.20575575 12.16173951], Loss = 0.6435\n",
      "Iteration 4711: Weights = [55.30333333  2.72615149  6.5285914   0.10737813  0.20574246 12.16219395], Loss = 0.6434\n",
      "Iteration 4712: Weights = [55.30333333  2.72597535  6.52816958  0.10737119  0.20572917 12.16264837], Loss = 0.6433\n",
      "Iteration 4713: Weights = [55.30333333  2.72579922  6.52774778  0.10736425  0.20571587 12.16310275], Loss = 0.6433\n",
      "Iteration 4714: Weights = [55.30333333  2.7256231   6.52732601  0.10735732  0.20570258 12.16355711], Loss = 0.6432\n",
      "Iteration 4715: Weights = [55.30333333  2.72544699  6.52690426  0.10735038  0.20568929 12.16401143], Loss = 0.6431\n",
      "Iteration 4716: Weights = [55.30333333  2.72527089  6.52648255  0.10734344  0.205676   12.16446573], Loss = 0.6430\n",
      "Iteration 4717: Weights = [55.30333333  2.72509481  6.52606086  0.10733651  0.20566271 12.16492   ], Loss = 0.6429\n",
      "Iteration 4718: Weights = [55.30333333  2.72491874  6.5256392   0.10732957  0.20564942 12.16537423], Loss = 0.6428\n",
      "Iteration 4719: Weights = [55.30333333  2.72474267  6.52521756  0.10732264  0.20563614 12.16582844], Loss = 0.6428\n",
      "Iteration 4720: Weights = [55.30333333  2.72456662  6.52479596  0.1073157   0.20562285 12.16628262], Loss = 0.6427\n",
      "Iteration 4721: Weights = [55.30333333  2.72439058  6.52437437  0.10730877  0.20560956 12.16673677], Loss = 0.6426\n",
      "Iteration 4722: Weights = [55.30333333  2.72421455  6.52395282  0.10730184  0.20559628 12.16719089], Loss = 0.6425\n",
      "Iteration 4723: Weights = [55.30333333  2.72403854  6.5235313   0.1072949   0.205583   12.16764498], Loss = 0.6424\n",
      "Iteration 4724: Weights = [55.30333333  2.72386253  6.5231098   0.10728797  0.20556971 12.16809905], Loss = 0.6424\n",
      "Iteration 4725: Weights = [55.30333333  2.72368654  6.52268833  0.10728104  0.20555643 12.16855308], Loss = 0.6423\n",
      "Iteration 4726: Weights = [55.30333333  2.72351055  6.52226688  0.10727411  0.20554315 12.16900708], Loss = 0.6422\n",
      "Iteration 4727: Weights = [55.30333333  2.72333458  6.52184547  0.10726718  0.20552987 12.16946106], Loss = 0.6421\n",
      "Iteration 4728: Weights = [55.30333333  2.72315862  6.52142408  0.10726025  0.20551659 12.169915  ], Loss = 0.6420\n",
      "Iteration 4729: Weights = [55.30333333  2.72298267  6.52100271  0.10725332  0.20550331 12.17036891], Loss = 0.6419\n",
      "Iteration 4730: Weights = [55.30333333  2.72280674  6.52058138  0.10724639  0.20549003 12.1708228 ], Loss = 0.6419\n",
      "Iteration 4731: Weights = [55.30333333  2.72263081  6.52016007  0.10723946  0.20547675 12.17127666], Loss = 0.6418\n",
      "Iteration 4732: Weights = [55.30333333  2.7224549   6.51973879  0.10723253  0.20546348 12.17173048], Loss = 0.6417\n",
      "Iteration 4733: Weights = [55.30333333  2.72227899  6.51931754  0.1072256   0.2054502  12.17218428], Loss = 0.6416\n",
      "Iteration 4734: Weights = [55.30333333  2.7221031   6.51889631  0.10721867  0.20543693 12.17263805], Loss = 0.6415\n",
      "Iteration 4735: Weights = [55.30333333  2.72192722  6.51847511  0.10721174  0.20542365 12.17309179], Loss = 0.6414\n",
      "Iteration 4736: Weights = [55.30333333  2.72175135  6.51805394  0.10720482  0.20541038 12.1735455 ], Loss = 0.6414\n",
      "Iteration 4737: Weights = [55.30333333  2.72157549  6.5176328   0.10719789  0.20539711 12.17399918], Loss = 0.6413\n",
      "Iteration 4738: Weights = [55.30333333  2.72139965  6.51721168  0.10719096  0.20538384 12.17445283], Loss = 0.6412\n",
      "Iteration 4739: Weights = [55.30333333  2.72122381  6.51679059  0.10718404  0.20537057 12.17490645], Loss = 0.6411\n",
      "Iteration 4740: Weights = [55.30333333  2.72104799  6.51636952  0.10717711  0.2053573  12.17536005], Loss = 0.6410\n",
      "Iteration 4741: Weights = [55.30333333  2.72087217  6.51594849  0.10717019  0.20534403 12.17581361], Loss = 0.6409\n",
      "Iteration 4742: Weights = [55.30333333  2.72069637  6.51552748  0.10716326  0.20533076 12.17626715], Loss = 0.6409\n",
      "Iteration 4743: Weights = [55.30333333  2.72052058  6.5151065   0.10715634  0.2053175  12.17672065], Loss = 0.6408\n",
      "Iteration 4744: Weights = [55.30333333  2.72034481  6.51468555  0.10714941  0.20530423 12.17717413], Loss = 0.6407\n",
      "Iteration 4745: Weights = [55.30333333  2.72016904  6.51426462  0.10714249  0.20529096 12.17762757], Loss = 0.6406\n",
      "Iteration 4746: Weights = [55.30333333  2.71999328  6.51384372  0.10713557  0.2052777  12.17808099], Loss = 0.6405\n",
      "Iteration 4747: Weights = [55.30333333  2.71981754  6.51342285  0.10712865  0.20526444 12.17853438], Loss = 0.6404\n",
      "Iteration 4748: Weights = [55.30333333  2.71964181  6.513002    0.10712172  0.20525117 12.17898773], Loss = 0.6404\n",
      "Iteration 4749: Weights = [55.30333333  2.71946608  6.51258118  0.1071148   0.20523791 12.17944106], Loss = 0.6403\n",
      "Iteration 4750: Weights = [55.30333333  2.71929037  6.51216039  0.10710788  0.20522465 12.17989436], Loss = 0.6402\n",
      "Iteration 4751: Weights = [55.30333333  2.71911468  6.51173963  0.10710096  0.20521139 12.18034763], Loss = 0.6401\n",
      "Iteration 4752: Weights = [55.30333333  2.71893899  6.51131889  0.10709404  0.20519813 12.18080088], Loss = 0.6400\n",
      "Iteration 4753: Weights = [55.30333333  2.71876331  6.51089818  0.10708712  0.20518487 12.18125409], Loss = 0.6399\n",
      "Iteration 4754: Weights = [55.30333333  2.71858765  6.5104775   0.1070802   0.20517162 12.18170727], Loss = 0.6399\n",
      "Iteration 4755: Weights = [55.30333333  2.71841199  6.51005684  0.10707328  0.20515836 12.18216042], Loss = 0.6398\n",
      "Iteration 4756: Weights = [55.30333333  2.71823635  6.50963622  0.10706637  0.2051451  12.18261355], Loss = 0.6397\n",
      "Iteration 4757: Weights = [55.30333333  2.71806072  6.50921562  0.10705945  0.20513185 12.18306664], Loss = 0.6396\n",
      "Iteration 4758: Weights = [55.30333333  2.7178851   6.50879504  0.10705253  0.2051186  12.18351971], Loss = 0.6395\n",
      "Iteration 4759: Weights = [55.30333333  2.71770949  6.5083745   0.10704561  0.20510534 12.18397274], Loss = 0.6395\n",
      "Iteration 4760: Weights = [55.30333333  2.7175339   6.50795398  0.1070387   0.20509209 12.18442575], Loss = 0.6394\n",
      "Iteration 4761: Weights = [55.30333333  2.71735831  6.50753349  0.10703178  0.20507884 12.18487873], Loss = 0.6393\n",
      "Iteration 4762: Weights = [55.30333333  2.71718274  6.50711302  0.10702487  0.20506559 12.18533168], Loss = 0.6392\n",
      "Iteration 4763: Weights = [55.30333333  2.71700718  6.50669258  0.10701795  0.20505234 12.1857846 ], Loss = 0.6391\n",
      "Iteration 4764: Weights = [55.30333333  2.71683162  6.50627217  0.10701104  0.20503909 12.18623749], Loss = 0.6390\n",
      "Iteration 4765: Weights = [55.30333333  2.71665608  6.50585179  0.10700412  0.20502584 12.18669035], Loss = 0.6390\n",
      "Iteration 4766: Weights = [55.30333333  2.71648056  6.50543143  0.10699721  0.20501259 12.18714318], Loss = 0.6389\n",
      "Iteration 4767: Weights = [55.30333333  2.71630504  6.5050111   0.1069903   0.20499935 12.18759598], Loss = 0.6388\n",
      "Iteration 4768: Weights = [55.30333333  2.71612953  6.5045908   0.10698338  0.2049861  12.18804875], Loss = 0.6387\n",
      "Iteration 4769: Weights = [55.30333333  2.71595404  6.50417053  0.10697647  0.20497286 12.1885015 ], Loss = 0.6386\n",
      "Iteration 4770: Weights = [55.30333333  2.71577855  6.50375028  0.10696956  0.20495962 12.18895421], Loss = 0.6385\n",
      "Iteration 4771: Weights = [55.30333333  2.71560308  6.50333006  0.10696265  0.20494637 12.1894069 ], Loss = 0.6385\n",
      "Iteration 4772: Weights = [55.30333333  2.71542762  6.50290987  0.10695574  0.20493313 12.18985955], Loss = 0.6384\n",
      "Iteration 4773: Weights = [55.30333333  2.71525217  6.5024897   0.10694883  0.20491989 12.19031218], Loss = 0.6383\n",
      "Iteration 4774: Weights = [55.30333333  2.71507673  6.50206956  0.10694191  0.20490665 12.19076478], Loss = 0.6382\n",
      "Iteration 4775: Weights = [55.30333333  2.71490131  6.50164945  0.10693501  0.20489341 12.19121734], Loss = 0.6381\n",
      "Iteration 4776: Weights = [55.30333333  2.71472589  6.50122936  0.1069281   0.20488017 12.19166988], Loss = 0.6380\n",
      "Iteration 4777: Weights = [55.30333333  2.71455049  6.50080931  0.10692119  0.20486693 12.19212239], Loss = 0.6380\n",
      "Iteration 4778: Weights = [55.30333333  2.7143751   6.50038928  0.10691428  0.2048537  12.19257487], Loss = 0.6379\n",
      "Iteration 4779: Weights = [55.30333333  2.71419972  6.49996927  0.10690737  0.20484046 12.19302732], Loss = 0.6378\n",
      "Iteration 4780: Weights = [55.30333333  2.71402435  6.4995493   0.10690046  0.20482723 12.19347975], Loss = 0.6377\n",
      "Iteration 4781: Weights = [55.30333333  2.71384899  6.49912935  0.10689356  0.20481399 12.19393214], Loss = 0.6376\n",
      "Iteration 4782: Weights = [55.30333333  2.71367364  6.49870943  0.10688665  0.20480076 12.1943845 ], Loss = 0.6376\n",
      "Iteration 4783: Weights = [55.30333333  2.7134983   6.49828953  0.10687974  0.20478752 12.19483684], Loss = 0.6375\n",
      "Iteration 4784: Weights = [55.30333333  2.71332298  6.49786966  0.10687284  0.20477429 12.19528914], Loss = 0.6374\n",
      "Iteration 4785: Weights = [55.30333333  2.71314767  6.49744982  0.10686593  0.20476106 12.19574142], Loss = 0.6373\n",
      "Iteration 4786: Weights = [55.30333333  2.71297236  6.49703001  0.10685903  0.20474783 12.19619366], Loss = 0.6372\n",
      "Iteration 4787: Weights = [55.30333333  2.71279707  6.49661022  0.10685212  0.2047346  12.19664588], Loss = 0.6371\n",
      "Iteration 4788: Weights = [55.30333333  2.71262179  6.49619047  0.10684522  0.20472137 12.19709807], Loss = 0.6371\n",
      "Iteration 4789: Weights = [55.30333333  2.71244653  6.49577073  0.10683832  0.20470815 12.19755023], Loss = 0.6370\n",
      "Iteration 4790: Weights = [55.30333333  2.71227127  6.49535103  0.10683141  0.20469492 12.19800236], Loss = 0.6369\n",
      "Iteration 4791: Weights = [55.30333333  2.71209602  6.49493135  0.10682451  0.20468169 12.19845446], Loss = 0.6368\n",
      "Iteration 4792: Weights = [55.30333333  2.71192079  6.4945117   0.10681761  0.20466847 12.19890653], Loss = 0.6367\n",
      "Iteration 4793: Weights = [55.30333333  2.71174557  6.49409208  0.10681071  0.20465525 12.19935857], Loss = 0.6366\n",
      "Iteration 4794: Weights = [55.30333333  2.71157036  6.49367248  0.10680381  0.20464202 12.19981058], Loss = 0.6366\n",
      "Iteration 4795: Weights = [55.30333333  2.71139516  6.49325291  0.1067969   0.2046288  12.20026257], Loss = 0.6365\n",
      "Iteration 4796: Weights = [55.30333333  2.71121997  6.49283337  0.10679     0.20461558 12.20071452], Loss = 0.6364\n",
      "Iteration 4797: Weights = [55.30333333  2.71104479  6.49241385  0.1067831   0.20460236 12.20116645], Loss = 0.6363\n",
      "Iteration 4798: Weights = [55.30333333  2.71086962  6.49199437  0.1067762   0.20458914 12.20161834], Loss = 0.6362\n",
      "Iteration 4799: Weights = [55.30333333  2.71069447  6.49157491  0.10676931  0.20457592 12.20207021], Loss = 0.6362\n",
      "Iteration 4800: Weights = [55.30333333  2.71051933  6.49115547  0.10676241  0.2045627  12.20252205], Loss = 0.6361\n",
      "Iteration 4801: Weights = [55.30333333  2.71034419  6.49073607  0.10675551  0.20454948 12.20297386], Loss = 0.6360\n",
      "Iteration 4802: Weights = [55.30333333  2.71016907  6.49031669  0.10674861  0.20453627 12.20342564], Loss = 0.6359\n",
      "Iteration 4803: Weights = [55.30333333  2.70999396  6.48989733  0.10674171  0.20452305 12.20387739], Loss = 0.6358\n",
      "Iteration 4804: Weights = [55.30333333  2.70981887  6.48947801  0.10673482  0.20450984 12.20432911], Loss = 0.6357\n",
      "Iteration 4805: Weights = [55.30333333  2.70964378  6.48905871  0.10672792  0.20449662 12.2047808 ], Loss = 0.6357\n",
      "Iteration 4806: Weights = [55.30333333  2.7094687   6.48863944  0.10672102  0.20448341 12.20523246], Loss = 0.6356\n",
      "Iteration 4807: Weights = [55.30333333  2.70929364  6.48822019  0.10671413  0.2044702  12.20568409], Loss = 0.6355\n",
      "Iteration 4808: Weights = [55.30333333  2.70911859  6.48780098  0.10670723  0.20445699 12.2061357 ], Loss = 0.6354\n",
      "Iteration 4809: Weights = [55.30333333  2.70894354  6.48738179  0.10670034  0.20444378 12.20658727], Loss = 0.6353\n",
      "Iteration 4810: Weights = [55.30333333  2.70876851  6.48696263  0.10669345  0.20443057 12.20703882], Loss = 0.6353\n",
      "Iteration 4811: Weights = [55.30333333  2.7085935   6.48654349  0.10668655  0.20441736 12.20749034], Loss = 0.6352\n",
      "Iteration 4812: Weights = [55.30333333  2.70841849  6.48612438  0.10667966  0.20440415 12.20794182], Loss = 0.6351\n",
      "Iteration 4813: Weights = [55.30333333  2.70824349  6.4857053   0.10667277  0.20439094 12.20839328], Loss = 0.6350\n",
      "Iteration 4814: Weights = [55.30333333  2.70806851  6.48528625  0.10666587  0.20437774 12.20884471], Loss = 0.6349\n",
      "Iteration 4815: Weights = [55.30333333  2.70789353  6.48486722  0.10665898  0.20436453 12.20929611], Loss = 0.6348\n",
      "Iteration 4816: Weights = [55.30333333  2.70771857  6.48444822  0.10665209  0.20435133 12.20974748], Loss = 0.6348\n",
      "Iteration 4817: Weights = [55.30333333  2.70754362  6.48402924  0.1066452   0.20433813 12.21019882], Loss = 0.6347\n",
      "Iteration 4818: Weights = [55.30333333  2.70736868  6.4836103   0.10663831  0.20432492 12.21065014], Loss = 0.6346\n",
      "Iteration 4819: Weights = [55.30333333  2.70719375  6.48319138  0.10663142  0.20431172 12.21110142], Loss = 0.6345\n",
      "Iteration 4820: Weights = [55.30333333  2.70701883  6.48277249  0.10662453  0.20429852 12.21155267], Loss = 0.6344\n",
      "Iteration 4821: Weights = [55.30333333  2.70684393  6.48235362  0.10661764  0.20428532 12.2120039 ], Loss = 0.6343\n",
      "Iteration 4822: Weights = [55.30333333  2.70666903  6.48193478  0.10661075  0.20427212 12.21245509], Loss = 0.6343\n",
      "Iteration 4823: Weights = [55.30333333  2.70649415  6.48151597  0.10660386  0.20425892 12.21290626], Loss = 0.6342\n",
      "Iteration 4824: Weights = [55.30333333  2.70631928  6.48109719  0.10659697  0.20424572 12.2133574 ], Loss = 0.6341\n",
      "Iteration 4825: Weights = [55.30333333  2.70614442  6.48067843  0.10659009  0.20423253 12.21380851], Loss = 0.6340\n",
      "Iteration 4826: Weights = [55.30333333  2.70596957  6.4802597   0.1065832   0.20421933 12.21425959], Loss = 0.6339\n",
      "Iteration 4827: Weights = [55.30333333  2.70579473  6.479841    0.10657631  0.20420614 12.21471064], Loss = 0.6339\n",
      "Iteration 4828: Weights = [55.30333333  2.7056199   6.47942232  0.10656943  0.20419294 12.21516166], Loss = 0.6338\n",
      "Iteration 4829: Weights = [55.30333333  2.70544509  6.47900368  0.10656254  0.20417975 12.21561265], Loss = 0.6337\n",
      "Iteration 4830: Weights = [55.30333333  2.70527028  6.47858505  0.10655566  0.20416656 12.21606361], Loss = 0.6336\n",
      "Iteration 4831: Weights = [55.30333333  2.70509549  6.47816646  0.10654877  0.20415336 12.21651455], Loss = 0.6335\n",
      "Iteration 4832: Weights = [55.30333333  2.70492071  6.47774789  0.10654189  0.20414017 12.21696545], Loss = 0.6334\n",
      "Iteration 4833: Weights = [55.30333333  2.70474594  6.47732935  0.106535    0.20412698 12.21741632], Loss = 0.6334\n",
      "Iteration 4834: Weights = [55.30333333  2.70457118  6.47691084  0.10652812  0.2041138  12.21786717], Loss = 0.6333\n",
      "Iteration 4835: Weights = [55.30333333  2.70439643  6.47649235  0.10652124  0.20410061 12.21831799], Loss = 0.6332\n",
      "Iteration 4836: Weights = [55.30333333  2.70422169  6.47607389  0.10651435  0.20408742 12.21876878], Loss = 0.6331\n",
      "Iteration 4837: Weights = [55.30333333  2.70404697  6.47565546  0.10650747  0.20407423 12.21921953], Loss = 0.6330\n",
      "Iteration 4838: Weights = [55.30333333  2.70387225  6.47523706  0.10650059  0.20406105 12.21967026], Loss = 0.6330\n",
      "Iteration 4839: Weights = [55.30333333  2.70369755  6.47481868  0.10649371  0.20404786 12.22012096], Loss = 0.6329\n",
      "Iteration 4840: Weights = [55.30333333  2.70352286  6.47440033  0.10648683  0.20403468 12.22057164], Loss = 0.6328\n",
      "Iteration 4841: Weights = [55.30333333  2.70334818  6.473982    0.10647995  0.2040215  12.22102228], Loss = 0.6327\n",
      "Iteration 4842: Weights = [55.30333333  2.70317351  6.47356371  0.10647307  0.20400831 12.22147289], Loss = 0.6326\n",
      "Iteration 4843: Weights = [55.30333333  2.70299885  6.47314544  0.10646619  0.20399513 12.22192348], Loss = 0.6325\n",
      "Iteration 4844: Weights = [55.30333333  2.70282421  6.47272719  0.10645931  0.20398195 12.22237403], Loss = 0.6325\n",
      "Iteration 4845: Weights = [55.30333333  2.70264957  6.47230898  0.10645243  0.20396877 12.22282456], Loss = 0.6324\n",
      "Iteration 4846: Weights = [55.30333333  2.70247495  6.47189079  0.10644555  0.20395559 12.22327505], Loss = 0.6323\n",
      "Iteration 4847: Weights = [55.30333333  2.70230034  6.47147263  0.10643868  0.20394241 12.22372552], Loss = 0.6322\n",
      "Iteration 4848: Weights = [55.30333333  2.70212574  6.47105449  0.1064318   0.20392924 12.22417596], Loss = 0.6321\n",
      "Iteration 4849: Weights = [55.30333333  2.70195115  6.47063638  0.10642492  0.20391606 12.22462637], Loss = 0.6321\n",
      "Iteration 4850: Weights = [55.30333333  2.70177657  6.4702183   0.10641805  0.20390289 12.22507675], Loss = 0.6320\n",
      "Iteration 4851: Weights = [55.30333333  2.701602    6.46980025  0.10641117  0.20388971 12.2255271 ], Loss = 0.6319\n",
      "Iteration 4852: Weights = [55.30333333  2.70142745  6.46938222  0.10640429  0.20387654 12.22597742], Loss = 0.6318\n",
      "Iteration 4853: Weights = [55.30333333  2.7012529   6.46896422  0.10639742  0.20386336 12.22642771], Loss = 0.6317\n",
      "Iteration 4854: Weights = [55.30333333  2.70107837  6.46854625  0.10639054  0.20385019 12.22687798], Loss = 0.6316\n",
      "Iteration 4855: Weights = [55.30333333  2.70090385  6.4681283   0.10638367  0.20383702 12.22732821], Loss = 0.6316\n",
      "Iteration 4856: Weights = [55.30333333  2.70072933  6.46771039  0.1063768   0.20382385 12.22777842], Loss = 0.6315\n",
      "Iteration 4857: Weights = [55.30333333  2.70055483  6.46729249  0.10636992  0.20381068 12.2282286 ], Loss = 0.6314\n",
      "Iteration 4858: Weights = [55.30333333  2.70038035  6.46687463  0.10636305  0.20379751 12.22867874], Loss = 0.6313\n",
      "Iteration 4859: Weights = [55.30333333  2.70020587  6.46645679  0.10635618  0.20378435 12.22912886], Loss = 0.6312\n",
      "Iteration 4860: Weights = [55.30333333  2.7000314   6.46603898  0.10634931  0.20377118 12.22957895], Loss = 0.6312\n",
      "Iteration 4861: Weights = [55.30333333  2.69985695  6.4656212   0.10634244  0.20375801 12.23002901], Loss = 0.6311\n",
      "Iteration 4862: Weights = [55.30333333  2.69968251  6.46520344  0.10633556  0.20374485 12.23047904], Loss = 0.6310\n",
      "Iteration 4863: Weights = [55.30333333  2.69950807  6.46478571  0.10632869  0.20373168 12.23092904], Loss = 0.6309\n",
      "Iteration 4864: Weights = [55.30333333  2.69933365  6.46436801  0.10632182  0.20371852 12.23137902], Loss = 0.6308\n",
      "Iteration 4865: Weights = [55.30333333  2.69915924  6.46395033  0.10631495  0.20370536 12.23182896], Loss = 0.6308\n",
      "Iteration 4866: Weights = [55.30333333  2.69898485  6.46353268  0.10630808  0.2036922  12.23227888], Loss = 0.6307\n",
      "Iteration 4867: Weights = [55.30333333  2.69881046  6.46311506  0.10630122  0.20367903 12.23272876], Loss = 0.6306\n",
      "Iteration 4868: Weights = [55.30333333  2.69863608  6.46269747  0.10629435  0.20366587 12.23317862], Loss = 0.6305\n",
      "Iteration 4869: Weights = [55.30333333  2.69846172  6.4622799   0.10628748  0.20365271 12.23362845], Loss = 0.6304\n",
      "Iteration 4870: Weights = [55.30333333  2.69828737  6.46186236  0.10628061  0.20363956 12.23407824], Loss = 0.6303\n",
      "Iteration 4871: Weights = [55.30333333  2.69811302  6.46144484  0.10627375  0.2036264  12.23452801], Loss = 0.6303\n",
      "Iteration 4872: Weights = [55.30333333  2.69793869  6.46102736  0.10626688  0.20361324 12.23497776], Loss = 0.6302\n",
      "Iteration 4873: Weights = [55.30333333  2.69776437  6.4606099   0.10626001  0.20360009 12.23542747], Loss = 0.6301\n",
      "Iteration 4874: Weights = [55.30333333  2.69759007  6.46019246  0.10625315  0.20358693 12.23587715], Loss = 0.6300\n",
      "Iteration 4875: Weights = [55.30333333  2.69741577  6.45977506  0.10624628  0.20357378 12.2363268 ], Loss = 0.6299\n",
      "Iteration 4876: Weights = [55.30333333  2.69724148  6.45935768  0.10623942  0.20356062 12.23677643], Loss = 0.6299\n",
      "Iteration 4877: Weights = [55.30333333  2.69706721  6.45894033  0.10623255  0.20354747 12.23722602], Loss = 0.6298\n",
      "Iteration 4878: Weights = [55.30333333  2.69689295  6.458523    0.10622569  0.20353432 12.23767559], Loss = 0.6297\n",
      "Iteration 4879: Weights = [55.30333333  2.6967187   6.4581057   0.10621883  0.20352117 12.23812512], Loss = 0.6296\n",
      "Iteration 4880: Weights = [55.30333333  2.69654446  6.45768843  0.10621196  0.20350802 12.23857463], Loss = 0.6295\n",
      "Iteration 4881: Weights = [55.30333333  2.69637023  6.45727119  0.1062051   0.20349487 12.23902411], Loss = 0.6294\n",
      "Iteration 4882: Weights = [55.30333333  2.69619601  6.45685397  0.10619824  0.20348172 12.23947356], Loss = 0.6294\n",
      "Iteration 4883: Weights = [55.30333333  2.6960218   6.45643678  0.10619138  0.20346857 12.23992298], Loss = 0.6293\n",
      "Iteration 4884: Weights = [55.30333333  2.69584761  6.45601962  0.10618451  0.20345543 12.24037237], Loss = 0.6292\n",
      "Iteration 4885: Weights = [55.30333333  2.69567342  6.45560248  0.10617765  0.20344228 12.24082174], Loss = 0.6291\n",
      "Iteration 4886: Weights = [55.30333333  2.69549925  6.45518537  0.10617079  0.20342914 12.24127107], Loss = 0.6290\n",
      "Iteration 4887: Weights = [55.30333333  2.69532509  6.45476829  0.10616393  0.20341599 12.24172038], Loss = 0.6290\n",
      "Iteration 4888: Weights = [55.30333333  2.69515094  6.45435123  0.10615707  0.20340285 12.24216965], Loss = 0.6289\n",
      "Iteration 4889: Weights = [55.30333333  2.6949768   6.4539342   0.10615022  0.20338971 12.2426189 ], Loss = 0.6288\n",
      "Iteration 4890: Weights = [55.30333333  2.69480267  6.4535172   0.10614336  0.20337657 12.24306812], Loss = 0.6287\n",
      "Iteration 4891: Weights = [55.30333333  2.69462855  6.45310023  0.1061365   0.20336343 12.24351731], Loss = 0.6286\n",
      "Iteration 4892: Weights = [55.30333333  2.69445445  6.45268328  0.10612964  0.20335029 12.24396647], Loss = 0.6286\n",
      "Iteration 4893: Weights = [55.30333333  2.69428035  6.45226636  0.10612278  0.20333715 12.2444156 ], Loss = 0.6285\n",
      "Iteration 4894: Weights = [55.30333333  2.69410627  6.45184947  0.10611593  0.20332401 12.2448647 ], Loss = 0.6284\n",
      "Iteration 4895: Weights = [55.30333333  2.6939322   6.4514326   0.10610907  0.20331087 12.24531377], Loss = 0.6283\n",
      "Iteration 4896: Weights = [55.30333333  2.69375814  6.45101576  0.10610221  0.20329774 12.24576281], Loss = 0.6282\n",
      "Iteration 4897: Weights = [55.30333333  2.69358409  6.45059895  0.10609536  0.2032846  12.24621183], Loss = 0.6281\n",
      "Iteration 4898: Weights = [55.30333333  2.69341005  6.45018216  0.1060885   0.20327147 12.24666081], Loss = 0.6281\n",
      "Iteration 4899: Weights = [55.30333333  2.69323603  6.4497654   0.10608165  0.20325833 12.24710977], Loss = 0.6280\n",
      "Iteration 4900: Weights = [55.30333333  2.69306201  6.44934867  0.1060748   0.2032452  12.2475587 ], Loss = 0.6279\n",
      "Iteration 4901: Weights = [55.30333333  2.69288801  6.44893196  0.10606794  0.20323207 12.2480076 ], Loss = 0.6278\n",
      "Iteration 4902: Weights = [55.30333333  2.69271401  6.44851528  0.10606109  0.20321894 12.24845647], Loss = 0.6277\n",
      "Iteration 4903: Weights = [55.30333333  2.69254003  6.44809863  0.10605424  0.20320581 12.24890531], Loss = 0.6277\n",
      "Iteration 4904: Weights = [55.30333333  2.69236606  6.44768201  0.10604738  0.20319268 12.24935412], Loss = 0.6276\n",
      "Iteration 4905: Weights = [55.30333333  2.6921921   6.44726541  0.10604053  0.20317955 12.2498029 ], Loss = 0.6275\n",
      "Iteration 4906: Weights = [55.30333333  2.69201815  6.44684884  0.10603368  0.20316642 12.25025165], Loss = 0.6274\n",
      "Iteration 4907: Weights = [55.30333333  2.69184422  6.4464323   0.10602683  0.20315329 12.25070038], Loss = 0.6273\n",
      "Iteration 4908: Weights = [55.30333333  2.69167029  6.44601578  0.10601998  0.20314017 12.25114907], Loss = 0.6273\n",
      "Iteration 4909: Weights = [55.30333333  2.69149638  6.44559929  0.10601313  0.20312704 12.25159774], Loss = 0.6272\n",
      "Iteration 4910: Weights = [55.30333333  2.69132247  6.44518283  0.10600628  0.20311392 12.25204638], Loss = 0.6271\n",
      "Iteration 4911: Weights = [55.30333333  2.69114858  6.44476639  0.10599943  0.20310079 12.25249499], Loss = 0.6270\n",
      "Iteration 4912: Weights = [55.30333333  2.6909747   6.44434998  0.10599258  0.20308767 12.25294357], Loss = 0.6269\n",
      "Iteration 4913: Weights = [55.30333333  2.69080083  6.4439336   0.10598573  0.20307455 12.25339212], Loss = 0.6269\n",
      "Iteration 4914: Weights = [55.30333333  2.69062698  6.44351724  0.10597888  0.20306143 12.25384064], Loss = 0.6268\n",
      "Iteration 4915: Weights = [55.30333333  2.69045313  6.44310091  0.10597204  0.20304831 12.25428913], Loss = 0.6267\n",
      "Iteration 4916: Weights = [55.30333333  2.69027929  6.44268461  0.10596519  0.20303519 12.2547376 ], Loss = 0.6266\n",
      "Iteration 4917: Weights = [55.30333333  2.69010547  6.44226834  0.10595834  0.20302207 12.25518603], Loss = 0.6265\n",
      "Iteration 4918: Weights = [55.30333333  2.68993166  6.44185209  0.1059515   0.20300895 12.25563444], Loss = 0.6264\n",
      "Iteration 4919: Weights = [55.30333333  2.68975785  6.44143587  0.10594465  0.20299583 12.25608281], Loss = 0.6264\n",
      "Iteration 4920: Weights = [55.30333333  2.68958406  6.44101967  0.10593781  0.20298272 12.25653116], Loss = 0.6263\n",
      "Iteration 4921: Weights = [55.30333333  2.68941028  6.44060351  0.10593096  0.2029696  12.25697948], Loss = 0.6262\n",
      "Iteration 4922: Weights = [55.30333333  2.68923652  6.44018737  0.10592412  0.20295649 12.25742777], Loss = 0.6261\n",
      "Iteration 4923: Weights = [55.30333333  2.68906276  6.43977125  0.10591727  0.20294338 12.25787603], Loss = 0.6260\n",
      "Iteration 4924: Weights = [55.30333333  2.68888901  6.43935517  0.10591043  0.20293026 12.25832426], Loss = 0.6260\n",
      "Iteration 4925: Weights = [55.30333333  2.68871528  6.43893911  0.10590359  0.20291715 12.25877246], Loss = 0.6259\n",
      "Iteration 4926: Weights = [55.30333333  2.68854155  6.43852307  0.10589674  0.20290404 12.25922064], Loss = 0.6258\n",
      "Iteration 4927: Weights = [55.30333333  2.68836784  6.43810707  0.1058899   0.20289093 12.25966878], Loss = 0.6257\n",
      "Iteration 4928: Weights = [55.30333333  2.68819414  6.43769109  0.10588306  0.20287782 12.2601169 ], Loss = 0.6256\n",
      "Iteration 4929: Weights = [55.30333333  2.68802045  6.43727514  0.10587622  0.20286471 12.26056499], Loss = 0.6256\n",
      "Iteration 4930: Weights = [55.30333333  2.68784677  6.43685921  0.10586938  0.20285161 12.26101305], Loss = 0.6255\n",
      "Iteration 4931: Weights = [55.30333333  2.68767311  6.43644331  0.10586254  0.2028385  12.26146107], Loss = 0.6254\n",
      "Iteration 4932: Weights = [55.30333333  2.68749945  6.43602744  0.1058557   0.20282539 12.26190907], Loss = 0.6253\n",
      "Iteration 4933: Weights = [55.30333333  2.68732581  6.4356116   0.10584886  0.20281229 12.26235705], Loss = 0.6252\n",
      "Iteration 4934: Weights = [55.30333333  2.68715217  6.43519578  0.10584202  0.20279918 12.26280499], Loss = 0.6252\n",
      "Iteration 4935: Weights = [55.30333333  2.68697855  6.43477999  0.10583518  0.20278608 12.2632529 ], Loss = 0.6251\n",
      "Iteration 4936: Weights = [55.30333333  2.68680494  6.43436422  0.10582834  0.20277298 12.26370079], Loss = 0.6250\n",
      "Iteration 4937: Weights = [55.30333333  2.68663134  6.43394849  0.1058215   0.20275988 12.26414864], Loss = 0.6249\n",
      "Iteration 4938: Weights = [55.30333333  2.68645775  6.43353278  0.10581467  0.20274678 12.26459647], Loss = 0.6248\n",
      "Iteration 4939: Weights = [55.30333333  2.68628417  6.43311709  0.10580783  0.20273368 12.26504427], Loss = 0.6247\n",
      "Iteration 4940: Weights = [55.30333333  2.68611061  6.43270144  0.10580099  0.20272058 12.26549204], Loss = 0.6247\n",
      "Iteration 4941: Weights = [55.30333333  2.68593705  6.43228581  0.10579416  0.20270748 12.26593978], Loss = 0.6246\n",
      "Iteration 4942: Weights = [55.30333333  2.68576351  6.4318702   0.10578732  0.20269438 12.26638749], Loss = 0.6245\n",
      "Iteration 4943: Weights = [55.30333333  2.68558998  6.43145463  0.10578049  0.20268129 12.26683517], Loss = 0.6244\n",
      "Iteration 4944: Weights = [55.30333333  2.68541645  6.43103908  0.10577365  0.20266819 12.26728282], Loss = 0.6243\n",
      "Iteration 4945: Weights = [55.30333333  2.68524294  6.43062356  0.10576682  0.20265509 12.26773045], Loss = 0.6243\n",
      "Iteration 4946: Weights = [55.30333333  2.68506944  6.43020806  0.10575998  0.202642   12.26817804], Loss = 0.6242\n",
      "Iteration 4947: Weights = [55.30333333  2.68489596  6.42979259  0.10575315  0.20262891 12.26862561], Loss = 0.6241\n",
      "Iteration 4948: Weights = [55.30333333  2.68472248  6.42937715  0.10574632  0.20261582 12.26907314], Loss = 0.6240\n",
      "Iteration 4949: Weights = [55.30333333  2.68454902  6.42896174  0.10573948  0.20260272 12.26952065], Loss = 0.6239\n",
      "Iteration 4950: Weights = [55.30333333  2.68437556  6.42854635  0.10573265  0.20258963 12.26996813], Loss = 0.6239\n",
      "Iteration 4951: Weights = [55.30333333  2.68420212  6.42813099  0.10572582  0.20257654 12.27041558], Loss = 0.6238\n",
      "Iteration 4952: Weights = [55.30333333  2.68402869  6.42771565  0.10571899  0.20256345 12.270863  ], Loss = 0.6237\n",
      "Iteration 4953: Weights = [55.30333333  2.68385527  6.42730034  0.10571216  0.20255037 12.2713104 ], Loss = 0.6236\n",
      "Iteration 4954: Weights = [55.30333333  2.68368186  6.42688506  0.10570533  0.20253728 12.27175776], Loss = 0.6235\n",
      "Iteration 4955: Weights = [55.30333333  2.68350846  6.42646981  0.1056985   0.20252419 12.2722051 ], Loss = 0.6235\n",
      "Iteration 4956: Weights = [55.30333333  2.68333507  6.42605458  0.10569167  0.20251111 12.2726524 ], Loss = 0.6234\n",
      "Iteration 4957: Weights = [55.30333333  2.6831617   6.42563938  0.10568484  0.20249802 12.27309968], Loss = 0.6233\n",
      "Iteration 4958: Weights = [55.30333333  2.68298833  6.42522421  0.10567801  0.20248494 12.27354693], Loss = 0.6232\n",
      "Iteration 4959: Weights = [55.30333333  2.68281498  6.42480906  0.10567118  0.20247186 12.27399415], Loss = 0.6231\n",
      "Iteration 4960: Weights = [55.30333333  2.68264164  6.42439394  0.10566436  0.20245877 12.27444134], Loss = 0.6231\n",
      "Iteration 4961: Weights = [55.30333333  2.68246831  6.42397885  0.10565753  0.20244569 12.2748885 ], Loss = 0.6230\n",
      "Iteration 4962: Weights = [55.30333333  2.68229499  6.42356378  0.1056507   0.20243261 12.27533563], Loss = 0.6229\n",
      "Iteration 4963: Weights = [55.30333333  2.68212168  6.42314874  0.10564388  0.20241953 12.27578274], Loss = 0.6228\n",
      "Iteration 4964: Weights = [55.30333333  2.68194838  6.42273373  0.10563705  0.20240645 12.27622981], Loss = 0.6227\n",
      "Iteration 4965: Weights = [55.30333333  2.68177509  6.42231874  0.10563022  0.20239338 12.27667686], Loss = 0.6227\n",
      "Iteration 4966: Weights = [55.30333333  2.68160182  6.42190379  0.1056234   0.2023803  12.27712387], Loss = 0.6226\n",
      "Iteration 4967: Weights = [55.30333333  2.68142856  6.42148885  0.10561657  0.20236722 12.27757086], Loss = 0.6225\n",
      "Iteration 4968: Weights = [55.30333333  2.6812553   6.42107395  0.10560975  0.20235415 12.27801782], Loss = 0.6224\n",
      "Iteration 4969: Weights = [55.30333333  2.68108206  6.42065907  0.10560293  0.20234107 12.27846475], Loss = 0.6223\n",
      "Iteration 4970: Weights = [55.30333333  2.68090883  6.42024422  0.1055961   0.202328   12.27891165], Loss = 0.6223\n",
      "Iteration 4971: Weights = [55.30333333  2.68073561  6.41982939  0.10558928  0.20231493 12.27935853], Loss = 0.6222\n",
      "Iteration 4972: Weights = [55.30333333  2.6805624   6.4194146   0.10558246  0.20230185 12.27980537], Loss = 0.6221\n",
      "Iteration 4973: Weights = [55.30333333  2.68038921  6.41899982  0.10557564  0.20228878 12.28025219], Loss = 0.6220\n",
      "Iteration 4974: Weights = [55.30333333  2.68021602  6.41858508  0.10556882  0.20227571 12.28069897], Loss = 0.6219\n",
      "Iteration 4975: Weights = [55.30333333  2.68004285  6.41817036  0.10556199  0.20226264 12.28114573], Loss = 0.6218\n",
      "Iteration 4976: Weights = [55.30333333  2.67986969  6.41775567  0.10555517  0.20224958 12.28159246], Loss = 0.6218\n",
      "Iteration 4977: Weights = [55.30333333  2.67969653  6.41734101  0.10554835  0.20223651 12.28203916], Loss = 0.6217\n",
      "Iteration 4978: Weights = [55.30333333  2.67952339  6.41692637  0.10554153  0.20222344 12.28248583], Loss = 0.6216\n",
      "Iteration 4979: Weights = [55.30333333  2.67935026  6.41651176  0.10553471  0.20221037 12.28293247], Loss = 0.6215\n",
      "Iteration 4980: Weights = [55.30333333  2.67917715  6.41609718  0.1055279   0.20219731 12.28337908], Loss = 0.6214\n",
      "Iteration 4981: Weights = [55.30333333  2.67900404  6.41568262  0.10552108  0.20218424 12.28382567], Loss = 0.6214\n",
      "Iteration 4982: Weights = [55.30333333  2.67883094  6.41526809  0.10551426  0.20217118 12.28427222], Loss = 0.6213\n",
      "Iteration 4983: Weights = [55.30333333  2.67865786  6.41485359  0.10550744  0.20215812 12.28471875], Loss = 0.6212\n",
      "Iteration 4984: Weights = [55.30333333  2.67848479  6.41443911  0.10550062  0.20214506 12.28516525], Loss = 0.6211\n",
      "Iteration 4985: Weights = [55.30333333  2.67831172  6.41402466  0.10549381  0.202132   12.28561171], Loss = 0.6210\n",
      "Iteration 4986: Weights = [55.30333333  2.67813867  6.41361024  0.10548699  0.20211894 12.28605815], Loss = 0.6210\n",
      "Iteration 4987: Weights = [55.30333333  2.67796563  6.41319584  0.10548018  0.20210588 12.28650457], Loss = 0.6209\n",
      "Iteration 4988: Weights = [55.30333333  2.6777926   6.41278147  0.10547336  0.20209282 12.28695095], Loss = 0.6208\n",
      "Iteration 4989: Weights = [55.30333333  2.67761959  6.41236713  0.10546655  0.20207976 12.2873973 ], Loss = 0.6207\n",
      "Iteration 4990: Weights = [55.30333333  2.67744658  6.41195281  0.10545973  0.2020667  12.28784363], Loss = 0.6206\n",
      "Iteration 4991: Weights = [55.30333333  2.67727358  6.41153852  0.10545292  0.20205365 12.28828992], Loss = 0.6206\n",
      "Iteration 4992: Weights = [55.30333333  2.6771006   6.41112426  0.1054461   0.20204059 12.28873619], Loss = 0.6205\n",
      "Iteration 4993: Weights = [55.30333333  2.67692763  6.41071002  0.10543929  0.20202754 12.28918243], Loss = 0.6204\n",
      "Iteration 4994: Weights = [55.30333333  2.67675467  6.41029582  0.10543248  0.20201448 12.28962864], Loss = 0.6203\n",
      "Iteration 4995: Weights = [55.30333333  2.67658171  6.40988163  0.10542567  0.20200143 12.29007482], Loss = 0.6202\n",
      "Iteration 4996: Weights = [55.30333333  2.67640878  6.40946748  0.10541885  0.20198838 12.29052097], Loss = 0.6202\n",
      "Iteration 4997: Weights = [55.30333333  2.67623585  6.40905335  0.10541204  0.20197533 12.29096709], Loss = 0.6201\n",
      "Iteration 4998: Weights = [55.30333333  2.67606293  6.40863925  0.10540523  0.20196228 12.29141318], Loss = 0.6200\n",
      "Iteration 4999: Weights = [55.30333333  2.67589002  6.40822517  0.10539842  0.20194923 12.29185925], Loss = 0.6199\n",
      "Iteration 5000: Weights = [55.30333333  2.67571713  6.40781112  0.10539161  0.20193618 12.29230528], Loss = 0.6198\n",
      "Iteration 5001: Weights = [55.30333333  2.67554425  6.4073971   0.1053848   0.20192313 12.29275129], Loss = 0.6198\n",
      "Iteration 5002: Weights = [55.30333333  2.67537137  6.40698311  0.10537799  0.20191009 12.29319727], Loss = 0.6197\n",
      "Iteration 5003: Weights = [55.30333333  2.67519851  6.40656914  0.10537118  0.20189704 12.29364322], Loss = 0.6196\n",
      "Iteration 5004: Weights = [55.30333333  2.67502566  6.4061552   0.10536438  0.201884   12.29408914], Loss = 0.6195\n",
      "Iteration 5005: Weights = [55.30333333  2.67485282  6.40574128  0.10535757  0.20187095 12.29453503], Loss = 0.6194\n",
      "Iteration 5006: Weights = [55.30333333  2.67468     6.4053274   0.10535076  0.20185791 12.2949809 ], Loss = 0.6194\n",
      "Iteration 5007: Weights = [55.30333333  2.67450718  6.40491353  0.10534395  0.20184487 12.29542673], Loss = 0.6193\n",
      "Iteration 5008: Weights = [55.30333333  2.67433437  6.4044997   0.10533715  0.20183183 12.29587254], Loss = 0.6192\n",
      "Iteration 5009: Weights = [55.30333333  2.67416158  6.40408589  0.10533034  0.20181878 12.29631831], Loss = 0.6191\n",
      "Iteration 5010: Weights = [55.30333333  2.6739888   6.40367211  0.10532354  0.20180575 12.29676406], Loss = 0.6190\n",
      "Iteration 5011: Weights = [55.30333333  2.67381603  6.40325836  0.10531673  0.20179271 12.29720978], Loss = 0.6190\n",
      "Iteration 5012: Weights = [55.30333333  2.67364327  6.40284463  0.10530993  0.20177967 12.29765547], Loss = 0.6189\n",
      "Iteration 5013: Weights = [55.30333333  2.67347052  6.40243093  0.10530312  0.20176663 12.29810113], Loss = 0.6188\n",
      "Iteration 5014: Weights = [55.30333333  2.67329778  6.40201726  0.10529632  0.20175359 12.29854677], Loss = 0.6187\n",
      "Iteration 5015: Weights = [55.30333333  2.67312505  6.40160361  0.10528951  0.20174056 12.29899237], Loss = 0.6186\n",
      "Iteration 5016: Weights = [55.30333333  2.67295233  6.40118999  0.10528271  0.20172752 12.29943795], Loss = 0.6186\n",
      "Iteration 5017: Weights = [55.30333333  2.67277963  6.40077639  0.10527591  0.20171449 12.29988349], Loss = 0.6185\n",
      "Iteration 5018: Weights = [55.30333333  2.67260694  6.40036283  0.10526911  0.20170146 12.30032901], Loss = 0.6184\n",
      "Iteration 5019: Weights = [55.30333333  2.67243425  6.39994929  0.10526231  0.20168842 12.3007745 ], Loss = 0.6183\n",
      "Iteration 5020: Weights = [55.30333333  2.67226158  6.39953577  0.1052555   0.20167539 12.30121996], Loss = 0.6182\n",
      "Iteration 5021: Weights = [55.30333333  2.67208892  6.39912229  0.1052487   0.20166236 12.30166539], Loss = 0.6182\n",
      "Iteration 5022: Weights = [55.30333333  2.67191627  6.39870883  0.1052419   0.20164933 12.30211079], Loss = 0.6181\n",
      "Iteration 5023: Weights = [55.30333333  2.67174364  6.39829539  0.1052351   0.2016363  12.30255617], Loss = 0.6180\n",
      "Iteration 5024: Weights = [55.30333333  2.67157101  6.39788199  0.1052283   0.20162327 12.30300151], Loss = 0.6179\n",
      "Iteration 5025: Weights = [55.30333333  2.67139839  6.39746861  0.1052215   0.20161025 12.30344683], Loss = 0.6178\n",
      "Iteration 5026: Weights = [55.30333333  2.67122579  6.39705525  0.10521471  0.20159722 12.30389212], Loss = 0.6178\n",
      "Iteration 5027: Weights = [55.30333333  2.6710532   6.39664193  0.10520791  0.2015842  12.30433737], Loss = 0.6177\n",
      "Iteration 5028: Weights = [55.30333333  2.67088061  6.39622863  0.10520111  0.20157117 12.3047826 ], Loss = 0.6176\n",
      "Iteration 5029: Weights = [55.30333333  2.67070804  6.39581535  0.10519431  0.20155815 12.30522781], Loss = 0.6175\n",
      "Iteration 5030: Weights = [55.30333333  2.67053548  6.39540211  0.10518752  0.20154512 12.30567298], Loss = 0.6174\n",
      "Iteration 5031: Weights = [55.30333333  2.67036293  6.39498889  0.10518072  0.2015321  12.30611812], Loss = 0.6174\n",
      "Iteration 5032: Weights = [55.30333333  2.6701904   6.39457569  0.10517392  0.20151908 12.30656324], Loss = 0.6173\n",
      "Iteration 5033: Weights = [55.30333333  2.67001787  6.39416253  0.10516713  0.20150606 12.30700832], Loss = 0.6172\n",
      "Iteration 5034: Weights = [55.30333333  2.66984535  6.39374939  0.10516033  0.20149304 12.30745338], Loss = 0.6171\n",
      "Iteration 5035: Weights = [55.30333333  2.66967285  6.39333628  0.10515354  0.20148002 12.30789841], Loss = 0.6170\n",
      "Iteration 5036: Weights = [55.30333333  2.66950036  6.39292319  0.10514674  0.201467   12.30834341], Loss = 0.6170\n",
      "Iteration 5037: Weights = [55.30333333  2.66932788  6.39251013  0.10513995  0.20145399 12.30878838], Loss = 0.6169\n",
      "Iteration 5038: Weights = [55.30333333  2.66915541  6.3920971   0.10513316  0.20144097 12.30923332], Loss = 0.6168\n",
      "Iteration 5039: Weights = [55.30333333  2.66898295  6.39168409  0.10512636  0.20142795 12.30967824], Loss = 0.6167\n",
      "Iteration 5040: Weights = [55.30333333  2.6688105   6.39127111  0.10511957  0.20141494 12.31012312], Loss = 0.6166\n",
      "Iteration 5041: Weights = [55.30333333  2.66863806  6.39085816  0.10511278  0.20140193 12.31056798], Loss = 0.6166\n",
      "Iteration 5042: Weights = [55.30333333  2.66846563  6.39044523  0.10510599  0.20138891 12.3110128 ], Loss = 0.6165\n",
      "Iteration 5043: Weights = [55.30333333  2.66829322  6.39003233  0.1050992   0.2013759  12.3114576 ], Loss = 0.6164\n",
      "Iteration 5044: Weights = [55.30333333  2.66812082  6.38961946  0.10509241  0.20136289 12.31190237], Loss = 0.6163\n",
      "Iteration 5045: Weights = [55.30333333  2.66794842  6.38920661  0.10508562  0.20134988 12.31234711], Loss = 0.6162\n",
      "Iteration 5046: Weights = [55.30333333  2.66777604  6.38879379  0.10507883  0.20133687 12.31279183], Loss = 0.6162\n",
      "Iteration 5047: Weights = [55.30333333  2.66760367  6.388381    0.10507204  0.20132386 12.31323651], Loss = 0.6161\n",
      "Iteration 5048: Weights = [55.30333333  2.66743131  6.38796823  0.10506525  0.20131085 12.31368117], Loss = 0.6160\n",
      "Iteration 5049: Weights = [55.30333333  2.66725896  6.3875555   0.10505846  0.20129784 12.31412579], Loss = 0.6159\n",
      "Iteration 5050: Weights = [55.30333333  2.66708663  6.38714278  0.10505167  0.20128484 12.31457039], Loss = 0.6159\n",
      "Iteration 5051: Weights = [55.30333333  2.6669143   6.3867301   0.10504488  0.20127183 12.31501496], Loss = 0.6158\n",
      "Iteration 5052: Weights = [55.30333333  2.66674199  6.38631744  0.1050381   0.20125883 12.3154595 ], Loss = 0.6157\n",
      "Iteration 5053: Weights = [55.30333333  2.66656968  6.3859048   0.10503131  0.20124582 12.31590401], Loss = 0.6156\n",
      "Iteration 5054: Weights = [55.30333333  2.66639739  6.3854922   0.10502452  0.20123282 12.31634849], Loss = 0.6155\n",
      "Iteration 5055: Weights = [55.30333333  2.66622511  6.38507962  0.10501774  0.20121982 12.31679295], Loss = 0.6155\n",
      "Iteration 5056: Weights = [55.30333333  2.66605284  6.38466707  0.10501095  0.20120682 12.31723737], Loss = 0.6154\n",
      "Iteration 5057: Weights = [55.30333333  2.66588058  6.38425454  0.10500417  0.20119382 12.31768177], Loss = 0.6153\n",
      "Iteration 5058: Weights = [55.30333333  2.66570833  6.38384204  0.10499738  0.20118082 12.31812614], Loss = 0.6152\n",
      "Iteration 5059: Weights = [55.30333333  2.66553609  6.38342957  0.1049906   0.20116782 12.31857047], Loss = 0.6151\n",
      "Iteration 5060: Weights = [55.30333333  2.66536387  6.38301712  0.10498382  0.20115482 12.31901478], Loss = 0.6151\n",
      "Iteration 5061: Weights = [55.30333333  2.66519165  6.3826047   0.10497703  0.20114183 12.31945907], Loss = 0.6150\n",
      "Iteration 5062: Weights = [55.30333333  2.66501945  6.38219231  0.10497025  0.20112883 12.31990332], Loss = 0.6149\n",
      "Iteration 5063: Weights = [55.30333333  2.66484726  6.38177994  0.10496347  0.20111583 12.32034754], Loss = 0.6148\n",
      "Iteration 5064: Weights = [55.30333333  2.66467508  6.3813676   0.10495669  0.20110284 12.32079174], Loss = 0.6147\n",
      "Iteration 5065: Weights = [55.30333333  2.66450291  6.38095529  0.1049499   0.20108985 12.32123591], Loss = 0.6147\n",
      "Iteration 5066: Weights = [55.30333333  2.66433075  6.380543    0.10494312  0.20107685 12.32168004], Loss = 0.6146\n",
      "Iteration 5067: Weights = [55.30333333  2.6641586   6.38013074  0.10493634  0.20106386 12.32212415], Loss = 0.6145\n",
      "Iteration 5068: Weights = [55.30333333  2.66398646  6.37971851  0.10492956  0.20105087 12.32256823], Loss = 0.6144\n",
      "Iteration 5069: Weights = [55.30333333  2.66381434  6.3793063   0.10492278  0.20103788 12.32301229], Loss = 0.6143\n",
      "Iteration 5070: Weights = [55.30333333  2.66364222  6.37889412  0.104916    0.20102489 12.32345631], Loss = 0.6143\n",
      "Iteration 5071: Weights = [55.30333333  2.66347012  6.37848197  0.10490922  0.2010119  12.3239003 ], Loss = 0.6142\n",
      "Iteration 5072: Weights = [55.30333333  2.66329803  6.37806984  0.10490245  0.20099891 12.32434427], Loss = 0.6141\n",
      "Iteration 5073: Weights = [55.30333333  2.66312595  6.37765774  0.10489567  0.20098593 12.32478821], Loss = 0.6140\n",
      "Iteration 5074: Weights = [55.30333333  2.66295388  6.37724567  0.10488889  0.20097294 12.32523212], Loss = 0.6139\n",
      "Iteration 5075: Weights = [55.30333333  2.66278182  6.37683362  0.10488211  0.20095996 12.325676  ], Loss = 0.6139\n",
      "Iteration 5076: Weights = [55.30333333  2.66260977  6.3764216   0.10487534  0.20094697 12.32611985], Loss = 0.6138\n",
      "Iteration 5077: Weights = [55.30333333  2.66243774  6.37600961  0.10486856  0.20093399 12.32656367], Loss = 0.6137\n",
      "Iteration 5078: Weights = [55.30333333  2.66226571  6.37559764  0.10486179  0.200921   12.32700746], Loss = 0.6136\n",
      "Iteration 5079: Weights = [55.30333333  2.6620937   6.3751857   0.10485501  0.20090802 12.32745123], Loss = 0.6135\n",
      "Iteration 5080: Weights = [55.30333333  2.66192169  6.37477379  0.10484823  0.20089504 12.32789497], Loss = 0.6135\n",
      "Iteration 5081: Weights = [55.30333333  2.6617497   6.3743619   0.10484146  0.20088206 12.32833867], Loss = 0.6134\n",
      "Iteration 5082: Weights = [55.30333333  2.66157772  6.37395004  0.10483469  0.20086908 12.32878235], Loss = 0.6133\n",
      "Iteration 5083: Weights = [55.30333333  2.66140575  6.37353821  0.10482791  0.2008561  12.329226  ], Loss = 0.6132\n",
      "Iteration 5084: Weights = [55.30333333  2.66123379  6.3731264   0.10482114  0.20084313 12.32966963], Loss = 0.6132\n",
      "Iteration 5085: Weights = [55.30333333  2.66106184  6.37271462  0.10481437  0.20083015 12.33011322], Loss = 0.6131\n",
      "Iteration 5086: Weights = [55.30333333  2.66088991  6.37230287  0.10480759  0.20081717 12.33055678], Loss = 0.6130\n",
      "Iteration 5087: Weights = [55.30333333  2.66071798  6.37189114  0.10480082  0.2008042  12.33100032], Loss = 0.6129\n",
      "Iteration 5088: Weights = [55.30333333  2.66054607  6.37147944  0.10479405  0.20079122 12.33144383], Loss = 0.6128\n",
      "Iteration 5089: Weights = [55.30333333  2.66037416  6.37106777  0.10478728  0.20077825 12.3318873 ], Loss = 0.6128\n",
      "Iteration 5090: Weights = [55.30333333  2.66020227  6.37065612  0.10478051  0.20076528 12.33233075], Loss = 0.6127\n",
      "Iteration 5091: Weights = [55.30333333  2.66003039  6.3702445   0.10477374  0.20075231 12.33277418], Loss = 0.6126\n",
      "Iteration 5092: Weights = [55.30333333  2.65985852  6.3698329   0.10476697  0.20073933 12.33321757], Loss = 0.6125\n",
      "Iteration 5093: Weights = [55.30333333  2.65968666  6.36942134  0.1047602   0.20072636 12.33366093], Loss = 0.6124\n",
      "Iteration 5094: Weights = [55.30333333  2.65951481  6.36900979  0.10475343  0.20071339 12.33410427], Loss = 0.6124\n",
      "Iteration 5095: Weights = [55.30333333  2.65934298  6.36859828  0.10474666  0.20070043 12.33454757], Loss = 0.6123\n",
      "Iteration 5096: Weights = [55.30333333  2.65917115  6.36818679  0.1047399   0.20068746 12.33499085], Loss = 0.6122\n",
      "Iteration 5097: Weights = [55.30333333  2.65899934  6.36777533  0.10473313  0.20067449 12.3354341 ], Loss = 0.6121\n",
      "Iteration 5098: Weights = [55.30333333  2.65882753  6.3673639   0.10472636  0.20066153 12.33587732], Loss = 0.6120\n",
      "Iteration 5099: Weights = [55.30333333  2.65865574  6.36695249  0.1047196   0.20064856 12.33632051], Loss = 0.6120\n",
      "Iteration 5100: Weights = [55.30333333  2.65848396  6.36654111  0.10471283  0.2006356  12.33676368], Loss = 0.6119\n",
      "Iteration 5101: Weights = [55.30333333  2.65831219  6.36612975  0.10470606  0.20062263 12.33720681], Loss = 0.6118\n",
      "Iteration 5102: Weights = [55.30333333  2.65814043  6.36571842  0.1046993   0.20060967 12.33764992], Loss = 0.6117\n",
      "Iteration 5103: Weights = [55.30333333  2.65796868  6.36530712  0.10469253  0.20059671 12.338093  ], Loss = 0.6116\n",
      "Iteration 5104: Weights = [55.30333333  2.65779695  6.36489585  0.10468577  0.20058375 12.33853605], Loss = 0.6116\n",
      "Iteration 5105: Weights = [55.30333333  2.65762522  6.3644846   0.104679    0.20057079 12.33897907], Loss = 0.6115\n",
      "Iteration 5106: Weights = [55.30333333  2.65745351  6.36407337  0.10467224  0.20055783 12.33942206], Loss = 0.6114\n",
      "Iteration 5107: Weights = [55.30333333  2.6572818   6.36366218  0.10466548  0.20054487 12.33986502], Loss = 0.6113\n",
      "Iteration 5108: Weights = [55.30333333  2.65711011  6.36325101  0.10465872  0.20053191 12.34030796], Loss = 0.6113\n",
      "Iteration 5109: Weights = [55.30333333  2.65693843  6.36283987  0.10465195  0.20051896 12.34075086], Loss = 0.6112\n",
      "Iteration 5110: Weights = [55.30333333  2.65676676  6.36242875  0.10464519  0.200506   12.34119374], Loss = 0.6111\n",
      "Iteration 5111: Weights = [55.30333333  2.6565951   6.36201766  0.10463843  0.20049304 12.34163659], Loss = 0.6110\n",
      "Iteration 5112: Weights = [55.30333333  2.65642345  6.3616066   0.10463167  0.20048009 12.34207941], Loss = 0.6109\n",
      "Iteration 5113: Weights = [55.30333333  2.65625182  6.36119556  0.10462491  0.20046714 12.3425222 ], Loss = 0.6109\n",
      "Iteration 5114: Weights = [55.30333333  2.65608019  6.36078455  0.10461815  0.20045418 12.34296496], Loss = 0.6108\n",
      "Iteration 5115: Weights = [55.30333333  2.65590858  6.36037357  0.10461139  0.20044123 12.3434077 ], Loss = 0.6107\n",
      "Iteration 5116: Weights = [55.30333333  2.65573697  6.35996261  0.10460463  0.20042828 12.3438504 ], Loss = 0.6106\n",
      "Iteration 5117: Weights = [55.30333333  2.65556538  6.35955168  0.10459787  0.20041533 12.34429308], Loss = 0.6105\n",
      "Iteration 5118: Weights = [55.30333333  2.6553938   6.35914078  0.10459111  0.20040238 12.34473573], Loss = 0.6105\n",
      "Iteration 5119: Weights = [55.30333333  2.65522223  6.3587299   0.10458436  0.20038943 12.34517835], Loss = 0.6104\n",
      "Iteration 5120: Weights = [55.30333333  2.65505067  6.35831905  0.1045776   0.20037649 12.34562094], Loss = 0.6103\n",
      "Iteration 5121: Weights = [55.30333333  2.65487912  6.35790823  0.10457084  0.20036354 12.3460635 ], Loss = 0.6102\n",
      "Iteration 5122: Weights = [55.30333333  2.65470758  6.35749743  0.10456408  0.20035059 12.34650603], Loss = 0.6101\n",
      "Iteration 5123: Weights = [55.30333333  2.65453606  6.35708666  0.10455733  0.20033765 12.34694854], Loss = 0.6101\n",
      "Iteration 5124: Weights = [55.30333333  2.65436454  6.35667592  0.10455057  0.2003247  12.34739102], Loss = 0.6100\n",
      "Iteration 5125: Weights = [55.30333333  2.65419304  6.3562652   0.10454382  0.20031176 12.34783346], Loss = 0.6099\n",
      "Iteration 5126: Weights = [55.30333333  2.65402155  6.35585451  0.10453706  0.20029882 12.34827588], Loss = 0.6098\n",
      "Iteration 5127: Weights = [55.30333333  2.65385006  6.35544384  0.10453031  0.20028588 12.34871827], Loss = 0.6098\n",
      "Iteration 5128: Weights = [55.30333333  2.65367859  6.35503321  0.10452355  0.20027294 12.34916064], Loss = 0.6097\n",
      "Iteration 5129: Weights = [55.30333333  2.65350713  6.3546226   0.1045168   0.20026    12.34960297], Loss = 0.6096\n",
      "Iteration 5130: Weights = [55.30333333  2.65333569  6.35421201  0.10451005  0.20024706 12.35004528], Loss = 0.6095\n",
      "Iteration 5131: Weights = [55.30333333  2.65316425  6.35380145  0.1045033   0.20023412 12.35048755], Loss = 0.6094\n",
      "Iteration 5132: Weights = [55.30333333  2.65299282  6.35339092  0.10449654  0.20022118 12.3509298 ], Loss = 0.6094\n",
      "Iteration 5133: Weights = [55.30333333  2.65282141  6.35298041  0.10448979  0.20020824 12.35137202], Loss = 0.6093\n",
      "Iteration 5134: Weights = [55.30333333  2.65265     6.35256994  0.10448304  0.20019531 12.35181421], Loss = 0.6092\n",
      "Iteration 5135: Weights = [55.30333333  2.65247861  6.35215948  0.10447629  0.20018237 12.35225637], Loss = 0.6091\n",
      "Iteration 5136: Weights = [55.30333333  2.65230723  6.35174906  0.10446954  0.20016944 12.35269851], Loss = 0.6090\n",
      "Iteration 5137: Weights = [55.30333333  2.65213586  6.35133866  0.10446279  0.20015651 12.35314061], Loss = 0.6090\n",
      "Iteration 5138: Weights = [55.30333333  2.6519645   6.35092829  0.10445604  0.20014357 12.35358269], Loss = 0.6089\n",
      "Iteration 5139: Weights = [55.30333333  2.65179315  6.35051794  0.10444929  0.20013064 12.35402474], Loss = 0.6088\n",
      "Iteration 5140: Weights = [55.30333333  2.65162181  6.35010762  0.10444254  0.20011771 12.35446676], Loss = 0.6087\n",
      "Iteration 5141: Weights = [55.30333333  2.65145048  6.34969733  0.10443579  0.20010478 12.35490875], Loss = 0.6087\n",
      "Iteration 5142: Weights = [55.30333333  2.65127917  6.34928706  0.10442905  0.20009185 12.35535071], Loss = 0.6086\n",
      "Iteration 5143: Weights = [55.30333333  2.65110786  6.34887682  0.1044223   0.20007892 12.35579264], Loss = 0.6085\n",
      "Iteration 5144: Weights = [55.30333333  2.65093657  6.34846661  0.10441555  0.200066   12.35623455], Loss = 0.6084\n",
      "Iteration 5145: Weights = [55.30333333  2.65076529  6.34805642  0.1044088   0.20005307 12.35667643], Loss = 0.6083\n",
      "Iteration 5146: Weights = [55.30333333  2.65059402  6.34764626  0.10440206  0.20004014 12.35711827], Loss = 0.6083\n",
      "Iteration 5147: Weights = [55.30333333  2.65042276  6.34723612  0.10439531  0.20002722 12.35756009], Loss = 0.6082\n",
      "Iteration 5148: Weights = [55.30333333  2.65025151  6.34682602  0.10438857  0.20001429 12.35800189], Loss = 0.6081\n",
      "Iteration 5149: Weights = [55.30333333  2.65008027  6.34641594  0.10438182  0.20000137 12.35844365], Loss = 0.6080\n",
      "Iteration 5150: Weights = [55.30333333  2.64990904  6.34600588  0.10437508  0.19998845 12.35888538], Loss = 0.6079\n",
      "Iteration 5151: Weights = [55.30333333  2.64973783  6.34559585  0.10436833  0.19997553 12.35932709], Loss = 0.6079\n",
      "Iteration 5152: Weights = [55.30333333  2.64956662  6.34518585  0.10436159  0.19996261 12.35976876], Loss = 0.6078\n",
      "Iteration 5153: Weights = [55.30333333  2.64939543  6.34477588  0.10435485  0.19994969 12.36021041], Loss = 0.6077\n",
      "Iteration 5154: Weights = [55.30333333  2.64922425  6.34436593  0.10434811  0.19993677 12.36065203], Loss = 0.6076\n",
      "Iteration 5155: Weights = [55.30333333  2.64905307  6.343956    0.10434136  0.19992385 12.36109362], Loss = 0.6076\n",
      "Iteration 5156: Weights = [55.30333333  2.64888191  6.34354611  0.10433462  0.19991093 12.36153519], Loss = 0.6075\n",
      "Iteration 5157: Weights = [55.30333333  2.64871076  6.34313624  0.10432788  0.19989801 12.36197672], Loss = 0.6074\n",
      "Iteration 5158: Weights = [55.30333333  2.64853962  6.3427264   0.10432114  0.1998851  12.36241823], Loss = 0.6073\n",
      "Iteration 5159: Weights = [55.30333333  2.6483685   6.34231658  0.1043144   0.19987218 12.3628597 ], Loss = 0.6072\n",
      "Iteration 5160: Weights = [55.30333333  2.64819738  6.34190679  0.10430766  0.19985927 12.36330115], Loss = 0.6072\n",
      "Iteration 5161: Weights = [55.30333333  2.64802628  6.34149703  0.10430092  0.19984636 12.36374257], Loss = 0.6071\n",
      "Iteration 5162: Weights = [55.30333333  2.64785518  6.34108729  0.10429418  0.19983344 12.36418397], Loss = 0.6070\n",
      "Iteration 5163: Weights = [55.30333333  2.6476841   6.34067758  0.10428744  0.19982053 12.36462533], Loss = 0.6069\n",
      "Iteration 5164: Weights = [55.30333333  2.64751303  6.3402679   0.1042807   0.19980762 12.36506666], Loss = 0.6068\n",
      "Iteration 5165: Weights = [55.30333333  2.64734196  6.33985824  0.10427397  0.19979471 12.36550797], Loss = 0.6068\n",
      "Iteration 5166: Weights = [55.30333333  2.64717091  6.33944861  0.10426723  0.1997818  12.36594925], Loss = 0.6067\n",
      "Iteration 5167: Weights = [55.30333333  2.64699988  6.339039    0.10426049  0.19976889 12.3663905 ], Loss = 0.6066\n",
      "Iteration 5168: Weights = [55.30333333  2.64682885  6.33862942  0.10425376  0.19975599 12.36683172], Loss = 0.6065\n",
      "Iteration 5169: Weights = [55.30333333  2.64665783  6.33821987  0.10424702  0.19974308 12.36727291], Loss = 0.6065\n",
      "Iteration 5170: Weights = [55.30333333  2.64648682  6.33781035  0.10424028  0.19973017 12.36771407], Loss = 0.6064\n",
      "Iteration 5171: Weights = [55.30333333  2.64631583  6.33740085  0.10423355  0.19971727 12.36815521], Loss = 0.6063\n",
      "Iteration 5172: Weights = [55.30333333  2.64614485  6.33699138  0.10422681  0.19970436 12.36859631], Loss = 0.6062\n",
      "Iteration 5173: Weights = [55.30333333  2.64597387  6.33658193  0.10422008  0.19969146 12.36903739], Loss = 0.6061\n",
      "Iteration 5174: Weights = [55.30333333  2.64580291  6.33617251  0.10421335  0.19967856 12.36947844], Loss = 0.6061\n",
      "Iteration 5175: Weights = [55.30333333  2.64563196  6.33576312  0.10420661  0.19966566 12.36991946], Loss = 0.6060\n",
      "Iteration 5176: Weights = [55.30333333  2.64546102  6.33535375  0.10419988  0.19965276 12.37036046], Loss = 0.6059\n",
      "Iteration 5177: Weights = [55.30333333  2.64529009  6.33494441  0.10419315  0.19963986 12.37080142], Loss = 0.6058\n",
      "Iteration 5178: Weights = [55.30333333  2.64511918  6.3345351   0.10418641  0.19962696 12.37124235], Loss = 0.6057\n",
      "Iteration 5179: Weights = [55.30333333  2.64494827  6.33412581  0.10417968  0.19961406 12.37168326], Loss = 0.6057\n",
      "Iteration 5180: Weights = [55.30333333  2.64477737  6.33371655  0.10417295  0.19960116 12.37212414], Loss = 0.6056\n",
      "Iteration 5181: Weights = [55.30333333  2.64460649  6.33330732  0.10416622  0.19958826 12.37256499], Loss = 0.6055\n",
      "Iteration 5182: Weights = [55.30333333  2.64443562  6.33289811  0.10415949  0.19957537 12.37300581], Loss = 0.6054\n",
      "Iteration 5183: Weights = [55.30333333  2.64426475  6.33248893  0.10415276  0.19956247 12.37344661], Loss = 0.6054\n",
      "Iteration 5184: Weights = [55.30333333  2.6440939   6.33207977  0.10414603  0.19954958 12.37388737], Loss = 0.6053\n",
      "Iteration 5185: Weights = [55.30333333  2.64392306  6.33167065  0.1041393   0.19953669 12.37432811], Loss = 0.6052\n",
      "Iteration 5186: Weights = [55.30333333  2.64375223  6.33126154  0.10413257  0.19952379 12.37476881], Loss = 0.6051\n",
      "Iteration 5187: Weights = [55.30333333  2.64358141  6.33085247  0.10412584  0.1995109  12.37520949], Loss = 0.6050\n",
      "Iteration 5188: Weights = [55.30333333  2.64341061  6.33044342  0.10411912  0.19949801 12.37565014], Loss = 0.6050\n",
      "Iteration 5189: Weights = [55.30333333  2.64323981  6.3300344   0.10411239  0.19948512 12.37609077], Loss = 0.6049\n",
      "Iteration 5190: Weights = [55.30333333  2.64306903  6.3296254   0.10410566  0.19947223 12.37653136], Loss = 0.6048\n",
      "Iteration 5191: Weights = [55.30333333  2.64289825  6.32921643  0.10409894  0.19945934 12.37697193], Loss = 0.6047\n",
      "Iteration 5192: Weights = [55.30333333  2.64272749  6.32880749  0.10409221  0.19944646 12.37741246], Loss = 0.6047\n",
      "Iteration 5193: Weights = [55.30333333  2.64255674  6.32839857  0.10408548  0.19943357 12.37785297], Loss = 0.6046\n",
      "Iteration 5194: Weights = [55.30333333  2.642386    6.32798968  0.10407876  0.19942068 12.37829345], Loss = 0.6045\n",
      "Iteration 5195: Weights = [55.30333333  2.64221527  6.32758082  0.10407203  0.1994078  12.3787339 ], Loss = 0.6044\n",
      "Iteration 5196: Weights = [55.30333333  2.64204455  6.32717198  0.10406531  0.19939491 12.37917433], Loss = 0.6043\n",
      "Iteration 5197: Weights = [55.30333333  2.64187384  6.32676317  0.10405859  0.19938203 12.37961472], Loss = 0.6043\n",
      "Iteration 5198: Weights = [55.30333333  2.64170314  6.32635438  0.10405186  0.19936915 12.38005509], Loss = 0.6042\n",
      "Iteration 5199: Weights = [55.30333333  2.64153246  6.32594562  0.10404514  0.19935627 12.38049542], Loss = 0.6041\n",
      "Iteration 5200: Weights = [55.30333333  2.64136178  6.32553689  0.10403842  0.19934339 12.38093573], Loss = 0.6040\n",
      "Iteration 5201: Weights = [55.30333333  2.64119112  6.32512819  0.1040317   0.19933051 12.38137601], Loss = 0.6040\n",
      "Iteration 5202: Weights = [55.30333333  2.64102047  6.32471951  0.10402497  0.19931763 12.38181627], Loss = 0.6039\n",
      "Iteration 5203: Weights = [55.30333333  2.64084982  6.32431085  0.10401825  0.19930475 12.38225649], Loss = 0.6038\n",
      "Iteration 5204: Weights = [55.30333333  2.64067919  6.32390223  0.10401153  0.19929187 12.38269669], Loss = 0.6037\n",
      "Iteration 5205: Weights = [55.30333333  2.64050857  6.32349363  0.10400481  0.19927899 12.38313685], Loss = 0.6036\n",
      "Iteration 5206: Weights = [55.30333333  2.64033797  6.32308505  0.10399809  0.19926612 12.38357699], Loss = 0.6036\n",
      "Iteration 5207: Weights = [55.30333333  2.64016737  6.32267651  0.10399137  0.19925324 12.3840171 ], Loss = 0.6035\n",
      "Iteration 5208: Weights = [55.30333333  2.63999678  6.32226798  0.10398465  0.19924037 12.38445718], Loss = 0.6034\n",
      "Iteration 5209: Weights = [55.30333333  2.63982621  6.32185949  0.10397793  0.1992275  12.38489724], Loss = 0.6033\n",
      "Iteration 5210: Weights = [55.30333333  2.63965564  6.32145102  0.10397122  0.19921462 12.38533726], Loss = 0.6032\n",
      "Iteration 5211: Weights = [55.30333333  2.63948509  6.32104258  0.1039645   0.19920175 12.38577726], Loss = 0.6032\n",
      "Iteration 5212: Weights = [55.30333333  2.63931455  6.32063417  0.10395778  0.19918888 12.38621723], Loss = 0.6031\n",
      "Iteration 5213: Weights = [55.30333333  2.63914401  6.32022578  0.10395106  0.19917601 12.38665717], Loss = 0.6030\n",
      "Iteration 5214: Weights = [55.30333333  2.63897349  6.31981741  0.10394435  0.19916314 12.38709708], Loss = 0.6029\n",
      "Iteration 5215: Weights = [55.30333333  2.63880298  6.31940908  0.10393763  0.19915027 12.38753696], Loss = 0.6029\n",
      "Iteration 5216: Weights = [55.30333333  2.63863249  6.31900077  0.10393092  0.19913741 12.38797681], Loss = 0.6028\n",
      "Iteration 5217: Weights = [55.30333333  2.638462    6.31859249  0.1039242   0.19912454 12.38841664], Loss = 0.6027\n",
      "Iteration 5218: Weights = [55.30333333  2.63829152  6.31818423  0.10391749  0.19911167 12.38885644], Loss = 0.6026\n",
      "Iteration 5219: Weights = [55.30333333  2.63812106  6.317776    0.10391077  0.19909881 12.38929621], Loss = 0.6025\n",
      "Iteration 5220: Weights = [55.30333333  2.6379506   6.31736779  0.10390406  0.19908595 12.38973595], Loss = 0.6025\n",
      "Iteration 5221: Weights = [55.30333333  2.63778016  6.31695962  0.10389734  0.19907308 12.39017566], Loss = 0.6024\n",
      "Iteration 5222: Weights = [55.30333333  2.63760973  6.31655146  0.10389063  0.19906022 12.39061534], Loss = 0.6023\n",
      "Iteration 5223: Weights = [55.30333333  2.63743931  6.31614334  0.10388392  0.19904736 12.391055  ], Loss = 0.6022\n",
      "Iteration 5224: Weights = [55.30333333  2.6372689   6.31573524  0.10387721  0.1990345  12.39149462], Loss = 0.6022\n",
      "Iteration 5225: Weights = [55.30333333  2.6370985   6.31532717  0.10387049  0.19902164 12.39193422], Loss = 0.6021\n",
      "Iteration 5226: Weights = [55.30333333  2.63692811  6.31491912  0.10386378  0.19900878 12.39237379], Loss = 0.6020\n",
      "Iteration 5227: Weights = [55.30333333  2.63675773  6.3145111   0.10385707  0.19899592 12.39281334], Loss = 0.6019\n",
      "Iteration 5228: Weights = [55.30333333  2.63658737  6.31410311  0.10385036  0.19898306 12.39325285], Loss = 0.6018\n",
      "Iteration 5229: Weights = [55.30333333  2.63641701  6.31369514  0.10384365  0.1989702  12.39369233], Loss = 0.6018\n",
      "Iteration 5230: Weights = [55.30333333  2.63624667  6.3132872   0.10383694  0.19895735 12.39413179], Loss = 0.6017\n",
      "Iteration 5231: Weights = [55.30333333  2.63607633  6.31287929  0.10383023  0.19894449 12.39457122], Loss = 0.6016\n",
      "Iteration 5232: Weights = [55.30333333  2.63590601  6.3124714   0.10382352  0.19893164 12.39501062], Loss = 0.6015\n",
      "Iteration 5233: Weights = [55.30333333  2.6357357   6.31206354  0.10381682  0.19891879 12.39544999], Loss = 0.6015\n",
      "Iteration 5234: Weights = [55.30333333  2.6355654   6.3116557   0.10381011  0.19890593 12.39588933], Loss = 0.6014\n",
      "Iteration 5235: Weights = [55.30333333  2.63539511  6.31124789  0.1038034   0.19889308 12.39632865], Loss = 0.6013\n",
      "Iteration 5236: Weights = [55.30333333  2.63522483  6.31084011  0.10379669  0.19888023 12.39676793], Loss = 0.6012\n",
      "Iteration 5237: Weights = [55.30333333  2.63505456  6.31043236  0.10378999  0.19886738 12.39720719], Loss = 0.6011\n",
      "Iteration 5238: Weights = [55.30333333  2.63488431  6.31002463  0.10378328  0.19885453 12.39764642], Loss = 0.6011\n",
      "Iteration 5239: Weights = [55.30333333  2.63471406  6.30961692  0.10377658  0.19884168 12.39808562], Loss = 0.6010\n",
      "Iteration 5240: Weights = [55.30333333  2.63454383  6.30920925  0.10376987  0.19882884 12.39852479], Loss = 0.6009\n",
      "Iteration 5241: Weights = [55.30333333  2.63437361  6.30880159  0.10376317  0.19881599 12.39896394], Loss = 0.6008\n",
      "Iteration 5242: Weights = [55.30333333  2.63420339  6.30839397  0.10375646  0.19880314 12.39940305], Loss = 0.6008\n",
      "Iteration 5243: Weights = [55.30333333  2.63403319  6.30798637  0.10374976  0.1987903  12.39984214], Loss = 0.6007\n",
      "Iteration 5244: Weights = [55.30333333  2.633863    6.3075788   0.10374305  0.19877745 12.4002812 ], Loss = 0.6006\n",
      "Iteration 5245: Weights = [55.30333333  2.63369282  6.30717126  0.10373635  0.19876461 12.40072023], Loss = 0.6005\n",
      "Iteration 5246: Weights = [55.30333333  2.63352266  6.30676374  0.10372965  0.19875177 12.40115923], Loss = 0.6004\n",
      "Iteration 5247: Weights = [55.30333333  2.6333525   6.30635624  0.10372295  0.19873893 12.40159821], Loss = 0.6004\n",
      "Iteration 5248: Weights = [55.30333333  2.63318235  6.30594878  0.10371624  0.19872609 12.40203715], Loss = 0.6003\n",
      "Iteration 5249: Weights = [55.30333333  2.63301222  6.30554134  0.10370954  0.19871325 12.40247607], Loss = 0.6002\n",
      "Iteration 5250: Weights = [55.30333333  2.63284209  6.30513392  0.10370284  0.19870041 12.40291496], Loss = 0.6001\n",
      "Iteration 5251: Weights = [55.30333333  2.63267198  6.30472654  0.10369614  0.19868757 12.40335382], Loss = 0.6001\n",
      "Iteration 5252: Weights = [55.30333333  2.63250188  6.30431918  0.10368944  0.19867473 12.40379265], Loss = 0.6000\n",
      "Iteration 5253: Weights = [55.30333333  2.63233179  6.30391184  0.10368274  0.19866189 12.40423146], Loss = 0.5999\n",
      "Iteration 5254: Weights = [55.30333333  2.63216171  6.30350453  0.10367604  0.19864906 12.40467023], Loss = 0.5998\n",
      "Iteration 5255: Weights = [55.30333333  2.63199164  6.30309725  0.10366934  0.19863622 12.40510898], Loss = 0.5998\n",
      "Iteration 5256: Weights = [55.30333333  2.63182158  6.30268999  0.10366265  0.19862339 12.4055477 ], Loss = 0.5997\n",
      "Iteration 5257: Weights = [55.30333333  2.63165153  6.30228276  0.10365595  0.19861055 12.40598639], Loss = 0.5996\n",
      "Iteration 5258: Weights = [55.30333333  2.63148149  6.30187556  0.10364925  0.19859772 12.40642505], Loss = 0.5995\n",
      "Iteration 5259: Weights = [55.30333333  2.63131147  6.30146838  0.10364255  0.19858489 12.40686369], Loss = 0.5994\n",
      "Iteration 5260: Weights = [55.30333333  2.63114145  6.30106123  0.10363586  0.19857206 12.40730229], Loss = 0.5994\n",
      "Iteration 5261: Weights = [55.30333333  2.63097145  6.30065411  0.10362916  0.19855923 12.40774087], Loss = 0.5993\n",
      "Iteration 5262: Weights = [55.30333333  2.63080146  6.30024701  0.10362247  0.1985464  12.40817942], Loss = 0.5992\n",
      "Iteration 5263: Weights = [55.30333333  2.63063148  6.29983994  0.10361577  0.19853357 12.40861794], Loss = 0.5991\n",
      "Iteration 5264: Weights = [55.30333333  2.63046151  6.2994329   0.10360908  0.19852074 12.40905643], Loss = 0.5991\n",
      "Iteration 5265: Weights = [55.30333333  2.63029155  6.29902588  0.10360238  0.19850792 12.40949489], Loss = 0.5990\n",
      "Iteration 5266: Weights = [55.30333333  2.6301216   6.29861888  0.10359569  0.19849509 12.40993333], Loss = 0.5989\n",
      "Iteration 5267: Weights = [55.30333333  2.62995166  6.29821192  0.10358899  0.19848227 12.41037174], Loss = 0.5988\n",
      "Iteration 5268: Weights = [55.30333333  2.62978174  6.29780498  0.1035823   0.19846944 12.41081012], Loss = 0.5987\n",
      "Iteration 5269: Weights = [55.30333333  2.62961182  6.29739806  0.10357561  0.19845662 12.41124847], Loss = 0.5987\n",
      "Iteration 5270: Weights = [55.30333333  2.62944192  6.29699118  0.10356892  0.19844379 12.41168679], Loss = 0.5986\n",
      "Iteration 5271: Weights = [55.30333333  2.62927202  6.29658431  0.10356222  0.19843097 12.41212508], Loss = 0.5985\n",
      "Iteration 5272: Weights = [55.30333333  2.62910214  6.29617748  0.10355553  0.19841815 12.41256335], Loss = 0.5984\n",
      "Iteration 5273: Weights = [55.30333333  2.62893227  6.29577067  0.10354884  0.19840533 12.41300159], Loss = 0.5984\n",
      "Iteration 5274: Weights = [55.30333333  2.62876241  6.29536389  0.10354215  0.19839251 12.41343979], Loss = 0.5983\n",
      "Iteration 5275: Weights = [55.30333333  2.62859256  6.29495713  0.10353546  0.19837969 12.41387798], Loss = 0.5982\n",
      "Iteration 5276: Weights = [55.30333333  2.62842272  6.2945504   0.10352877  0.19836688 12.41431613], Loss = 0.5981\n",
      "Iteration 5277: Weights = [55.30333333  2.62825289  6.2941437   0.10352208  0.19835406 12.41475425], Loss = 0.5980\n",
      "Iteration 5278: Weights = [55.30333333  2.62808307  6.29373702  0.10351539  0.19834124 12.41519235], Loss = 0.5980\n",
      "Iteration 5279: Weights = [55.30333333  2.62791327  6.29333037  0.10350871  0.19832843 12.41563041], Loss = 0.5979\n",
      "Iteration 5280: Weights = [55.30333333  2.62774347  6.29292375  0.10350202  0.19831561 12.41606845], Loss = 0.5978\n",
      "Iteration 5281: Weights = [55.30333333  2.62757369  6.29251715  0.10349533  0.1983028  12.41650646], Loss = 0.5977\n",
      "Iteration 5282: Weights = [55.30333333  2.62740392  6.29211058  0.10348864  0.19828999 12.41694445], Loss = 0.5977\n",
      "Iteration 5283: Weights = [55.30333333  2.62723416  6.29170403  0.10348196  0.19827718 12.4173824 ], Loss = 0.5976\n",
      "Iteration 5284: Weights = [55.30333333  2.6270644   6.29129751  0.10347527  0.19826436 12.41782033], Loss = 0.5975\n",
      "Iteration 5285: Weights = [55.30333333  2.62689466  6.29089102  0.10346858  0.19825155 12.41825822], Loss = 0.5974\n",
      "Iteration 5286: Weights = [55.30333333  2.62672494  6.29048455  0.1034619   0.19823874 12.41869609], Loss = 0.5974\n",
      "Iteration 5287: Weights = [55.30333333  2.62655522  6.29007811  0.10345521  0.19822594 12.41913394], Loss = 0.5973\n",
      "Iteration 5288: Weights = [55.30333333  2.62638551  6.28967169  0.10344853  0.19821313 12.41957175], Loss = 0.5972\n",
      "Iteration 5289: Weights = [55.30333333  2.62621581  6.28926531  0.10344185  0.19820032 12.42000953], Loss = 0.5971\n",
      "Iteration 5290: Weights = [55.30333333  2.62604613  6.28885894  0.10343516  0.19818752 12.42044729], Loss = 0.5970\n",
      "Iteration 5291: Weights = [55.30333333  2.62587646  6.28845261  0.10342848  0.19817471 12.42088502], Loss = 0.5970\n",
      "Iteration 5292: Weights = [55.30333333  2.62570679  6.2880463   0.1034218   0.19816191 12.42132272], Loss = 0.5969\n",
      "Iteration 5293: Weights = [55.30333333  2.62553714  6.28764001  0.10341511  0.1981491  12.42176039], Loss = 0.5968\n",
      "Iteration 5294: Weights = [55.30333333  2.6253675   6.28723376  0.10340843  0.1981363  12.42219803], Loss = 0.5967\n",
      "Iteration 5295: Weights = [55.30333333  2.62519787  6.28682753  0.10340175  0.1981235  12.42263565], Loss = 0.5967\n",
      "Iteration 5296: Weights = [55.30333333  2.62502825  6.28642132  0.10339507  0.1981107  12.42307323], Loss = 0.5966\n",
      "Iteration 5297: Weights = [55.30333333  2.62485864  6.28601514  0.10338839  0.1980979  12.42351079], Loss = 0.5965\n",
      "Iteration 5298: Weights = [55.30333333  2.62468904  6.28560899  0.10338171  0.1980851  12.42394832], Loss = 0.5964\n",
      "Iteration 5299: Weights = [55.30333333  2.62451946  6.28520287  0.10337503  0.1980723  12.42438582], Loss = 0.5964\n",
      "Iteration 5300: Weights = [55.30333333  2.62434988  6.28479677  0.10336835  0.1980595  12.42482329], Loss = 0.5963\n",
      "Iteration 5301: Weights = [55.30333333  2.62418032  6.28439069  0.10336167  0.1980467  12.42526074], Loss = 0.5962\n",
      "Iteration 5302: Weights = [55.30333333  2.62401076  6.28398465  0.10335499  0.19803391 12.42569816], Loss = 0.5961\n",
      "Iteration 5303: Weights = [55.30333333  2.62384122  6.28357863  0.10334831  0.19802111 12.42613555], Loss = 0.5960\n",
      "Iteration 5304: Weights = [55.30333333  2.62367169  6.28317263  0.10334164  0.19800832 12.42657291], Loss = 0.5960\n",
      "Iteration 5305: Weights = [55.30333333  2.62350217  6.28276666  0.10333496  0.19799552 12.42701024], Loss = 0.5959\n",
      "Iteration 5306: Weights = [55.30333333  2.62333266  6.28236072  0.10332828  0.19798273 12.42744754], Loss = 0.5958\n",
      "Iteration 5307: Weights = [55.30333333  2.62316316  6.2819548   0.10332161  0.19796994 12.42788482], Loss = 0.5957\n",
      "Iteration 5308: Weights = [55.30333333  2.62299367  6.28154891  0.10331493  0.19795715 12.42832206], Loss = 0.5957\n",
      "Iteration 5309: Weights = [55.30333333  2.6228242   6.28114305  0.10330826  0.19794436 12.42875928], Loss = 0.5956\n",
      "Iteration 5310: Weights = [55.30333333  2.62265473  6.28073721  0.10330158  0.19793157 12.42919647], Loss = 0.5955\n",
      "Iteration 5311: Weights = [55.30333333  2.62248527  6.2803314   0.10329491  0.19791878 12.42963364], Loss = 0.5954\n",
      "Iteration 5312: Weights = [55.30333333  2.62231583  6.27992562  0.10328823  0.19790599 12.43007077], Loss = 0.5953\n",
      "Iteration 5313: Weights = [55.30333333  2.6221464   6.27951986  0.10328156  0.1978932  12.43050788], Loss = 0.5953\n",
      "Iteration 5314: Weights = [55.30333333  2.62197698  6.27911413  0.10327489  0.19788042 12.43094496], Loss = 0.5952\n",
      "Iteration 5315: Weights = [55.30333333  2.62180756  6.27870842  0.10326821  0.19786763 12.431382  ], Loss = 0.5951\n",
      "Iteration 5316: Weights = [55.30333333  2.62163816  6.27830274  0.10326154  0.19785485 12.43181903], Loss = 0.5950\n",
      "Iteration 5317: Weights = [55.30333333  2.62146877  6.27789709  0.10325487  0.19784206 12.43225602], Loss = 0.5950\n",
      "Iteration 5318: Weights = [55.30333333  2.6212994   6.27749146  0.1032482   0.19782928 12.43269298], Loss = 0.5949\n",
      "Iteration 5319: Weights = [55.30333333  2.62113003  6.27708586  0.10324153  0.1978165  12.43312992], Loss = 0.5948\n",
      "Iteration 5320: Weights = [55.30333333  2.62096067  6.27668028  0.10323486  0.19780372 12.43356683], Loss = 0.5947\n",
      "Iteration 5321: Weights = [55.30333333  2.62079133  6.27627473  0.10322818  0.19779094 12.43400371], Loss = 0.5947\n",
      "Iteration 5322: Weights = [55.30333333  2.62062199  6.27586921  0.10322152  0.19777816 12.43444056], Loss = 0.5946\n",
      "Iteration 5323: Weights = [55.30333333  2.62045267  6.27546371  0.10321485  0.19776538 12.43487739], Loss = 0.5945\n",
      "Iteration 5324: Weights = [55.30333333  2.62028336  6.27505824  0.10320818  0.1977526  12.43531418], Loss = 0.5944\n",
      "Iteration 5325: Weights = [55.30333333  2.62011405  6.2746528   0.10320151  0.19773982 12.43575095], Loss = 0.5943\n",
      "Iteration 5326: Weights = [55.30333333  2.61994476  6.27424738  0.10319484  0.19772705 12.43618769], Loss = 0.5943\n",
      "Iteration 5327: Weights = [55.30333333  2.61977548  6.27384199  0.10318817  0.19771427 12.4366244 ], Loss = 0.5942\n",
      "Iteration 5328: Weights = [55.30333333  2.61960621  6.27343662  0.10318151  0.1977015  12.43706108], Loss = 0.5941\n",
      "Iteration 5329: Weights = [55.30333333  2.61943696  6.27303129  0.10317484  0.19768872 12.43749774], Loss = 0.5940\n",
      "Iteration 5330: Weights = [55.30333333  2.61926771  6.27262597  0.10316817  0.19767595 12.43793436], Loss = 0.5940\n",
      "Iteration 5331: Weights = [55.30333333  2.61909847  6.27222069  0.10316151  0.19766318 12.43837096], Loss = 0.5939\n",
      "Iteration 5332: Weights = [55.30333333  2.61892925  6.27181542  0.10315484  0.1976504  12.43880753], Loss = 0.5938\n",
      "Iteration 5333: Weights = [55.30333333  2.61876003  6.27141019  0.10314818  0.19763763 12.43924407], Loss = 0.5937\n",
      "Iteration 5334: Weights = [55.30333333  2.61859083  6.27100498  0.10314151  0.19762486 12.43968058], Loss = 0.5937\n",
      "Iteration 5335: Weights = [55.30333333  2.61842164  6.2705998   0.10313485  0.1976121  12.44011707], Loss = 0.5936\n",
      "Iteration 5336: Weights = [55.30333333  2.61825246  6.27019464  0.10312818  0.19759933 12.44055353], Loss = 0.5935\n",
      "Iteration 5337: Weights = [55.30333333  2.61808329  6.26978951  0.10312152  0.19758656 12.44098996], Loss = 0.5934\n",
      "Iteration 5338: Weights = [55.30333333  2.61791413  6.26938441  0.10311486  0.19757379 12.44142636], Loss = 0.5934\n",
      "Iteration 5339: Weights = [55.30333333  2.61774498  6.26897933  0.10310819  0.19756103 12.44186273], Loss = 0.5933\n",
      "Iteration 5340: Weights = [55.30333333  2.61757584  6.26857428  0.10310153  0.19754826 12.44229907], Loss = 0.5932\n",
      "Iteration 5341: Weights = [55.30333333  2.61740671  6.26816926  0.10309487  0.1975355  12.44273539], Loss = 0.5931\n",
      "Iteration 5342: Weights = [55.30333333  2.6172376   6.26776426  0.10308821  0.19752274 12.44317168], Loss = 0.5930\n",
      "Iteration 5343: Weights = [55.30333333  2.61706849  6.26735928  0.10308155  0.19750997 12.44360794], Loss = 0.5930\n",
      "Iteration 5344: Weights = [55.30333333  2.6168994   6.26695434  0.10307489  0.19749721 12.44404417], Loss = 0.5929\n",
      "Iteration 5345: Weights = [55.30333333  2.61673031  6.26654942  0.10306823  0.19748445 12.44448037], Loss = 0.5928\n",
      "Iteration 5346: Weights = [55.30333333  2.61656124  6.26614452  0.10306157  0.19747169 12.44491655], Loss = 0.5927\n",
      "Iteration 5347: Weights = [55.30333333  2.61639218  6.26573965  0.10305491  0.19745893 12.44535269], Loss = 0.5927\n",
      "Iteration 5348: Weights = [55.30333333  2.61622313  6.26533481  0.10304825  0.19744617 12.44578881], Loss = 0.5926\n",
      "Iteration 5349: Weights = [55.30333333  2.61605409  6.26493     0.10304159  0.19743342 12.4462249 ], Loss = 0.5925\n",
      "Iteration 5350: Weights = [55.30333333  2.61588506  6.26452521  0.10303494  0.19742066 12.44666096], Loss = 0.5924\n",
      "Iteration 5351: Weights = [55.30333333  2.61571605  6.26412044  0.10302828  0.1974079  12.447097  ], Loss = 0.5924\n",
      "Iteration 5352: Weights = [55.30333333  2.61554704  6.26371571  0.10302162  0.19739515 12.447533  ], Loss = 0.5923\n",
      "Iteration 5353: Weights = [55.30333333  2.61537804  6.26331099  0.10301497  0.1973824  12.44796898], Loss = 0.5922\n",
      "Iteration 5354: Weights = [55.30333333  2.61520906  6.26290631  0.10300831  0.19736964 12.44840493], Loss = 0.5921\n",
      "Iteration 5355: Weights = [55.30333333  2.61504008  6.26250165  0.10300165  0.19735689 12.44884085], Loss = 0.5921\n",
      "Iteration 5356: Weights = [55.30333333  2.61487112  6.26209702  0.102995    0.19734414 12.44927675], Loss = 0.5920\n",
      "Iteration 5357: Weights = [55.30333333  2.61470217  6.26169241  0.10298834  0.19733139 12.44971261], Loss = 0.5919\n",
      "Iteration 5358: Weights = [55.30333333  2.61453323  6.26128783  0.10298169  0.19731864 12.45014845], Loss = 0.5918\n",
      "Iteration 5359: Weights = [55.30333333  2.6143643   6.26088328  0.10297504  0.19730589 12.45058426], Loss = 0.5917\n",
      "Iteration 5360: Weights = [55.30333333  2.61419538  6.26047875  0.10296838  0.19729314 12.45102004], Loss = 0.5917\n",
      "Iteration 5361: Weights = [55.30333333  2.61402647  6.26007425  0.10296173  0.19728039 12.45145579], Loss = 0.5916\n",
      "Iteration 5362: Weights = [55.30333333  2.61385757  6.25966977  0.10295508  0.19726765 12.45189151], Loss = 0.5915\n",
      "Iteration 5363: Weights = [55.30333333  2.61368869  6.25926532  0.10294842  0.1972549  12.45232721], Loss = 0.5914\n",
      "Iteration 5364: Weights = [55.30333333  2.61351981  6.2588609   0.10294177  0.19724215 12.45276288], Loss = 0.5914\n",
      "Iteration 5365: Weights = [55.30333333  2.61335095  6.2584565   0.10293512  0.19722941 12.45319852], Loss = 0.5913\n",
      "Iteration 5366: Weights = [55.30333333  2.61318209  6.25805213  0.10292847  0.19721667 12.45363413], Loss = 0.5912\n",
      "Iteration 5367: Weights = [55.30333333  2.61301325  6.25764778  0.10292182  0.19720392 12.45406971], Loss = 0.5911\n",
      "Iteration 5368: Weights = [55.30333333  2.61284442  6.25724346  0.10291517  0.19719118 12.45450527], Loss = 0.5911\n",
      "Iteration 5369: Weights = [55.30333333  2.6126756   6.25683917  0.10290852  0.19717844 12.4549408 ], Loss = 0.5910\n",
      "Iteration 5370: Weights = [55.30333333  2.61250679  6.2564349   0.10290187  0.1971657  12.4553763 ], Loss = 0.5909\n",
      "Iteration 5371: Weights = [55.30333333  2.61233799  6.25603066  0.10289522  0.19715296 12.45581177], Loss = 0.5908\n",
      "Iteration 5372: Weights = [55.30333333  2.6121692   6.25562645  0.10288857  0.19714022 12.45624721], Loss = 0.5908\n",
      "Iteration 5373: Weights = [55.30333333  2.61200042  6.25522226  0.10288193  0.19712749 12.45668262], Loss = 0.5907\n",
      "Iteration 5374: Weights = [55.30333333  2.61183165  6.2548181   0.10287528  0.19711475 12.45711801], Loss = 0.5906\n",
      "Iteration 5375: Weights = [55.30333333  2.6116629   6.25441396  0.10286863  0.19710201 12.45755337], Loss = 0.5905\n",
      "Iteration 5376: Weights = [55.30333333  2.61149415  6.25400985  0.10286199  0.19708928 12.4579887 ], Loss = 0.5904\n",
      "Iteration 5377: Weights = [55.30333333  2.61132542  6.25360576  0.10285534  0.19707654 12.458424  ], Loss = 0.5904\n",
      "Iteration 5378: Weights = [55.30333333  2.6111567   6.25320171  0.10284869  0.19706381 12.45885928], Loss = 0.5903\n",
      "Iteration 5379: Weights = [55.30333333  2.61098798  6.25279767  0.10284205  0.19705108 12.45929452], Loss = 0.5902\n",
      "Iteration 5380: Weights = [55.30333333  2.61081928  6.25239367  0.1028354   0.19703835 12.45972974], Loss = 0.5901\n",
      "Iteration 5381: Weights = [55.30333333  2.61065059  6.25198969  0.10282876  0.19702562 12.46016493], Loss = 0.5901\n",
      "Iteration 5382: Weights = [55.30333333  2.61048191  6.25158574  0.10282212  0.19701288 12.46060009], Loss = 0.5900\n",
      "Iteration 5383: Weights = [55.30333333  2.61031324  6.25118181  0.10281547  0.19700016 12.46103523], Loss = 0.5899\n",
      "Iteration 5384: Weights = [55.30333333  2.61014459  6.25077791  0.10280883  0.19698743 12.46147033], Loss = 0.5898\n",
      "Iteration 5385: Weights = [55.30333333  2.60997594  6.25037403  0.10280219  0.1969747  12.46190541], Loss = 0.5898\n",
      "Iteration 5386: Weights = [55.30333333  2.6098073   6.24997018  0.10279554  0.19696197 12.46234046], Loss = 0.5897\n",
      "Iteration 5387: Weights = [55.30333333  2.60963868  6.24956636  0.1027889   0.19694925 12.46277548], Loss = 0.5896\n",
      "Iteration 5388: Weights = [55.30333333  2.60947007  6.24916256  0.10278226  0.19693652 12.46321047], Loss = 0.5895\n",
      "Iteration 5389: Weights = [55.30333333  2.60930146  6.24875879  0.10277562  0.1969238  12.46364544], Loss = 0.5895\n",
      "Iteration 5390: Weights = [55.30333333  2.60913287  6.24835505  0.10276898  0.19691107 12.46408037], Loss = 0.5894\n",
      "Iteration 5391: Weights = [55.30333333  2.60896429  6.24795133  0.10276234  0.19689835 12.46451528], Loss = 0.5893\n",
      "Iteration 5392: Weights = [55.30333333  2.60879572  6.24754763  0.1027557   0.19688563 12.46495016], Loss = 0.5892\n",
      "Iteration 5393: Weights = [55.30333333  2.60862716  6.24714397  0.10274906  0.19687291 12.46538501], Loss = 0.5891\n",
      "Iteration 5394: Weights = [55.30333333  2.60845861  6.24674033  0.10274242  0.19686019 12.46581984], Loss = 0.5891\n",
      "Iteration 5395: Weights = [55.30333333  2.60829007  6.24633671  0.10273578  0.19684747 12.46625464], Loss = 0.5890\n",
      "Iteration 5396: Weights = [55.30333333  2.60812155  6.24593312  0.10272915  0.19683475 12.4666894 ], Loss = 0.5889\n",
      "Iteration 5397: Weights = [55.30333333  2.60795303  6.24552956  0.10272251  0.19682203 12.46712414], Loss = 0.5888\n",
      "Iteration 5398: Weights = [55.30333333  2.60778452  6.24512603  0.10271587  0.19680931 12.46755886], Loss = 0.5888\n",
      "Iteration 5399: Weights = [55.30333333  2.60761603  6.24472252  0.10270923  0.1967966  12.46799354], Loss = 0.5887\n",
      "Iteration 5400: Weights = [55.30333333  2.60744755  6.24431903  0.1027026   0.19678388 12.4684282 ], Loss = 0.5886\n",
      "Iteration 5401: Weights = [55.30333333  2.60727907  6.24391557  0.10269596  0.19677117 12.46886282], Loss = 0.5885\n",
      "Iteration 5402: Weights = [55.30333333  2.60711061  6.24351214  0.10268933  0.19675845 12.46929742], Loss = 0.5885\n",
      "Iteration 5403: Weights = [55.30333333  2.60694216  6.24310874  0.10268269  0.19674574 12.46973199], Loss = 0.5884\n",
      "Iteration 5404: Weights = [55.30333333  2.60677372  6.24270536  0.10267606  0.19673303 12.47016654], Loss = 0.5883\n",
      "Iteration 5405: Weights = [55.30333333  2.60660529  6.242302    0.10266942  0.19672032 12.47060105], Loss = 0.5882\n",
      "Iteration 5406: Weights = [55.30333333  2.60643688  6.24189868  0.10266279  0.19670761 12.47103554], Loss = 0.5882\n",
      "Iteration 5407: Weights = [55.30333333  2.60626847  6.24149537  0.10265616  0.1966949  12.47147   ], Loss = 0.5881\n",
      "Iteration 5408: Weights = [55.30333333  2.60610007  6.2410921   0.10264952  0.19668219 12.47190443], Loss = 0.5880\n",
      "Iteration 5409: Weights = [55.30333333  2.60593169  6.24068885  0.10264289  0.19666948 12.47233883], Loss = 0.5879\n",
      "Iteration 5410: Weights = [55.30333333  2.60576331  6.24028563  0.10263626  0.19665677 12.47277321], Loss = 0.5879\n",
      "Iteration 5411: Weights = [55.30333333  2.60559495  6.23988243  0.10262963  0.19664407 12.47320756], Loss = 0.5878\n",
      "Iteration 5412: Weights = [55.30333333  2.6054266   6.23947926  0.102623    0.19663136 12.47364187], Loss = 0.5877\n",
      "Iteration 5413: Weights = [55.30333333  2.60525825  6.23907611  0.10261637  0.19661866 12.47407617], Loss = 0.5876\n",
      "Iteration 5414: Weights = [55.30333333  2.60508992  6.23867299  0.10260974  0.19660595 12.47451043], Loss = 0.5876\n",
      "Iteration 5415: Weights = [55.30333333  2.6049216   6.2382699   0.10260311  0.19659325 12.47494466], Loss = 0.5875\n",
      "Iteration 5416: Weights = [55.30333333  2.60475329  6.23786683  0.10259648  0.19658055 12.47537887], Loss = 0.5874\n",
      "Iteration 5417: Weights = [55.30333333  2.604585    6.23746379  0.10258985  0.19656785 12.47581305], Loss = 0.5873\n",
      "Iteration 5418: Weights = [55.30333333  2.60441671  6.23706078  0.10258322  0.19655514 12.4762472 ], Loss = 0.5872\n",
      "Iteration 5419: Weights = [55.30333333  2.60424843  6.23665779  0.10257659  0.19654244 12.47668132], Loss = 0.5872\n",
      "Iteration 5420: Weights = [55.30333333  2.60408017  6.23625482  0.10256996  0.19652975 12.47711542], Loss = 0.5871\n",
      "Iteration 5421: Weights = [55.30333333  2.60391191  6.23585189  0.10256334  0.19651705 12.47754948], Loss = 0.5870\n",
      "Iteration 5422: Weights = [55.30333333  2.60374367  6.23544898  0.10255671  0.19650435 12.47798352], Loss = 0.5869\n",
      "Iteration 5423: Weights = [55.30333333  2.60357543  6.23504609  0.10255008  0.19649165 12.47841753], Loss = 0.5869\n",
      "Iteration 5424: Weights = [55.30333333  2.60340721  6.23464323  0.10254346  0.19647896 12.47885151], Loss = 0.5868\n",
      "Iteration 5425: Weights = [55.30333333  2.603239    6.2342404   0.10253683  0.19646626 12.47928547], Loss = 0.5867\n",
      "Iteration 5426: Weights = [55.30333333  2.6030708   6.23383759  0.10253021  0.19645357 12.47971939], Loss = 0.5866\n",
      "Iteration 5427: Weights = [55.30333333  2.60290261  6.23343481  0.10252358  0.19644088 12.48015329], Loss = 0.5866\n",
      "Iteration 5428: Weights = [55.30333333  2.60273443  6.23303206  0.10251696  0.19642818 12.48058716], Loss = 0.5865\n",
      "Iteration 5429: Weights = [55.30333333  2.60256626  6.23262933  0.10251033  0.19641549 12.481021  ], Loss = 0.5864\n",
      "Iteration 5430: Weights = [55.30333333  2.60239811  6.23222663  0.10250371  0.1964028  12.48145482], Loss = 0.5863\n",
      "Iteration 5431: Weights = [55.30333333  2.60222996  6.23182395  0.10249709  0.19639011 12.4818886 ], Loss = 0.5863\n",
      "Iteration 5432: Weights = [55.30333333  2.60206183  6.2314213   0.10249046  0.19637742 12.48232236], Loss = 0.5862\n",
      "Iteration 5433: Weights = [55.30333333  2.6018937   6.23101867  0.10248384  0.19636473 12.48275609], Loss = 0.5861\n",
      "Iteration 5434: Weights = [55.30333333  2.60172559  6.23061607  0.10247722  0.19635205 12.48318979], Loss = 0.5860\n",
      "Iteration 5435: Weights = [55.30333333  2.60155748  6.2302135   0.1024706   0.19633936 12.48362347], Loss = 0.5860\n",
      "Iteration 5436: Weights = [55.30333333  2.60138939  6.22981096  0.10246398  0.19632667 12.48405711], Loss = 0.5859\n",
      "Iteration 5437: Weights = [55.30333333  2.60122131  6.22940843  0.10245736  0.19631399 12.48449073], Loss = 0.5858\n",
      "Iteration 5438: Weights = [55.30333333  2.60105324  6.22900594  0.10245074  0.1963013  12.48492432], Loss = 0.5857\n",
      "Iteration 5439: Weights = [55.30333333  2.60088518  6.22860347  0.10244412  0.19628862 12.48535788], Loss = 0.5857\n",
      "Iteration 5440: Weights = [55.30333333  2.60071713  6.22820103  0.1024375   0.19627594 12.48579142], Loss = 0.5856\n",
      "Iteration 5441: Weights = [55.30333333  2.6005491   6.22779861  0.10243088  0.19626326 12.48622492], Loss = 0.5855\n",
      "Iteration 5442: Weights = [55.30333333  2.60038107  6.22739622  0.10242426  0.19625058 12.4866584 ], Loss = 0.5854\n",
      "Iteration 5443: Weights = [55.30333333  2.60021305  6.22699386  0.10241764  0.19623789 12.48709185], Loss = 0.5854\n",
      "Iteration 5444: Weights = [55.30333333  2.60004505  6.22659152  0.10241103  0.19622522 12.48752527], Loss = 0.5853\n",
      "Iteration 5445: Weights = [55.30333333  2.59987705  6.22618921  0.10240441  0.19621254 12.48795867], Loss = 0.5852\n",
      "Iteration 5446: Weights = [55.30333333  2.59970907  6.22578692  0.10239779  0.19619986 12.48839203], Loss = 0.5851\n",
      "Iteration 5447: Weights = [55.30333333  2.5995411   6.22538466  0.10239118  0.19618718 12.48882537], Loss = 0.5851\n",
      "Iteration 5448: Weights = [55.30333333  2.59937314  6.22498242  0.10238456  0.19617451 12.48925868], Loss = 0.5850\n",
      "Iteration 5449: Weights = [55.30333333  2.59920519  6.22458022  0.10237795  0.19616183 12.48969196], Loss = 0.5849\n",
      "Iteration 5450: Weights = [55.30333333  2.59903725  6.22417803  0.10237133  0.19614916 12.49012522], Loss = 0.5848\n",
      "Iteration 5451: Weights = [55.30333333  2.59886932  6.22377588  0.10236472  0.19613648 12.49055844], Loss = 0.5848\n",
      "Iteration 5452: Weights = [55.30333333  2.5987014   6.22337375  0.1023581   0.19612381 12.49099164], Loss = 0.5847\n",
      "Iteration 5453: Weights = [55.30333333  2.59853349  6.22297164  0.10235149  0.19611114 12.49142481], Loss = 0.5846\n",
      "Iteration 5454: Weights = [55.30333333  2.5983656   6.22256956  0.10234488  0.19609847 12.49185795], Loss = 0.5845\n",
      "Iteration 5455: Weights = [55.30333333  2.59819771  6.22216751  0.10233826  0.1960858  12.49229107], Loss = 0.5844\n",
      "Iteration 5456: Weights = [55.30333333  2.59802984  6.22176548  0.10233165  0.19607313 12.49272415], Loss = 0.5844\n",
      "Iteration 5457: Weights = [55.30333333  2.59786197  6.22136348  0.10232504  0.19606046 12.49315721], Loss = 0.5843\n",
      "Iteration 5458: Weights = [55.30333333  2.59769412  6.22096151  0.10231843  0.19604779 12.49359024], Loss = 0.5842\n",
      "Iteration 5459: Weights = [55.30333333  2.59752628  6.22055956  0.10231182  0.19603512 12.49402324], Loss = 0.5841\n",
      "Iteration 5460: Weights = [55.30333333  2.59735844  6.22015764  0.10230521  0.19602246 12.49445622], Loss = 0.5841\n",
      "Iteration 5461: Weights = [55.30333333  2.59719062  6.21975574  0.1022986   0.19600979 12.49488916], Loss = 0.5840\n",
      "Iteration 5462: Weights = [55.30333333  2.59702281  6.21935387  0.10229199  0.19599713 12.49532208], Loss = 0.5839\n",
      "Iteration 5463: Weights = [55.30333333  2.59685502  6.21895202  0.10228538  0.19598446 12.49575497], Loss = 0.5838\n",
      "Iteration 5464: Weights = [55.30333333  2.59668723  6.2185502   0.10227877  0.1959718  12.49618783], Loss = 0.5838\n",
      "Iteration 5465: Weights = [55.30333333  2.59651945  6.21814841  0.10227216  0.19595914 12.49662067], Loss = 0.5837\n",
      "Iteration 5466: Weights = [55.30333333  2.59635168  6.21774664  0.10226555  0.19594648 12.49705347], Loss = 0.5836\n",
      "Iteration 5467: Weights = [55.30333333  2.59618393  6.2173449   0.10225894  0.19593382 12.49748625], Loss = 0.5835\n",
      "Iteration 5468: Weights = [55.30333333  2.59601618  6.21694319  0.10225234  0.19592116 12.497919  ], Loss = 0.5835\n",
      "Iteration 5469: Weights = [55.30333333  2.59584845  6.2165415   0.10224573  0.1959085  12.49835172], Loss = 0.5834\n",
      "Iteration 5470: Weights = [55.30333333  2.59568073  6.21613983  0.10223912  0.19589584 12.49878442], Loss = 0.5833\n",
      "Iteration 5471: Weights = [55.30333333  2.59551301  6.2157382   0.10223252  0.19588318 12.49921708], Loss = 0.5832\n",
      "Iteration 5472: Weights = [55.30333333  2.59534531  6.21533659  0.10222591  0.19587053 12.49964972], Loss = 0.5832\n",
      "Iteration 5473: Weights = [55.30333333  2.59517762  6.214935    0.10221931  0.19585787 12.50008233], Loss = 0.5831\n",
      "Iteration 5474: Weights = [55.30333333  2.59500994  6.21453344  0.1022127   0.19584522 12.50051491], Loss = 0.5830\n",
      "Iteration 5475: Weights = [55.30333333  2.59484227  6.21413191  0.1022061   0.19583256 12.50094747], Loss = 0.5829\n",
      "Iteration 5476: Weights = [55.30333333  2.59467462  6.2137304   0.1021995   0.19581991 12.50138   ], Loss = 0.5829\n",
      "Iteration 5477: Weights = [55.30333333  2.59450697  6.21332892  0.10219289  0.19580726 12.50181249], Loss = 0.5828\n",
      "Iteration 5478: Weights = [55.30333333  2.59433933  6.21292746  0.10218629  0.19579461 12.50224497], Loss = 0.5827\n",
      "Iteration 5479: Weights = [55.30333333  2.59417171  6.21252603  0.10217969  0.19578195 12.50267741], Loss = 0.5826\n",
      "Iteration 5480: Weights = [55.30333333  2.59400409  6.21212463  0.10217308  0.19576931 12.50310982], Loss = 0.5826\n",
      "Iteration 5481: Weights = [55.30333333  2.59383649  6.21172325  0.10216648  0.19575666 12.50354221], Loss = 0.5825\n",
      "Iteration 5482: Weights = [55.30333333  2.5936689   6.2113219   0.10215988  0.19574401 12.50397457], Loss = 0.5824\n",
      "Iteration 5483: Weights = [55.30333333  2.59350131  6.21092057  0.10215328  0.19573136 12.5044069 ], Loss = 0.5823\n",
      "Iteration 5484: Weights = [55.30333333  2.59333374  6.21051927  0.10214668  0.19571871 12.5048392 ], Loss = 0.5823\n",
      "Iteration 5485: Weights = [55.30333333  2.59316618  6.210118    0.10214008  0.19570607 12.50527148], Loss = 0.5822\n",
      "Iteration 5486: Weights = [55.30333333  2.59299863  6.20971675  0.10213348  0.19569342 12.50570373], Loss = 0.5821\n",
      "Iteration 5487: Weights = [55.30333333  2.59283109  6.20931553  0.10212688  0.19568078 12.50613594], Loss = 0.5820\n",
      "Iteration 5488: Weights = [55.30333333  2.59266357  6.20891433  0.10212028  0.19566814 12.50656814], Loss = 0.5820\n",
      "Iteration 5489: Weights = [55.30333333  2.59249605  6.20851316  0.10211369  0.19565549 12.5070003 ], Loss = 0.5819\n",
      "Iteration 5490: Weights = [55.30333333  2.59232854  6.20811202  0.10210709  0.19564285 12.50743243], Loss = 0.5818\n",
      "Iteration 5491: Weights = [55.30333333  2.59216105  6.2077109   0.10210049  0.19563021 12.50786454], Loss = 0.5817\n",
      "Iteration 5492: Weights = [55.30333333  2.59199356  6.20730981  0.10209389  0.19561757 12.50829662], Loss = 0.5817\n",
      "Iteration 5493: Weights = [55.30333333  2.59182609  6.20690874  0.1020873   0.19560493 12.50872867], Loss = 0.5816\n",
      "Iteration 5494: Weights = [55.30333333  2.59165862  6.2065077   0.1020807   0.19559229 12.5091607 ], Loss = 0.5815\n",
      "Iteration 5495: Weights = [55.30333333  2.59149117  6.20610668  0.10207411  0.19557966 12.50959269], Loss = 0.5814\n",
      "Iteration 5496: Weights = [55.30333333  2.59132373  6.20570569  0.10206751  0.19556702 12.51002466], Loss = 0.5814\n",
      "Iteration 5497: Weights = [55.30333333  2.5911563   6.20530473  0.10206092  0.19555438 12.5104566 ], Loss = 0.5813\n",
      "Iteration 5498: Weights = [55.30333333  2.59098888  6.20490379  0.10205432  0.19554175 12.51088851], Loss = 0.5812\n",
      "Iteration 5499: Weights = [55.30333333  2.59082147  6.20450288  0.10204773  0.19552911 12.5113204 ], Loss = 0.5811\n",
      "Iteration 5500: Weights = [55.30333333  2.59065407  6.204102    0.10204113  0.19551648 12.51175225], Loss = 0.5811\n",
      "Iteration 5501: Weights = [55.30333333  2.59048669  6.20370114  0.10203454  0.19550385 12.51218408], Loss = 0.5810\n",
      "Iteration 5502: Weights = [55.30333333  2.59031931  6.2033003   0.10202795  0.19549121 12.51261588], Loss = 0.5809\n",
      "Iteration 5503: Weights = [55.30333333  2.59015194  6.2028995   0.10202136  0.19547858 12.51304766], Loss = 0.5808\n",
      "Iteration 5504: Weights = [55.30333333  2.58998459  6.20249872  0.10201476  0.19546595 12.5134794 ], Loss = 0.5808\n",
      "Iteration 5505: Weights = [55.30333333  2.58981724  6.20209796  0.10200817  0.19545332 12.51391112], Loss = 0.5807\n",
      "Iteration 5506: Weights = [55.30333333  2.58964991  6.20169723  0.10200158  0.1954407  12.51434281], Loss = 0.5806\n",
      "Iteration 5507: Weights = [55.30333333  2.58948259  6.20129653  0.10199499  0.19542807 12.51477447], Loss = 0.5805\n",
      "Iteration 5508: Weights = [55.30333333  2.58931528  6.20089585  0.1019884   0.19541544 12.5152061 ], Loss = 0.5805\n",
      "Iteration 5509: Weights = [55.30333333  2.58914798  6.20049519  0.10198181  0.19540281 12.51563771], Loss = 0.5804\n",
      "Iteration 5510: Weights = [55.30333333  2.58898069  6.20009457  0.10197522  0.19539019 12.51606928], Loss = 0.5803\n",
      "Iteration 5511: Weights = [55.30333333  2.58881341  6.19969397  0.10196863  0.19537756 12.51650083], Loss = 0.5802\n",
      "Iteration 5512: Weights = [55.30333333  2.58864614  6.19929339  0.10196204  0.19536494 12.51693236], Loss = 0.5802\n",
      "Iteration 5513: Weights = [55.30333333  2.58847888  6.19889284  0.10195546  0.19535232 12.51736385], Loss = 0.5801\n",
      "Iteration 5514: Weights = [55.30333333  2.58831163  6.19849232  0.10194887  0.1953397  12.51779531], Loss = 0.5800\n",
      "Iteration 5515: Weights = [55.30333333  2.5881444   6.19809182  0.10194228  0.19532707 12.51822675], Loss = 0.5799\n",
      "Iteration 5516: Weights = [55.30333333  2.58797717  6.19769135  0.1019357   0.19531445 12.51865816], Loss = 0.5799\n",
      "Iteration 5517: Weights = [55.30333333  2.58780996  6.19729091  0.10192911  0.19530183 12.51908955], Loss = 0.5798\n",
      "Iteration 5518: Weights = [55.30333333  2.58764276  6.19689049  0.10192252  0.19528922 12.5195209 ], Loss = 0.5797\n",
      "Iteration 5519: Weights = [55.30333333  2.58747556  6.19649009  0.10191594  0.1952766  12.51995223], Loss = 0.5796\n",
      "Iteration 5520: Weights = [55.30333333  2.58730838  6.19608973  0.10190935  0.19526398 12.52038353], Loss = 0.5796\n",
      "Iteration 5521: Weights = [55.30333333  2.58714121  6.19568939  0.10190277  0.19525136 12.5208148 ], Loss = 0.5795\n",
      "Iteration 5522: Weights = [55.30333333  2.58697405  6.19528907  0.10189618  0.19523875 12.52124604], Loss = 0.5794\n",
      "Iteration 5523: Weights = [55.30333333  2.5868069   6.19488878  0.1018896   0.19522613 12.52167725], Loss = 0.5793\n",
      "Iteration 5524: Weights = [55.30333333  2.58663976  6.19448852  0.10188302  0.19521352 12.52210844], Loss = 0.5793\n",
      "Iteration 5525: Weights = [55.30333333  2.58647263  6.19408828  0.10187643  0.19520091 12.5225396 ], Loss = 0.5792\n",
      "Iteration 5526: Weights = [55.30333333  2.58630552  6.19368806  0.10186985  0.19518829 12.52297073], Loss = 0.5791\n",
      "Iteration 5527: Weights = [55.30333333  2.58613841  6.19328788  0.10186327  0.19517568 12.52340184], Loss = 0.5790\n",
      "Iteration 5528: Weights = [55.30333333  2.58597131  6.19288772  0.10185669  0.19516307 12.52383291], Loss = 0.5790\n",
      "Iteration 5529: Weights = [55.30333333  2.58580423  6.19248758  0.10185011  0.19515046 12.52426396], Loss = 0.5789\n",
      "Iteration 5530: Weights = [55.30333333  2.58563715  6.19208747  0.10184353  0.19513785 12.52469498], Loss = 0.5788\n",
      "Iteration 5531: Weights = [55.30333333  2.58547009  6.19168739  0.10183695  0.19512524 12.52512597], Loss = 0.5787\n",
      "Iteration 5532: Weights = [55.30333333  2.58530304  6.19128733  0.10183037  0.19511264 12.52555694], Loss = 0.5787\n",
      "Iteration 5533: Weights = [55.30333333  2.585136    6.1908873   0.10182379  0.19510003 12.52598787], Loss = 0.5786\n",
      "Iteration 5534: Weights = [55.30333333  2.58496897  6.1904873   0.10181721  0.19508742 12.52641878], Loss = 0.5785\n",
      "Iteration 5535: Weights = [55.30333333  2.58480195  6.19008732  0.10181063  0.19507482 12.52684966], Loss = 0.5784\n",
      "Iteration 5536: Weights = [55.30333333  2.58463494  6.18968736  0.10180405  0.19506222 12.52728051], Loss = 0.5784\n",
      "Iteration 5537: Weights = [55.30333333  2.58446794  6.18928743  0.10179747  0.19504961 12.52771134], Loss = 0.5783\n",
      "Iteration 5538: Weights = [55.30333333  2.58430095  6.18888753  0.1017909   0.19503701 12.52814214], Loss = 0.5782\n",
      "Iteration 5539: Weights = [55.30333333  2.58413397  6.18848765  0.10178432  0.19502441 12.52857291], Loss = 0.5781\n",
      "Iteration 5540: Weights = [55.30333333  2.58396701  6.1880878   0.10177774  0.19501181 12.52900365], Loss = 0.5781\n",
      "Iteration 5541: Weights = [55.30333333  2.58380005  6.18768798  0.10177117  0.19499921 12.52943436], Loss = 0.5780\n",
      "Iteration 5542: Weights = [55.30333333  2.58363311  6.18728818  0.10176459  0.19498661 12.52986505], Loss = 0.5779\n",
      "Iteration 5543: Weights = [55.30333333  2.58346617  6.18688841  0.10175802  0.19497401 12.53029571], Loss = 0.5778\n",
      "Iteration 5544: Weights = [55.30333333  2.58329925  6.18648866  0.10175144  0.19496141 12.53072634], Loss = 0.5778\n",
      "Iteration 5545: Weights = [55.30333333  2.58313234  6.18608894  0.10174487  0.19494881 12.53115694], Loss = 0.5777\n",
      "Iteration 5546: Weights = [55.30333333  2.58296544  6.18568924  0.10173829  0.19493622 12.53158751], Loss = 0.5776\n",
      "Iteration 5547: Weights = [55.30333333  2.58279855  6.18528957  0.10173172  0.19492362 12.53201806], Loss = 0.5775\n",
      "Iteration 5548: Weights = [55.30333333  2.58263167  6.18488993  0.10172515  0.19491103 12.53244858], Loss = 0.5775\n",
      "Iteration 5549: Weights = [55.30333333  2.5824648   6.18449031  0.10171857  0.19489844 12.53287907], Loss = 0.5774\n",
      "Iteration 5550: Weights = [55.30333333  2.58229794  6.18409072  0.101712    0.19488584 12.53330953], Loss = 0.5773\n",
      "Iteration 5551: Weights = [55.30333333  2.58213109  6.18369115  0.10170543  0.19487325 12.53373997], Loss = 0.5772\n",
      "Iteration 5552: Weights = [55.30333333  2.58196426  6.18329161  0.10169886  0.19486066 12.53417038], Loss = 0.5772\n",
      "Iteration 5553: Weights = [55.30333333  2.58179743  6.1828921   0.10169229  0.19484807 12.53460076], Loss = 0.5771\n",
      "Iteration 5554: Weights = [55.30333333  2.58163062  6.18249261  0.10168572  0.19483548 12.53503111], Loss = 0.5770\n",
      "Iteration 5555: Weights = [55.30333333  2.58146381  6.18209314  0.10167915  0.19482289 12.53546143], Loss = 0.5769\n",
      "Iteration 5556: Weights = [55.30333333  2.58129702  6.18169371  0.10167258  0.1948103  12.53589173], Loss = 0.5769\n",
      "Iteration 5557: Weights = [55.30333333  2.58113023  6.18129429  0.10166601  0.19479772 12.536322  ], Loss = 0.5768\n",
      "Iteration 5558: Weights = [55.30333333  2.58096346  6.18089491  0.10165944  0.19478513 12.53675224], Loss = 0.5767\n",
      "Iteration 5559: Weights = [55.30333333  2.5807967   6.18049555  0.10165287  0.19477254 12.53718245], Loss = 0.5766\n",
      "Iteration 5560: Weights = [55.30333333  2.58062995  6.18009621  0.1016463   0.19475996 12.53761264], Loss = 0.5766\n",
      "Iteration 5561: Weights = [55.30333333  2.58046321  6.17969691  0.10163973  0.19474738 12.5380428 ], Loss = 0.5765\n",
      "Iteration 5562: Weights = [55.30333333  2.58029648  6.17929762  0.10163317  0.19473479 12.53847293], Loss = 0.5764\n",
      "Iteration 5563: Weights = [55.30333333  2.58012976  6.17889837  0.1016266   0.19472221 12.53890303], Loss = 0.5763\n",
      "Iteration 5564: Weights = [55.30333333  2.57996306  6.17849914  0.10162003  0.19470963 12.5393331 ], Loss = 0.5763\n",
      "Iteration 5565: Weights = [55.30333333  2.57979636  6.17809993  0.10161347  0.19469705 12.53976315], Loss = 0.5762\n",
      "Iteration 5566: Weights = [55.30333333  2.57962967  6.17770075  0.1016069   0.19468447 12.54019317], Loss = 0.5761\n",
      "Iteration 5567: Weights = [55.30333333  2.579463    6.1773016   0.10160034  0.19467189 12.54062316], Loss = 0.5761\n",
      "Iteration 5568: Weights = [55.30333333  2.57929633  6.17690247  0.10159377  0.19465931 12.54105312], Loss = 0.5760\n",
      "Iteration 5569: Weights = [55.30333333  2.57912968  6.17650337  0.10158721  0.19464673 12.54148306], Loss = 0.5759\n",
      "Iteration 5570: Weights = [55.30333333  2.57896304  6.17610429  0.10158065  0.19463416 12.54191297], Loss = 0.5758\n",
      "Iteration 5571: Weights = [55.30333333  2.57879641  6.17570524  0.10157408  0.19462158 12.54234284], Loss = 0.5758\n",
      "Iteration 5572: Weights = [55.30333333  2.57862979  6.17530622  0.10156752  0.19460901 12.5427727 ], Loss = 0.5757\n",
      "Iteration 5573: Weights = [55.30333333  2.57846317  6.17490722  0.10156096  0.19459643 12.54320252], Loss = 0.5756\n",
      "Iteration 5574: Weights = [55.30333333  2.57829658  6.17450824  0.10155439  0.19458386 12.54363232], Loss = 0.5755\n",
      "Iteration 5575: Weights = [55.30333333  2.57812999  6.1741093   0.10154783  0.19457129 12.54406209], Loss = 0.5755\n",
      "Iteration 5576: Weights = [55.30333333  2.57796341  6.17371037  0.10154127  0.19455872 12.54449183], Loss = 0.5754\n",
      "Iteration 5577: Weights = [55.30333333  2.57779684  6.17331148  0.10153471  0.19454614 12.54492154], Loss = 0.5753\n",
      "Iteration 5578: Weights = [55.30333333  2.57763028  6.17291261  0.10152815  0.19453357 12.54535123], Loss = 0.5752\n",
      "Iteration 5579: Weights = [55.30333333  2.57746374  6.17251376  0.10152159  0.19452101 12.54578088], Loss = 0.5752\n",
      "Iteration 5580: Weights = [55.30333333  2.5772972   6.17211495  0.10151503  0.19450844 12.54621051], Loss = 0.5751\n",
      "Iteration 5581: Weights = [55.30333333  2.57713068  6.17171615  0.10150847  0.19449587 12.54664011], Loss = 0.5750\n",
      "Iteration 5582: Weights = [55.30333333  2.57696417  6.17131739  0.10150191  0.1944833  12.54706969], Loss = 0.5749\n",
      "Iteration 5583: Weights = [55.30333333  2.57679766  6.17091864  0.10149535  0.19447074 12.54749924], Loss = 0.5749\n",
      "Iteration 5584: Weights = [55.30333333  2.57663117  6.17051993  0.1014888   0.19445817 12.54792875], Loss = 0.5748\n",
      "Iteration 5585: Weights = [55.30333333  2.57646469  6.17012124  0.10148224  0.19444561 12.54835825], Loss = 0.5747\n",
      "Iteration 5586: Weights = [55.30333333  2.57629822  6.16972258  0.10147568  0.19443304 12.54878771], Loss = 0.5746\n",
      "Iteration 5587: Weights = [55.30333333  2.57613176  6.16932394  0.10146913  0.19442048 12.54921714], Loss = 0.5746\n",
      "Iteration 5588: Weights = [55.30333333  2.57596531  6.16892533  0.10146257  0.19440792 12.54964655], Loss = 0.5745\n",
      "Iteration 5589: Weights = [55.30333333  2.57579887  6.16852674  0.10145601  0.19439536 12.55007593], Loss = 0.5744\n",
      "Iteration 5590: Weights = [55.30333333  2.57563244  6.16812818  0.10144946  0.1943828  12.55050528], Loss = 0.5743\n",
      "Iteration 5591: Weights = [55.30333333  2.57546603  6.16772964  0.1014429   0.19437024 12.55093461], Loss = 0.5743\n",
      "Iteration 5592: Weights = [55.30333333  2.57529962  6.16733113  0.10143635  0.19435768 12.55136391], Loss = 0.5742\n",
      "Iteration 5593: Weights = [55.30333333  2.57513323  6.16693265  0.1014298   0.19434512 12.55179317], Loss = 0.5741\n",
      "Iteration 5594: Weights = [55.30333333  2.57496684  6.16653419  0.10142324  0.19433257 12.55222242], Loss = 0.5740\n",
      "Iteration 5595: Weights = [55.30333333  2.57480047  6.16613576  0.10141669  0.19432001 12.55265163], Loss = 0.5740\n",
      "Iteration 5596: Weights = [55.30333333  2.5746341   6.16573735  0.10141014  0.19430745 12.55308082], Loss = 0.5739\n",
      "Iteration 5597: Weights = [55.30333333  2.57446775  6.16533897  0.10140358  0.1942949  12.55350997], Loss = 0.5738\n",
      "Iteration 5598: Weights = [55.30333333  2.57430141  6.16494062  0.10139703  0.19428235 12.5539391 ], Loss = 0.5737\n",
      "Iteration 5599: Weights = [55.30333333  2.57413508  6.16454229  0.10139048  0.19426979 12.55436821], Loss = 0.5737\n",
      "Iteration 5600: Weights = [55.30333333  2.57396876  6.16414398  0.10138393  0.19425724 12.55479728], Loss = 0.5736\n",
      "Iteration 5601: Weights = [55.30333333  2.57380245  6.16374571  0.10137738  0.19424469 12.55522633], Loss = 0.5735\n",
      "Iteration 5602: Weights = [55.30333333  2.57363615  6.16334746  0.10137083  0.19423214 12.55565535], Loss = 0.5735\n",
      "Iteration 5603: Weights = [55.30333333  2.57346986  6.16294923  0.10136428  0.19421959 12.55608434], Loss = 0.5734\n",
      "Iteration 5604: Weights = [55.30333333  2.57330359  6.16255103  0.10135773  0.19420704 12.5565133 ], Loss = 0.5733\n",
      "Iteration 5605: Weights = [55.30333333  2.57313732  6.16215285  0.10135118  0.19419449 12.55694224], Loss = 0.5732\n",
      "Iteration 5606: Weights = [55.30333333  2.57297107  6.1617547   0.10134463  0.19418194 12.55737115], Loss = 0.5732\n",
      "Iteration 5607: Weights = [55.30333333  2.57280482  6.16135658  0.10133808  0.1941694  12.55780003], Loss = 0.5731\n",
      "Iteration 5608: Weights = [55.30333333  2.57263859  6.16095848  0.10133154  0.19415685 12.55822888], Loss = 0.5730\n",
      "Iteration 5609: Weights = [55.30333333  2.57247236  6.16056041  0.10132499  0.19414431 12.55865771], Loss = 0.5729\n",
      "Iteration 5610: Weights = [55.30333333  2.57230615  6.16016237  0.10131844  0.19413176 12.55908651], Loss = 0.5729\n",
      "Iteration 5611: Weights = [55.30333333  2.57213995  6.15976434  0.1013119   0.19411922 12.55951528], Loss = 0.5728\n",
      "Iteration 5612: Weights = [55.30333333  2.57197376  6.15936635  0.10130535  0.19410668 12.55994402], Loss = 0.5727\n",
      "Iteration 5613: Weights = [55.30333333  2.57180758  6.15896838  0.1012988   0.19409414 12.56037273], Loss = 0.5726\n",
      "Iteration 5614: Weights = [55.30333333  2.57164141  6.15857044  0.10129226  0.19408159 12.56080142], Loss = 0.5726\n",
      "Iteration 5615: Weights = [55.30333333  2.57147525  6.15817252  0.10128571  0.19406905 12.56123008], Loss = 0.5725\n",
      "Iteration 5616: Weights = [55.30333333  2.5713091   6.15777463  0.10127917  0.19405652 12.56165871], Loss = 0.5724\n",
      "Iteration 5617: Weights = [55.30333333  2.57114296  6.15737676  0.10127263  0.19404398 12.56208732], Loss = 0.5723\n",
      "Iteration 5618: Weights = [55.30333333  2.57097684  6.15697892  0.10126608  0.19403144 12.56251589], Loss = 0.5723\n",
      "Iteration 5619: Weights = [55.30333333  2.57081072  6.15658111  0.10125954  0.1940189  12.56294444], Loss = 0.5722\n",
      "Iteration 5620: Weights = [55.30333333  2.57064461  6.15618332  0.101253    0.19400637 12.56337296], Loss = 0.5721\n",
      "Iteration 5621: Weights = [55.30333333  2.57047852  6.15578555  0.10124646  0.19399383 12.56380146], Loss = 0.5720\n",
      "Iteration 5622: Weights = [55.30333333  2.57031244  6.15538782  0.10123991  0.1939813  12.56422992], Loss = 0.5720\n",
      "Iteration 5623: Weights = [55.30333333  2.57014636  6.1549901   0.10123337  0.19396876 12.56465836], Loss = 0.5719\n",
      "Iteration 5624: Weights = [55.30333333  2.5699803   6.15459242  0.10122683  0.19395623 12.56508677], Loss = 0.5718\n",
      "Iteration 5625: Weights = [55.30333333  2.56981425  6.15419476  0.10122029  0.1939437  12.56551515], Loss = 0.5717\n",
      "Iteration 5626: Weights = [55.30333333  2.56964821  6.15379712  0.10121375  0.19393117 12.56594351], Loss = 0.5717\n",
      "Iteration 5627: Weights = [55.30333333  2.56948218  6.15339951  0.10120721  0.19391864 12.56637183], Loss = 0.5716\n",
      "Iteration 5628: Weights = [55.30333333  2.56931616  6.15300193  0.10120067  0.19390611 12.56680013], Loss = 0.5715\n",
      "Iteration 5629: Weights = [55.30333333  2.56915015  6.15260437  0.10119413  0.19389358 12.5672284 ], Loss = 0.5715\n",
      "Iteration 5630: Weights = [55.30333333  2.56898415  6.15220684  0.10118759  0.19388105 12.56765665], Loss = 0.5714\n",
      "Iteration 5631: Weights = [55.30333333  2.56881817  6.15180933  0.10118106  0.19386852 12.56808486], Loss = 0.5713\n",
      "Iteration 5632: Weights = [55.30333333  2.56865219  6.15141185  0.10117452  0.193856   12.56851305], Loss = 0.5712\n",
      "Iteration 5633: Weights = [55.30333333  2.56848622  6.1510144   0.10116798  0.19384347 12.56894121], Loss = 0.5712\n",
      "Iteration 5634: Weights = [55.30333333  2.56832027  6.15061697  0.10116145  0.19383095 12.56936935], Loss = 0.5711\n",
      "Iteration 5635: Weights = [55.30333333  2.56815432  6.15021956  0.10115491  0.19381842 12.56979745], Loss = 0.5710\n",
      "Iteration 5636: Weights = [55.30333333  2.56798839  6.14982219  0.10114837  0.1938059  12.57022553], Loss = 0.5709\n",
      "Iteration 5637: Weights = [55.30333333  2.56782247  6.14942483  0.10114184  0.19379338 12.57065358], Loss = 0.5709\n",
      "Iteration 5638: Weights = [55.30333333  2.56765655  6.14902751  0.1011353   0.19378086 12.5710816 ], Loss = 0.5708\n",
      "Iteration 5639: Weights = [55.30333333  2.56749065  6.14863021  0.10112877  0.19376834 12.5715096 ], Loss = 0.5707\n",
      "Iteration 5640: Weights = [55.30333333  2.56732476  6.14823293  0.10112223  0.19375582 12.57193757], Loss = 0.5706\n",
      "Iteration 5641: Weights = [55.30333333  2.56715888  6.14783568  0.1011157   0.1937433  12.57236551], Loss = 0.5706\n",
      "Iteration 5642: Weights = [55.30333333  2.56699301  6.14743846  0.10110917  0.19373078 12.57279342], Loss = 0.5705\n",
      "Iteration 5643: Weights = [55.30333333  2.56682715  6.14704126  0.10110263  0.19371826 12.5732213 ], Loss = 0.5704\n",
      "Iteration 5644: Weights = [55.30333333  2.56666131  6.14664409  0.1010961   0.19370575 12.57364916], Loss = 0.5703\n",
      "Iteration 5645: Weights = [55.30333333  2.56649547  6.14624694  0.10108957  0.19369323 12.57407699], Loss = 0.5703\n",
      "Iteration 5646: Weights = [55.30333333  2.56632964  6.14584982  0.10108304  0.19368072 12.57450479], Loss = 0.5702\n",
      "Iteration 5647: Weights = [55.30333333  2.56616383  6.14545272  0.10107651  0.1936682  12.57493256], Loss = 0.5701\n",
      "Iteration 5648: Weights = [55.30333333  2.56599802  6.14505565  0.10106998  0.19365569 12.57536031], Loss = 0.5701\n",
      "Iteration 5649: Weights = [55.30333333  2.56583223  6.14465861  0.10106345  0.19364318 12.57578803], Loss = 0.5700\n",
      "Iteration 5650: Weights = [55.30333333  2.56566644  6.14426159  0.10105692  0.19363066 12.57621572], Loss = 0.5699\n",
      "Iteration 5651: Weights = [55.30333333  2.56550067  6.1438646   0.10105039  0.19361815 12.57664338], Loss = 0.5698\n",
      "Iteration 5652: Weights = [55.30333333  2.56533491  6.14346763  0.10104386  0.19360564 12.57707102], Loss = 0.5698\n",
      "Iteration 5653: Weights = [55.30333333  2.56516916  6.14307069  0.10103733  0.19359313 12.57749863], Loss = 0.5697\n",
      "Iteration 5654: Weights = [55.30333333  2.56500342  6.14267377  0.1010308   0.19358063 12.57792621], Loss = 0.5696\n",
      "Iteration 5655: Weights = [55.30333333  2.56483769  6.14227688  0.10102427  0.19356812 12.57835376], Loss = 0.5695\n",
      "Iteration 5656: Weights = [55.30333333  2.56467197  6.14188002  0.10101775  0.19355561 12.57878129], Loss = 0.5695\n",
      "Iteration 5657: Weights = [55.30333333  2.56450626  6.14148318  0.10101122  0.19354311 12.57920878], Loss = 0.5694\n",
      "Iteration 5658: Weights = [55.30333333  2.56434056  6.14108636  0.10100469  0.1935306  12.57963625], Loss = 0.5693\n",
      "Iteration 5659: Weights = [55.30333333  2.56417487  6.14068957  0.10099817  0.1935181  12.5800637 ], Loss = 0.5692\n",
      "Iteration 5660: Weights = [55.30333333  2.5640092   6.14029281  0.10099164  0.19350559 12.58049111], Loss = 0.5692\n",
      "Iteration 5661: Weights = [55.30333333  2.56384353  6.13989608  0.10098512  0.19349309 12.5809185 ], Loss = 0.5691\n",
      "Iteration 5662: Weights = [55.30333333  2.56367788  6.13949936  0.10097859  0.19348059 12.58134586], Loss = 0.5690\n",
      "Iteration 5663: Weights = [55.30333333  2.56351223  6.13910268  0.10097207  0.19346809 12.58177319], Loss = 0.5689\n",
      "Iteration 5664: Weights = [55.30333333  2.5633466   6.13870602  0.10096554  0.19345559 12.58220049], Loss = 0.5689\n",
      "Iteration 5665: Weights = [55.30333333  2.56318098  6.13830939  0.10095902  0.19344309 12.58262777], Loss = 0.5688\n",
      "Iteration 5666: Weights = [55.30333333  2.56301536  6.13791278  0.1009525   0.19343059 12.58305502], Loss = 0.5687\n",
      "Iteration 5667: Weights = [55.30333333  2.56284976  6.13751619  0.10094597  0.19341809 12.58348224], Loss = 0.5687\n",
      "Iteration 5668: Weights = [55.30333333  2.56268417  6.13711964  0.10093945  0.19340559 12.58390944], Loss = 0.5686\n",
      "Iteration 5669: Weights = [55.30333333  2.56251859  6.13672311  0.10093293  0.1933931  12.5843366 ], Loss = 0.5685\n",
      "Iteration 5670: Weights = [55.30333333  2.56235302  6.1363266   0.10092641  0.1933806  12.58476374], Loss = 0.5684\n",
      "Iteration 5671: Weights = [55.30333333  2.56218746  6.13593012  0.10091989  0.19336811 12.58519085], Loss = 0.5684\n",
      "Iteration 5672: Weights = [55.30333333  2.56202191  6.13553366  0.10091337  0.19335561 12.58561793], Loss = 0.5683\n",
      "Iteration 5673: Weights = [55.30333333  2.56185638  6.13513723  0.10090684  0.19334312 12.58604499], Loss = 0.5682\n",
      "Iteration 5674: Weights = [55.30333333  2.56169085  6.13474083  0.10090033  0.19333063 12.58647202], Loss = 0.5681\n",
      "Iteration 5675: Weights = [55.30333333  2.56152533  6.13434445  0.10089381  0.19331814 12.58689902], Loss = 0.5681\n",
      "Iteration 5676: Weights = [55.30333333  2.56135983  6.1339481   0.10088729  0.19330564 12.58732599], Loss = 0.5680\n",
      "Iteration 5677: Weights = [55.30333333  2.56119433  6.13355177  0.10088077  0.19329315 12.58775294], Loss = 0.5679\n",
      "Iteration 5678: Weights = [55.30333333  2.56102885  6.13315547  0.10087425  0.19328067 12.58817986], Loss = 0.5678\n",
      "Iteration 5679: Weights = [55.30333333  2.56086338  6.1327592   0.10086773  0.19326818 12.58860675], Loss = 0.5678\n",
      "Iteration 5680: Weights = [55.30333333  2.56069792  6.13236295  0.10086122  0.19325569 12.58903361], Loss = 0.5677\n",
      "Iteration 5681: Weights = [55.30333333  2.56053246  6.13196672  0.1008547   0.1932432  12.58946044], Loss = 0.5676\n",
      "Iteration 5682: Weights = [55.30333333  2.56036702  6.13157052  0.10084818  0.19323072 12.58988725], Loss = 0.5676\n",
      "Iteration 5683: Weights = [55.30333333  2.56020159  6.13117435  0.10084167  0.19321823 12.59031403], Loss = 0.5675\n",
      "Iteration 5684: Weights = [55.30333333  2.56003617  6.1307782   0.10083515  0.19320575 12.59074078], Loss = 0.5674\n",
      "Iteration 5685: Weights = [55.30333333  2.55987076  6.13038208  0.10082864  0.19319327 12.59116751], Loss = 0.5673\n",
      "Iteration 5686: Weights = [55.30333333  2.55970536  6.12998599  0.10082212  0.19318078 12.59159421], Loss = 0.5673\n",
      "Iteration 5687: Weights = [55.30333333  2.55953998  6.12958992  0.10081561  0.1931683  12.59202088], Loss = 0.5672\n",
      "Iteration 5688: Weights = [55.30333333  2.5593746   6.12919387  0.10080909  0.19315582 12.59244752], Loss = 0.5671\n",
      "Iteration 5689: Weights = [55.30333333  2.55920923  6.12879785  0.10080258  0.19314334 12.59287413], Loss = 0.5670\n",
      "Iteration 5690: Weights = [55.30333333  2.55904388  6.12840186  0.10079607  0.19313086 12.59330072], Loss = 0.5670\n",
      "Iteration 5691: Weights = [55.30333333  2.55887853  6.12800589  0.10078955  0.19311838 12.59372728], Loss = 0.5669\n",
      "Iteration 5692: Weights = [55.30333333  2.5587132   6.12760995  0.10078304  0.1931059  12.59415381], Loss = 0.5668\n",
      "Iteration 5693: Weights = [55.30333333  2.55854787  6.12721403  0.10077653  0.19309343 12.59458032], Loss = 0.5667\n",
      "Iteration 5694: Weights = [55.30333333  2.55838256  6.12681814  0.10077002  0.19308095 12.59500679], Loss = 0.5667\n",
      "Iteration 5695: Weights = [55.30333333  2.55821726  6.12642227  0.10076351  0.19306848 12.59543324], Loss = 0.5666\n",
      "Iteration 5696: Weights = [55.30333333  2.55805197  6.12602643  0.100757    0.193056   12.59585966], Loss = 0.5665\n",
      "Iteration 5697: Weights = [55.30333333  2.55788669  6.12563062  0.10075049  0.19304353 12.59628606], Loss = 0.5665\n",
      "Iteration 5698: Weights = [55.30333333  2.55772142  6.12523483  0.10074398  0.19303105 12.59671242], Loss = 0.5664\n",
      "Iteration 5699: Weights = [55.30333333  2.55755616  6.12483906  0.10073747  0.19301858 12.59713876], Loss = 0.5663\n",
      "Iteration 5700: Weights = [55.30333333  2.55739091  6.12444332  0.10073096  0.19300611 12.59756507], Loss = 0.5662\n",
      "Iteration 5701: Weights = [55.30333333  2.55722567  6.12404761  0.10072445  0.19299364 12.59799136], Loss = 0.5662\n",
      "Iteration 5702: Weights = [55.30333333  2.55706044  6.12365192  0.10071794  0.19298117 12.59841761], Loss = 0.5661\n",
      "Iteration 5703: Weights = [55.30333333  2.55689523  6.12325626  0.10071143  0.1929687  12.59884384], Loss = 0.5660\n",
      "Iteration 5704: Weights = [55.30333333  2.55673002  6.12286063  0.10070493  0.19295623 12.59927005], Loss = 0.5659\n",
      "Iteration 5705: Weights = [55.30333333  2.55656482  6.12246502  0.10069842  0.19294377 12.59969622], Loss = 0.5659\n",
      "Iteration 5706: Weights = [55.30333333  2.55639964  6.12206943  0.10069191  0.1929313  12.60012237], Loss = 0.5658\n",
      "Iteration 5707: Weights = [55.30333333  2.55623447  6.12167387  0.10068541  0.19291883 12.60054848], Loss = 0.5657\n",
      "Iteration 5708: Weights = [55.30333333  2.5560693   6.12127834  0.1006789   0.19290637 12.60097457], Loss = 0.5656\n",
      "Iteration 5709: Weights = [55.30333333  2.55590415  6.12088283  0.1006724   0.19289391 12.60140064], Loss = 0.5656\n",
      "Iteration 5710: Weights = [55.30333333  2.55573901  6.12048735  0.10066589  0.19288144 12.60182667], Loss = 0.5655\n",
      "Iteration 5711: Weights = [55.30333333  2.55557388  6.12009189  0.10065939  0.19286898 12.60225268], Loss = 0.5654\n",
      "Iteration 5712: Weights = [55.30333333  2.55540876  6.11969646  0.10065288  0.19285652 12.60267866], Loss = 0.5654\n",
      "Iteration 5713: Weights = [55.30333333  2.55524365  6.11930105  0.10064638  0.19284406 12.60310462], Loss = 0.5653\n",
      "Iteration 5714: Weights = [55.30333333  2.55507855  6.11890567  0.10063988  0.1928316  12.60353054], Loss = 0.5652\n",
      "Iteration 5715: Weights = [55.30333333  2.55491346  6.11851032  0.10063338  0.19281914 12.60395644], Loss = 0.5651\n",
      "Iteration 5716: Weights = [55.30333333  2.55474838  6.11811499  0.10062687  0.19280668 12.60438231], Loss = 0.5651\n",
      "Iteration 5717: Weights = [55.30333333  2.55458331  6.11771969  0.10062037  0.19279422 12.60480816], Loss = 0.5650\n",
      "Iteration 5718: Weights = [55.30333333  2.55441825  6.11732441  0.10061387  0.19278176 12.60523397], Loss = 0.5649\n",
      "Iteration 5719: Weights = [55.30333333  2.55425321  6.11692916  0.10060737  0.19276931 12.60565976], Loss = 0.5648\n",
      "Iteration 5720: Weights = [55.30333333  2.55408817  6.11653393  0.10060087  0.19275685 12.60608552], Loss = 0.5648\n",
      "Iteration 5721: Weights = [55.30333333  2.55392315  6.11613873  0.10059437  0.1927444  12.60651126], Loss = 0.5647\n",
      "Iteration 5722: Weights = [55.30333333  2.55375813  6.11574355  0.10058787  0.19273195 12.60693696], Loss = 0.5646\n",
      "Iteration 5723: Weights = [55.30333333  2.55359313  6.1153484   0.10058137  0.19271949 12.60736264], Loss = 0.5646\n",
      "Iteration 5724: Weights = [55.30333333  2.55342814  6.11495328  0.10057487  0.19270704 12.60778829], Loss = 0.5645\n",
      "Iteration 5725: Weights = [55.30333333  2.55326316  6.11455818  0.10056837  0.19269459 12.60821391], Loss = 0.5644\n",
      "Iteration 5726: Weights = [55.30333333  2.55309818  6.1141631   0.10056188  0.19268214 12.60863951], Loss = 0.5643\n",
      "Iteration 5727: Weights = [55.30333333  2.55293322  6.11376805  0.10055538  0.19266969 12.60906508], Loss = 0.5643\n",
      "Iteration 5728: Weights = [55.30333333  2.55276827  6.11337303  0.10054888  0.19265724 12.60949062], Loss = 0.5642\n",
      "Iteration 5729: Weights = [55.30333333  2.55260333  6.11297803  0.10054238  0.19264479 12.60991613], Loss = 0.5641\n",
      "Iteration 5730: Weights = [55.30333333  2.55243841  6.11258306  0.10053589  0.19263235 12.61034162], Loss = 0.5640\n",
      "Iteration 5731: Weights = [55.30333333  2.55227349  6.11218812  0.10052939  0.1926199  12.61076708], Loss = 0.5640\n",
      "Iteration 5732: Weights = [55.30333333  2.55210858  6.11179319  0.1005229   0.19260745 12.61119251], Loss = 0.5639\n",
      "Iteration 5733: Weights = [55.30333333  2.55194368  6.1113983   0.1005164   0.19259501 12.61161791], Loss = 0.5638\n",
      "Iteration 5734: Weights = [55.30333333  2.5517788   6.11100343  0.10050991  0.19258256 12.61204329], Loss = 0.5638\n",
      "Iteration 5735: Weights = [55.30333333  2.55161392  6.11060859  0.10050341  0.19257012 12.61246864], Loss = 0.5637\n",
      "Iteration 5736: Weights = [55.30333333  2.55144906  6.11021377  0.10049692  0.19255768 12.61289396], Loss = 0.5636\n",
      "Iteration 5737: Weights = [55.30333333  2.5512842   6.10981897  0.10049043  0.19254524 12.61331925], Loss = 0.5635\n",
      "Iteration 5738: Weights = [55.30333333  2.55111936  6.10942421  0.10048393  0.1925328  12.61374452], Loss = 0.5635\n",
      "Iteration 5739: Weights = [55.30333333  2.55095453  6.10902946  0.10047744  0.19252036 12.61416976], Loss = 0.5634\n",
      "Iteration 5740: Weights = [55.30333333  2.5507897   6.10863475  0.10047095  0.19250792 12.61459497], Loss = 0.5633\n",
      "Iteration 5741: Weights = [55.30333333  2.55062489  6.10824006  0.10046446  0.19249548 12.61502015], Loss = 0.5632\n",
      "Iteration 5742: Weights = [55.30333333  2.55046009  6.10784539  0.10045797  0.19248304 12.61544531], Loss = 0.5632\n",
      "Iteration 5743: Weights = [55.30333333  2.5502953   6.10745075  0.10045148  0.19247061 12.61587043], Loss = 0.5631\n",
      "Iteration 5744: Weights = [55.30333333  2.55013052  6.10705614  0.10044498  0.19245817 12.61629554], Loss = 0.5630\n",
      "Iteration 5745: Weights = [55.30333333  2.54996575  6.10666155  0.10043849  0.19244573 12.61672061], Loss = 0.5630\n",
      "Iteration 5746: Weights = [55.30333333  2.54980099  6.10626698  0.10043201  0.1924333  12.61714566], Loss = 0.5629\n",
      "Iteration 5747: Weights = [55.30333333  2.54963624  6.10587244  0.10042552  0.19242087 12.61757067], Loss = 0.5628\n",
      "Iteration 5748: Weights = [55.30333333  2.54947151  6.10547793  0.10041903  0.19240843 12.61799567], Loss = 0.5627\n",
      "Iteration 5749: Weights = [55.30333333  2.54930678  6.10508344  0.10041254  0.192396   12.61842063], Loss = 0.5627\n",
      "Iteration 5750: Weights = [55.30333333  2.54914207  6.10468898  0.10040605  0.19238357 12.61884557], Loss = 0.5626\n",
      "Iteration 5751: Weights = [55.30333333  2.54897736  6.10429455  0.10039956  0.19237114 12.61927048], Loss = 0.5625\n",
      "Iteration 5752: Weights = [55.30333333  2.54881267  6.10390014  0.10039308  0.19235871 12.61969536], Loss = 0.5624\n",
      "Iteration 5753: Weights = [55.30333333  2.54864798  6.10350575  0.10038659  0.19234628 12.62012021], Loss = 0.5624\n",
      "Iteration 5754: Weights = [55.30333333  2.54848331  6.10311139  0.1003801   0.19233385 12.62054504], Loss = 0.5623\n",
      "Iteration 5755: Weights = [55.30333333  2.54831865  6.10271706  0.10037362  0.19232143 12.62096984], Loss = 0.5622\n",
      "Iteration 5756: Weights = [55.30333333  2.54815399  6.10232275  0.10036713  0.192309   12.62139461], Loss = 0.5622\n",
      "Iteration 5757: Weights = [55.30333333  2.54798935  6.10192846  0.10036065  0.19229658 12.62181935], Loss = 0.5621\n",
      "Iteration 5758: Weights = [55.30333333  2.54782472  6.10153421  0.10035416  0.19228415 12.62224407], Loss = 0.5620\n",
      "Iteration 5759: Weights = [55.30333333  2.5476601   6.10113997  0.10034768  0.19227173 12.62266876], Loss = 0.5619\n",
      "Iteration 5760: Weights = [55.30333333  2.54749549  6.10074577  0.1003412   0.1922593  12.62309342], Loss = 0.5619\n",
      "Iteration 5761: Weights = [55.30333333  2.54733089  6.10035159  0.10033471  0.19224688 12.62351806], Loss = 0.5618\n",
      "Iteration 5762: Weights = [55.30333333  2.54716631  6.09995743  0.10032823  0.19223446 12.62394266], Loss = 0.5617\n",
      "Iteration 5763: Weights = [55.30333333  2.54700173  6.0995633   0.10032175  0.19222204 12.62436724], Loss = 0.5616\n",
      "Iteration 5764: Weights = [55.30333333  2.54683716  6.09916919  0.10031527  0.19220962 12.62479179], Loss = 0.5616\n",
      "Iteration 5765: Weights = [55.30333333  2.5466726   6.09877511  0.10030878  0.1921972  12.62521632], Loss = 0.5615\n",
      "Iteration 5766: Weights = [55.30333333  2.54650806  6.09838106  0.1003023   0.19218478 12.62564082], Loss = 0.5614\n",
      "Iteration 5767: Weights = [55.30333333  2.54634352  6.09798703  0.10029582  0.19217237 12.62606529], Loss = 0.5614\n",
      "Iteration 5768: Weights = [55.30333333  2.546179    6.09759303  0.10028934  0.19215995 12.62648973], Loss = 0.5613\n",
      "Iteration 5769: Weights = [55.30333333  2.54601449  6.09719905  0.10028286  0.19214753 12.62691414], Loss = 0.5612\n",
      "Iteration 5770: Weights = [55.30333333  2.54584998  6.0968051   0.10027638  0.19213512 12.62733853], Loss = 0.5611\n",
      "Iteration 5771: Weights = [55.30333333  2.54568549  6.09641117  0.1002699   0.1921227  12.62776289], Loss = 0.5611\n",
      "Iteration 5772: Weights = [55.30333333  2.54552101  6.09601727  0.10026342  0.19211029 12.62818722], Loss = 0.5610\n",
      "Iteration 5773: Weights = [55.30333333  2.54535654  6.09562339  0.10025695  0.19209788 12.62861153], Loss = 0.5609\n",
      "Iteration 5774: Weights = [55.30333333  2.54519208  6.09522954  0.10025047  0.19208547 12.62903581], Loss = 0.5608\n",
      "Iteration 5775: Weights = [55.30333333  2.54502763  6.09483572  0.10024399  0.19207305 12.62946006], Loss = 0.5608\n",
      "Iteration 5776: Weights = [55.30333333  2.54486319  6.09444192  0.10023751  0.19206064 12.62988428], Loss = 0.5607\n",
      "Iteration 5777: Weights = [55.30333333  2.54469876  6.09404815  0.10023104  0.19204823 12.63030848], Loss = 0.5606\n",
      "Iteration 5778: Weights = [55.30333333  2.54453434  6.0936544   0.10022456  0.19203583 12.63073265], Loss = 0.5606\n",
      "Iteration 5779: Weights = [55.30333333  2.54436993  6.09326067  0.10021809  0.19202342 12.63115679], Loss = 0.5605\n",
      "Iteration 5780: Weights = [55.30333333  2.54420554  6.09286698  0.10021161  0.19201101 12.6315809 ], Loss = 0.5604\n",
      "Iteration 5781: Weights = [55.30333333  2.54404115  6.0924733   0.10020514  0.19199861 12.63200499], Loss = 0.5603\n",
      "Iteration 5782: Weights = [55.30333333  2.54387677  6.09207966  0.10019866  0.1919862  12.63242904], Loss = 0.5603\n",
      "Iteration 5783: Weights = [55.30333333  2.54371241  6.09168603  0.10019219  0.1919738  12.63285308], Loss = 0.5602\n",
      "Iteration 5784: Weights = [55.30333333  2.54354806  6.09129244  0.10018571  0.19196139 12.63327708], Loss = 0.5601\n",
      "Iteration 5785: Weights = [55.30333333  2.54338371  6.09089887  0.10017924  0.19194899 12.63370106], Loss = 0.5600\n",
      "Iteration 5786: Weights = [55.30333333  2.54321938  6.09050532  0.10017277  0.19193659 12.63412501], Loss = 0.5600\n",
      "Iteration 5787: Weights = [55.30333333  2.54305506  6.0901118   0.1001663   0.19192418 12.63454893], Loss = 0.5599\n",
      "Iteration 5788: Weights = [55.30333333  2.54289074  6.08971831  0.10015982  0.19191178 12.63497282], Loss = 0.5598\n",
      "Iteration 5789: Weights = [55.30333333  2.54272644  6.08932484  0.10015335  0.19189938 12.63539669], Loss = 0.5598\n",
      "Iteration 5790: Weights = [55.30333333  2.54256215  6.0889314   0.10014688  0.19188699 12.63582053], Loss = 0.5597\n",
      "Iteration 5791: Weights = [55.30333333  2.54239787  6.08853798  0.10014041  0.19187459 12.63624434], Loss = 0.5596\n",
      "Iteration 5792: Weights = [55.30333333  2.5422336   6.08814459  0.10013394  0.19186219 12.63666812], Loss = 0.5595\n",
      "Iteration 5793: Weights = [55.30333333  2.54206934  6.08775122  0.10012747  0.19184979 12.63709188], Loss = 0.5595\n",
      "Iteration 5794: Weights = [55.30333333  2.5419051   6.08735788  0.100121    0.1918374  12.63751561], Loss = 0.5594\n",
      "Iteration 5795: Weights = [55.30333333  2.54174086  6.08696456  0.10011453  0.191825   12.63793931], Loss = 0.5593\n",
      "Iteration 5796: Weights = [55.30333333  2.54157663  6.08657127  0.10010806  0.19181261 12.63836299], Loss = 0.5593\n",
      "Iteration 5797: Weights = [55.30333333  2.54141241  6.086178    0.10010159  0.19180021 12.63878664], Loss = 0.5592\n",
      "Iteration 5798: Weights = [55.30333333  2.54124821  6.08578476  0.10009513  0.19178782 12.63921026], Loss = 0.5591\n",
      "Iteration 5799: Weights = [55.30333333  2.54108401  6.08539155  0.10008866  0.19177543 12.63963385], Loss = 0.5590\n",
      "Iteration 5800: Weights = [55.30333333  2.54091983  6.08499836  0.10008219  0.19176304 12.64005742], Loss = 0.5590\n",
      "Iteration 5801: Weights = [55.30333333  2.54075565  6.0846052   0.10007573  0.19175065 12.64048096], Loss = 0.5589\n",
      "Iteration 5802: Weights = [55.30333333  2.54059149  6.08421206  0.10006926  0.19173826 12.64090447], Loss = 0.5588\n",
      "Iteration 5803: Weights = [55.30333333  2.54042734  6.08381894  0.10006279  0.19172587 12.64132795], Loss = 0.5587\n",
      "Iteration 5804: Weights = [55.30333333  2.5402632   6.08342586  0.10005633  0.19171348 12.64175141], Loss = 0.5587\n",
      "Iteration 5805: Weights = [55.30333333  2.54009907  6.08303279  0.10004986  0.1917011  12.64217484], Loss = 0.5586\n",
      "Iteration 5806: Weights = [55.30333333  2.53993494  6.08263976  0.1000434   0.19168871 12.64259824], Loss = 0.5585\n",
      "Iteration 5807: Weights = [55.30333333  2.53977083  6.08224674  0.10003694  0.19167632 12.64302161], Loss = 0.5585\n",
      "Iteration 5808: Weights = [55.30333333  2.53960673  6.08185376  0.10003047  0.19166394 12.64344496], Loss = 0.5584\n",
      "Iteration 5809: Weights = [55.30333333  2.53944265  6.0814608   0.10002401  0.19165156 12.64386828], Loss = 0.5583\n",
      "Iteration 5810: Weights = [55.30333333  2.53927857  6.08106786  0.10001755  0.19163917 12.64429157], Loss = 0.5582\n",
      "Iteration 5811: Weights = [55.30333333  2.5391145   6.08067495  0.10001108  0.19162679 12.64471484], Loss = 0.5582\n",
      "Iteration 5812: Weights = [55.30333333  2.53895044  6.08028207  0.10000462  0.19161441 12.64513807], Loss = 0.5581\n",
      "Iteration 5813: Weights = [55.30333333  2.53878639  6.07988921  0.09999816  0.19160203 12.64556128], Loss = 0.5580\n",
      "Iteration 5814: Weights = [55.30333333  2.53862236  6.07949637  0.0999917   0.19158965 12.64598447], Loss = 0.5580\n",
      "Iteration 5815: Weights = [55.30333333  2.53845833  6.07910357  0.09998524  0.19157727 12.64640762], Loss = 0.5579\n",
      "Iteration 5816: Weights = [55.30333333  2.53829432  6.07871078  0.09997878  0.19156489 12.64683075], Loss = 0.5578\n",
      "Iteration 5817: Weights = [55.30333333  2.53813031  6.07831803  0.09997232  0.19155251 12.64725385], Loss = 0.5577\n",
      "Iteration 5818: Weights = [55.30333333  2.53796632  6.07792529  0.09996586  0.19154014 12.64767692], Loss = 0.5577\n",
      "Iteration 5819: Weights = [55.30333333  2.53780234  6.07753259  0.0999594   0.19152776 12.64809997], Loss = 0.5576\n",
      "Iteration 5820: Weights = [55.30333333  2.53763836  6.0771399   0.09995294  0.19151539 12.64852299], Loss = 0.5575\n",
      "Iteration 5821: Weights = [55.30333333  2.5374744   6.07674725  0.09994648  0.19150301 12.64894598], Loss = 0.5574\n",
      "Iteration 5822: Weights = [55.30333333  2.53731045  6.07635462  0.09994003  0.19149064 12.64936894], Loss = 0.5574\n",
      "Iteration 5823: Weights = [55.30333333  2.53714651  6.07596201  0.09993357  0.19147827 12.64979188], Loss = 0.5573\n",
      "Iteration 5824: Weights = [55.30333333  2.53698258  6.07556943  0.09992711  0.1914659  12.65021479], Loss = 0.5572\n",
      "Iteration 5825: Weights = [55.30333333  2.53681866  6.07517688  0.09992065  0.19145352 12.65063767], Loss = 0.5572\n",
      "Iteration 5826: Weights = [55.30333333  2.53665475  6.07478435  0.0999142   0.19144115 12.65106053], Loss = 0.5571\n",
      "Iteration 5827: Weights = [55.30333333  2.53649085  6.07439184  0.09990774  0.19142878 12.65148335], Loss = 0.5570\n",
      "Iteration 5828: Weights = [55.30333333  2.53632697  6.07399936  0.09990129  0.19141642 12.65190616], Loss = 0.5569\n",
      "Iteration 5829: Weights = [55.30333333  2.53616309  6.07360691  0.09989483  0.19140405 12.65232893], Loss = 0.5569\n",
      "Iteration 5830: Weights = [55.30333333  2.53599922  6.07321448  0.09988838  0.19139168 12.65275167], Loss = 0.5568\n",
      "Iteration 5831: Weights = [55.30333333  2.53583537  6.07282208  0.09988192  0.19137932 12.65317439], Loss = 0.5567\n",
      "Iteration 5832: Weights = [55.30333333  2.53567152  6.0724297   0.09987547  0.19136695 12.65359708], Loss = 0.5567\n",
      "Iteration 5833: Weights = [55.30333333  2.53550769  6.07203735  0.09986902  0.19135459 12.65401975], Loss = 0.5566\n",
      "Iteration 5834: Weights = [55.30333333  2.53534386  6.07164503  0.09986257  0.19134222 12.65444238], Loss = 0.5565\n",
      "Iteration 5835: Weights = [55.30333333  2.53518005  6.07125272  0.09985611  0.19132986 12.65486499], Loss = 0.5564\n",
      "Iteration 5836: Weights = [55.30333333  2.53501625  6.07086045  0.09984966  0.1913175  12.65528757], Loss = 0.5564\n",
      "Iteration 5837: Weights = [55.30333333  2.53485245  6.0704682   0.09984321  0.19130513 12.65571013], Loss = 0.5563\n",
      "Iteration 5838: Weights = [55.30333333  2.53468867  6.07007597  0.09983676  0.19129277 12.65613265], Loss = 0.5562\n",
      "Iteration 5839: Weights = [55.30333333  2.5345249   6.06968377  0.09983031  0.19128041 12.65655515], Loss = 0.5562\n",
      "Iteration 5840: Weights = [55.30333333  2.53436114  6.0692916   0.09982386  0.19126806 12.65697763], Loss = 0.5561\n",
      "Iteration 5841: Weights = [55.30333333  2.53419739  6.06889945  0.09981741  0.1912557  12.65740007], Loss = 0.5560\n",
      "Iteration 5842: Weights = [55.30333333  2.53403365  6.06850733  0.09981096  0.19124334 12.65782249], Loss = 0.5559\n",
      "Iteration 5843: Weights = [55.30333333  2.53386992  6.06811523  0.09980451  0.19123098 12.65824488], Loss = 0.5559\n",
      "Iteration 5844: Weights = [55.30333333  2.5337062   6.06772315  0.09979806  0.19121863 12.65866724], Loss = 0.5558\n",
      "Iteration 5845: Weights = [55.30333333  2.53354249  6.06733111  0.09979161  0.19120627 12.65908958], Loss = 0.5557\n",
      "Iteration 5846: Weights = [55.30333333  2.5333788   6.06693908  0.09978516  0.19119392 12.65951189], Loss = 0.5557\n",
      "Iteration 5847: Weights = [55.30333333  2.53321511  6.06654709  0.09977872  0.19118156 12.65993417], Loss = 0.5556\n",
      "Iteration 5848: Weights = [55.30333333  2.53305143  6.06615511  0.09977227  0.19116921 12.66035642], Loss = 0.5555\n",
      "Iteration 5849: Weights = [55.30333333  2.53288777  6.06576317  0.09976582  0.19115686 12.66077865], Loss = 0.5554\n",
      "Iteration 5850: Weights = [55.30333333  2.53272411  6.06537125  0.09975938  0.19114451 12.66120085], Loss = 0.5554\n",
      "Iteration 5851: Weights = [55.30333333  2.53256047  6.06497935  0.09975293  0.19113216 12.66162302], Loss = 0.5553\n",
      "Iteration 5852: Weights = [55.30333333  2.53239683  6.06458748  0.09974649  0.19111981 12.66204517], Loss = 0.5552\n",
      "Iteration 5853: Weights = [55.30333333  2.53223321  6.06419564  0.09974004  0.19110746 12.66246729], Loss = 0.5551\n",
      "Iteration 5854: Weights = [55.30333333  2.5320696   6.06380382  0.0997336   0.19109511 12.66288938], Loss = 0.5551\n",
      "Iteration 5855: Weights = [55.30333333  2.531906    6.06341202  0.09972715  0.19108277 12.66331144], Loss = 0.5550\n",
      "Iteration 5856: Weights = [55.30333333  2.53174241  6.06302025  0.09972071  0.19107042 12.66373348], Loss = 0.5549\n",
      "Iteration 5857: Weights = [55.30333333  2.53157882  6.06262851  0.09971427  0.19105807 12.66415548], Loss = 0.5549\n",
      "Iteration 5858: Weights = [55.30333333  2.53141525  6.06223679  0.09970782  0.19104573 12.66457747], Loss = 0.5548\n",
      "Iteration 5859: Weights = [55.30333333  2.53125169  6.0618451   0.09970138  0.19103339 12.66499942], Loss = 0.5547\n",
      "Iteration 5860: Weights = [55.30333333  2.53108814  6.06145343  0.09969494  0.19102104 12.66542135], Loss = 0.5546\n",
      "Iteration 5861: Weights = [55.30333333  2.53092461  6.06106179  0.0996885   0.1910087  12.66584325], Loss = 0.5546\n",
      "Iteration 5862: Weights = [55.30333333  2.53076108  6.06067017  0.09968206  0.19099636 12.66626512], Loss = 0.5545\n",
      "Iteration 5863: Weights = [55.30333333  2.53059756  6.06027858  0.09967562  0.19098402 12.66668696], Loss = 0.5544\n",
      "Iteration 5864: Weights = [55.30333333  2.53043405  6.05988701  0.09966918  0.19097168 12.66710878], Loss = 0.5544\n",
      "Iteration 5865: Weights = [55.30333333  2.53027056  6.05949547  0.09966274  0.19095934 12.66753057], Loss = 0.5543\n",
      "Iteration 5866: Weights = [55.30333333  2.53010707  6.05910395  0.0996563   0.190947   12.66795234], Loss = 0.5542\n",
      "Iteration 5867: Weights = [55.30333333  2.5299436   6.05871246  0.09964986  0.19093466 12.66837407], Loss = 0.5541\n",
      "Iteration 5868: Weights = [55.30333333  2.52978013  6.058321    0.09964342  0.19092233 12.66879578], Loss = 0.5541\n",
      "Iteration 5869: Weights = [55.30333333  2.52961668  6.05792955  0.09963698  0.19090999 12.66921746], Loss = 0.5540\n",
      "Iteration 5870: Weights = [55.30333333  2.52945323  6.05753814  0.09963054  0.19089766 12.66963912], Loss = 0.5539\n",
      "Iteration 5871: Weights = [55.30333333  2.5292898   6.05714675  0.09962411  0.19088532 12.67006074], Loss = 0.5539\n",
      "Iteration 5872: Weights = [55.30333333  2.52912638  6.05675539  0.09961767  0.19087299 12.67048234], Loss = 0.5538\n",
      "Iteration 5873: Weights = [55.30333333  2.52896297  6.05636405  0.09961123  0.19086066 12.67090392], Loss = 0.5537\n",
      "Iteration 5874: Weights = [55.30333333  2.52879956  6.05597273  0.0996048   0.19084832 12.67132546], Loss = 0.5536\n",
      "Iteration 5875: Weights = [55.30333333  2.52863617  6.05558144  0.09959836  0.19083599 12.67174698], Loss = 0.5536\n",
      "Iteration 5876: Weights = [55.30333333  2.52847279  6.05519018  0.09959193  0.19082366 12.67216847], Loss = 0.5535\n",
      "Iteration 5877: Weights = [55.30333333  2.52830942  6.05479894  0.09958549  0.19081133 12.67258993], Loss = 0.5534\n",
      "Iteration 5878: Weights = [55.30333333  2.52814606  6.05440773  0.09957906  0.190799   12.67301137], Loss = 0.5534\n",
      "Iteration 5879: Weights = [55.30333333  2.52798272  6.05401654  0.09957262  0.19078668 12.67343278], Loss = 0.5533\n",
      "Iteration 5880: Weights = [55.30333333  2.52781938  6.05362538  0.09956619  0.19077435 12.67385416], Loss = 0.5532\n",
      "Iteration 5881: Weights = [55.30333333  2.52765605  6.05323424  0.09955976  0.19076202 12.67427552], Loss = 0.5531\n",
      "Iteration 5882: Weights = [55.30333333  2.52749273  6.05284313  0.09955332  0.1907497  12.67469684], Loss = 0.5531\n",
      "Iteration 5883: Weights = [55.30333333  2.52732943  6.05245205  0.09954689  0.19073737 12.67511814], Loss = 0.5530\n",
      "Iteration 5884: Weights = [55.30333333  2.52716613  6.05206098  0.09954046  0.19072505 12.67553942], Loss = 0.5529\n",
      "Iteration 5885: Weights = [55.30333333  2.52700285  6.05166995  0.09953403  0.19071273 12.67596066], Loss = 0.5529\n",
      "Iteration 5886: Weights = [55.30333333  2.52683957  6.05127894  0.0995276   0.1907004  12.67638188], Loss = 0.5528\n",
      "Iteration 5887: Weights = [55.30333333  2.52667631  6.05088795  0.09952117  0.19068808 12.67680307], Loss = 0.5527\n",
      "Iteration 5888: Weights = [55.30333333  2.52651305  6.05049699  0.09951474  0.19067576 12.67722424], Loss = 0.5526\n",
      "Iteration 5889: Weights = [55.30333333  2.52634981  6.05010606  0.09950831  0.19066344 12.67764537], Loss = 0.5526\n",
      "Iteration 5890: Weights = [55.30333333  2.52618658  6.04971515  0.09950188  0.19065112 12.67806648], Loss = 0.5525\n",
      "Iteration 5891: Weights = [55.30333333  2.52602336  6.04932426  0.09949545  0.1906388  12.67848757], Loss = 0.5524\n",
      "Iteration 5892: Weights = [55.30333333  2.52586014  6.04893341  0.09948902  0.19062649 12.67890862], Loss = 0.5524\n",
      "Iteration 5893: Weights = [55.30333333  2.52569694  6.04854257  0.09948259  0.19061417 12.67932965], Loss = 0.5523\n",
      "Iteration 5894: Weights = [55.30333333  2.52553375  6.04815176  0.09947616  0.19060185 12.67975065], Loss = 0.5522\n",
      "Iteration 5895: Weights = [55.30333333  2.52537057  6.04776098  0.09946974  0.19058954 12.68017162], Loss = 0.5521\n",
      "Iteration 5896: Weights = [55.30333333  2.5252074   6.04737022  0.09946331  0.19057722 12.68059257], Loss = 0.5521\n",
      "Iteration 5897: Weights = [55.30333333  2.52504425  6.04697949  0.09945688  0.19056491 12.68101349], Loss = 0.5520\n",
      "Iteration 5898: Weights = [55.30333333  2.5248811   6.04658878  0.09945046  0.1905526  12.68143438], Loss = 0.5519\n",
      "Iteration 5899: Weights = [55.30333333  2.52471796  6.0461981   0.09944403  0.19054029 12.68185525], Loss = 0.5519\n",
      "Iteration 5900: Weights = [55.30333333  2.52455483  6.04580744  0.09943761  0.19052797 12.68227609], Loss = 0.5518\n",
      "Iteration 5901: Weights = [55.30333333  2.52439172  6.04541681  0.09943118  0.19051566 12.6826969 ], Loss = 0.5517\n",
      "Iteration 5902: Weights = [55.30333333  2.52422861  6.0450262   0.09942476  0.19050335 12.68311768], Loss = 0.5516\n",
      "Iteration 5903: Weights = [55.30333333  2.52406551  6.04463562  0.09941833  0.19049105 12.68353844], Loss = 0.5516\n",
      "Iteration 5904: Weights = [55.30333333  2.52390243  6.04424507  0.09941191  0.19047874 12.68395916], Loss = 0.5515\n",
      "Iteration 5905: Weights = [55.30333333  2.52373935  6.04385454  0.09940548  0.19046643 12.68437987], Loss = 0.5514\n",
      "Iteration 5906: Weights = [55.30333333  2.52357629  6.04346403  0.09939906  0.19045412 12.68480054], Loss = 0.5514\n",
      "Iteration 5907: Weights = [55.30333333  2.52341324  6.04307355  0.09939264  0.19044182 12.68522119], Loss = 0.5513\n",
      "Iteration 5908: Weights = [55.30333333  2.52325019  6.04268309  0.09938622  0.19042951 12.68564181], Loss = 0.5512\n",
      "Iteration 5909: Weights = [55.30333333  2.52308716  6.04229266  0.0993798   0.19041721 12.6860624 ], Loss = 0.5511\n",
      "Iteration 5910: Weights = [55.30333333  2.52292414  6.04190226  0.09937338  0.19040491 12.68648297], Loss = 0.5511\n",
      "Iteration 5911: Weights = [55.30333333  2.52276113  6.04151188  0.09936695  0.1903926  12.68690351], Loss = 0.5510\n",
      "Iteration 5912: Weights = [55.30333333  2.52259813  6.04112153  0.09936053  0.1903803  12.68732402], Loss = 0.5509\n",
      "Iteration 5913: Weights = [55.30333333  2.52243514  6.0407312   0.09935411  0.190368   12.6877445 ], Loss = 0.5509\n",
      "Iteration 5914: Weights = [55.30333333  2.52227216  6.04034089  0.09934769  0.1903557  12.68816496], Loss = 0.5508\n",
      "Iteration 5915: Weights = [55.30333333  2.52210919  6.03995062  0.09934128  0.1903434  12.68858539], Loss = 0.5507\n",
      "Iteration 5916: Weights = [55.30333333  2.52194623  6.03956036  0.09933486  0.1903311  12.68900579], Loss = 0.5506\n",
      "Iteration 5917: Weights = [55.30333333  2.52178328  6.03917013  0.09932844  0.19031881 12.68942617], Loss = 0.5506\n",
      "Iteration 5918: Weights = [55.30333333  2.52162035  6.03877993  0.09932202  0.19030651 12.68984652], Loss = 0.5505\n",
      "Iteration 5919: Weights = [55.30333333  2.52145742  6.03838975  0.0993156   0.19029421 12.69026684], Loss = 0.5504\n",
      "Iteration 5920: Weights = [55.30333333  2.5212945   6.0379996   0.09930919  0.19028192 12.69068713], Loss = 0.5504\n",
      "Iteration 5921: Weights = [55.30333333  2.5211316   6.03760947  0.09930277  0.19026962 12.6911074 ], Loss = 0.5503\n",
      "Iteration 5922: Weights = [55.30333333  2.5209687   6.03721937  0.09929635  0.19025733 12.69152764], Loss = 0.5502\n",
      "Iteration 5923: Weights = [55.30333333  2.52080582  6.03682929  0.09928994  0.19024504 12.69194785], Loss = 0.5501\n",
      "Iteration 5924: Weights = [55.30333333  2.52064294  6.03643924  0.09928352  0.19023274 12.69236804], Loss = 0.5501\n",
      "Iteration 5925: Weights = [55.30333333  2.52048008  6.03604922  0.09927711  0.19022045 12.6927882 ], Loss = 0.5500\n",
      "Iteration 5926: Weights = [55.30333333  2.52031723  6.03565922  0.09927069  0.19020816 12.69320833], Loss = 0.5499\n",
      "Iteration 5927: Weights = [55.30333333  2.52015438  6.03526924  0.09926428  0.19019587 12.69362843], Loss = 0.5499\n",
      "Iteration 5928: Weights = [55.30333333  2.51999155  6.03487929  0.09925787  0.19018358 12.69404851], Loss = 0.5498\n",
      "Iteration 5929: Weights = [55.30333333  2.51982873  6.03448936  0.09925145  0.1901713  12.69446856], Loss = 0.5497\n",
      "Iteration 5930: Weights = [55.30333333  2.51966592  6.03409946  0.09924504  0.19015901 12.69488858], Loss = 0.5497\n",
      "Iteration 5931: Weights = [55.30333333  2.51950312  6.03370959  0.09923863  0.19014672 12.69530858], Loss = 0.5496\n",
      "Iteration 5932: Weights = [55.30333333  2.51934033  6.03331974  0.09923222  0.19013444 12.69572855], Loss = 0.5495\n",
      "Iteration 5933: Weights = [55.30333333  2.51917755  6.03292991  0.0992258   0.19012215 12.69614849], Loss = 0.5494\n",
      "Iteration 5934: Weights = [55.30333333  2.51901478  6.03254011  0.09921939  0.19010987 12.6965684 ], Loss = 0.5494\n",
      "Iteration 5935: Weights = [55.30333333  2.51885202  6.03215034  0.09921298  0.19009758 12.69698829], Loss = 0.5493\n",
      "Iteration 5936: Weights = [55.30333333  2.51868927  6.03176059  0.09920657  0.1900853  12.69740815], Loss = 0.5492\n",
      "Iteration 5937: Weights = [55.30333333  2.51852653  6.03137086  0.09920016  0.19007302 12.69782798], Loss = 0.5492\n",
      "Iteration 5938: Weights = [55.30333333  2.51836381  6.03098116  0.09919375  0.19006074 12.69824779], Loss = 0.5491\n",
      "Iteration 5939: Weights = [55.30333333  2.51820109  6.03059149  0.09918734  0.19004846 12.69866757], Loss = 0.5490\n",
      "Iteration 5940: Weights = [55.30333333  2.51803838  6.03020184  0.09918093  0.19003618 12.69908732], Loss = 0.5489\n",
      "Iteration 5941: Weights = [55.30333333  2.51787569  6.02981222  0.09917453  0.1900239  12.69950704], Loss = 0.5489\n",
      "Iteration 5942: Weights = [55.30333333  2.517713    6.02942262  0.09916812  0.19001162 12.69992674], Loss = 0.5488\n",
      "Iteration 5943: Weights = [55.30333333  2.51755033  6.02903305  0.09916171  0.18999934 12.70034641], Loss = 0.5487\n",
      "Iteration 5944: Weights = [55.30333333  2.51738766  6.0286435   0.0991553   0.18998707 12.70076605], Loss = 0.5487\n",
      "Iteration 5945: Weights = [55.30333333  2.51722501  6.02825398  0.0991489   0.18997479 12.70118567], Loss = 0.5486\n",
      "Iteration 5946: Weights = [55.30333333  2.51706237  6.02786448  0.09914249  0.18996252 12.70160526], Loss = 0.5485\n",
      "Iteration 5947: Weights = [55.30333333  2.51689974  6.02747501  0.09913608  0.18995024 12.70202482], Loss = 0.5484\n",
      "Iteration 5948: Weights = [55.30333333  2.51673711  6.02708556  0.09912968  0.18993797 12.70244435], Loss = 0.5484\n",
      "Iteration 5949: Weights = [55.30333333  2.5165745   6.02669614  0.09912327  0.1899257  12.70286386], Loss = 0.5483\n",
      "Iteration 5950: Weights = [55.30333333  2.5164119   6.02630674  0.09911687  0.18991343 12.70328334], Loss = 0.5482\n",
      "Iteration 5951: Weights = [55.30333333  2.51624931  6.02591737  0.09911047  0.18990116 12.7037028 ], Loss = 0.5482\n",
      "Iteration 5952: Weights = [55.30333333  2.51608673  6.02552802  0.09910406  0.18988889 12.70412222], Loss = 0.5481\n",
      "Iteration 5953: Weights = [55.30333333  2.51592416  6.0251387   0.09909766  0.18987662 12.70454162], Loss = 0.5480\n",
      "Iteration 5954: Weights = [55.30333333  2.5157616   6.0247494   0.09909126  0.18986435 12.70496099], Loss = 0.5479\n",
      "Iteration 5955: Weights = [55.30333333  2.51559905  6.02436013  0.09908485  0.18985208 12.70538034], Loss = 0.5479\n",
      "Iteration 5956: Weights = [55.30333333  2.51543652  6.02397089  0.09907845  0.18983982 12.70579966], Loss = 0.5478\n",
      "Iteration 5957: Weights = [55.30333333  2.51527399  6.02358167  0.09907205  0.18982755 12.70621895], Loss = 0.5477\n",
      "Iteration 5958: Weights = [55.30333333  2.51511147  6.02319247  0.09906565  0.18981528 12.70663821], Loss = 0.5477\n",
      "Iteration 5959: Weights = [55.30333333  2.51494897  6.0228033   0.09905925  0.18980302 12.70705745], Loss = 0.5476\n",
      "Iteration 5960: Weights = [55.30333333  2.51478647  6.02241415  0.09905285  0.18979076 12.70747666], Loss = 0.5475\n",
      "Iteration 5961: Weights = [55.30333333  2.51462398  6.02202503  0.09904645  0.18977849 12.70789584], Loss = 0.5475\n",
      "Iteration 5962: Weights = [55.30333333  2.51446151  6.02163594  0.09904005  0.18976623 12.70831499], Loss = 0.5474\n",
      "Iteration 5963: Weights = [55.30333333  2.51429905  6.02124687  0.09903365  0.18975397 12.70873412], Loss = 0.5473\n",
      "Iteration 5964: Weights = [55.30333333  2.51413659  6.02085782  0.09902725  0.18974171 12.70915322], Loss = 0.5472\n",
      "Iteration 5965: Weights = [55.30333333  2.51397415  6.0204688   0.09902085  0.18972945 12.7095723 ], Loss = 0.5472\n",
      "Iteration 5966: Weights = [55.30333333  2.51381171  6.02007981  0.09901445  0.18971719 12.70999134], Loss = 0.5471\n",
      "Iteration 5967: Weights = [55.30333333  2.51364929  6.01969084  0.09900806  0.18970493 12.71041036], Loss = 0.5470\n",
      "Iteration 5968: Weights = [55.30333333  2.51348688  6.0193019   0.09900166  0.18969268 12.71082936], Loss = 0.5470\n",
      "Iteration 5969: Weights = [55.30333333  2.51332448  6.01891298  0.09899526  0.18968042 12.71124832], Loss = 0.5469\n",
      "Iteration 5970: Weights = [55.30333333  2.51316209  6.01852408  0.09898887  0.18966816 12.71166726], Loss = 0.5468\n",
      "Iteration 5971: Weights = [55.30333333  2.51299971  6.01813521  0.09898247  0.18965591 12.71208617], Loss = 0.5467\n",
      "Iteration 5972: Weights = [55.30333333  2.51283734  6.01774637  0.09897607  0.18964366 12.71250506], Loss = 0.5467\n",
      "Iteration 5973: Weights = [55.30333333  2.51267498  6.01735755  0.09896968  0.1896314  12.71292391], Loss = 0.5466\n",
      "Iteration 5974: Weights = [55.30333333  2.51251263  6.01696876  0.09896328  0.18961915 12.71334274], Loss = 0.5465\n",
      "Iteration 5975: Weights = [55.30333333  2.51235029  6.01657999  0.09895689  0.1896069  12.71376155], Loss = 0.5465\n",
      "Iteration 5976: Weights = [55.30333333  2.51218796  6.01619125  0.0989505   0.18959465 12.71418032], Loss = 0.5464\n",
      "Iteration 5977: Weights = [55.30333333  2.51202565  6.01580253  0.0989441   0.1895824  12.71459907], Loss = 0.5463\n",
      "Iteration 5978: Weights = [55.30333333  2.51186334  6.01541383  0.09893771  0.18957015 12.7150178 ], Loss = 0.5463\n",
      "Iteration 5979: Weights = [55.30333333  2.51170104  6.01502517  0.09893132  0.1895579  12.71543649], Loss = 0.5462\n",
      "Iteration 5980: Weights = [55.30333333  2.51153876  6.01463652  0.09892493  0.18954565 12.71585516], Loss = 0.5461\n",
      "Iteration 5981: Weights = [55.30333333  2.51137648  6.01424791  0.09891853  0.1895334  12.7162738 ], Loss = 0.5460\n",
      "Iteration 5982: Weights = [55.30333333  2.51121422  6.01385931  0.09891214  0.18952116 12.71669241], Loss = 0.5460\n",
      "Iteration 5983: Weights = [55.30333333  2.51105196  6.01347075  0.09890575  0.18950891 12.717111  ], Loss = 0.5459\n",
      "Iteration 5984: Weights = [55.30333333  2.51088972  6.0130822   0.09889936  0.18949667 12.71752956], Loss = 0.5458\n",
      "Iteration 5985: Weights = [55.30333333  2.51072748  6.01269369  0.09889297  0.18948443 12.71794809], Loss = 0.5458\n",
      "Iteration 5986: Weights = [55.30333333  2.51056526  6.01230519  0.09888658  0.18947218 12.7183666 ], Loss = 0.5457\n",
      "Iteration 5987: Weights = [55.30333333  2.51040305  6.01191673  0.09888019  0.18945994 12.71878508], Loss = 0.5456\n",
      "Iteration 5988: Weights = [55.30333333  2.51024084  6.01152828  0.0988738   0.1894477  12.71920353], Loss = 0.5455\n",
      "Iteration 5989: Weights = [55.30333333  2.51007865  6.01113987  0.09886741  0.18943546 12.71962195], Loss = 0.5455\n",
      "Iteration 5990: Weights = [55.30333333  2.50991647  6.01075148  0.09886103  0.18942322 12.72004035], Loss = 0.5454\n",
      "Iteration 5991: Weights = [55.30333333  2.5097543   6.01036311  0.09885464  0.18941098 12.72045872], Loss = 0.5453\n",
      "Iteration 5992: Weights = [55.30333333  2.50959214  6.00997477  0.09884825  0.18939874 12.72087707], Loss = 0.5453\n",
      "Iteration 5993: Weights = [55.30333333  2.50942999  6.00958645  0.09884187  0.1893865  12.72129538], Loss = 0.5452\n",
      "Iteration 5994: Weights = [55.30333333  2.50926785  6.00919816  0.09883548  0.18937427 12.72171367], Loss = 0.5451\n",
      "Iteration 5995: Weights = [55.30333333  2.50910572  6.00880989  0.09882909  0.18936203 12.72213193], Loss = 0.5451\n",
      "Iteration 5996: Weights = [55.30333333  2.5089436   6.00842165  0.09882271  0.1893498  12.72255017], Loss = 0.5450\n",
      "Iteration 5997: Weights = [55.30333333  2.5087815   6.00803344  0.09881632  0.18933756 12.72296838], Loss = 0.5449\n",
      "Iteration 5998: Weights = [55.30333333  2.5086194   6.00764524  0.09880994  0.18932533 12.72338656], Loss = 0.5448\n",
      "Iteration 5999: Weights = [55.30333333  2.50845731  6.00725708  0.09880355  0.1893131  12.72380471], Loss = 0.5448\n",
      "Iteration 6000: Weights = [55.30333333  2.50829524  6.00686894  0.09879717  0.18930086 12.72422284], Loss = 0.5447\n",
      "Iteration 6001: Weights = [55.30333333  2.50813317  6.00648082  0.09879079  0.18928863 12.72464094], Loss = 0.5446\n",
      "Iteration 6002: Weights = [55.30333333  2.50797111  6.00609273  0.0987844   0.1892764  12.72505901], Loss = 0.5446\n",
      "Iteration 6003: Weights = [55.30333333  2.50780907  6.00570467  0.09877802  0.18926417 12.72547706], Loss = 0.5445\n",
      "Iteration 6004: Weights = [55.30333333  2.50764703  6.00531662  0.09877164  0.18925194 12.72589508], Loss = 0.5444\n",
      "Iteration 6005: Weights = [55.30333333  2.50748501  6.00492861  0.09876526  0.18923972 12.72631307], Loss = 0.5444\n",
      "Iteration 6006: Weights = [55.30333333  2.507323    6.00454062  0.09875887  0.18922749 12.72673104], Loss = 0.5443\n",
      "Iteration 6007: Weights = [55.30333333  2.50716099  6.00415265  0.09875249  0.18921526 12.72714898], Loss = 0.5442\n",
      "Iteration 6008: Weights = [55.30333333  2.506999    6.00376471  0.09874611  0.18920304 12.72756689], Loss = 0.5441\n",
      "Iteration 6009: Weights = [55.30333333  2.50683702  6.0033768   0.09873973  0.18919081 12.72798477], Loss = 0.5441\n",
      "Iteration 6010: Weights = [55.30333333  2.50667505  6.00298891  0.09873335  0.18917859 12.72840263], Loss = 0.5440\n",
      "Iteration 6011: Weights = [55.30333333  2.50651309  6.00260104  0.09872697  0.18916636 12.72882046], Loss = 0.5439\n",
      "Iteration 6012: Weights = [55.30333333  2.50635113  6.0022132   0.09872059  0.18915414 12.72923826], Loss = 0.5439\n",
      "Iteration 6013: Weights = [55.30333333  2.50618919  6.00182539  0.09871422  0.18914192 12.72965604], Loss = 0.5438\n",
      "Iteration 6014: Weights = [55.30333333  2.50602726  6.0014376   0.09870784  0.1891297  12.73007379], Loss = 0.5437\n",
      "Iteration 6015: Weights = [55.30333333  2.50586535  6.00104983  0.09870146  0.18911748 12.73049151], Loss = 0.5436\n",
      "Iteration 6016: Weights = [55.30333333  2.50570344  6.00066209  0.09869508  0.18910526 12.7309092 ], Loss = 0.5436\n",
      "Iteration 6017: Weights = [55.30333333  2.50554154  6.00027438  0.09868871  0.18909304 12.73132687], Loss = 0.5435\n",
      "Iteration 6018: Weights = [55.30333333  2.50537965  5.99988669  0.09868233  0.18908082 12.73174451], Loss = 0.5434\n",
      "Iteration 6019: Weights = [55.30333333  2.50521777  5.99949902  0.09867595  0.18906861 12.73216213], Loss = 0.5434\n",
      "Iteration 6020: Weights = [55.30333333  2.5050559   5.99911138  0.09866958  0.18905639 12.73257972], Loss = 0.5433\n",
      "Iteration 6021: Weights = [55.30333333  2.50489405  5.99872377  0.0986632   0.18904418 12.73299728], Loss = 0.5432\n",
      "Iteration 6022: Weights = [55.30333333  2.5047322   5.99833618  0.09865683  0.18903196 12.73341481], Loss = 0.5432\n",
      "Iteration 6023: Weights = [55.30333333  2.50457037  5.99794861  0.09865045  0.18901975 12.73383232], Loss = 0.5431\n",
      "Iteration 6024: Weights = [55.30333333  2.50440854  5.99756107  0.09864408  0.18900754 12.7342498 ], Loss = 0.5430\n",
      "Iteration 6025: Weights = [55.30333333  2.50424673  5.99717356  0.09863771  0.18899532 12.73466725], Loss = 0.5429\n",
      "Iteration 6026: Weights = [55.30333333  2.50408492  5.99678607  0.09863133  0.18898311 12.73508467], Loss = 0.5429\n",
      "Iteration 6027: Weights = [55.30333333  2.50392313  5.99639861  0.09862496  0.1889709  12.73550207], Loss = 0.5428\n",
      "Iteration 6028: Weights = [55.30333333  2.50376134  5.99601117  0.09861859  0.18895869 12.73591944], Loss = 0.5427\n",
      "Iteration 6029: Weights = [55.30333333  2.50359957  5.99562375  0.09861222  0.18894648 12.73633679], Loss = 0.5427\n",
      "Iteration 6030: Weights = [55.30333333  2.50343781  5.99523636  0.09860584  0.18893427 12.73675411], Loss = 0.5426\n",
      "Iteration 6031: Weights = [55.30333333  2.50327606  5.994849    0.09859947  0.18892207 12.7371714 ], Loss = 0.5425\n",
      "Iteration 6032: Weights = [55.30333333  2.50311431  5.99446166  0.0985931   0.18890986 12.73758866], Loss = 0.5425\n",
      "Iteration 6033: Weights = [55.30333333  2.50295258  5.99407435  0.09858673  0.18889765 12.7380059 ], Loss = 0.5424\n",
      "Iteration 6034: Weights = [55.30333333  2.50279086  5.99368706  0.09858036  0.18888545 12.73842311], Loss = 0.5423\n",
      "Iteration 6035: Weights = [55.30333333  2.50262915  5.99329979  0.09857399  0.18887324 12.73884029], Loss = 0.5422\n",
      "Iteration 6036: Weights = [55.30333333  2.50246745  5.99291255  0.09856762  0.18886104 12.73925745], Loss = 0.5422\n",
      "Iteration 6037: Weights = [55.30333333  2.50230576  5.99252534  0.09856125  0.18884884 12.73967457], Loss = 0.5421\n",
      "Iteration 6038: Weights = [55.30333333  2.50214408  5.99213815  0.09855489  0.18883664 12.74009168], Loss = 0.5420\n",
      "Iteration 6039: Weights = [55.30333333  2.50198242  5.99175099  0.09854852  0.18882444 12.74050875], Loss = 0.5420\n",
      "Iteration 6040: Weights = [55.30333333  2.50182076  5.99136385  0.09854215  0.18881224 12.7409258 ], Loss = 0.5419\n",
      "Iteration 6041: Weights = [55.30333333  2.50165911  5.99097673  0.09853578  0.18880004 12.74134282], Loss = 0.5418\n",
      "Iteration 6042: Weights = [55.30333333  2.50149747  5.99058964  0.09852942  0.18878784 12.74175981], Loss = 0.5418\n",
      "Iteration 6043: Weights = [55.30333333  2.50133585  5.99020258  0.09852305  0.18877564 12.74217678], Loss = 0.5417\n",
      "Iteration 6044: Weights = [55.30333333  2.50117423  5.98981554  0.09851669  0.18876344 12.74259372], Loss = 0.5416\n",
      "Iteration 6045: Weights = [55.30333333  2.50101262  5.98942853  0.09851032  0.18875125 12.74301064], Loss = 0.5415\n",
      "Iteration 6046: Weights = [55.30333333  2.50085103  5.98904154  0.09850396  0.18873905 12.74342752], Loss = 0.5415\n",
      "Iteration 6047: Weights = [55.30333333  2.50068944  5.98865457  0.09849759  0.18872686 12.74384438], Loss = 0.5414\n",
      "Iteration 6048: Weights = [55.30333333  2.50052787  5.98826763  0.09849123  0.18871466 12.74426121], Loss = 0.5413\n",
      "Iteration 6049: Weights = [55.30333333  2.5003663   5.98788072  0.09848486  0.18870247 12.74467802], Loss = 0.5413\n",
      "Iteration 6050: Weights = [55.30333333  2.50020475  5.98749383  0.0984785   0.18869028 12.7450948 ], Loss = 0.5412\n",
      "Iteration 6051: Weights = [55.30333333  2.50004321  5.98710697  0.09847214  0.18867808 12.74551155], Loss = 0.5411\n",
      "Iteration 6052: Weights = [55.30333333  2.49988167  5.98672013  0.09846577  0.18866589 12.74592828], Loss = 0.5411\n",
      "Iteration 6053: Weights = [55.30333333  2.49972015  5.98633331  0.09845941  0.1886537  12.74634497], Loss = 0.5410\n",
      "Iteration 6054: Weights = [55.30333333  2.49955864  5.98594653  0.09845305  0.18864151 12.74676164], Loss = 0.5409\n",
      "Iteration 6055: Weights = [55.30333333  2.49939714  5.98555976  0.09844669  0.18862932 12.74717829], Loss = 0.5408\n",
      "Iteration 6056: Weights = [55.30333333  2.49923565  5.98517302  0.09844033  0.18861714 12.74759491], Loss = 0.5408\n",
      "Iteration 6057: Weights = [55.30333333  2.49907417  5.98478631  0.09843397  0.18860495 12.7480115 ], Loss = 0.5407\n",
      "Iteration 6058: Weights = [55.30333333  2.4989127   5.98439962  0.09842761  0.18859276 12.74842806], Loss = 0.5406\n",
      "Iteration 6059: Weights = [55.30333333  2.49875124  5.98401296  0.09842125  0.18858058 12.74884459], Loss = 0.5406\n",
      "Iteration 6060: Weights = [55.30333333  2.49858979  5.98362632  0.09841489  0.18856839 12.7492611 ], Loss = 0.5405\n",
      "Iteration 6061: Weights = [55.30333333  2.49842835  5.9832397   0.09840853  0.18855621 12.74967759], Loss = 0.5404\n",
      "Iteration 6062: Weights = [55.30333333  2.49826692  5.98285311  0.09840217  0.18854403 12.75009404], Loss = 0.5404\n",
      "Iteration 6063: Weights = [55.30333333  2.4981055   5.98246655  0.09839581  0.18853185 12.75051047], Loss = 0.5403\n",
      "Iteration 6064: Weights = [55.30333333  2.49794409  5.98208001  0.09838946  0.18851966 12.75092687], Loss = 0.5402\n",
      "Iteration 6065: Weights = [55.30333333  2.4977827   5.9816935   0.0983831   0.18850748 12.75134325], Loss = 0.5401\n",
      "Iteration 6066: Weights = [55.30333333  2.49762131  5.98130701  0.09837674  0.1884953  12.7517596 ], Loss = 0.5401\n",
      "Iteration 6067: Weights = [55.30333333  2.49745993  5.98092054  0.09837039  0.18848312 12.75217592], Loss = 0.5400\n",
      "Iteration 6068: Weights = [55.30333333  2.49729857  5.9805341   0.09836403  0.18847095 12.75259221], Loss = 0.5399\n",
      "Iteration 6069: Weights = [55.30333333  2.49713721  5.98014769  0.09835768  0.18845877 12.75300848], Loss = 0.5399\n",
      "Iteration 6070: Weights = [55.30333333  2.49697587  5.9797613   0.09835132  0.18844659 12.75342472], Loss = 0.5398\n",
      "Iteration 6071: Weights = [55.30333333  2.49681453  5.97937493  0.09834497  0.18843442 12.75384093], Loss = 0.5397\n",
      "Iteration 6072: Weights = [55.30333333  2.49665321  5.9789886   0.09833861  0.18842224 12.75425712], Loss = 0.5397\n",
      "Iteration 6073: Weights = [55.30333333  2.49649189  5.97860228  0.09833226  0.18841007 12.75467328], Loss = 0.5396\n",
      "Iteration 6074: Weights = [55.30333333  2.49633059  5.97821599  0.0983259   0.18839789 12.75508941], Loss = 0.5395\n",
      "Iteration 6075: Weights = [55.30333333  2.4961693   5.97782973  0.09831955  0.18838572 12.75550552], Loss = 0.5394\n",
      "Iteration 6076: Weights = [55.30333333  2.49600802  5.97744349  0.0983132   0.18837355 12.7559216 ], Loss = 0.5394\n",
      "Iteration 6077: Weights = [55.30333333  2.49584674  5.97705727  0.09830685  0.18836138 12.75633765], Loss = 0.5393\n",
      "Iteration 6078: Weights = [55.30333333  2.49568548  5.97667108  0.09830049  0.18834921 12.75675367], Loss = 0.5392\n",
      "Iteration 6079: Weights = [55.30333333  2.49552423  5.97628492  0.09829414  0.18833704 12.75716967], Loss = 0.5392\n",
      "Iteration 6080: Weights = [55.30333333  2.49536299  5.97589878  0.09828779  0.18832487 12.75758564], Loss = 0.5391\n",
      "Iteration 6081: Weights = [55.30333333  2.49520176  5.97551266  0.09828144  0.1883127  12.75800159], Loss = 0.5390\n",
      "Iteration 6082: Weights = [55.30333333  2.49504054  5.97512657  0.09827509  0.18830053 12.75841751], Loss = 0.5390\n",
      "Iteration 6083: Weights = [55.30333333  2.49487933  5.97474051  0.09826874  0.18828837 12.7588334 ], Loss = 0.5389\n",
      "Iteration 6084: Weights = [55.30333333  2.49471813  5.97435447  0.09826239  0.1882762  12.75924926], Loss = 0.5388\n",
      "Iteration 6085: Weights = [55.30333333  2.49455694  5.97396845  0.09825604  0.18826404 12.7596651 ], Loss = 0.5388\n",
      "Iteration 6086: Weights = [55.30333333  2.49439576  5.97358246  0.09824969  0.18825187 12.76008091], Loss = 0.5387\n",
      "Iteration 6087: Weights = [55.30333333  2.4942346   5.9731965   0.09824335  0.18823971 12.76049669], Loss = 0.5386\n",
      "Iteration 6088: Weights = [55.30333333  2.49407344  5.97281056  0.098237    0.18822755 12.76091245], Loss = 0.5385\n",
      "Iteration 6089: Weights = [55.30333333  2.49391229  5.97242464  0.09823065  0.18821538 12.76132818], Loss = 0.5385\n",
      "Iteration 6090: Weights = [55.30333333  2.49375115  5.97203875  0.0982243   0.18820322 12.76174388], Loss = 0.5384\n",
      "Iteration 6091: Weights = [55.30333333  2.49359003  5.97165289  0.09821796  0.18819106 12.76215956], Loss = 0.5383\n",
      "Iteration 6092: Weights = [55.30333333  2.49342891  5.97126705  0.09821161  0.1881789  12.76257521], Loss = 0.5383\n",
      "Iteration 6093: Weights = [55.30333333  2.49326781  5.97088123  0.09820527  0.18816674 12.76299083], Loss = 0.5382\n",
      "Iteration 6094: Weights = [55.30333333  2.49310671  5.97049544  0.09819892  0.18815459 12.76340643], Loss = 0.5381\n",
      "Iteration 6095: Weights = [55.30333333  2.49294563  5.97010967  0.09819258  0.18814243 12.76382199], Loss = 0.5381\n",
      "Iteration 6096: Weights = [55.30333333  2.49278455  5.96972393  0.09818623  0.18813027 12.76423754], Loss = 0.5380\n",
      "Iteration 6097: Weights = [55.30333333  2.49262349  5.96933822  0.09817989  0.18811812 12.76465305], Loss = 0.5379\n",
      "Iteration 6098: Weights = [55.30333333  2.49246244  5.96895253  0.09817354  0.18810596 12.76506854], Loss = 0.5378\n",
      "Iteration 6099: Weights = [55.30333333  2.49230139  5.96856686  0.0981672   0.18809381 12.765484  ], Loss = 0.5378\n",
      "Iteration 6100: Weights = [55.30333333  2.49214036  5.96818122  0.09816086  0.18808166 12.76589943], Loss = 0.5377\n",
      "Iteration 6101: Weights = [55.30333333  2.49197934  5.9677956   0.09815452  0.1880695  12.76631484], Loss = 0.5376\n",
      "Iteration 6102: Weights = [55.30333333  2.49181833  5.96741001  0.09814817  0.18805735 12.76673022], Loss = 0.5376\n",
      "Iteration 6103: Weights = [55.30333333  2.49165733  5.96702445  0.09814183  0.1880452  12.76714558], Loss = 0.5375\n",
      "Iteration 6104: Weights = [55.30333333  2.49149633  5.9666389   0.09813549  0.18803305 12.7675609 ], Loss = 0.5374\n",
      "Iteration 6105: Weights = [55.30333333  2.49133535  5.96625339  0.09812915  0.1880209  12.7679762 ], Loss = 0.5374\n",
      "Iteration 6106: Weights = [55.30333333  2.49117438  5.9658679   0.09812281  0.18800875 12.76839148], Loss = 0.5373\n",
      "Iteration 6107: Weights = [55.30333333  2.49101342  5.96548243  0.09811647  0.18799661 12.76880672], Loss = 0.5372\n",
      "Iteration 6108: Weights = [55.30333333  2.49085247  5.96509699  0.09811013  0.18798446 12.76922194], Loss = 0.5372\n",
      "Iteration 6109: Weights = [55.30333333  2.49069154  5.96471157  0.09810379  0.18797231 12.76963714], Loss = 0.5371\n",
      "Iteration 6110: Weights = [55.30333333  2.49053061  5.96432618  0.09809745  0.18796017 12.7700523 ], Loss = 0.5370\n",
      "Iteration 6111: Weights = [55.30333333  2.49036969  5.96394081  0.09809111  0.18794802 12.77046744], Loss = 0.5369\n",
      "Iteration 6112: Weights = [55.30333333  2.49020878  5.96355547  0.09808478  0.18793588 12.77088255], Loss = 0.5369\n",
      "Iteration 6113: Weights = [55.30333333  2.49004788  5.96317015  0.09807844  0.18792374 12.77129764], Loss = 0.5368\n",
      "Iteration 6114: Weights = [55.30333333  2.489887    5.96278486  0.0980721   0.1879116  12.7717127 ], Loss = 0.5367\n",
      "Iteration 6115: Weights = [55.30333333  2.48972612  5.96239959  0.09806577  0.18789945 12.77212773], Loss = 0.5367\n",
      "Iteration 6116: Weights = [55.30333333  2.48956525  5.96201435  0.09805943  0.18788731 12.77254273], Loss = 0.5366\n",
      "Iteration 6117: Weights = [55.30333333  2.4894044   5.96162913  0.09805309  0.18787517 12.77295771], Loss = 0.5365\n",
      "Iteration 6118: Weights = [55.30333333  2.48924355  5.96124394  0.09804676  0.18786303 12.77337266], Loss = 0.5365\n",
      "Iteration 6119: Weights = [55.30333333  2.48908272  5.96085877  0.09804042  0.1878509  12.77378759], Loss = 0.5364\n",
      "Iteration 6120: Weights = [55.30333333  2.48892189  5.96047363  0.09803409  0.18783876 12.77420249], Loss = 0.5363\n",
      "Iteration 6121: Weights = [55.30333333  2.48876108  5.96008851  0.09802775  0.18782662 12.77461736], Loss = 0.5363\n",
      "Iteration 6122: Weights = [55.30333333  2.48860027  5.95970342  0.09802142  0.18781449 12.7750322 ], Loss = 0.5362\n",
      "Iteration 6123: Weights = [55.30333333  2.48843948  5.95931835  0.09801509  0.18780235 12.77544702], Loss = 0.5361\n",
      "Iteration 6124: Weights = [55.30333333  2.4882787   5.9589333   0.09800875  0.18779022 12.77586181], Loss = 0.5360\n",
      "Iteration 6125: Weights = [55.30333333  2.48811793  5.95854829  0.09800242  0.18777808 12.77627657], Loss = 0.5360\n",
      "Iteration 6126: Weights = [55.30333333  2.48795716  5.95816329  0.09799609  0.18776595 12.77669131], Loss = 0.5359\n",
      "Iteration 6127: Weights = [55.30333333  2.48779641  5.95777832  0.09798976  0.18775382 12.77710602], Loss = 0.5358\n",
      "Iteration 6128: Weights = [55.30333333  2.48763567  5.95739338  0.09798343  0.18774169 12.7775207 ], Loss = 0.5358\n",
      "Iteration 6129: Weights = [55.30333333  2.48747494  5.95700846  0.0979771   0.18772956 12.77793536], Loss = 0.5357\n",
      "Iteration 6130: Weights = [55.30333333  2.48731422  5.95662357  0.09797077  0.18771743 12.77834999], Loss = 0.5356\n",
      "Iteration 6131: Weights = [55.30333333  2.48715351  5.9562387   0.09796444  0.1877053  12.77876459], Loss = 0.5356\n",
      "Iteration 6132: Weights = [55.30333333  2.48699281  5.95585385  0.09795811  0.18769317 12.77917917], Loss = 0.5355\n",
      "Iteration 6133: Weights = [55.30333333  2.48683212  5.95546903  0.09795178  0.18768104 12.77959372], Loss = 0.5354\n",
      "Iteration 6134: Weights = [55.30333333  2.48667144  5.95508424  0.09794545  0.18766892 12.78000824], Loss = 0.5354\n",
      "Iteration 6135: Weights = [55.30333333  2.48651077  5.95469947  0.09793912  0.18765679 12.78042274], Loss = 0.5353\n",
      "Iteration 6136: Weights = [55.30333333  2.48635011  5.95431472  0.09793279  0.18764467 12.78083721], Loss = 0.5352\n",
      "Iteration 6137: Weights = [55.30333333  2.48618946  5.95393     0.09792646  0.18763254 12.78125165], Loss = 0.5351\n",
      "Iteration 6138: Weights = [55.30333333  2.48602883  5.95354531  0.09792014  0.18762042 12.78166607], Loss = 0.5351\n",
      "Iteration 6139: Weights = [55.30333333  2.4858682   5.95316064  0.09791381  0.1876083  12.78208045], Loss = 0.5350\n",
      "Iteration 6140: Weights = [55.30333333  2.48570758  5.95277599  0.09790748  0.18759617 12.78249482], Loss = 0.5349\n",
      "Iteration 6141: Weights = [55.30333333  2.48554697  5.95239137  0.09790116  0.18758405 12.78290915], Loss = 0.5349\n",
      "Iteration 6142: Weights = [55.30333333  2.48538638  5.95200677  0.09789483  0.18757193 12.78332346], Loss = 0.5348\n",
      "Iteration 6143: Weights = [55.30333333  2.48522579  5.9516222   0.09788851  0.18755981 12.78373774], Loss = 0.5347\n",
      "Iteration 6144: Weights = [55.30333333  2.48506522  5.95123766  0.09788218  0.1875477  12.784152  ], Loss = 0.5347\n",
      "Iteration 6145: Weights = [55.30333333  2.48490465  5.95085314  0.09787586  0.18753558 12.78456623], Loss = 0.5346\n",
      "Iteration 6146: Weights = [55.30333333  2.4847441   5.95046864  0.09786953  0.18752346 12.78498043], Loss = 0.5345\n",
      "Iteration 6147: Weights = [55.30333333  2.48458355  5.95008417  0.09786321  0.18751134 12.7853946 ], Loss = 0.5345\n",
      "Iteration 6148: Weights = [55.30333333  2.48442302  5.94969972  0.09785689  0.18749923 12.78580875], Loss = 0.5344\n",
      "Iteration 6149: Weights = [55.30333333  2.4842625   5.9493153   0.09785056  0.18748711 12.78622287], Loss = 0.5343\n",
      "Iteration 6150: Weights = [55.30333333  2.48410198  5.9489309   0.09784424  0.187475   12.78663696], Loss = 0.5342\n",
      "Iteration 6151: Weights = [55.30333333  2.48394148  5.94854653  0.09783792  0.18746289 12.78705103], Loss = 0.5342\n",
      "Iteration 6152: Weights = [55.30333333  2.48378099  5.94816218  0.0978316   0.18745078 12.78746507], Loss = 0.5341\n",
      "Iteration 6153: Weights = [55.30333333  2.4836205   5.94777786  0.09782528  0.18743866 12.78787909], Loss = 0.5340\n",
      "Iteration 6154: Weights = [55.30333333  2.48346003  5.94739356  0.09781896  0.18742655 12.78829307], Loss = 0.5340\n",
      "Iteration 6155: Weights = [55.30333333  2.48329957  5.94700929  0.09781264  0.18741444 12.78870704], Loss = 0.5339\n",
      "Iteration 6156: Weights = [55.30333333  2.48313912  5.94662504  0.09780632  0.18740233 12.78912097], Loss = 0.5338\n",
      "Iteration 6157: Weights = [55.30333333  2.48297868  5.94624082  0.0978      0.18739023 12.78953488], Loss = 0.5338\n",
      "Iteration 6158: Weights = [55.30333333  2.48281825  5.94585662  0.09779368  0.18737812 12.78994876], Loss = 0.5337\n",
      "Iteration 6159: Weights = [55.30333333  2.48265783  5.94547244  0.09778736  0.18736601 12.79036261], Loss = 0.5336\n",
      "Iteration 6160: Weights = [55.30333333  2.48249742  5.94508829  0.09778104  0.1873539  12.79077644], Loss = 0.5336\n",
      "Iteration 6161: Weights = [55.30333333  2.48233702  5.94470417  0.09777472  0.1873418  12.79119024], Loss = 0.5335\n",
      "Iteration 6162: Weights = [55.30333333  2.48217663  5.94432007  0.09776841  0.18732969 12.79160401], Loss = 0.5334\n",
      "Iteration 6163: Weights = [55.30333333  2.48201625  5.943936    0.09776209  0.18731759 12.79201776], Loss = 0.5333\n",
      "Iteration 6164: Weights = [55.30333333  2.48185589  5.94355195  0.09775577  0.18730549 12.79243148], Loss = 0.5333\n",
      "Iteration 6165: Weights = [55.30333333  2.48169553  5.94316792  0.09774946  0.18729339 12.79284517], Loss = 0.5332\n",
      "Iteration 6166: Weights = [55.30333333  2.48153518  5.94278392  0.09774314  0.18728128 12.79325884], Loss = 0.5331\n",
      "Iteration 6167: Weights = [55.30333333  2.48137484  5.94239995  0.09773682  0.18726918 12.79367248], Loss = 0.5331\n",
      "Iteration 6168: Weights = [55.30333333  2.48121452  5.942016    0.09773051  0.18725708 12.79408609], Loss = 0.5330\n",
      "Iteration 6169: Weights = [55.30333333  2.4810542   5.94163207  0.09772419  0.18724498 12.79449968], Loss = 0.5329\n",
      "Iteration 6170: Weights = [55.30333333  2.4808939   5.94124817  0.09771788  0.18723289 12.79491324], Loss = 0.5329\n",
      "Iteration 6171: Weights = [55.30333333  2.4807336   5.94086429  0.09771157  0.18722079 12.79532677], Loss = 0.5328\n",
      "Iteration 6172: Weights = [55.30333333  2.48057331  5.94048044  0.09770525  0.18720869 12.79574027], Loss = 0.5327\n",
      "Iteration 6173: Weights = [55.30333333  2.48041304  5.94009662  0.09769894  0.1871966  12.79615375], Loss = 0.5327\n",
      "Iteration 6174: Weights = [55.30333333  2.48025278  5.93971281  0.09769263  0.1871845  12.79656721], Loss = 0.5326\n",
      "Iteration 6175: Weights = [55.30333333  2.48009252  5.93932904  0.09768632  0.18717241 12.79698063], Loss = 0.5325\n",
      "Iteration 6176: Weights = [55.30333333  2.47993228  5.93894529  0.09768     0.18716031 12.79739403], Loss = 0.5325\n",
      "Iteration 6177: Weights = [55.30333333  2.47977204  5.93856156  0.09767369  0.18714822 12.7978074 ], Loss = 0.5324\n",
      "Iteration 6178: Weights = [55.30333333  2.47961182  5.93817786  0.09766738  0.18713613 12.79822075], Loss = 0.5323\n",
      "Iteration 6179: Weights = [55.30333333  2.47945161  5.93779418  0.09766107  0.18712404 12.79863407], Loss = 0.5322\n",
      "Iteration 6180: Weights = [55.30333333  2.47929141  5.93741053  0.09765476  0.18711195 12.79904736], Loss = 0.5322\n",
      "Iteration 6181: Weights = [55.30333333  2.47913121  5.9370269   0.09764845  0.18709986 12.79946063], Loss = 0.5321\n",
      "Iteration 6182: Weights = [55.30333333  2.47897103  5.93664329  0.09764214  0.18708777 12.79987387], Loss = 0.5320\n",
      "Iteration 6183: Weights = [55.30333333  2.47881086  5.93625972  0.09763583  0.18707568 12.80028708], Loss = 0.5320\n",
      "Iteration 6184: Weights = [55.30333333  2.4786507   5.93587616  0.09762953  0.18706359 12.80070027], Loss = 0.5319\n",
      "Iteration 6185: Weights = [55.30333333  2.47849055  5.93549263  0.09762322  0.18705151 12.80111342], Loss = 0.5318\n",
      "Iteration 6186: Weights = [55.30333333  2.47833041  5.93510913  0.09761691  0.18703942 12.80152656], Loss = 0.5318\n",
      "Iteration 6187: Weights = [55.30333333  2.47817028  5.93472565  0.0976106   0.18702734 12.80193966], Loss = 0.5317\n",
      "Iteration 6188: Weights = [55.30333333  2.47801016  5.9343422   0.0976043   0.18701525 12.80235274], Loss = 0.5316\n",
      "Iteration 6189: Weights = [55.30333333  2.47785005  5.93395877  0.09759799  0.18700317 12.80276579], Loss = 0.5316\n",
      "Iteration 6190: Weights = [55.30333333  2.47768995  5.93357536  0.09759168  0.18699109 12.80317882], Loss = 0.5315\n",
      "Iteration 6191: Weights = [55.30333333  2.47752986  5.93319198  0.09758538  0.186979   12.80359182], Loss = 0.5314\n",
      "Iteration 6192: Weights = [55.30333333  2.47736979  5.93280863  0.09757907  0.18696692 12.80400479], Loss = 0.5314\n",
      "Iteration 6193: Weights = [55.30333333  2.47720972  5.93242529  0.09757277  0.18695484 12.80441774], Loss = 0.5313\n",
      "Iteration 6194: Weights = [55.30333333  2.47704966  5.93204199  0.09756646  0.18694276 12.80483065], Loss = 0.5312\n",
      "Iteration 6195: Weights = [55.30333333  2.47688961  5.93165871  0.09756016  0.18693068 12.80524355], Loss = 0.5311\n",
      "Iteration 6196: Weights = [55.30333333  2.47672958  5.93127545  0.09755386  0.18691861 12.80565641], Loss = 0.5311\n",
      "Iteration 6197: Weights = [55.30333333  2.47656955  5.93089222  0.09754755  0.18690653 12.80606925], Loss = 0.5310\n",
      "Iteration 6198: Weights = [55.30333333  2.47640953  5.93050901  0.09754125  0.18689445 12.80648206], Loss = 0.5309\n",
      "Iteration 6199: Weights = [55.30333333  2.47624953  5.93012583  0.09753495  0.18688238 12.80689485], Loss = 0.5309\n",
      "Iteration 6200: Weights = [55.30333333  2.47608953  5.92974267  0.09752865  0.1868703  12.80730761], Loss = 0.5308\n",
      "Iteration 6201: Weights = [55.30333333  2.47592955  5.92935954  0.09752234  0.18685823 12.80772034], Loss = 0.5307\n",
      "Iteration 6202: Weights = [55.30333333  2.47576957  5.92897643  0.09751604  0.18684615 12.80813304], Loss = 0.5307\n",
      "Iteration 6203: Weights = [55.30333333  2.47560961  5.92859335  0.09750974  0.18683408 12.80854572], Loss = 0.5306\n",
      "Iteration 6204: Weights = [55.30333333  2.47544965  5.92821029  0.09750344  0.18682201 12.80895837], Loss = 0.5305\n",
      "Iteration 6205: Weights = [55.30333333  2.47528971  5.92782726  0.09749714  0.18680994 12.809371  ], Loss = 0.5305\n",
      "Iteration 6206: Weights = [55.30333333  2.47512978  5.92744425  0.09749084  0.18679787 12.8097836 ], Loss = 0.5304\n",
      "Iteration 6207: Weights = [55.30333333  2.47496985  5.92706126  0.09748454  0.1867858  12.81019617], Loss = 0.5303\n",
      "Iteration 6208: Weights = [55.30333333  2.47480994  5.92667831  0.09747824  0.18677373 12.81060872], Loss = 0.5303\n",
      "Iteration 6209: Weights = [55.30333333  2.47465004  5.92629537  0.09747195  0.18676166 12.81102123], Loss = 0.5302\n",
      "Iteration 6210: Weights = [55.30333333  2.47449015  5.92591246  0.09746565  0.1867496  12.81143373], Loss = 0.5301\n",
      "Iteration 6211: Weights = [55.30333333  2.47433026  5.92552958  0.09745935  0.18673753 12.81184619], Loss = 0.5301\n",
      "Iteration 6212: Weights = [55.30333333  2.47417039  5.92514672  0.09745305  0.18672546 12.81225863], Loss = 0.5300\n",
      "Iteration 6213: Weights = [55.30333333  2.47401053  5.92476388  0.09744676  0.1867134  12.81267104], Loss = 0.5299\n",
      "Iteration 6214: Weights = [55.30333333  2.47385068  5.92438107  0.09744046  0.18670134 12.81308343], Loss = 0.5298\n",
      "Iteration 6215: Weights = [55.30333333  2.47369084  5.92399828  0.09743417  0.18668927 12.81349579], Loss = 0.5298\n",
      "Iteration 6216: Weights = [55.30333333  2.47353101  5.92361552  0.09742787  0.18667721 12.81390812], Loss = 0.5297\n",
      "Iteration 6217: Weights = [55.30333333  2.47337119  5.92323278  0.09742157  0.18666515 12.81432043], Loss = 0.5296\n",
      "Iteration 6218: Weights = [55.30333333  2.47321138  5.92285007  0.09741528  0.18665309 12.8147327 ], Loss = 0.5296\n",
      "Iteration 6219: Weights = [55.30333333  2.47305158  5.92246739  0.09740899  0.18664103 12.81514496], Loss = 0.5295\n",
      "Iteration 6220: Weights = [55.30333333  2.47289179  5.92208472  0.09740269  0.18662897 12.81555718], Loss = 0.5294\n",
      "Iteration 6221: Weights = [55.30333333  2.47273201  5.92170209  0.0973964   0.18661691 12.81596938], Loss = 0.5294\n",
      "Iteration 6222: Weights = [55.30333333  2.47257225  5.92131947  0.09739011  0.18660485 12.81638155], Loss = 0.5293\n",
      "Iteration 6223: Weights = [55.30333333  2.47241249  5.92093688  0.09738381  0.1865928  12.8167937 ], Loss = 0.5292\n",
      "Iteration 6224: Weights = [55.30333333  2.47225274  5.92055432  0.09737752  0.18658074 12.81720582], Loss = 0.5292\n",
      "Iteration 6225: Weights = [55.30333333  2.472093    5.92017178  0.09737123  0.18656868 12.81761791], Loss = 0.5291\n",
      "Iteration 6226: Weights = [55.30333333  2.47193328  5.91978927  0.09736494  0.18655663 12.81802998], Loss = 0.5290\n",
      "Iteration 6227: Weights = [55.30333333  2.47177356  5.91940678  0.09735865  0.18654458 12.81844202], Loss = 0.5290\n",
      "Iteration 6228: Weights = [55.30333333  2.47161385  5.91902431  0.09735236  0.18653252 12.81885403], Loss = 0.5289\n",
      "Iteration 6229: Weights = [55.30333333  2.47145416  5.91864187  0.09734607  0.18652047 12.81926602], Loss = 0.5288\n",
      "Iteration 6230: Weights = [55.30333333  2.47129447  5.91825946  0.09733978  0.18650842 12.81967797], Loss = 0.5288\n",
      "Iteration 6231: Weights = [55.30333333  2.4711348   5.91787707  0.09733349  0.18649637 12.82008991], Loss = 0.5287\n",
      "Iteration 6232: Weights = [55.30333333  2.47097513  5.9174947   0.0973272   0.18648432 12.82050181], Loss = 0.5286\n",
      "Iteration 6233: Weights = [55.30333333  2.47081548  5.91711236  0.09732091  0.18647227 12.82091369], Loss = 0.5285\n",
      "Iteration 6234: Weights = [55.30333333  2.47065583  5.91673004  0.09731462  0.18646022 12.82132555], Loss = 0.5285\n",
      "Iteration 6235: Weights = [55.30333333  2.4704962   5.91634775  0.09730833  0.18644817 12.82173737], Loss = 0.5284\n",
      "Iteration 6236: Weights = [55.30333333  2.47033658  5.91596548  0.09730205  0.18643613 12.82214917], Loss = 0.5283\n",
      "Iteration 6237: Weights = [55.30333333  2.47017696  5.91558324  0.09729576  0.18642408 12.82256095], Loss = 0.5283\n",
      "Iteration 6238: Weights = [55.30333333  2.47001736  5.91520102  0.09728947  0.18641203 12.82297269], Loss = 0.5282\n",
      "Iteration 6239: Weights = [55.30333333  2.46985777  5.91481883  0.09728319  0.18639999 12.82338441], Loss = 0.5281\n",
      "Iteration 6240: Weights = [55.30333333  2.46969818  5.91443666  0.0972769   0.18638795 12.8237961 ], Loss = 0.5281\n",
      "Iteration 6241: Weights = [55.30333333  2.46953861  5.91405452  0.09727062  0.1863759  12.82420777], Loss = 0.5280\n",
      "Iteration 6242: Weights = [55.30333333  2.46937905  5.9136724   0.09726433  0.18636386 12.82461941], Loss = 0.5279\n",
      "Iteration 6243: Weights = [55.30333333  2.4692195   5.91329031  0.09725805  0.18635182 12.82503103], Loss = 0.5279\n",
      "Iteration 6244: Weights = [55.30333333  2.46905996  5.91290824  0.09725176  0.18633978 12.82544261], Loss = 0.5278\n",
      "Iteration 6245: Weights = [55.30333333  2.46890043  5.91252619  0.09724548  0.18632774 12.82585417], Loss = 0.5277\n",
      "Iteration 6246: Weights = [55.30333333  2.4687409   5.91214417  0.0972392   0.1863157  12.82626571], Loss = 0.5277\n",
      "Iteration 6247: Weights = [55.30333333  2.46858139  5.91176218  0.09723291  0.18630366 12.82667721], Loss = 0.5276\n",
      "Iteration 6248: Weights = [55.30333333  2.46842189  5.9113802   0.09722663  0.18629163 12.82708869], Loss = 0.5275\n",
      "Iteration 6249: Weights = [55.30333333  2.4682624   5.91099826  0.09722035  0.18627959 12.82750015], Loss = 0.5275\n",
      "Iteration 6250: Weights = [55.30333333  2.46810293  5.91061634  0.09721407  0.18626755 12.82791158], Loss = 0.5274\n",
      "Iteration 6251: Weights = [55.30333333  2.46794346  5.91023444  0.09720779  0.18625552 12.82832298], Loss = 0.5273\n",
      "Iteration 6252: Weights = [55.30333333  2.467784    5.90985257  0.09720151  0.18624348 12.82873435], Loss = 0.5272\n",
      "Iteration 6253: Weights = [55.30333333  2.46762455  5.90947072  0.09719523  0.18623145 12.8291457 ], Loss = 0.5272\n",
      "Iteration 6254: Weights = [55.30333333  2.46746511  5.9090889   0.09718895  0.18621942 12.82955702], Loss = 0.5271\n",
      "Iteration 6255: Weights = [55.30333333  2.46730568  5.9087071   0.09718267  0.18620739 12.82996831], Loss = 0.5270\n",
      "Iteration 6256: Weights = [55.30333333  2.46714627  5.90832533  0.09717639  0.18619535 12.83037958], Loss = 0.5270\n",
      "Iteration 6257: Weights = [55.30333333  2.46698686  5.90794358  0.09717011  0.18618332 12.83079082], Loss = 0.5269\n",
      "Iteration 6258: Weights = [55.30333333  2.46682746  5.90756185  0.09716383  0.18617129 12.83120204], Loss = 0.5268\n",
      "Iteration 6259: Weights = [55.30333333  2.46666807  5.90718015  0.09715755  0.18615926 12.83161323], Loss = 0.5268\n",
      "Iteration 6260: Weights = [55.30333333  2.4665087   5.90679848  0.09715127  0.18614724 12.83202439], Loss = 0.5267\n",
      "Iteration 6261: Weights = [55.30333333  2.46634933  5.90641683  0.097145    0.18613521 12.83243552], Loss = 0.5266\n",
      "Iteration 6262: Weights = [55.30333333  2.46618998  5.9060352   0.09713872  0.18612318 12.83284663], Loss = 0.5266\n",
      "Iteration 6263: Weights = [55.30333333  2.46603063  5.9056536   0.09713244  0.18611116 12.83325771], Loss = 0.5265\n",
      "Iteration 6264: Weights = [55.30333333  2.4658713   5.90527203  0.09712617  0.18609913 12.83366877], Loss = 0.5264\n",
      "Iteration 6265: Weights = [55.30333333  2.46571197  5.90489048  0.09711989  0.18608711 12.8340798 ], Loss = 0.5264\n",
      "Iteration 6266: Weights = [55.30333333  2.46555266  5.90450895  0.09711362  0.18607508 12.8344908 ], Loss = 0.5263\n",
      "Iteration 6267: Weights = [55.30333333  2.46539335  5.90412745  0.09710734  0.18606306 12.83490177], Loss = 0.5262\n",
      "Iteration 6268: Weights = [55.30333333  2.46523406  5.90374597  0.09710107  0.18605104 12.83531272], Loss = 0.5262\n",
      "Iteration 6269: Weights = [55.30333333  2.46507477  5.90336452  0.09709479  0.18603902 12.83572365], Loss = 0.5261\n",
      "Iteration 6270: Weights = [55.30333333  2.4649155   5.90298309  0.09708852  0.186027   12.83613454], Loss = 0.5260\n",
      "Iteration 6271: Weights = [55.30333333  2.46475624  5.90260168  0.09708225  0.18601498 12.83654541], Loss = 0.5260\n",
      "Iteration 6272: Weights = [55.30333333  2.46459698  5.90222031  0.09707597  0.18600296 12.83695625], Loss = 0.5259\n",
      "Iteration 6273: Weights = [55.30333333  2.46443774  5.90183895  0.0970697   0.18599094 12.83736707], Loss = 0.5258\n",
      "Iteration 6274: Weights = [55.30333333  2.46427851  5.90145762  0.09706343  0.18597892 12.83777786], Loss = 0.5258\n",
      "Iteration 6275: Weights = [55.30333333  2.46411929  5.90107632  0.09705716  0.18596691 12.83818862], Loss = 0.5257\n",
      "Iteration 6276: Weights = [55.30333333  2.46396008  5.90069504  0.09705089  0.18595489 12.83859936], Loss = 0.5256\n",
      "Iteration 6277: Weights = [55.30333333  2.46380087  5.90031378  0.09704462  0.18594288 12.83901007], Loss = 0.5255\n",
      "Iteration 6278: Weights = [55.30333333  2.46364168  5.89993255  0.09703835  0.18593086 12.83942076], Loss = 0.5255\n",
      "Iteration 6279: Weights = [55.30333333  2.4634825   5.89955134  0.09703208  0.18591885 12.83983141], Loss = 0.5254\n",
      "Iteration 6280: Weights = [55.30333333  2.46332333  5.89917016  0.09702581  0.18590684 12.84024204], Loss = 0.5253\n",
      "Iteration 6281: Weights = [55.30333333  2.46316417  5.898789    0.09701954  0.18589483 12.84065265], Loss = 0.5253\n",
      "Iteration 6282: Weights = [55.30333333  2.46300502  5.89840787  0.09701327  0.18588281 12.84106322], Loss = 0.5252\n",
      "Iteration 6283: Weights = [55.30333333  2.46284588  5.89802676  0.097007    0.1858708  12.84147378], Loss = 0.5251\n",
      "Iteration 6284: Weights = [55.30333333  2.46268675  5.89764568  0.09700073  0.18585879 12.8418843 ], Loss = 0.5251\n",
      "Iteration 6285: Weights = [55.30333333  2.46252763  5.89726462  0.09699447  0.18584679 12.8422948 ], Loss = 0.5250\n",
      "Iteration 6286: Weights = [55.30333333  2.46236852  5.89688359  0.0969882   0.18583478 12.84270527], Loss = 0.5249\n",
      "Iteration 6287: Weights = [55.30333333  2.46220943  5.89650258  0.09698193  0.18582277 12.84311571], Loss = 0.5249\n",
      "Iteration 6288: Weights = [55.30333333  2.46205034  5.89612159  0.09697567  0.18581076 12.84352613], Loss = 0.5248\n",
      "Iteration 6289: Weights = [55.30333333  2.46189126  5.89574063  0.0969694   0.18579876 12.84393652], Loss = 0.5247\n",
      "Iteration 6290: Weights = [55.30333333  2.46173219  5.8953597   0.09696314  0.18578675 12.84434689], Loss = 0.5247\n",
      "Iteration 6291: Weights = [55.30333333  2.46157313  5.89497879  0.09695687  0.18577475 12.84475723], Loss = 0.5246\n",
      "Iteration 6292: Weights = [55.30333333  2.46141409  5.8945979   0.09695061  0.18576275 12.84516754], Loss = 0.5245\n",
      "Iteration 6293: Weights = [55.30333333  2.46125505  5.89421704  0.09694434  0.18575074 12.84557783], Loss = 0.5245\n",
      "Iteration 6294: Weights = [55.30333333  2.46109602  5.8938362   0.09693808  0.18573874 12.84598809], Loss = 0.5244\n",
      "Iteration 6295: Weights = [55.30333333  2.46093701  5.89345539  0.09693182  0.18572674 12.84639832], Loss = 0.5243\n",
      "Iteration 6296: Weights = [55.30333333  2.460778    5.8930746   0.09692555  0.18571474 12.84680853], Loss = 0.5243\n",
      "Iteration 6297: Weights = [55.30333333  2.46061901  5.89269384  0.09691929  0.18570274 12.84721871], Loss = 0.5242\n",
      "Iteration 6298: Weights = [55.30333333  2.46046002  5.8923131   0.09691303  0.18569074 12.84762886], Loss = 0.5241\n",
      "Iteration 6299: Weights = [55.30333333  2.46030104  5.89193238  0.09690677  0.18567875 12.84803899], Loss = 0.5241\n",
      "Iteration 6300: Weights = [55.30333333  2.46014208  5.89155169  0.0969005   0.18566675 12.84844909], Loss = 0.5240\n",
      "Iteration 6301: Weights = [55.30333333  2.45998313  5.89117103  0.09689424  0.18565475 12.84885916], Loss = 0.5239\n",
      "Iteration 6302: Weights = [55.30333333  2.45982418  5.89079039  0.09688798  0.18564276 12.84926921], Loss = 0.5239\n",
      "Iteration 6303: Weights = [55.30333333  2.45966525  5.89040977  0.09688172  0.18563076 12.84967923], Loss = 0.5238\n",
      "Iteration 6304: Weights = [55.30333333  2.45950632  5.89002918  0.09687546  0.18561877 12.85008922], Loss = 0.5237\n",
      "Iteration 6305: Weights = [55.30333333  2.45934741  5.88964861  0.0968692   0.18560677 12.85049919], Loss = 0.5237\n",
      "Iteration 6306: Weights = [55.30333333  2.45918851  5.88926807  0.09686294  0.18559478 12.85090913], Loss = 0.5236\n",
      "Iteration 6307: Weights = [55.30333333  2.45902961  5.88888755  0.09685669  0.18558279 12.85131905], Loss = 0.5235\n",
      "Iteration 6308: Weights = [55.30333333  2.45887073  5.88850706  0.09685043  0.1855708  12.85172893], Loss = 0.5234\n",
      "Iteration 6309: Weights = [55.30333333  2.45871186  5.88812659  0.09684417  0.18555881 12.8521388 ], Loss = 0.5234\n",
      "Iteration 6310: Weights = [55.30333333  2.45855299  5.88774615  0.09683791  0.18554682 12.85254863], Loss = 0.5233\n",
      "Iteration 6311: Weights = [55.30333333  2.45839414  5.88736573  0.09683166  0.18553483 12.85295844], Loss = 0.5232\n",
      "Iteration 6312: Weights = [55.30333333  2.4582353   5.88698534  0.0968254   0.18552284 12.85336822], Loss = 0.5232\n",
      "Iteration 6313: Weights = [55.30333333  2.45807647  5.88660497  0.09681914  0.18551086 12.85377798], Loss = 0.5231\n",
      "Iteration 6314: Weights = [55.30333333  2.45791765  5.88622462  0.09681289  0.18549887 12.85418771], Loss = 0.5230\n",
      "Iteration 6315: Weights = [55.30333333  2.45775884  5.8858443   0.09680663  0.18548689 12.85459741], Loss = 0.5230\n",
      "Iteration 6316: Weights = [55.30333333  2.45760004  5.885464    0.09680038  0.1854749  12.85500709], Loss = 0.5229\n",
      "Iteration 6317: Weights = [55.30333333  2.45744125  5.88508373  0.09679412  0.18546292 12.85541674], Loss = 0.5228\n",
      "Iteration 6318: Weights = [55.30333333  2.45728247  5.88470349  0.09678787  0.18545093 12.85582636], Loss = 0.5228\n",
      "Iteration 6319: Weights = [55.30333333  2.4571237   5.88432326  0.09678162  0.18543895 12.85623596], Loss = 0.5227\n",
      "Iteration 6320: Weights = [55.30333333  2.45696494  5.88394306  0.09677536  0.18542697 12.85664553], Loss = 0.5226\n",
      "Iteration 6321: Weights = [55.30333333  2.45680619  5.88356289  0.09676911  0.18541499 12.85705507], Loss = 0.5226\n",
      "Iteration 6322: Weights = [55.30333333  2.45664745  5.88318274  0.09676286  0.18540301 12.85746459], Loss = 0.5225\n",
      "Iteration 6323: Weights = [55.30333333  2.45648872  5.88280262  0.09675661  0.18539103 12.85787408], Loss = 0.5224\n",
      "Iteration 6324: Weights = [55.30333333  2.45633     5.88242252  0.09675035  0.18537905 12.85828355], Loss = 0.5224\n",
      "Iteration 6325: Weights = [55.30333333  2.45617129  5.88204244  0.0967441   0.18536707 12.85869299], Loss = 0.5223\n",
      "Iteration 6326: Weights = [55.30333333  2.45601259  5.88166239  0.09673785  0.1853551  12.8591024 ], Loss = 0.5222\n",
      "Iteration 6327: Weights = [55.30333333  2.45585391  5.88128237  0.0967316   0.18534312 12.85951178], Loss = 0.5222\n",
      "Iteration 6328: Weights = [55.30333333  2.45569523  5.88090237  0.09672535  0.18533114 12.85992114], Loss = 0.5221\n",
      "Iteration 6329: Weights = [55.30333333  2.45553656  5.88052239  0.0967191   0.18531917 12.86033048], Loss = 0.5220\n",
      "Iteration 6330: Weights = [55.30333333  2.4553779   5.88014244  0.09671285  0.1853072  12.86073978], Loss = 0.5220\n",
      "Iteration 6331: Weights = [55.30333333  2.45521926  5.87976251  0.0967066   0.18529522 12.86114906], Loss = 0.5219\n",
      "Iteration 6332: Weights = [55.30333333  2.45506062  5.8793826   0.09670035  0.18528325 12.86155831], Loss = 0.5218\n",
      "Iteration 6333: Weights = [55.30333333  2.45490199  5.87900273  0.09669411  0.18527128 12.86196754], Loss = 0.5218\n",
      "Iteration 6334: Weights = [55.30333333  2.45474338  5.87862287  0.09668786  0.18525931 12.86237674], Loss = 0.5217\n",
      "Iteration 6335: Weights = [55.30333333  2.45458477  5.87824304  0.09668161  0.18524734 12.86278592], Loss = 0.5216\n",
      "Iteration 6336: Weights = [55.30333333  2.45442618  5.87786324  0.09667537  0.18523537 12.86319506], Loss = 0.5216\n",
      "Iteration 6337: Weights = [55.30333333  2.45426759  5.87748346  0.09666912  0.1852234  12.86360418], Loss = 0.5215\n",
      "Iteration 6338: Weights = [55.30333333  2.45410901  5.8771037   0.09666287  0.18521143 12.86401328], Loss = 0.5214\n",
      "Iteration 6339: Weights = [55.30333333  2.45395045  5.87672397  0.09665663  0.18519947 12.86442235], Loss = 0.5214\n",
      "Iteration 6340: Weights = [55.30333333  2.4537919   5.87634426  0.09665038  0.1851875  12.86483139], Loss = 0.5213\n",
      "Iteration 6341: Weights = [55.30333333  2.45363335  5.87596458  0.09664414  0.18517554 12.8652404 ], Loss = 0.5212\n",
      "Iteration 6342: Weights = [55.30333333  2.45347482  5.87558492  0.09663789  0.18516357 12.86564939], Loss = 0.5212\n",
      "Iteration 6343: Weights = [55.30333333  2.45331629  5.87520529  0.09663165  0.18515161 12.86605836], Loss = 0.5211\n",
      "Iteration 6344: Weights = [55.30333333  2.45315778  5.87482568  0.09662541  0.18513964 12.86646729], Loss = 0.5210\n",
      "Iteration 6345: Weights = [55.30333333  2.45299928  5.8744461   0.09661916  0.18512768 12.8668762 ], Loss = 0.5210\n",
      "Iteration 6346: Weights = [55.30333333  2.45284078  5.87406654  0.09661292  0.18511572 12.86728508], Loss = 0.5209\n",
      "Iteration 6347: Weights = [55.30333333  2.4526823   5.873687    0.09660668  0.18510376 12.86769394], Loss = 0.5208\n",
      "Iteration 6348: Weights = [55.30333333  2.45252383  5.87330749  0.09660044  0.1850918  12.86810277], Loss = 0.5207\n",
      "Iteration 6349: Weights = [55.30333333  2.45236536  5.872928    0.09659419  0.18507984 12.86851158], Loss = 0.5207\n",
      "Iteration 6350: Weights = [55.30333333  2.45220691  5.87254854  0.09658795  0.18506788 12.86892035], Loss = 0.5206\n",
      "Iteration 6351: Weights = [55.30333333  2.45204847  5.8721691   0.09658171  0.18505592 12.8693291 ], Loss = 0.5205\n",
      "Iteration 6352: Weights = [55.30333333  2.45189004  5.87178969  0.09657547  0.18504397 12.86973783], Loss = 0.5205\n",
      "Iteration 6353: Weights = [55.30333333  2.45173162  5.8714103   0.09656923  0.18503201 12.87014653], Loss = 0.5204\n",
      "Iteration 6354: Weights = [55.30333333  2.45157321  5.87103094  0.09656299  0.18502006 12.8705552 ], Loss = 0.5203\n",
      "Iteration 6355: Weights = [55.30333333  2.4514148   5.8706516   0.09655675  0.1850081  12.87096384], Loss = 0.5203\n",
      "Iteration 6356: Weights = [55.30333333  2.45125641  5.87027229  0.09655051  0.18499615 12.87137246], Loss = 0.5202\n",
      "Iteration 6357: Weights = [55.30333333  2.45109803  5.869893    0.09654428  0.18498419 12.87178106], Loss = 0.5201\n",
      "Iteration 6358: Weights = [55.30333333  2.45093966  5.86951373  0.09653804  0.18497224 12.87218962], Loss = 0.5201\n",
      "Iteration 6359: Weights = [55.30333333  2.4507813   5.86913449  0.0965318   0.18496029 12.87259816], Loss = 0.5200\n",
      "Iteration 6360: Weights = [55.30333333  2.45062295  5.86875527  0.09652556  0.18494834 12.87300668], Loss = 0.5199\n",
      "Iteration 6361: Weights = [55.30333333  2.45046461  5.86837608  0.09651933  0.18493639 12.87341516], Loss = 0.5199\n",
      "Iteration 6362: Weights = [55.30333333  2.45030628  5.86799691  0.09651309  0.18492444 12.87382362], Loss = 0.5198\n",
      "Iteration 6363: Weights = [55.30333333  2.45014796  5.86761777  0.09650685  0.18491249 12.87423206], Loss = 0.5197\n",
      "Iteration 6364: Weights = [55.30333333  2.44998966  5.86723865  0.09650062  0.18490055 12.87464047], Loss = 0.5197\n",
      "Iteration 6365: Weights = [55.30333333  2.44983136  5.86685956  0.09649438  0.1848886  12.87504885], Loss = 0.5196\n",
      "Iteration 6366: Weights = [55.30333333  2.44967307  5.86648049  0.09648815  0.18487665 12.8754572 ], Loss = 0.5195\n",
      "Iteration 6367: Weights = [55.30333333  2.44951479  5.86610144  0.09648191  0.18486471 12.87586553], Loss = 0.5195\n",
      "Iteration 6368: Weights = [55.30333333  2.44935652  5.86572242  0.09647568  0.18485276 12.87627383], Loss = 0.5194\n",
      "Iteration 6369: Weights = [55.30333333  2.44919826  5.86534343  0.09646945  0.18484082 12.87668211], Loss = 0.5193\n",
      "Iteration 6370: Weights = [55.30333333  2.44904002  5.86496445  0.09646321  0.18482888 12.87709036], Loss = 0.5193\n",
      "Iteration 6371: Weights = [55.30333333  2.44888178  5.86458551  0.09645698  0.18481693 12.87749858], Loss = 0.5192\n",
      "Iteration 6372: Weights = [55.30333333  2.44872355  5.86420659  0.09645075  0.18480499 12.87790678], Loss = 0.5191\n",
      "Iteration 6373: Weights = [55.30333333  2.44856533  5.86382769  0.09644452  0.18479305 12.87831495], Loss = 0.5191\n",
      "Iteration 6374: Weights = [55.30333333  2.44840713  5.86344881  0.09643829  0.18478111 12.87872309], Loss = 0.5190\n",
      "Iteration 6375: Weights = [55.30333333  2.44824893  5.86306996  0.09643205  0.18476917 12.87913121], Loss = 0.5189\n",
      "Iteration 6376: Weights = [55.30333333  2.44809075  5.86269114  0.09642582  0.18475724 12.8795393 ], Loss = 0.5189\n",
      "Iteration 6377: Weights = [55.30333333  2.44793257  5.86231234  0.09641959  0.1847453  12.87994737], Loss = 0.5188\n",
      "Iteration 6378: Weights = [55.30333333  2.4477744   5.86193356  0.09641336  0.18473336 12.88035541], Loss = 0.5187\n",
      "Iteration 6379: Weights = [55.30333333  2.44761625  5.86155481  0.09640713  0.18472142 12.88076342], Loss = 0.5187\n",
      "Iteration 6380: Weights = [55.30333333  2.4474581   5.86117609  0.09640091  0.18470949 12.88117141], Loss = 0.5186\n",
      "Iteration 6381: Weights = [55.30333333  2.44729997  5.86079738  0.09639468  0.18469756 12.88157937], Loss = 0.5185\n",
      "Iteration 6382: Weights = [55.30333333  2.44714184  5.8604187   0.09638845  0.18468562 12.8819873 ], Loss = 0.5185\n",
      "Iteration 6383: Weights = [55.30333333  2.44698373  5.86004005  0.09638222  0.18467369 12.88239521], Loss = 0.5184\n",
      "Iteration 6384: Weights = [55.30333333  2.44682562  5.85966142  0.09637599  0.18466176 12.88280309], Loss = 0.5183\n",
      "Iteration 6385: Weights = [55.30333333  2.44666753  5.85928282  0.09636977  0.18464982 12.88321094], Loss = 0.5183\n",
      "Iteration 6386: Weights = [55.30333333  2.44650944  5.85890424  0.09636354  0.18463789 12.88361877], Loss = 0.5182\n",
      "Iteration 6387: Weights = [55.30333333  2.44635137  5.85852568  0.09635731  0.18462596 12.88402657], Loss = 0.5181\n",
      "Iteration 6388: Weights = [55.30333333  2.44619331  5.85814715  0.09635109  0.18461404 12.88443434], Loss = 0.5181\n",
      "Iteration 6389: Weights = [55.30333333  2.44603525  5.85776864  0.09634486  0.18460211 12.88484209], Loss = 0.5180\n",
      "Iteration 6390: Weights = [55.30333333  2.44587721  5.85739016  0.09633864  0.18459018 12.88524982], Loss = 0.5179\n",
      "Iteration 6391: Weights = [55.30333333  2.44571918  5.8570117   0.09633241  0.18457825 12.88565751], Loss = 0.5179\n",
      "Iteration 6392: Weights = [55.30333333  2.44556115  5.85663327  0.09632619  0.18456633 12.88606518], Loss = 0.5178\n",
      "Iteration 6393: Weights = [55.30333333  2.44540314  5.85625486  0.09631996  0.1845544  12.88647282], Loss = 0.5177\n",
      "Iteration 6394: Weights = [55.30333333  2.44524514  5.85587648  0.09631374  0.18454248 12.88688044], Loss = 0.5177\n",
      "Iteration 6395: Weights = [55.30333333  2.44508715  5.85549812  0.09630752  0.18453055 12.88728803], Loss = 0.5176\n",
      "Iteration 6396: Weights = [55.30333333  2.44492916  5.85511978  0.0963013   0.18451863 12.8876956 ], Loss = 0.5175\n",
      "Iteration 6397: Weights = [55.30333333  2.44477119  5.85474147  0.09629507  0.18450671 12.88810313], Loss = 0.5175\n",
      "Iteration 6398: Weights = [55.30333333  2.44461323  5.85436318  0.09628885  0.18449479 12.88851065], Loss = 0.5174\n",
      "Iteration 6399: Weights = [55.30333333  2.44445528  5.85398492  0.09628263  0.18448287 12.88891813], Loss = 0.5173\n",
      "Iteration 6400: Weights = [55.30333333  2.44429734  5.85360669  0.09627641  0.18447095 12.88932559], Loss = 0.5173\n",
      "Iteration 6401: Weights = [55.30333333  2.44413941  5.85322847  0.09627019  0.18445903 12.88973302], Loss = 0.5172\n",
      "Iteration 6402: Weights = [55.30333333  2.44398149  5.85285028  0.09626397  0.18444711 12.89014043], Loss = 0.5171\n",
      "Iteration 6403: Weights = [55.30333333  2.44382358  5.85247212  0.09625775  0.18443519 12.89054781], Loss = 0.5171\n",
      "Iteration 6404: Weights = [55.30333333  2.44366568  5.85209398  0.09625153  0.18442328 12.89095516], Loss = 0.5170\n",
      "Iteration 6405: Weights = [55.30333333  2.44350779  5.85171586  0.09624531  0.18441136 12.89136249], Loss = 0.5169\n",
      "Iteration 6406: Weights = [55.30333333  2.44334991  5.85133777  0.09623909  0.18439944 12.89176979], Loss = 0.5169\n",
      "Iteration 6407: Weights = [55.30333333  2.44319204  5.85095971  0.09623287  0.18438753 12.89217707], Loss = 0.5168\n",
      "Iteration 6408: Weights = [55.30333333  2.44303418  5.85058166  0.09622666  0.18437562 12.89258432], Loss = 0.5167\n",
      "Iteration 6409: Weights = [55.30333333  2.44287633  5.85020365  0.09622044  0.1843637  12.89299154], Loss = 0.5167\n",
      "Iteration 6410: Weights = [55.30333333  2.44271849  5.84982565  0.09621422  0.18435179 12.89339873], Loss = 0.5166\n",
      "Iteration 6411: Weights = [55.30333333  2.44256066  5.84944768  0.096208    0.18433988 12.8938059 ], Loss = 0.5165\n",
      "Iteration 6412: Weights = [55.30333333  2.44240284  5.84906974  0.09620179  0.18432797 12.89421305], Loss = 0.5165\n",
      "Iteration 6413: Weights = [55.30333333  2.44224503  5.84869182  0.09619557  0.18431606 12.89462016], Loss = 0.5164\n",
      "Iteration 6414: Weights = [55.30333333  2.44208723  5.84831392  0.09618936  0.18430415 12.89502725], Loss = 0.5163\n",
      "Iteration 6415: Weights = [55.30333333  2.44192945  5.84793605  0.09618314  0.18429224 12.89543432], Loss = 0.5163\n",
      "Iteration 6416: Weights = [55.30333333  2.44177167  5.8475582   0.09617693  0.18428033 12.89584136], Loss = 0.5162\n",
      "Iteration 6417: Weights = [55.30333333  2.4416139   5.84718038  0.09617071  0.18426843 12.89624837], Loss = 0.5161\n",
      "Iteration 6418: Weights = [55.30333333  2.44145614  5.84680258  0.0961645   0.18425652 12.89665535], Loss = 0.5161\n",
      "Iteration 6419: Weights = [55.30333333  2.44129839  5.84642481  0.09615829  0.18424462 12.89706231], Loss = 0.5160\n",
      "Iteration 6420: Weights = [55.30333333  2.44114066  5.84604706  0.09615207  0.18423271 12.89746924], Loss = 0.5159\n",
      "Iteration 6421: Weights = [55.30333333  2.44098293  5.84566934  0.09614586  0.18422081 12.89787615], Loss = 0.5159\n",
      "Iteration 6422: Weights = [55.30333333  2.44082521  5.84529164  0.09613965  0.18420891 12.89828303], Loss = 0.5158\n",
      "Iteration 6423: Weights = [55.30333333  2.44066751  5.84491396  0.09613344  0.184197   12.89868989], Loss = 0.5157\n",
      "Iteration 6424: Weights = [55.30333333  2.44050981  5.84453631  0.09612723  0.1841851  12.89909671], Loss = 0.5157\n",
      "Iteration 6425: Weights = [55.30333333  2.44035212  5.84415868  0.09612101  0.1841732  12.89950351], Loss = 0.5156\n",
      "Iteration 6426: Weights = [55.30333333  2.44019445  5.84378108  0.0961148   0.1841613  12.89991029], Loss = 0.5155\n",
      "Iteration 6427: Weights = [55.30333333  2.44003678  5.8434035   0.09610859  0.1841494  12.90031704], Loss = 0.5155\n",
      "Iteration 6428: Weights = [55.30333333  2.43987913  5.84302594  0.09610238  0.1841375  12.90072376], Loss = 0.5154\n",
      "Iteration 6429: Weights = [55.30333333  2.43972148  5.84264842  0.09609617  0.18412561 12.90113046], Loss = 0.5153\n",
      "Iteration 6430: Weights = [55.30333333  2.43956385  5.84227091  0.09608997  0.18411371 12.90153713], Loss = 0.5153\n",
      "Iteration 6431: Weights = [55.30333333  2.43940622  5.84189343  0.09608376  0.18410181 12.90194377], Loss = 0.5152\n",
      "Iteration 6432: Weights = [55.30333333  2.43924861  5.84151597  0.09607755  0.18408992 12.90235039], Loss = 0.5151\n",
      "Iteration 6433: Weights = [55.30333333  2.439091    5.84113854  0.09607134  0.18407803 12.90275698], Loss = 0.5151\n",
      "Iteration 6434: Weights = [55.30333333  2.43893341  5.84076113  0.09606513  0.18406613 12.90316354], Loss = 0.5150\n",
      "Iteration 6435: Weights = [55.30333333  2.43877582  5.84038375  0.09605893  0.18405424 12.90357008], Loss = 0.5149\n",
      "Iteration 6436: Weights = [55.30333333  2.43861825  5.84000639  0.09605272  0.18404235 12.9039766 ], Loss = 0.5149\n",
      "Iteration 6437: Weights = [55.30333333  2.43846068  5.83962906  0.09604651  0.18403046 12.90438308], Loss = 0.5148\n",
      "Iteration 6438: Weights = [55.30333333  2.43830313  5.83925175  0.09604031  0.18401856 12.90478954], Loss = 0.5147\n",
      "Iteration 6439: Weights = [55.30333333  2.43814559  5.83887446  0.0960341   0.18400667 12.90519597], Loss = 0.5147\n",
      "Iteration 6440: Weights = [55.30333333  2.43798805  5.8384972   0.0960279   0.18399479 12.90560238], Loss = 0.5146\n",
      "Iteration 6441: Weights = [55.30333333  2.43783053  5.83811996  0.09602169  0.1839829  12.90600876], Loss = 0.5145\n",
      "Iteration 6442: Weights = [55.30333333  2.43767302  5.83774275  0.09601549  0.18397101 12.90641512], Loss = 0.5145\n",
      "Iteration 6443: Weights = [55.30333333  2.43751551  5.83736556  0.09600929  0.18395912 12.90682145], Loss = 0.5144\n",
      "Iteration 6444: Weights = [55.30333333  2.43735802  5.8369884   0.09600308  0.18394724 12.90722775], Loss = 0.5143\n",
      "Iteration 6445: Weights = [55.30333333  2.43720054  5.83661126  0.09599688  0.18393535 12.90763402], Loss = 0.5143\n",
      "Iteration 6446: Weights = [55.30333333  2.43704307  5.83623414  0.09599068  0.18392347 12.90804027], Loss = 0.5142\n",
      "Iteration 6447: Weights = [55.30333333  2.4368856   5.83585705  0.09598447  0.18391158 12.9084465 ], Loss = 0.5141\n",
      "Iteration 6448: Weights = [55.30333333  2.43672815  5.83547998  0.09597827  0.1838997  12.9088527 ], Loss = 0.5141\n",
      "Iteration 6449: Weights = [55.30333333  2.43657071  5.83510294  0.09597207  0.18388782 12.90925887], Loss = 0.5140\n",
      "Iteration 6450: Weights = [55.30333333  2.43641328  5.83472593  0.09596587  0.18387594 12.90966501], Loss = 0.5139\n",
      "Iteration 6451: Weights = [55.30333333  2.43625586  5.83434893  0.09595967  0.18386406 12.91007113], Loss = 0.5139\n",
      "Iteration 6452: Weights = [55.30333333  2.43609845  5.83397196  0.09595347  0.18385218 12.91047722], Loss = 0.5138\n",
      "Iteration 6453: Weights = [55.30333333  2.43594104  5.83359502  0.09594727  0.1838403  12.91088329], Loss = 0.5137\n",
      "Iteration 6454: Weights = [55.30333333  2.43578365  5.8332181   0.09594107  0.18382842 12.91128933], Loss = 0.5137\n",
      "Iteration 6455: Weights = [55.30333333  2.43562627  5.8328412   0.09593487  0.18381654 12.91169534], Loss = 0.5136\n",
      "Iteration 6456: Weights = [55.30333333  2.4354689   5.83246433  0.09592867  0.18380467 12.91210133], Loss = 0.5135\n",
      "Iteration 6457: Weights = [55.30333333  2.43531154  5.83208748  0.09592247  0.18379279 12.91250729], Loss = 0.5135\n",
      "Iteration 6458: Weights = [55.30333333  2.43515419  5.83171066  0.09591628  0.18378091 12.91291322], Loss = 0.5134\n",
      "Iteration 6459: Weights = [55.30333333  2.43499685  5.83133386  0.09591008  0.18376904 12.91331913], Loss = 0.5133\n",
      "Iteration 6460: Weights = [55.30333333  2.43483952  5.83095709  0.09590388  0.18375717 12.91372502], Loss = 0.5133\n",
      "Iteration 6461: Weights = [55.30333333  2.4346822   5.83058034  0.09589769  0.18374529 12.91413087], Loss = 0.5132\n",
      "Iteration 6462: Weights = [55.30333333  2.43452489  5.83020361  0.09589149  0.18373342 12.9145367 ], Loss = 0.5131\n",
      "Iteration 6463: Weights = [55.30333333  2.43436759  5.82982691  0.09588529  0.18372155 12.91494251], Loss = 0.5131\n",
      "Iteration 6464: Weights = [55.30333333  2.4342103   5.82945023  0.0958791   0.18370968 12.91534828], Loss = 0.5130\n",
      "Iteration 6465: Weights = [55.30333333  2.43405302  5.82907358  0.0958729   0.18369781 12.91575403], Loss = 0.5129\n",
      "Iteration 6466: Weights = [55.30333333  2.43389575  5.82869695  0.09586671  0.18368594 12.91615976], Loss = 0.5129\n",
      "Iteration 6467: Weights = [55.30333333  2.43373849  5.82832035  0.09586052  0.18367407 12.91656546], Loss = 0.5128\n",
      "Iteration 6468: Weights = [55.30333333  2.43358125  5.82794377  0.09585432  0.1836622  12.91697113], Loss = 0.5127\n",
      "Iteration 6469: Weights = [55.30333333  2.43342401  5.82756721  0.09584813  0.18365034 12.91737678], Loss = 0.5127\n",
      "Iteration 6470: Weights = [55.30333333  2.43326678  5.82719068  0.09584194  0.18363847 12.9177824 ], Loss = 0.5126\n",
      "Iteration 6471: Weights = [55.30333333  2.43310956  5.82681418  0.09583574  0.18362661 12.91818799], Loss = 0.5125\n",
      "Iteration 6472: Weights = [55.30333333  2.43295235  5.8264377   0.09582955  0.18361474 12.91859356], Loss = 0.5125\n",
      "Iteration 6473: Weights = [55.30333333  2.43279515  5.82606124  0.09582336  0.18360288 12.9189991 ], Loss = 0.5124\n",
      "Iteration 6474: Weights = [55.30333333  2.43263797  5.8256848   0.09581717  0.18359101 12.91940462], Loss = 0.5123\n",
      "Iteration 6475: Weights = [55.30333333  2.43248079  5.82530839  0.09581098  0.18357915 12.91981011], Loss = 0.5123\n",
      "Iteration 6476: Weights = [55.30333333  2.43232362  5.82493201  0.09580479  0.18356729 12.92021557], Loss = 0.5122\n",
      "Iteration 6477: Weights = [55.30333333  2.43216646  5.82455565  0.0957986   0.18355543 12.92062101], Loss = 0.5121\n",
      "Iteration 6478: Weights = [55.30333333  2.43200932  5.82417931  0.09579241  0.18354357 12.92102642], Loss = 0.5121\n",
      "Iteration 6479: Weights = [55.30333333  2.43185218  5.823803    0.09578622  0.18353171 12.9214318 ], Loss = 0.5120\n",
      "Iteration 6480: Weights = [55.30333333  2.43169505  5.82342671  0.09578003  0.18351985 12.92183716], Loss = 0.5119\n",
      "Iteration 6481: Weights = [55.30333333  2.43153794  5.82305045  0.09577384  0.183508   12.92224249], Loss = 0.5119\n",
      "Iteration 6482: Weights = [55.30333333  2.43138083  5.82267421  0.09576765  0.18349614 12.9226478 ], Loss = 0.5118\n",
      "Iteration 6483: Weights = [55.30333333  2.43122373  5.822298    0.09576146  0.18348428 12.92305308], Loss = 0.5117\n",
      "Iteration 6484: Weights = [55.30333333  2.43106665  5.82192181  0.09575528  0.18347243 12.92345833], Loss = 0.5117\n",
      "Iteration 6485: Weights = [55.30333333  2.43090957  5.82154564  0.09574909  0.18346057 12.92386356], Loss = 0.5116\n",
      "Iteration 6486: Weights = [55.30333333  2.43075251  5.8211695   0.0957429   0.18344872 12.92426876], Loss = 0.5115\n",
      "Iteration 6487: Weights = [55.30333333  2.43059545  5.82079338  0.09573672  0.18343687 12.92467393], Loss = 0.5115\n",
      "Iteration 6488: Weights = [55.30333333  2.4304384   5.82041729  0.09573053  0.18342501 12.92507908], Loss = 0.5114\n",
      "Iteration 6489: Weights = [55.30333333  2.43028137  5.82004122  0.09572435  0.18341316 12.92548421], Loss = 0.5113\n",
      "Iteration 6490: Weights = [55.30333333  2.43012434  5.81966517  0.09571816  0.18340131 12.9258893 ], Loss = 0.5113\n",
      "Iteration 6491: Weights = [55.30333333  2.42996733  5.81928915  0.09571198  0.18338946 12.92629437], Loss = 0.5112\n",
      "Iteration 6492: Weights = [55.30333333  2.42981032  5.81891316  0.09570579  0.18337761 12.92669942], Loss = 0.5111\n",
      "Iteration 6493: Weights = [55.30333333  2.42965333  5.81853719  0.09569961  0.18336576 12.92710443], Loss = 0.5111\n",
      "Iteration 6494: Weights = [55.30333333  2.42949634  5.81816124  0.09569342  0.18335392 12.92750943], Loss = 0.5110\n",
      "Iteration 6495: Weights = [55.30333333  2.42933937  5.81778532  0.09568724  0.18334207 12.92791439], Loss = 0.5109\n",
      "Iteration 6496: Weights = [55.30333333  2.4291824   5.81740942  0.09568106  0.18333022 12.92831933], Loss = 0.5109\n",
      "Iteration 6497: Weights = [55.30333333  2.42902545  5.81703354  0.09567488  0.18331838 12.92872424], Loss = 0.5108\n",
      "Iteration 6498: Weights = [55.30333333  2.42886851  5.81665769  0.0956687   0.18330653 12.92912913], Loss = 0.5108\n",
      "Iteration 6499: Weights = [55.30333333  2.42871157  5.81628187  0.09566251  0.18329469 12.92953399], Loss = 0.5107\n",
      "Iteration 6500: Weights = [55.30333333  2.42855465  5.81590606  0.09565633  0.18328285 12.92993883], Loss = 0.5106\n",
      "Iteration 6501: Weights = [55.30333333  2.42839773  5.81553029  0.09565015  0.183271   12.93034364], Loss = 0.5106\n",
      "Iteration 6502: Weights = [55.30333333  2.42824083  5.81515453  0.09564397  0.18325916 12.93074842], Loss = 0.5105\n",
      "Iteration 6503: Weights = [55.30333333  2.42808394  5.81477881  0.09563779  0.18324732 12.93115317], Loss = 0.5104\n",
      "Iteration 6504: Weights = [55.30333333  2.42792705  5.8144031   0.09563161  0.18323548 12.9315579 ], Loss = 0.5104\n",
      "Iteration 6505: Weights = [55.30333333  2.42777018  5.81402742  0.09562543  0.18322364 12.93196261], Loss = 0.5103\n",
      "Iteration 6506: Weights = [55.30333333  2.42761332  5.81365176  0.09561926  0.1832118  12.93236729], Loss = 0.5102\n",
      "Iteration 6507: Weights = [55.30333333  2.42745646  5.81327613  0.09561308  0.18319997 12.93277194], Loss = 0.5102\n",
      "Iteration 6508: Weights = [55.30333333  2.42729962  5.81290053  0.0956069   0.18318813 12.93317656], Loss = 0.5101\n",
      "Iteration 6509: Weights = [55.30333333  2.42714279  5.81252494  0.09560072  0.18317629 12.93358116], Loss = 0.5100\n",
      "Iteration 6510: Weights = [55.30333333  2.42698597  5.81214938  0.09559455  0.18316446 12.93398574], Loss = 0.5100\n",
      "Iteration 6511: Weights = [55.30333333  2.42682915  5.81177385  0.09558837  0.18315262 12.93439028], Loss = 0.5099\n",
      "Iteration 6512: Weights = [55.30333333  2.42667235  5.81139834  0.09558219  0.18314079 12.9347948 ], Loss = 0.5098\n",
      "Iteration 6513: Weights = [55.30333333  2.42651556  5.81102285  0.09557602  0.18312896 12.9351993 ], Loss = 0.5098\n",
      "Iteration 6514: Weights = [55.30333333  2.42635878  5.81064739  0.09556984  0.18311712 12.93560377], Loss = 0.5097\n",
      "Iteration 6515: Weights = [55.30333333  2.426202    5.81027195  0.09556367  0.18310529 12.93600821], Loss = 0.5096\n",
      "Iteration 6516: Weights = [55.30333333  2.42604524  5.80989654  0.09555749  0.18309346 12.93641263], Loss = 0.5096\n",
      "Iteration 6517: Weights = [55.30333333  2.42588849  5.80952115  0.09555132  0.18308163 12.93681702], Loss = 0.5095\n",
      "Iteration 6518: Weights = [55.30333333  2.42573175  5.80914579  0.09554514  0.1830698  12.93722138], Loss = 0.5094\n",
      "Iteration 6519: Weights = [55.30333333  2.42557502  5.80877044  0.09553897  0.18305797 12.93762572], Loss = 0.5094\n",
      "Iteration 6520: Weights = [55.30333333  2.4254183   5.80839513  0.0955328   0.18304615 12.93803003], Loss = 0.5093\n",
      "Iteration 6521: Weights = [55.30333333  2.42526159  5.80801984  0.09552663  0.18303432 12.93843432], Loss = 0.5092\n",
      "Iteration 6522: Weights = [55.30333333  2.42510488  5.80764457  0.09552045  0.18302249 12.93883858], Loss = 0.5092\n",
      "Iteration 6523: Weights = [55.30333333  2.42494819  5.80726933  0.09551428  0.18301067 12.93924281], Loss = 0.5091\n",
      "Iteration 6524: Weights = [55.30333333  2.42479151  5.80689411  0.09550811  0.18299884 12.93964702], Loss = 0.5090\n",
      "Iteration 6525: Weights = [55.30333333  2.42463484  5.80651891  0.09550194  0.18298702 12.9400512 ], Loss = 0.5090\n",
      "Iteration 6526: Weights = [55.30333333  2.42447818  5.80614374  0.09549577  0.1829752  12.94045535], Loss = 0.5089\n",
      "Iteration 6527: Weights = [55.30333333  2.42432153  5.80576859  0.0954896   0.18296337 12.94085948], Loss = 0.5088\n",
      "Iteration 6528: Weights = [55.30333333  2.42416489  5.80539347  0.09548343  0.18295155 12.94126359], Loss = 0.5088\n",
      "Iteration 6529: Weights = [55.30333333  2.42400826  5.80501837  0.09547726  0.18293973 12.94166766], Loss = 0.5087\n",
      "Iteration 6530: Weights = [55.30333333  2.42385164  5.8046433   0.09547109  0.18292791 12.94207171], Loss = 0.5086\n",
      "Iteration 6531: Weights = [55.30333333  2.42369503  5.80426825  0.09546492  0.18291609 12.94247574], Loss = 0.5086\n",
      "Iteration 6532: Weights = [55.30333333  2.42353843  5.80389322  0.09545875  0.18290427 12.94287974], Loss = 0.5085\n",
      "Iteration 6533: Weights = [55.30333333  2.42338184  5.80351822  0.09545259  0.18289246 12.94328371], Loss = 0.5084\n",
      "Iteration 6534: Weights = [55.30333333  2.42322526  5.80314325  0.09544642  0.18288064 12.94368766], Loss = 0.5084\n",
      "Iteration 6535: Weights = [55.30333333  2.42306869  5.80276829  0.09544025  0.18286882 12.94409158], Loss = 0.5083\n",
      "Iteration 6536: Weights = [55.30333333  2.42291213  5.80239336  0.09543408  0.18285701 12.94449547], Loss = 0.5082\n",
      "Iteration 6537: Weights = [55.30333333  2.42275558  5.80201846  0.09542792  0.18284519 12.94489934], Loss = 0.5082\n",
      "Iteration 6538: Weights = [55.30333333  2.42259904  5.80164358  0.09542175  0.18283338 12.94530318], Loss = 0.5081\n",
      "Iteration 6539: Weights = [55.30333333  2.42244252  5.80126872  0.09541559  0.18282156 12.945707  ], Loss = 0.5081\n",
      "Iteration 6540: Weights = [55.30333333  2.422286    5.80089389  0.09540942  0.18280975 12.94611079], Loss = 0.5080\n",
      "Iteration 6541: Weights = [55.30333333  2.42212949  5.80051908  0.09540326  0.18279794 12.94651455], Loss = 0.5079\n",
      "Iteration 6542: Weights = [55.30333333  2.42197299  5.8001443   0.09539709  0.18278613 12.94691829], Loss = 0.5079\n",
      "Iteration 6543: Weights = [55.30333333  2.4218165   5.79976954  0.09539093  0.18277432 12.947322  ], Loss = 0.5078\n",
      "Iteration 6544: Weights = [55.30333333  2.42166002  5.79939481  0.09538477  0.18276251 12.94772568], Loss = 0.5077\n",
      "Iteration 6545: Weights = [55.30333333  2.42150355  5.7990201   0.0953786   0.1827507  12.94812934], Loss = 0.5077\n",
      "Iteration 6546: Weights = [55.30333333  2.4213471   5.79864541  0.09537244  0.18273889 12.94853298], Loss = 0.5076\n",
      "Iteration 6547: Weights = [55.30333333  2.42119065  5.79827075  0.09536628  0.18272709 12.94893658], Loss = 0.5075\n",
      "Iteration 6548: Weights = [55.30333333  2.42103421  5.79789611  0.09536012  0.18271528 12.94934017], Loss = 0.5075\n",
      "Iteration 6549: Weights = [55.30333333  2.42087778  5.7975215   0.09535396  0.18270347 12.94974372], Loss = 0.5074\n",
      "Iteration 6550: Weights = [55.30333333  2.42072136  5.79714691  0.09534779  0.18269167 12.95014725], Loss = 0.5073\n",
      "Iteration 6551: Weights = [55.30333333  2.42056496  5.79677234  0.09534163  0.18267987 12.95055075], Loss = 0.5073\n",
      "Iteration 6552: Weights = [55.30333333  2.42040856  5.7963978   0.09533547  0.18266806 12.95095423], Loss = 0.5072\n",
      "Iteration 6553: Weights = [55.30333333  2.42025217  5.79602328  0.09532931  0.18265626 12.95135768], Loss = 0.5071\n",
      "Iteration 6554: Weights = [55.30333333  2.42009579  5.79564879  0.09532315  0.18264446 12.9517611 ], Loss = 0.5071\n",
      "Iteration 6555: Weights = [55.30333333  2.41993943  5.79527432  0.095317    0.18263266 12.9521645 ], Loss = 0.5070\n",
      "Iteration 6556: Weights = [55.30333333  2.41978307  5.79489988  0.09531084  0.18262086 12.95256788], Loss = 0.5069\n",
      "Iteration 6557: Weights = [55.30333333  2.41962672  5.79452546  0.09530468  0.18260906 12.95297122], Loss = 0.5069\n",
      "Iteration 6558: Weights = [55.30333333  2.41947039  5.79415106  0.09529852  0.18259726 12.95337454], Loss = 0.5068\n",
      "Iteration 6559: Weights = [55.30333333  2.41931406  5.79377669  0.09529236  0.18258546 12.95377784], Loss = 0.5067\n",
      "Iteration 6560: Weights = [55.30333333  2.41915774  5.79340234  0.09528621  0.18257366 12.95418111], Loss = 0.5067\n",
      "Iteration 6561: Weights = [55.30333333  2.41900144  5.79302802  0.09528005  0.18256187 12.95458435], Loss = 0.5066\n",
      "Iteration 6562: Weights = [55.30333333  2.41884514  5.79265372  0.09527389  0.18255007 12.95498756], Loss = 0.5065\n",
      "Iteration 6563: Weights = [55.30333333  2.41868885  5.79227945  0.09526774  0.18253828 12.95539075], Loss = 0.5065\n",
      "Iteration 6564: Weights = [55.30333333  2.41853258  5.7919052   0.09526158  0.18252648 12.95579392], Loss = 0.5064\n",
      "Iteration 6565: Weights = [55.30333333  2.41837631  5.79153097  0.09525543  0.18251469 12.95619706], Loss = 0.5063\n",
      "Iteration 6566: Weights = [55.30333333  2.41822005  5.79115677  0.09524927  0.1825029  12.95660017], Loss = 0.5063\n",
      "Iteration 6567: Weights = [55.30333333  2.41806381  5.79078259  0.09524312  0.1824911  12.95700325], Loss = 0.5062\n",
      "Iteration 6568: Weights = [55.30333333  2.41790757  5.79040843  0.09523696  0.18247931 12.95740631], Loss = 0.5062\n",
      "Iteration 6569: Weights = [55.30333333  2.41775135  5.7900343   0.09523081  0.18246752 12.95780935], Loss = 0.5061\n",
      "Iteration 6570: Weights = [55.30333333  2.41759513  5.7896602   0.09522466  0.18245573 12.95821236], Loss = 0.5060\n",
      "Iteration 6571: Weights = [55.30333333  2.41743892  5.78928612  0.09521851  0.18244394 12.95861534], Loss = 0.5060\n",
      "Iteration 6572: Weights = [55.30333333  2.41728273  5.78891206  0.09521235  0.18243216 12.95901829], Loss = 0.5059\n",
      "Iteration 6573: Weights = [55.30333333  2.41712654  5.78853803  0.0952062   0.18242037 12.95942122], Loss = 0.5058\n",
      "Iteration 6574: Weights = [55.30333333  2.41697037  5.78816402  0.09520005  0.18240858 12.95982413], Loss = 0.5058\n",
      "Iteration 6575: Weights = [55.30333333  2.4168142   5.78779003  0.0951939   0.1823968  12.960227  ], Loss = 0.5057\n",
      "Iteration 6576: Weights = [55.30333333  2.41665805  5.78741607  0.09518775  0.18238501 12.96062986], Loss = 0.5056\n",
      "Iteration 6577: Weights = [55.30333333  2.4165019   5.78704213  0.0951816   0.18237323 12.96103268], Loss = 0.5056\n",
      "Iteration 6578: Weights = [55.30333333  2.41634577  5.78666822  0.09517545  0.18236144 12.96143548], Loss = 0.5055\n",
      "Iteration 6579: Weights = [55.30333333  2.41618964  5.78629433  0.0951693   0.18234966 12.96183825], Loss = 0.5054\n",
      "Iteration 6580: Weights = [55.30333333  2.41603353  5.78592047  0.09516315  0.18233788 12.962241  ], Loss = 0.5054\n",
      "Iteration 6581: Weights = [55.30333333  2.41587742  5.78554663  0.095157    0.1823261  12.96264372], Loss = 0.5053\n",
      "Iteration 6582: Weights = [55.30333333  2.41572133  5.78517281  0.09515085  0.18231432 12.96304642], Loss = 0.5052\n",
      "Iteration 6583: Weights = [55.30333333  2.41556524  5.78479902  0.0951447   0.18230254 12.96344909], Loss = 0.5052\n",
      "Iteration 6584: Weights = [55.30333333  2.41540917  5.78442525  0.09513856  0.18229076 12.96385173], Loss = 0.5051\n",
      "Iteration 6585: Weights = [55.30333333  2.4152531   5.78405151  0.09513241  0.18227898 12.96425435], Loss = 0.5050\n",
      "Iteration 6586: Weights = [55.30333333  2.41509705  5.78367779  0.09512626  0.1822672  12.96465694], Loss = 0.5050\n",
      "Iteration 6587: Weights = [55.30333333  2.41494101  5.7833041   0.09512012  0.18225543 12.96505951], Loss = 0.5049\n",
      "Iteration 6588: Weights = [55.30333333  2.41478497  5.78293043  0.09511397  0.18224365 12.96546205], Loss = 0.5048\n",
      "Iteration 6589: Weights = [55.30333333  2.41462895  5.78255678  0.09510783  0.18223188 12.96586456], Loss = 0.5048\n",
      "Iteration 6590: Weights = [55.30333333  2.41447293  5.78218316  0.09510168  0.1822201  12.96626705], Loss = 0.5047\n",
      "Iteration 6591: Weights = [55.30333333  2.41431693  5.78180956  0.09509554  0.18220833 12.96666951], Loss = 0.5047\n",
      "Iteration 6592: Weights = [55.30333333  2.41416094  5.78143598  0.09508939  0.18219655 12.96707194], Loss = 0.5046\n",
      "Iteration 6593: Weights = [55.30333333  2.41400495  5.78106243  0.09508325  0.18218478 12.96747435], Loss = 0.5045\n",
      "Iteration 6594: Weights = [55.30333333  2.41384898  5.78068891  0.0950771   0.18217301 12.96787674], Loss = 0.5045\n",
      "Iteration 6595: Weights = [55.30333333  2.41369301  5.78031541  0.09507096  0.18216124 12.96827909], Loss = 0.5044\n",
      "Iteration 6596: Weights = [55.30333333  2.41353706  5.77994193  0.09506482  0.18214947 12.96868142], Loss = 0.5043\n",
      "Iteration 6597: Weights = [55.30333333  2.41338112  5.77956847  0.09505868  0.1821377  12.96908373], Loss = 0.5043\n",
      "Iteration 6598: Weights = [55.30333333  2.41322518  5.77919505  0.09505253  0.18212593 12.96948601], Loss = 0.5042\n",
      "Iteration 6599: Weights = [55.30333333  2.41306926  5.77882164  0.09504639  0.18211417 12.96988826], Loss = 0.5041\n",
      "Iteration 6600: Weights = [55.30333333  2.41291335  5.77844826  0.09504025  0.1821024  12.97029049], Loss = 0.5041\n",
      "Iteration 6601: Weights = [55.30333333  2.41275744  5.7780749   0.09503411  0.18209063 12.97069269], Loss = 0.5040\n",
      "Iteration 6602: Weights = [55.30333333  2.41260155  5.77770157  0.09502797  0.18207887 12.97109487], Loss = 0.5039\n",
      "Iteration 6603: Weights = [55.30333333  2.41244567  5.77732826  0.09502183  0.1820671  12.97149702], Loss = 0.5039\n",
      "Iteration 6604: Weights = [55.30333333  2.4122898   5.77695497  0.09501569  0.18205534 12.97189914], Loss = 0.5038\n",
      "Iteration 6605: Weights = [55.30333333  2.41213393  5.77658171  0.09500955  0.18204358 12.97230124], Loss = 0.5037\n",
      "Iteration 6606: Weights = [55.30333333  2.41197808  5.77620848  0.09500341  0.18203181 12.97270331], Loss = 0.5037\n",
      "Iteration 6607: Weights = [55.30333333  2.41182224  5.77583526  0.09499727  0.18202005 12.97310535], Loss = 0.5036\n",
      "Iteration 6608: Weights = [55.30333333  2.4116664   5.77546208  0.09499114  0.18200829 12.97350737], Loss = 0.5035\n",
      "Iteration 6609: Weights = [55.30333333  2.41151058  5.77508891  0.094985    0.18199653 12.97390937], Loss = 0.5035\n",
      "Iteration 6610: Weights = [55.30333333  2.41135477  5.77471577  0.09497886  0.18198477 12.97431134], Loss = 0.5034\n",
      "Iteration 6611: Weights = [55.30333333  2.41119897  5.77434266  0.09497272  0.18197302 12.97471328], Loss = 0.5033\n",
      "Iteration 6612: Weights = [55.30333333  2.41104317  5.77396956  0.09496659  0.18196126 12.97511519], Loss = 0.5033\n",
      "Iteration 6613: Weights = [55.30333333  2.41088739  5.7735965   0.09496045  0.1819495  12.97551708], Loss = 0.5032\n",
      "Iteration 6614: Weights = [55.30333333  2.41073162  5.77322345  0.09495432  0.18193774 12.97591895], Loss = 0.5032\n",
      "Iteration 6615: Weights = [55.30333333  2.41057586  5.77285043  0.09494818  0.18192599 12.97632078], Loss = 0.5031\n",
      "Iteration 6616: Weights = [55.30333333  2.41042011  5.77247744  0.09494205  0.18191423 12.9767226 ], Loss = 0.5030\n",
      "Iteration 6617: Weights = [55.30333333  2.41026436  5.77210447  0.09493591  0.18190248 12.97712438], Loss = 0.5030\n",
      "Iteration 6618: Weights = [55.30333333  2.41010863  5.77173152  0.09492978  0.18189073 12.97752614], Loss = 0.5029\n",
      "Iteration 6619: Weights = [55.30333333  2.40995291  5.7713586   0.09492364  0.18187898 12.97792787], Loss = 0.5028\n",
      "Iteration 6620: Weights = [55.30333333  2.4097972   5.7709857   0.09491751  0.18186722 12.97832958], Loss = 0.5028\n",
      "Iteration 6621: Weights = [55.30333333  2.4096415   5.77061282  0.09491138  0.18185547 12.97873126], Loss = 0.5027\n",
      "Iteration 6622: Weights = [55.30333333  2.4094858   5.77023997  0.09490525  0.18184372 12.97913292], Loss = 0.5026\n",
      "Iteration 6623: Weights = [55.30333333  2.40933012  5.76986714  0.09489911  0.18183197 12.97953455], Loss = 0.5026\n",
      "Iteration 6624: Weights = [55.30333333  2.40917445  5.76949434  0.09489298  0.18182022 12.97993615], Loss = 0.5025\n",
      "Iteration 6625: Weights = [55.30333333  2.40901879  5.76912156  0.09488685  0.18180848 12.98033773], Loss = 0.5024\n",
      "Iteration 6626: Weights = [55.30333333  2.40886314  5.76874881  0.09488072  0.18179673 12.98073929], Loss = 0.5024\n",
      "Iteration 6627: Weights = [55.30333333  2.4087075   5.76837608  0.09487459  0.18178498 12.98114081], Loss = 0.5023\n",
      "Iteration 6628: Weights = [55.30333333  2.40855186  5.76800337  0.09486846  0.18177324 12.98154231], Loss = 0.5022\n",
      "Iteration 6629: Weights = [55.30333333  2.40839624  5.76763069  0.09486233  0.18176149 12.98194379], Loss = 0.5022\n",
      "Iteration 6630: Weights = [55.30333333  2.40824063  5.76725803  0.0948562   0.18174975 12.98234523], Loss = 0.5021\n",
      "Iteration 6631: Weights = [55.30333333  2.40808503  5.7668854   0.09485007  0.18173801 12.98274666], Loss = 0.5020\n",
      "Iteration 6632: Weights = [55.30333333  2.40792944  5.76651279  0.09484394  0.18172626 12.98314805], Loss = 0.5020\n",
      "Iteration 6633: Weights = [55.30333333  2.40777386  5.7661402   0.09483782  0.18171452 12.98354942], Loss = 0.5019\n",
      "Iteration 6634: Weights = [55.30333333  2.40761829  5.76576764  0.09483169  0.18170278 12.98395077], Loss = 0.5019\n",
      "Iteration 6635: Weights = [55.30333333  2.40746273  5.7653951   0.09482556  0.18169104 12.98435209], Loss = 0.5018\n",
      "Iteration 6636: Weights = [55.30333333  2.40730717  5.76502259  0.09481943  0.1816793  12.98475338], Loss = 0.5017\n",
      "Iteration 6637: Weights = [55.30333333  2.40715163  5.7646501   0.09481331  0.18166756 12.98515465], Loss = 0.5017\n",
      "Iteration 6638: Weights = [55.30333333  2.4069961   5.76427763  0.09480718  0.18165583 12.98555589], Loss = 0.5016\n",
      "Iteration 6639: Weights = [55.30333333  2.40684058  5.76390519  0.09480106  0.18164409 12.9859571 ], Loss = 0.5015\n",
      "Iteration 6640: Weights = [55.30333333  2.40668507  5.76353277  0.09479493  0.18163235 12.98635829], Loss = 0.5015\n",
      "Iteration 6641: Weights = [55.30333333  2.40652957  5.76316038  0.09478881  0.18162062 12.98675946], Loss = 0.5014\n",
      "Iteration 6642: Weights = [55.30333333  2.40637408  5.76278801  0.09478268  0.18160888 12.98716059], Loss = 0.5013\n",
      "Iteration 6643: Weights = [55.30333333  2.4062186   5.76241566  0.09477656  0.18159715 12.9875617 ], Loss = 0.5013\n",
      "Iteration 6644: Weights = [55.30333333  2.40606313  5.76204334  0.09477043  0.18158541 12.98796279], Loss = 0.5012\n",
      "Iteration 6645: Weights = [55.30333333  2.40590767  5.76167105  0.09476431  0.18157368 12.98836385], Loss = 0.5011\n",
      "Iteration 6646: Weights = [55.30333333  2.40575222  5.76129877  0.09475819  0.18156195 12.98876488], Loss = 0.5011\n",
      "Iteration 6647: Weights = [55.30333333  2.40559678  5.76092652  0.09475206  0.18155022 12.98916589], Loss = 0.5010\n",
      "Iteration 6648: Weights = [55.30333333  2.40544135  5.7605543   0.09474594  0.18153849 12.98956687], Loss = 0.5009\n",
      "Iteration 6649: Weights = [55.30333333  2.40528593  5.7601821   0.09473982  0.18152676 12.98996783], Loss = 0.5009\n",
      "Iteration 6650: Weights = [55.30333333  2.40513052  5.75980992  0.0947337   0.18151503 12.99036876], Loss = 0.5008\n",
      "Iteration 6651: Weights = [55.30333333  2.40497512  5.75943777  0.09472758  0.1815033  12.99076966], Loss = 0.5008\n",
      "Iteration 6652: Weights = [55.30333333  2.40481972  5.75906564  0.09472146  0.18149157 12.99117054], Loss = 0.5007\n",
      "Iteration 6653: Weights = [55.30333333  2.40466434  5.75869353  0.09471534  0.18147985 12.99157139], Loss = 0.5006\n",
      "Iteration 6654: Weights = [55.30333333  2.40450897  5.75832145  0.09470922  0.18146812 12.99197222], Loss = 0.5006\n",
      "Iteration 6655: Weights = [55.30333333  2.40435361  5.7579494   0.0947031   0.1814564  12.99237302], Loss = 0.5005\n",
      "Iteration 6656: Weights = [55.30333333  2.40419826  5.75757736  0.09469698  0.18144467 12.9927738 ], Loss = 0.5004\n",
      "Iteration 6657: Weights = [55.30333333  2.40404292  5.75720536  0.09469086  0.18143295 12.99317454], Loss = 0.5004\n",
      "Iteration 6658: Weights = [55.30333333  2.40388759  5.75683337  0.09468474  0.18142123 12.99357527], Loss = 0.5003\n",
      "Iteration 6659: Weights = [55.30333333  2.40373227  5.75646141  0.09467863  0.1814095  12.99397596], Loss = 0.5002\n",
      "Iteration 6660: Weights = [55.30333333  2.40357696  5.75608947  0.09467251  0.18139778 12.99437663], Loss = 0.5002\n",
      "Iteration 6661: Weights = [55.30333333  2.40342166  5.75571756  0.09466639  0.18138606 12.99477728], Loss = 0.5001\n",
      "Iteration 6662: Weights = [55.30333333  2.40326637  5.75534567  0.09466027  0.18137434 12.9951779 ], Loss = 0.5000\n",
      "Iteration 6663: Weights = [55.30333333  2.40311109  5.75497381  0.09465416  0.18136262 12.99557849], Loss = 0.5000\n",
      "Iteration 6664: Weights = [55.30333333  2.40295582  5.75460197  0.09464804  0.18135091 12.99597906], Loss = 0.4999\n",
      "Iteration 6665: Weights = [55.30333333  2.40280056  5.75423015  0.09464193  0.18133919 12.9963796 ], Loss = 0.4998\n",
      "Iteration 6666: Weights = [55.30333333  2.40264531  5.75385836  0.09463581  0.18132747 12.99678012], Loss = 0.4998\n",
      "Iteration 6667: Weights = [55.30333333  2.40249008  5.75348659  0.0946297   0.18131576 12.99718061], Loss = 0.4997\n",
      "Iteration 6668: Weights = [55.30333333  2.40233485  5.75311485  0.09462358  0.18130404 12.99758107], Loss = 0.4997\n",
      "Iteration 6669: Weights = [55.30333333  2.40217963  5.75274313  0.09461747  0.18129233 12.99798151], Loss = 0.4996\n",
      "Iteration 6670: Weights = [55.30333333  2.40202442  5.75237143  0.09461136  0.18128061 12.99838192], Loss = 0.4995\n",
      "Iteration 6671: Weights = [55.30333333  2.40186922  5.75199976  0.09460524  0.1812689  12.99878231], Loss = 0.4995\n",
      "Iteration 6672: Weights = [55.30333333  2.40171403  5.75162811  0.09459913  0.18125719 12.99918267], Loss = 0.4994\n",
      "Iteration 6673: Weights = [55.30333333  2.40155885  5.75125649  0.09459302  0.18124548 12.999583  ], Loss = 0.4993\n",
      "Iteration 6674: Weights = [55.30333333  2.40140368  5.75088489  0.09458691  0.18123377 12.99998331], Loss = 0.4993\n",
      "Iteration 6675: Weights = [55.30333333  2.40124852  5.75051331  0.09458079  0.18122206 13.00038359], Loss = 0.4992\n",
      "Iteration 6676: Weights = [55.30333333  2.40109337  5.75014176  0.09457468  0.18121035 13.00078385], Loss = 0.4991\n",
      "Iteration 6677: Weights = [55.30333333  2.40093823  5.74977023  0.09456857  0.18119864 13.00118408], Loss = 0.4991\n",
      "Iteration 6678: Weights = [55.30333333  2.4007831   5.74939872  0.09456246  0.18118693 13.00158429], Loss = 0.4990\n",
      "Iteration 6679: Weights = [55.30333333  2.40062798  5.74902724  0.09455635  0.18117522 13.00198447], Loss = 0.4989\n",
      "Iteration 6680: Weights = [55.30333333  2.40047287  5.74865579  0.09455024  0.18116352 13.00238462], Loss = 0.4989\n",
      "Iteration 6681: Weights = [55.30333333  2.40031777  5.74828436  0.09454413  0.18115181 13.00278475], Loss = 0.4988\n",
      "Iteration 6682: Weights = [55.30333333  2.40016268  5.74791295  0.09453803  0.18114011 13.00318485], Loss = 0.4988\n",
      "Iteration 6683: Weights = [55.30333333  2.4000076   5.74754156  0.09453192  0.1811284  13.00358493], Loss = 0.4987\n",
      "Iteration 6684: Weights = [55.30333333  2.39985253  5.7471702   0.09452581  0.1811167  13.00398498], Loss = 0.4986\n",
      "Iteration 6685: Weights = [55.30333333  2.39969747  5.74679887  0.0945197   0.181105   13.004385  ], Loss = 0.4986\n",
      "Iteration 6686: Weights = [55.30333333  2.39954243  5.74642755  0.09451359  0.1810933  13.004785  ], Loss = 0.4985\n",
      "Iteration 6687: Weights = [55.30333333  2.39938739  5.74605627  0.09450749  0.1810816  13.00518497], Loss = 0.4984\n",
      "Iteration 6688: Weights = [55.30333333  2.39923236  5.745685    0.09450138  0.1810699  13.00558492], Loss = 0.4984\n",
      "Iteration 6689: Weights = [55.30333333  2.39907734  5.74531376  0.09449528  0.1810582  13.00598484], Loss = 0.4983\n",
      "Iteration 6690: Weights = [55.30333333  2.39892233  5.74494255  0.09448917  0.1810465  13.00638474], Loss = 0.4982\n",
      "Iteration 6691: Weights = [55.30333333  2.39876733  5.74457135  0.09448307  0.1810348  13.0067846 ], Loss = 0.4982\n",
      "Iteration 6692: Weights = [55.30333333  2.39861234  5.74420019  0.09447696  0.1810231  13.00718445], Loss = 0.4981\n",
      "Iteration 6693: Weights = [55.30333333  2.39845736  5.74382904  0.09447086  0.18101141 13.00758427], Loss = 0.4980\n",
      "Iteration 6694: Weights = [55.30333333  2.39830239  5.74345792  0.09446475  0.18099971 13.00798406], Loss = 0.4980\n",
      "Iteration 6695: Weights = [55.30333333  2.39814743  5.74308682  0.09445865  0.18098802 13.00838382], Loss = 0.4979\n",
      "Iteration 6696: Weights = [55.30333333  2.39799248  5.74271575  0.09445255  0.18097632 13.00878356], Loss = 0.4978\n",
      "Iteration 6697: Weights = [55.30333333  2.39783754  5.7423447   0.09444644  0.18096463 13.00918328], Loss = 0.4978\n",
      "Iteration 6698: Weights = [55.30333333  2.39768261  5.74197368  0.09444034  0.18095294 13.00958297], Loss = 0.4977\n",
      "Iteration 6699: Weights = [55.30333333  2.3975277   5.74160268  0.09443424  0.18094125 13.00998263], Loss = 0.4977\n",
      "Iteration 6700: Weights = [55.30333333  2.39737279  5.7412317   0.09442814  0.18092955 13.01038227], Loss = 0.4976\n",
      "Iteration 6701: Weights = [55.30333333  2.39721789  5.74086075  0.09442204  0.18091786 13.01078188], Loss = 0.4975\n",
      "Iteration 6702: Weights = [55.30333333  2.397063    5.74048982  0.09441593  0.18090617 13.01118146], Loss = 0.4975\n",
      "Iteration 6703: Weights = [55.30333333  2.39690812  5.74011892  0.09440983  0.18089449 13.01158102], Loss = 0.4974\n",
      "Iteration 6704: Weights = [55.30333333  2.39675325  5.73974804  0.09440373  0.1808828  13.01198055], Loss = 0.4973\n",
      "Iteration 6705: Weights = [55.30333333  2.39659839  5.73937718  0.09439763  0.18087111 13.01238006], Loss = 0.4973\n",
      "Iteration 6706: Weights = [55.30333333  2.39644354  5.73900635  0.09439154  0.18085942 13.01277954], Loss = 0.4972\n",
      "Iteration 6707: Weights = [55.30333333  2.3962887   5.73863554  0.09438544  0.18084774 13.013179  ], Loss = 0.4971\n",
      "Iteration 6708: Weights = [55.30333333  2.39613387  5.73826475  0.09437934  0.18083605 13.01357843], Loss = 0.4971\n",
      "Iteration 6709: Weights = [55.30333333  2.39597906  5.73789399  0.09437324  0.18082437 13.01397784], Loss = 0.4970\n",
      "Iteration 6710: Weights = [55.30333333  2.39582425  5.73752326  0.09436714  0.18081269 13.01437721], Loss = 0.4969\n",
      "Iteration 6711: Weights = [55.30333333  2.39566945  5.73715254  0.09436105  0.180801   13.01477657], Loss = 0.4969\n",
      "Iteration 6712: Weights = [55.30333333  2.39551466  5.73678185  0.09435495  0.18078932 13.01517589], Loss = 0.4968\n",
      "Iteration 6713: Weights = [55.30333333  2.39535988  5.73641119  0.09434885  0.18077764 13.0155752 ], Loss = 0.4968\n",
      "Iteration 6714: Weights = [55.30333333  2.39520511  5.73604055  0.09434276  0.18076596 13.01597447], Loss = 0.4967\n",
      "Iteration 6715: Weights = [55.30333333  2.39505035  5.73566993  0.09433666  0.18075428 13.01637372], Loss = 0.4966\n",
      "Iteration 6716: Weights = [55.30333333  2.3948956   5.73529934  0.09433056  0.1807426  13.01677294], Loss = 0.4966\n",
      "Iteration 6717: Weights = [55.30333333  2.39474086  5.73492877  0.09432447  0.18073092 13.01717214], Loss = 0.4965\n",
      "Iteration 6718: Weights = [55.30333333  2.39458613  5.73455822  0.09431838  0.18071925 13.01757131], Loss = 0.4964\n",
      "Iteration 6719: Weights = [55.30333333  2.39443142  5.7341877   0.09431228  0.18070757 13.01797046], Loss = 0.4964\n",
      "Iteration 6720: Weights = [55.30333333  2.39427671  5.7338172   0.09430619  0.18069589 13.01836958], Loss = 0.4963\n",
      "Iteration 6721: Weights = [55.30333333  2.39412201  5.73344673  0.09430009  0.18068422 13.01876868], Loss = 0.4962\n",
      "Iteration 6722: Weights = [55.30333333  2.39396732  5.73307628  0.094294    0.18067254 13.01916775], Loss = 0.4962\n",
      "Iteration 6723: Weights = [55.30333333  2.39381264  5.73270586  0.09428791  0.18066087 13.01956679], Loss = 0.4961\n",
      "Iteration 6724: Weights = [55.30333333  2.39365797  5.73233545  0.09428182  0.1806492  13.01996581], Loss = 0.4961\n",
      "Iteration 6725: Weights = [55.30333333  2.39350331  5.73196508  0.09427572  0.18063753 13.0203648 ], Loss = 0.4960\n",
      "Iteration 6726: Weights = [55.30333333  2.39334866  5.73159472  0.09426963  0.18062585 13.02076376], Loss = 0.4959\n",
      "Iteration 6727: Weights = [55.30333333  2.39319402  5.73122439  0.09426354  0.18061418 13.0211627 ], Loss = 0.4959\n",
      "Iteration 6728: Weights = [55.30333333  2.39303939  5.73085409  0.09425745  0.18060251 13.02156162], Loss = 0.4958\n",
      "Iteration 6729: Weights = [55.30333333  2.39288477  5.7304838   0.09425136  0.18059084 13.02196051], Loss = 0.4957\n",
      "Iteration 6730: Weights = [55.30333333  2.39273017  5.73011355  0.09424527  0.18057918 13.02235937], Loss = 0.4957\n",
      "Iteration 6731: Weights = [55.30333333  2.39257557  5.72974331  0.09423918  0.18056751 13.02275821], Loss = 0.4956\n",
      "Iteration 6732: Weights = [55.30333333  2.39242098  5.7293731   0.09423309  0.18055584 13.02315702], Loss = 0.4955\n",
      "Iteration 6733: Weights = [55.30333333  2.3922664   5.72900292  0.09422701  0.18054418 13.0235558 ], Loss = 0.4955\n",
      "Iteration 6734: Weights = [55.30333333  2.39211183  5.72863275  0.09422092  0.18053251 13.02395456], Loss = 0.4954\n",
      "Iteration 6735: Weights = [55.30333333  2.39195727  5.72826261  0.09421483  0.18052085 13.0243533 ], Loss = 0.4953\n",
      "Iteration 6736: Weights = [55.30333333  2.39180272  5.7278925   0.09420874  0.18050918 13.02475201], Loss = 0.4953\n",
      "Iteration 6737: Weights = [55.30333333  2.39164818  5.72752241  0.09420265  0.18049752 13.02515069], Loss = 0.4952\n",
      "Iteration 6738: Weights = [55.30333333  2.39149365  5.72715234  0.09419657  0.18048586 13.02554935], Loss = 0.4952\n",
      "Iteration 6739: Weights = [55.30333333  2.39133913  5.7267823   0.09419048  0.18047419 13.02594798], Loss = 0.4951\n",
      "Iteration 6740: Weights = [55.30333333  2.39118462  5.72641228  0.0941844   0.18046253 13.02634658], Loss = 0.4950\n",
      "Iteration 6741: Weights = [55.30333333  2.39103012  5.72604229  0.09417831  0.18045087 13.02674516], Loss = 0.4950\n",
      "Iteration 6742: Weights = [55.30333333  2.39087564  5.72567231  0.09417223  0.18043921 13.02714372], Loss = 0.4949\n",
      "Iteration 6743: Weights = [55.30333333  2.39072116  5.72530237  0.09416614  0.18042756 13.02754224], Loss = 0.4948\n",
      "Iteration 6744: Weights = [55.30333333  2.39056669  5.72493244  0.09416006  0.1804159  13.02794075], Loss = 0.4948\n",
      "Iteration 6745: Weights = [55.30333333  2.39041223  5.72456255  0.09415397  0.18040424 13.02833922], Loss = 0.4947\n",
      "Iteration 6746: Weights = [55.30333333  2.39025778  5.72419267  0.09414789  0.18039259 13.02873767], Loss = 0.4946\n",
      "Iteration 6747: Weights = [55.30333333  2.39010334  5.72382282  0.09414181  0.18038093 13.0291361 ], Loss = 0.4946\n",
      "Iteration 6748: Weights = [55.30333333  2.38994891  5.72345299  0.09413572  0.18036927 13.0295345 ], Loss = 0.4945\n",
      "Iteration 6749: Weights = [55.30333333  2.38979449  5.72308319  0.09412964  0.18035762 13.02993287], Loss = 0.4945\n",
      "Iteration 6750: Weights = [55.30333333  2.38964008  5.72271341  0.09412356  0.18034597 13.03033122], Loss = 0.4944\n",
      "Iteration 6751: Weights = [55.30333333  2.38948568  5.72234365  0.09411748  0.18033432 13.03072954], Loss = 0.4943\n",
      "Iteration 6752: Weights = [55.30333333  2.38933129  5.72197392  0.0941114   0.18032266 13.03112784], Loss = 0.4943\n",
      "Iteration 6753: Weights = [55.30333333  2.38917691  5.72160421  0.09410532  0.18031101 13.03152611], Loss = 0.4942\n",
      "Iteration 6754: Weights = [55.30333333  2.38902254  5.72123453  0.09409924  0.18029936 13.03192435], Loss = 0.4941\n",
      "Iteration 6755: Weights = [55.30333333  2.38886818  5.72086487  0.09409316  0.18028771 13.03232257], Loss = 0.4941\n",
      "Iteration 6756: Weights = [55.30333333  2.38871383  5.72049523  0.09408708  0.18027606 13.03272077], Loss = 0.4940\n",
      "Iteration 6757: Weights = [55.30333333  2.38855949  5.72012562  0.094081    0.18026442 13.03311893], Loss = 0.4939\n",
      "Iteration 6758: Weights = [55.30333333  2.38840516  5.71975603  0.09407492  0.18025277 13.03351707], Loss = 0.4939\n",
      "Iteration 6759: Weights = [55.30333333  2.38825084  5.71938646  0.09406884  0.18024112 13.03391519], Loss = 0.4938\n",
      "Iteration 6760: Weights = [55.30333333  2.38809654  5.71901692  0.09406276  0.18022948 13.03431328], Loss = 0.4937\n",
      "Iteration 6761: Weights = [55.30333333  2.38794224  5.7186474   0.09405668  0.18021783 13.03471135], Loss = 0.4937\n",
      "Iteration 6762: Weights = [55.30333333  2.38778795  5.71827791  0.09405061  0.18020619 13.03510939], Loss = 0.4936\n",
      "Iteration 6763: Weights = [55.30333333  2.38763367  5.71790844  0.09404453  0.18019454 13.0355074 ], Loss = 0.4936\n",
      "Iteration 6764: Weights = [55.30333333  2.3874794   5.717539    0.09403845  0.1801829  13.03590539], Loss = 0.4935\n",
      "Iteration 6765: Weights = [55.30333333  2.38732514  5.71716957  0.09403238  0.18017126 13.03630335], Loss = 0.4934\n",
      "Iteration 6766: Weights = [55.30333333  2.38717089  5.71680018  0.0940263   0.18015962 13.03670128], Loss = 0.4934\n",
      "Iteration 6767: Weights = [55.30333333  2.38701665  5.7164308   0.09402023  0.18014798 13.03709919], Loss = 0.4933\n",
      "Iteration 6768: Weights = [55.30333333  2.38686242  5.71606145  0.09401415  0.18013634 13.03749708], Loss = 0.4932\n",
      "Iteration 6769: Weights = [55.30333333  2.3867082   5.71569213  0.09400808  0.1801247  13.03789494], Loss = 0.4932\n",
      "Iteration 6770: Weights = [55.30333333  2.38655399  5.71532282  0.094002    0.18011306 13.03829277], Loss = 0.4931\n",
      "Iteration 6771: Weights = [55.30333333  2.38639979  5.71495355  0.09399593  0.18010142 13.03869058], Loss = 0.4930\n",
      "Iteration 6772: Weights = [55.30333333  2.3862456   5.71458429  0.09398986  0.18008979 13.03908836], Loss = 0.4930\n",
      "Iteration 6773: Weights = [55.30333333  2.38609142  5.71421506  0.09398378  0.18007815 13.03948612], Loss = 0.4929\n",
      "Iteration 6774: Weights = [55.30333333  2.38593725  5.71384585  0.09397771  0.18006651 13.03988385], Loss = 0.4929\n",
      "Iteration 6775: Weights = [55.30333333  2.38578309  5.71347667  0.09397164  0.18005488 13.04028155], Loss = 0.4928\n",
      "Iteration 6776: Weights = [55.30333333  2.38562894  5.71310751  0.09396557  0.18004325 13.04067923], Loss = 0.4927\n",
      "Iteration 6777: Weights = [55.30333333  2.3854748   5.71273838  0.0939595   0.18003161 13.04107689], Loss = 0.4927\n",
      "Iteration 6778: Weights = [55.30333333  2.38532067  5.71236926  0.09395343  0.18001998 13.04147451], Loss = 0.4926\n",
      "Iteration 6779: Weights = [55.30333333  2.38516654  5.71200018  0.09394735  0.18000835 13.04187212], Loss = 0.4925\n",
      "Iteration 6780: Weights = [55.30333333  2.38501243  5.71163111  0.09394128  0.17999672 13.04226969], Loss = 0.4925\n",
      "Iteration 6781: Weights = [55.30333333  2.38485833  5.71126207  0.09393522  0.17998509 13.04266724], Loss = 0.4924\n",
      "Iteration 6782: Weights = [55.30333333  2.38470424  5.71089306  0.09392915  0.17997346 13.04306477], Loss = 0.4923\n",
      "Iteration 6783: Weights = [55.30333333  2.38455016  5.71052406  0.09392308  0.17996183 13.04346227], Loss = 0.4923\n",
      "Iteration 6784: Weights = [55.30333333  2.38439609  5.7101551   0.09391701  0.1799502  13.04385974], Loss = 0.4922\n",
      "Iteration 6785: Weights = [55.30333333  2.38424203  5.70978615  0.09391094  0.17993858 13.04425719], Loss = 0.4922\n",
      "Iteration 6786: Weights = [55.30333333  2.38408798  5.70941723  0.09390487  0.17992695 13.04465461], Loss = 0.4921\n",
      "Iteration 6787: Weights = [55.30333333  2.38393394  5.70904833  0.0938988   0.17991533 13.04505201], Loss = 0.4920\n",
      "Iteration 6788: Weights = [55.30333333  2.38377991  5.70867946  0.09389274  0.1799037  13.04544938], Loss = 0.4920\n",
      "Iteration 6789: Weights = [55.30333333  2.38362589  5.70831061  0.09388667  0.17989208 13.04584672], Loss = 0.4919\n",
      "Iteration 6790: Weights = [55.30333333  2.38347188  5.70794179  0.09388061  0.17988045 13.04624404], Loss = 0.4918\n",
      "Iteration 6791: Weights = [55.30333333  2.38331788  5.70757298  0.09387454  0.17986883 13.04664134], Loss = 0.4918\n",
      "Iteration 6792: Weights = [55.30333333  2.38316389  5.70720421  0.09386847  0.17985721 13.0470386 ], Loss = 0.4917\n",
      "Iteration 6793: Weights = [55.30333333  2.3830099   5.70683545  0.09386241  0.17984559 13.04743585], Loss = 0.4916\n",
      "Iteration 6794: Weights = [55.30333333  2.38285593  5.70646672  0.09385634  0.17983397 13.04783306], Loss = 0.4916\n",
      "Iteration 6795: Weights = [55.30333333  2.38270197  5.70609802  0.09385028  0.17982235 13.04823026], Loss = 0.4915\n",
      "Iteration 6796: Weights = [55.30333333  2.38254802  5.70572933  0.09384422  0.17981073 13.04862742], Loss = 0.4915\n",
      "Iteration 6797: Weights = [55.30333333  2.38239408  5.70536067  0.09383815  0.17979911 13.04902456], Loss = 0.4914\n",
      "Iteration 6798: Weights = [55.30333333  2.38224015  5.70499204  0.09383209  0.1797875  13.04942168], Loss = 0.4913\n",
      "Iteration 6799: Weights = [55.30333333  2.38208623  5.70462343  0.09382603  0.17977588 13.04981876], Loss = 0.4913\n",
      "Iteration 6800: Weights = [55.30333333  2.38193232  5.70425484  0.09381996  0.17976426 13.05021583], Loss = 0.4912\n",
      "Iteration 6801: Weights = [55.30333333  2.38177841  5.70388628  0.0938139   0.17975265 13.05061286], Loss = 0.4911\n",
      "Iteration 6802: Weights = [55.30333333  2.38162452  5.70351774  0.09380784  0.17974103 13.05100988], Loss = 0.4911\n",
      "Iteration 6803: Weights = [55.30333333  2.38147064  5.70314922  0.09380178  0.17972942 13.05140686], Loss = 0.4910\n",
      "Iteration 6804: Weights = [55.30333333  2.38131677  5.70278073  0.09379572  0.17971781 13.05180382], Loss = 0.4909\n",
      "Iteration 6805: Weights = [55.30333333  2.38116291  5.70241226  0.09378966  0.1797062  13.05220076], Loss = 0.4909\n",
      "Iteration 6806: Weights = [55.30333333  2.38100906  5.70204382  0.0937836   0.17969458 13.05259767], Loss = 0.4908\n",
      "Iteration 6807: Weights = [55.30333333  2.38085522  5.7016754   0.09377754  0.17968297 13.05299455], Loss = 0.4908\n",
      "Iteration 6808: Weights = [55.30333333  2.38070138  5.701307    0.09377148  0.17967136 13.05339141], Loss = 0.4907\n",
      "Iteration 6809: Weights = [55.30333333  2.38054756  5.70093863  0.09376542  0.17965976 13.05378824], Loss = 0.4906\n",
      "Iteration 6810: Weights = [55.30333333  2.38039375  5.70057028  0.09375936  0.17964815 13.05418505], Loss = 0.4906\n",
      "Iteration 6811: Weights = [55.30333333  2.38023995  5.70020195  0.09375331  0.17963654 13.05458183], Loss = 0.4905\n",
      "Iteration 6812: Weights = [55.30333333  2.38008616  5.69983365  0.09374725  0.17962493 13.05497858], Loss = 0.4904\n",
      "Iteration 6813: Weights = [55.30333333  2.37993237  5.69946538  0.09374119  0.17961333 13.05537531], Loss = 0.4904\n",
      "Iteration 6814: Weights = [55.30333333  2.3797786   5.69909712  0.09373513  0.17960172 13.05577201], Loss = 0.4903\n",
      "Iteration 6815: Weights = [55.30333333  2.37962484  5.69872889  0.09372908  0.17959012 13.05616869], Loss = 0.4903\n",
      "Iteration 6816: Weights = [55.30333333  2.37947109  5.69836068  0.09372302  0.17957851 13.05656535], Loss = 0.4902\n",
      "Iteration 6817: Weights = [55.30333333  2.37931734  5.6979925   0.09371697  0.17956691 13.05696197], Loss = 0.4901\n",
      "Iteration 6818: Weights = [55.30333333  2.37916361  5.69762434  0.09371091  0.17955531 13.05735857], Loss = 0.4901\n",
      "Iteration 6819: Weights = [55.30333333  2.37900989  5.69725621  0.09370486  0.17954371 13.05775515], Loss = 0.4900\n",
      "Iteration 6820: Weights = [55.30333333  2.37885618  5.6968881   0.0936988   0.17953211 13.0581517 ], Loss = 0.4899\n",
      "Iteration 6821: Weights = [55.30333333  2.37870247  5.69652001  0.09369275  0.17952051 13.05854822], Loss = 0.4899\n",
      "Iteration 6822: Weights = [55.30333333  2.37854878  5.69615195  0.09368669  0.17950891 13.05894472], Loss = 0.4898\n",
      "Iteration 6823: Weights = [55.30333333  2.3783951   5.69578391  0.09368064  0.17949731 13.0593412 ], Loss = 0.4897\n",
      "Iteration 6824: Weights = [55.30333333  2.37824143  5.69541589  0.09367459  0.17948571 13.05973764], Loss = 0.4897\n",
      "Iteration 6825: Weights = [55.30333333  2.37808776  5.6950479   0.09366853  0.17947411 13.06013407], Loss = 0.4896\n",
      "Iteration 6826: Weights = [55.30333333  2.37793411  5.69467993  0.09366248  0.17946252 13.06053046], Loss = 0.4896\n",
      "Iteration 6827: Weights = [55.30333333  2.37778047  5.69431198  0.09365643  0.17945092 13.06092683], Loss = 0.4895\n",
      "Iteration 6828: Weights = [55.30333333  2.37762683  5.69394406  0.09365038  0.17943933 13.06132318], Loss = 0.4894\n",
      "Iteration 6829: Weights = [55.30333333  2.37747321  5.69357617  0.09364433  0.17942773 13.0617195 ], Loss = 0.4894\n",
      "Iteration 6830: Weights = [55.30333333  2.3773196   5.69320829  0.09363828  0.17941614 13.06211579], Loss = 0.4893\n",
      "Iteration 6831: Weights = [55.30333333  2.37716599  5.69284044  0.09363223  0.17940455 13.06251206], Loss = 0.4892\n",
      "Iteration 6832: Weights = [55.30333333  2.3770124   5.69247262  0.09362618  0.17939296 13.0629083 ], Loss = 0.4892\n",
      "Iteration 6833: Weights = [55.30333333  2.37685882  5.69210482  0.09362013  0.17938137 13.06330452], Loss = 0.4891\n",
      "Iteration 6834: Weights = [55.30333333  2.37670524  5.69173704  0.09361408  0.17936978 13.06370071], Loss = 0.4890\n",
      "Iteration 6835: Weights = [55.30333333  2.37655168  5.69136928  0.09360803  0.17935819 13.06409688], Loss = 0.4890\n",
      "Iteration 6836: Weights = [55.30333333  2.37639812  5.69100155  0.09360198  0.1793466  13.06449302], Loss = 0.4889\n",
      "Iteration 6837: Weights = [55.30333333  2.37624458  5.69063385  0.09359594  0.17933501 13.06488913], Loss = 0.4889\n",
      "Iteration 6838: Weights = [55.30333333  2.37609105  5.69026616  0.09358989  0.17932342 13.06528522], Loss = 0.4888\n",
      "Iteration 6839: Weights = [55.30333333  2.37593752  5.6898985   0.09358384  0.17931184 13.06568129], Loss = 0.4887\n",
      "Iteration 6840: Weights = [55.30333333  2.37578401  5.68953087  0.09357779  0.17930025 13.06607732], Loss = 0.4887\n",
      "Iteration 6841: Weights = [55.30333333  2.3756305   5.68916325  0.09357175  0.17928867 13.06647334], Loss = 0.4886\n",
      "Iteration 6842: Weights = [55.30333333  2.37547701  5.68879567  0.0935657   0.17927708 13.06686932], Loss = 0.4885\n",
      "Iteration 6843: Weights = [55.30333333  2.37532353  5.6884281   0.09355966  0.1792655  13.06726528], Loss = 0.4885\n",
      "Iteration 6844: Weights = [55.30333333  2.37517005  5.68806056  0.09355361  0.17925392 13.06766122], Loss = 0.4884\n",
      "Iteration 6845: Weights = [55.30333333  2.37501659  5.68769304  0.09354757  0.17924233 13.06805713], Loss = 0.4884\n",
      "Iteration 6846: Weights = [55.30333333  2.37486313  5.68732555  0.09354152  0.17923075 13.06845301], Loss = 0.4883\n",
      "Iteration 6847: Weights = [55.30333333  2.37470969  5.68695808  0.09353548  0.17921917 13.06884887], Loss = 0.4882\n",
      "Iteration 6848: Weights = [55.30333333  2.37455625  5.68659064  0.09352943  0.17920759 13.06924471], Loss = 0.4882\n",
      "Iteration 6849: Weights = [55.30333333  2.37440283  5.68622321  0.09352339  0.17919601 13.06964051], Loss = 0.4881\n",
      "Iteration 6850: Weights = [55.30333333  2.37424941  5.68585581  0.09351735  0.17918443 13.0700363 ], Loss = 0.4880\n",
      "Iteration 6851: Weights = [55.30333333  2.37409601  5.68548844  0.09351131  0.17917286 13.07043205], Loss = 0.4880\n",
      "Iteration 6852: Weights = [55.30333333  2.37394261  5.68512109  0.09350526  0.17916128 13.07082778], Loss = 0.4879\n",
      "Iteration 6853: Weights = [55.30333333  2.37378923  5.68475376  0.09349922  0.1791497  13.07122349], Loss = 0.4878\n",
      "Iteration 6854: Weights = [55.30333333  2.37363585  5.68438646  0.09349318  0.17913813 13.07161917], Loss = 0.4878\n",
      "Iteration 6855: Weights = [55.30333333  2.37348249  5.68401918  0.09348714  0.17912656 13.07201482], Loss = 0.4877\n",
      "Iteration 6856: Weights = [55.30333333  2.37332913  5.68365192  0.0934811   0.17911498 13.07241045], Loss = 0.4877\n",
      "Iteration 6857: Weights = [55.30333333  2.37317579  5.68328469  0.09347506  0.17910341 13.07280606], Loss = 0.4876\n",
      "Iteration 6858: Weights = [55.30333333  2.37302245  5.68291748  0.09346902  0.17909184 13.07320163], Loss = 0.4875\n",
      "Iteration 6859: Weights = [55.30333333  2.37286912  5.6825503   0.09346298  0.17908026 13.07359719], Loss = 0.4875\n",
      "Iteration 6860: Weights = [55.30333333  2.37271581  5.68218314  0.09345694  0.17906869 13.07399271], Loss = 0.4874\n",
      "Iteration 6861: Weights = [55.30333333  2.3725625   5.681816    0.0934509   0.17905712 13.07438821], Loss = 0.4873\n",
      "Iteration 6862: Weights = [55.30333333  2.37240921  5.68144889  0.09344487  0.17904555 13.07478369], Loss = 0.4873\n",
      "Iteration 6863: Weights = [55.30333333  2.37225592  5.6810818   0.09343883  0.17903399 13.07517914], Loss = 0.4872\n",
      "Iteration 6864: Weights = [55.30333333  2.37210264  5.68071473  0.09343279  0.17902242 13.07557456], Loss = 0.4872\n",
      "Iteration 6865: Weights = [55.30333333  2.37194938  5.68034769  0.09342675  0.17901085 13.07596996], Loss = 0.4871\n",
      "Iteration 6866: Weights = [55.30333333  2.37179612  5.67998067  0.09342072  0.17899929 13.07636534], Loss = 0.4870\n",
      "Iteration 6867: Weights = [55.30333333  2.37164288  5.67961367  0.09341468  0.17898772 13.07676068], Loss = 0.4870\n",
      "Iteration 6868: Weights = [55.30333333  2.37148964  5.6792467   0.09340865  0.17897615 13.07715601], Loss = 0.4869\n",
      "Iteration 6869: Weights = [55.30333333  2.37133641  5.67887976  0.09340261  0.17896459 13.0775513 ], Loss = 0.4868\n",
      "Iteration 6870: Weights = [55.30333333  2.3711832   5.67851283  0.09339658  0.17895303 13.07794657], Loss = 0.4868\n",
      "Iteration 6871: Weights = [55.30333333  2.37102999  5.67814593  0.09339054  0.17894147 13.07834182], Loss = 0.4867\n",
      "Iteration 6872: Weights = [55.30333333  2.37087679  5.67777906  0.09338451  0.1789299  13.07873704], Loss = 0.4867\n",
      "Iteration 6873: Weights = [55.30333333  2.3707236   5.6774122   0.09337847  0.17891834 13.07913223], Loss = 0.4866\n",
      "Iteration 6874: Weights = [55.30333333  2.37057043  5.67704537  0.09337244  0.17890678 13.0795274 ], Loss = 0.4865\n",
      "Iteration 6875: Weights = [55.30333333  2.37041726  5.67667857  0.09336641  0.17889522 13.07992255], Loss = 0.4865\n",
      "Iteration 6876: Weights = [55.30333333  2.3702641   5.67631179  0.09336037  0.17888366 13.08031766], Loss = 0.4864\n",
      "Iteration 6877: Weights = [55.30333333  2.37011095  5.67594503  0.09335434  0.17887211 13.08071276], Loss = 0.4863\n",
      "Iteration 6878: Weights = [55.30333333  2.36995782  5.67557829  0.09334831  0.17886055 13.08110782], Loss = 0.4863\n",
      "Iteration 6879: Weights = [55.30333333  2.36980469  5.67521158  0.09334228  0.17884899 13.08150286], Loss = 0.4862\n",
      "Iteration 6880: Weights = [55.30333333  2.36965157  5.6748449   0.09333625  0.17883744 13.08189788], Loss = 0.4862\n",
      "Iteration 6881: Weights = [55.30333333  2.36949846  5.67447823  0.09333022  0.17882588 13.08229287], Loss = 0.4861\n",
      "Iteration 6882: Weights = [55.30333333  2.36934537  5.67411159  0.09332419  0.17881433 13.08268783], Loss = 0.4860\n",
      "Iteration 6883: Weights = [55.30333333  2.36919228  5.67374498  0.09331816  0.17880277 13.08308277], Loss = 0.4860\n",
      "Iteration 6884: Weights = [55.30333333  2.3690392   5.67337839  0.09331213  0.17879122 13.08347769], Loss = 0.4859\n",
      "Iteration 6885: Weights = [55.30333333  2.36888613  5.67301182  0.0933061   0.17877967 13.08387258], Loss = 0.4858\n",
      "Iteration 6886: Weights = [55.30333333  2.36873307  5.67264527  0.09330007  0.17876812 13.08426744], Loss = 0.4858\n",
      "Iteration 6887: Weights = [55.30333333  2.36858002  5.67227875  0.09329404  0.17875657 13.08466227], Loss = 0.4857\n",
      "Iteration 6888: Weights = [55.30333333  2.36842698  5.67191226  0.09328801  0.17874502 13.08505709], Loss = 0.4856\n",
      "Iteration 6889: Weights = [55.30333333  2.36827396  5.67154578  0.09328199  0.17873347 13.08545187], Loss = 0.4856\n",
      "Iteration 6890: Weights = [55.30333333  2.36812094  5.67117933  0.09327596  0.17872192 13.08584663], Loss = 0.4855\n",
      "Iteration 6891: Weights = [55.30333333  2.36796793  5.67081291  0.09326993  0.17871037 13.08624137], Loss = 0.4855\n",
      "Iteration 6892: Weights = [55.30333333  2.36781493  5.6704465   0.09326391  0.17869882 13.08663608], Loss = 0.4854\n",
      "Iteration 6893: Weights = [55.30333333  2.36766194  5.67008012  0.09325788  0.17868728 13.08703076], Loss = 0.4853\n",
      "Iteration 6894: Weights = [55.30333333  2.36750896  5.66971377  0.09325185  0.17867573 13.08742542], Loss = 0.4853\n",
      "Iteration 6895: Weights = [55.30333333  2.36735599  5.66934744  0.09324583  0.17866419 13.08782005], Loss = 0.4852\n",
      "Iteration 6896: Weights = [55.30333333  2.36720303  5.66898113  0.0932398   0.17865264 13.08821466], Loss = 0.4851\n",
      "Iteration 6897: Weights = [55.30333333  2.36705008  5.66861484  0.09323378  0.1786411  13.08860924], Loss = 0.4851\n",
      "Iteration 6898: Weights = [55.30333333  2.36689714  5.66824858  0.09322776  0.17862956 13.0890038 ], Loss = 0.4850\n",
      "Iteration 6899: Weights = [55.30333333  2.36674421  5.66788235  0.09322173  0.17861802 13.08939833], Loss = 0.4850\n",
      "Iteration 6900: Weights = [55.30333333  2.36659129  5.66751613  0.09321571  0.17860648 13.08979284], Loss = 0.4849\n",
      "Iteration 6901: Weights = [55.30333333  2.36643838  5.66714994  0.09320969  0.17859494 13.09018732], Loss = 0.4848\n",
      "Iteration 6902: Weights = [55.30333333  2.36628548  5.66678378  0.09320366  0.1785834  13.09058177], Loss = 0.4848\n",
      "Iteration 6903: Weights = [55.30333333  2.36613259  5.66641764  0.09319764  0.17857186 13.0909762 ], Loss = 0.4847\n",
      "Iteration 6904: Weights = [55.30333333  2.36597971  5.66605152  0.09319162  0.17856032 13.0913706 ], Loss = 0.4846\n",
      "Iteration 6905: Weights = [55.30333333  2.36582684  5.66568542  0.0931856   0.17854878 13.09176498], Loss = 0.4846\n",
      "Iteration 6906: Weights = [55.30333333  2.36567398  5.66531935  0.09317958  0.17853725 13.09215934], Loss = 0.4845\n",
      "Iteration 6907: Weights = [55.30333333  2.36552113  5.6649533   0.09317356  0.17852571 13.09255366], Loss = 0.4845\n",
      "Iteration 6908: Weights = [55.30333333  2.36536829  5.66458728  0.09316754  0.17851418 13.09294796], Loss = 0.4844\n",
      "Iteration 6909: Weights = [55.30333333  2.36521546  5.66422128  0.09316152  0.17850264 13.09334224], Loss = 0.4843\n",
      "Iteration 6910: Weights = [55.30333333  2.36506263  5.6638553   0.0931555   0.17849111 13.09373649], Loss = 0.4843\n",
      "Iteration 6911: Weights = [55.30333333  2.36490982  5.66348935  0.09314948  0.17847958 13.09413072], Loss = 0.4842\n",
      "Iteration 6912: Weights = [55.30333333  2.36475702  5.66312342  0.09314346  0.17846804 13.09452492], Loss = 0.4841\n",
      "Iteration 6913: Weights = [55.30333333  2.36460423  5.66275751  0.09313744  0.17845651 13.09491909], Loss = 0.4841\n",
      "Iteration 6914: Weights = [55.30333333  2.36445145  5.66239163  0.09313143  0.17844498 13.09531324], Loss = 0.4840\n",
      "Iteration 6915: Weights = [55.30333333  2.36429868  5.66202577  0.09312541  0.17843345 13.09570736], Loss = 0.4840\n",
      "Iteration 6916: Weights = [55.30333333  2.36414591  5.66165994  0.09311939  0.17842192 13.09610146], Loss = 0.4839\n",
      "Iteration 6917: Weights = [55.30333333  2.36399316  5.66129413  0.09311337  0.1784104  13.09649553], Loss = 0.4838\n",
      "Iteration 6918: Weights = [55.30333333  2.36384042  5.66092834  0.09310736  0.17839887 13.09688958], Loss = 0.4838\n",
      "Iteration 6919: Weights = [55.30333333  2.36368769  5.66056258  0.09310134  0.17838734 13.0972836 ], Loss = 0.4837\n",
      "Iteration 6920: Weights = [55.30333333  2.36353496  5.66019684  0.09309533  0.17837582 13.0976776 ], Loss = 0.4836\n",
      "Iteration 6921: Weights = [55.30333333  2.36338225  5.65983112  0.09308931  0.17836429 13.09807157], Loss = 0.4836\n",
      "Iteration 6922: Weights = [55.30333333  2.36322955  5.65946543  0.0930833   0.17835277 13.09846551], Loss = 0.4835\n",
      "Iteration 6923: Weights = [55.30333333  2.36307685  5.65909976  0.09307728  0.17834124 13.09885943], Loss = 0.4835\n",
      "Iteration 6924: Weights = [55.30333333  2.36292417  5.65873411  0.09307127  0.17832972 13.09925333], Loss = 0.4834\n",
      "Iteration 6925: Weights = [55.30333333  2.3627715   5.65836849  0.09306525  0.1783182  13.0996472 ], Loss = 0.4833\n",
      "Iteration 6926: Weights = [55.30333333  2.36261884  5.65800289  0.09305924  0.17830668 13.10004104], Loss = 0.4833\n",
      "Iteration 6927: Weights = [55.30333333  2.36246618  5.65763731  0.09305323  0.17829516 13.10043486], Loss = 0.4832\n",
      "Iteration 6928: Weights = [55.30333333  2.36231354  5.65727176  0.09304722  0.17828364 13.10082865], Loss = 0.4831\n",
      "Iteration 6929: Weights = [55.30333333  2.3621609   5.65690624  0.0930412   0.17827212 13.10122242], Loss = 0.4831\n",
      "Iteration 6930: Weights = [55.30333333  2.36200828  5.65654073  0.09303519  0.1782606  13.10161616], Loss = 0.4830\n",
      "Iteration 6931: Weights = [55.30333333  2.36185567  5.65617525  0.09302918  0.17824908 13.10200988], Loss = 0.4830\n",
      "Iteration 6932: Weights = [55.30333333  2.36170306  5.65580979  0.09302317  0.17823756 13.10240357], Loss = 0.4829\n",
      "Iteration 6933: Weights = [55.30333333  2.36155047  5.65544436  0.09301716  0.17822605 13.10279723], Loss = 0.4828\n",
      "Iteration 6934: Weights = [55.30333333  2.36139788  5.65507895  0.09301115  0.17821453 13.10319087], Loss = 0.4828\n",
      "Iteration 6935: Weights = [55.30333333  2.36124531  5.65471357  0.09300514  0.17820302 13.10358449], Loss = 0.4827\n",
      "Iteration 6936: Weights = [55.30333333  2.36109274  5.6543482   0.09299913  0.1781915  13.10397807], Loss = 0.4826\n",
      "Iteration 6937: Weights = [55.30333333  2.36094019  5.65398286  0.09299312  0.17817999 13.10437164], Loss = 0.4826\n",
      "Iteration 6938: Weights = [55.30333333  2.36078764  5.65361755  0.09298711  0.17816848 13.10476518], Loss = 0.4825\n",
      "Iteration 6939: Weights = [55.30333333  2.36063511  5.65325226  0.09298111  0.17815696 13.10515869], Loss = 0.4825\n",
      "Iteration 6940: Weights = [55.30333333  2.36048258  5.65288699  0.0929751   0.17814545 13.10555218], Loss = 0.4824\n",
      "Iteration 6941: Weights = [55.30333333  2.36033007  5.65252175  0.09296909  0.17813394 13.10594564], Loss = 0.4823\n",
      "Iteration 6942: Weights = [55.30333333  2.36017756  5.65215652  0.09296308  0.17812243 13.10633908], Loss = 0.4823\n",
      "Iteration 6943: Weights = [55.30333333  2.36002507  5.65179133  0.09295708  0.17811092 13.10673249], Loss = 0.4822\n",
      "Iteration 6944: Weights = [55.30333333  2.35987258  5.65142615  0.09295107  0.17809942 13.10712587], Loss = 0.4821\n",
      "Iteration 6945: Weights = [55.30333333  2.3597201   5.651061    0.09294507  0.17808791 13.10751923], Loss = 0.4821\n",
      "Iteration 6946: Weights = [55.30333333  2.35956764  5.65069588  0.09293906  0.1780764  13.10791257], Loss = 0.4820\n",
      "Iteration 6947: Weights = [55.30333333  2.35941518  5.65033077  0.09293306  0.1780649  13.10830588], Loss = 0.4820\n",
      "Iteration 6948: Weights = [55.30333333  2.35926273  5.6499657   0.09292705  0.17805339 13.10869916], Loss = 0.4819\n",
      "Iteration 6949: Weights = [55.30333333  2.3591103   5.64960064  0.09292105  0.17804189 13.10909242], Loss = 0.4818\n",
      "Iteration 6950: Weights = [55.30333333  2.35895787  5.64923561  0.09291504  0.17803038 13.10948565], Loss = 0.4818\n",
      "Iteration 6951: Weights = [55.30333333  2.35880545  5.6488706   0.09290904  0.17801888 13.10987886], Loss = 0.4817\n",
      "Iteration 6952: Weights = [55.30333333  2.35865305  5.64850561  0.09290304  0.17800738 13.11027204], Loss = 0.4816\n",
      "Iteration 6953: Weights = [55.30333333  2.35850065  5.64814065  0.09289703  0.17799588 13.1106652 ], Loss = 0.4816\n",
      "Iteration 6954: Weights = [55.30333333  2.35834826  5.64777572  0.09289103  0.17798438 13.11105833], Loss = 0.4815\n",
      "Iteration 6955: Weights = [55.30333333  2.35819588  5.6474108   0.09288503  0.17797288 13.11145144], Loss = 0.4815\n",
      "Iteration 6956: Weights = [55.30333333  2.35804352  5.64704591  0.09287903  0.17796138 13.11184452], Loss = 0.4814\n",
      "Iteration 6957: Weights = [55.30333333  2.35789116  5.64668104  0.09287303  0.17794988 13.11223757], Loss = 0.4813\n",
      "Iteration 6958: Weights = [55.30333333  2.35773881  5.6463162   0.09286703  0.17793838 13.1126306 ], Loss = 0.4813\n",
      "Iteration 6959: Weights = [55.30333333  2.35758647  5.64595138  0.09286103  0.17792688 13.11302361], Loss = 0.4812\n",
      "Iteration 6960: Weights = [55.30333333  2.35743414  5.64558658  0.09285503  0.17791539 13.11341659], Loss = 0.4812\n",
      "Iteration 6961: Weights = [55.30333333  2.35728183  5.64522181  0.09284903  0.17790389 13.11380954], Loss = 0.4811\n",
      "Iteration 6962: Weights = [55.30333333  2.35712952  5.64485706  0.09284303  0.1778924  13.11420247], Loss = 0.4810\n",
      "Iteration 6963: Weights = [55.30333333  2.35697722  5.64449234  0.09283703  0.1778809  13.11459537], Loss = 0.4810\n",
      "Iteration 6964: Weights = [55.30333333  2.35682493  5.64412763  0.09283103  0.17786941 13.11498825], Loss = 0.4809\n",
      "Iteration 6965: Weights = [55.30333333  2.35667265  5.64376296  0.09282503  0.17785792 13.1153811 ], Loss = 0.4808\n",
      "Iteration 6966: Weights = [55.30333333  2.35652038  5.6433983   0.09281903  0.17784643 13.11577393], Loss = 0.4808\n",
      "Iteration 6967: Weights = [55.30333333  2.35636812  5.64303367  0.09281304  0.17783494 13.11616673], Loss = 0.4807\n",
      "Iteration 6968: Weights = [55.30333333  2.35621587  5.64266906  0.09280704  0.17782344 13.1165595 ], Loss = 0.4807\n",
      "Iteration 6969: Weights = [55.30333333  2.35606363  5.64230448  0.09280104  0.17781196 13.11695226], Loss = 0.4806\n",
      "Iteration 6970: Weights = [55.30333333  2.3559114   5.64193992  0.09279505  0.17780047 13.11734498], Loss = 0.4805\n",
      "Iteration 6971: Weights = [55.30333333  2.35575918  5.64157538  0.09278905  0.17778898 13.11773768], Loss = 0.4805\n",
      "Iteration 6972: Weights = [55.30333333  2.35560697  5.64121087  0.09278306  0.17777749 13.11813036], Loss = 0.4804\n",
      "Iteration 6973: Weights = [55.30333333  2.35545477  5.64084638  0.09277706  0.177766   13.118523  ], Loss = 0.4803\n",
      "Iteration 6974: Weights = [55.30333333  2.35530258  5.64048191  0.09277107  0.17775452 13.11891563], Loss = 0.4803\n",
      "Iteration 6975: Weights = [55.30333333  2.3551504   5.64011747  0.09276507  0.17774303 13.11930823], Loss = 0.4802\n",
      "Iteration 6976: Weights = [55.30333333  2.35499823  5.63975305  0.09275908  0.17773155 13.1197008 ], Loss = 0.4802\n",
      "Iteration 6977: Weights = [55.30333333  2.35484607  5.63938865  0.09275309  0.17772007 13.12009335], Loss = 0.4801\n",
      "Iteration 6978: Weights = [55.30333333  2.35469392  5.63902428  0.09274709  0.17770858 13.12048587], Loss = 0.4800\n",
      "Iteration 6979: Weights = [55.30333333  2.35454177  5.63865993  0.0927411   0.1776971  13.12087837], Loss = 0.4800\n",
      "Iteration 6980: Weights = [55.30333333  2.35438964  5.63829561  0.09273511  0.17768562 13.12127084], Loss = 0.4799\n",
      "Iteration 6981: Weights = [55.30333333  2.35423752  5.6379313   0.09272912  0.17767414 13.12166328], Loss = 0.4798\n",
      "Iteration 6982: Weights = [55.30333333  2.35408541  5.63756703  0.09272313  0.17766266 13.12205571], Loss = 0.4798\n",
      "Iteration 6983: Weights = [55.30333333  2.35393331  5.63720277  0.09271713  0.17765118 13.1224481 ], Loss = 0.4797\n",
      "Iteration 6984: Weights = [55.30333333  2.35378121  5.63683854  0.09271114  0.1776397  13.12284047], Loss = 0.4797\n",
      "Iteration 6985: Weights = [55.30333333  2.35362913  5.63647433  0.09270515  0.17762822 13.12323282], Loss = 0.4796\n",
      "Iteration 6986: Weights = [55.30333333  2.35347706  5.63611015  0.09269916  0.17761675 13.12362514], Loss = 0.4795\n",
      "Iteration 6987: Weights = [55.30333333  2.353325    5.63574599  0.09269317  0.17760527 13.12401743], Loss = 0.4795\n",
      "Iteration 6988: Weights = [55.30333333  2.35317294  5.63538185  0.09268719  0.1775938  13.1244097 ], Loss = 0.4794\n",
      "Iteration 6989: Weights = [55.30333333  2.3530209   5.63501774  0.0926812   0.17758232 13.12480194], Loss = 0.4794\n",
      "Iteration 6990: Weights = [55.30333333  2.35286887  5.63465365  0.09267521  0.17757085 13.12519416], Loss = 0.4793\n",
      "Iteration 6991: Weights = [55.30333333  2.35271684  5.63428958  0.09266922  0.17755937 13.12558635], Loss = 0.4792\n",
      "Iteration 6992: Weights = [55.30333333  2.35256483  5.63392554  0.09266323  0.1775479  13.12597852], Loss = 0.4792\n",
      "Iteration 6993: Weights = [55.30333333  2.35241282  5.63356152  0.09265725  0.17753643 13.12637066], Loss = 0.4791\n",
      "Iteration 6994: Weights = [55.30333333  2.35226083  5.63319753  0.09265126  0.17752496 13.12676278], Loss = 0.4790\n",
      "Iteration 6995: Weights = [55.30333333  2.35210885  5.63283355  0.09264527  0.17751349 13.12715487], Loss = 0.4790\n",
      "Iteration 6996: Weights = [55.30333333  2.35195687  5.6324696   0.09263929  0.17750202 13.12754694], Loss = 0.4789\n",
      "Iteration 6997: Weights = [55.30333333  2.35180491  5.63210568  0.0926333   0.17749055 13.12793898], Loss = 0.4789\n",
      "Iteration 6998: Weights = [55.30333333  2.35165295  5.63174178  0.09262732  0.17747908 13.12833099], Loss = 0.4788\n",
      "Iteration 6999: Weights = [55.30333333  2.35150101  5.6313779   0.09262133  0.17746761 13.12872298], Loss = 0.4787\n",
      "Iteration 7000: Weights = [55.30333333  2.35134907  5.63101404  0.09261535  0.17745615 13.12911495], Loss = 0.4787\n",
      "Iteration 7001: Weights = [55.30333333  2.35119715  5.63065021  0.09260936  0.17744468 13.12950689], Loss = 0.4786\n",
      "Iteration 7002: Weights = [55.30333333  2.35104523  5.63028641  0.09260338  0.17743322 13.1298988 ], Loss = 0.4785\n",
      "Iteration 7003: Weights = [55.30333333  2.35089333  5.62992262  0.0925974   0.17742175 13.13029069], Loss = 0.4785\n",
      "Iteration 7004: Weights = [55.30333333  2.35074143  5.62955886  0.09259141  0.17741029 13.13068255], Loss = 0.4784\n",
      "Iteration 7005: Weights = [55.30333333  2.35058954  5.62919512  0.09258543  0.17739883 13.13107439], Loss = 0.4784\n",
      "Iteration 7006: Weights = [55.30333333  2.35043767  5.62883141  0.09257945  0.17738736 13.1314662 ], Loss = 0.4783\n",
      "Iteration 7007: Weights = [55.30333333  2.3502858   5.62846772  0.09257347  0.1773759  13.13185799], Loss = 0.4782\n",
      "Iteration 7008: Weights = [55.30333333  2.35013394  5.62810405  0.09256748  0.17736444 13.13224975], Loss = 0.4782\n",
      "Iteration 7009: Weights = [55.30333333  2.3499821   5.62774041  0.0925615   0.17735298 13.13264149], Loss = 0.4781\n",
      "Iteration 7010: Weights = [55.30333333  2.34983026  5.62737679  0.09255552  0.17734152 13.1330332 ], Loss = 0.4781\n",
      "Iteration 7011: Weights = [55.30333333  2.34967843  5.6270132   0.09254954  0.17733006 13.13342489], Loss = 0.4780\n",
      "Iteration 7012: Weights = [55.30333333  2.34952661  5.62664962  0.09254356  0.17731861 13.13381655], Loss = 0.4779\n",
      "Iteration 7013: Weights = [55.30333333  2.34937481  5.62628607  0.09253758  0.17730715 13.13420819], Loss = 0.4779\n",
      "Iteration 7014: Weights = [55.30333333  2.34922301  5.62592255  0.0925316   0.17729569 13.1345998 ], Loss = 0.4778\n",
      "Iteration 7015: Weights = [55.30333333  2.34907122  5.62555905  0.09252563  0.17728424 13.13499138], Loss = 0.4777\n",
      "Iteration 7016: Weights = [55.30333333  2.34891944  5.62519557  0.09251965  0.17727278 13.13538294], Loss = 0.4777\n",
      "Iteration 7017: Weights = [55.30333333  2.34876767  5.62483211  0.09251367  0.17726133 13.13577448], Loss = 0.4776\n",
      "Iteration 7018: Weights = [55.30333333  2.34861592  5.62446868  0.09250769  0.17724988 13.13616598], Loss = 0.4776\n",
      "Iteration 7019: Weights = [55.30333333  2.34846417  5.62410527  0.09250172  0.17723842 13.13655747], Loss = 0.4775\n",
      "Iteration 7020: Weights = [55.30333333  2.34831243  5.62374189  0.09249574  0.17722697 13.13694893], Loss = 0.4774\n",
      "Iteration 7021: Weights = [55.30333333  2.3481607   5.62337853  0.09248976  0.17721552 13.13734036], Loss = 0.4774\n",
      "Iteration 7022: Weights = [55.30333333  2.34800898  5.62301519  0.09248379  0.17720407 13.13773177], Loss = 0.4773\n",
      "Iteration 7023: Weights = [55.30333333  2.34785727  5.62265187  0.09247781  0.17719262 13.13812315], Loss = 0.4772\n",
      "Iteration 7024: Weights = [55.30333333  2.34770557  5.62228858  0.09247184  0.17718117 13.13851451], Loss = 0.4772\n",
      "Iteration 7025: Weights = [55.30333333  2.34755388  5.62192532  0.09246586  0.17716972 13.13890584], Loss = 0.4771\n",
      "Iteration 7026: Weights = [55.30333333  2.3474022   5.62156207  0.09245989  0.17715828 13.13929715], Loss = 0.4771\n",
      "Iteration 7027: Weights = [55.30333333  2.34725053  5.62119885  0.09245391  0.17714683 13.13968843], Loss = 0.4770\n",
      "Iteration 7028: Weights = [55.30333333  2.34709887  5.62083565  0.09244794  0.17713539 13.14007968], Loss = 0.4769\n",
      "Iteration 7029: Weights = [55.30333333  2.34694722  5.62047248  0.09244197  0.17712394 13.14047092], Loss = 0.4769\n",
      "Iteration 7030: Weights = [55.30333333  2.34679558  5.62010933  0.09243599  0.1771125  13.14086212], Loss = 0.4768\n",
      "Iteration 7031: Weights = [55.30333333  2.34664395  5.6197462   0.09243002  0.17710105 13.1412533 ], Loss = 0.4768\n",
      "Iteration 7032: Weights = [55.30333333  2.34649233  5.6193831   0.09242405  0.17708961 13.14164446], Loss = 0.4767\n",
      "Iteration 7033: Weights = [55.30333333  2.34634071  5.61902002  0.09241808  0.17707817 13.14203559], Loss = 0.4766\n",
      "Iteration 7034: Weights = [55.30333333  2.34618911  5.61865697  0.09241211  0.17706673 13.14242669], Loss = 0.4766\n",
      "Iteration 7035: Weights = [55.30333333  2.34603752  5.61829393  0.09240613  0.17705529 13.14281777], Loss = 0.4765\n",
      "Iteration 7036: Weights = [55.30333333  2.34588594  5.61793092  0.09240016  0.17704385 13.14320882], Loss = 0.4764\n",
      "Iteration 7037: Weights = [55.30333333  2.34573437  5.61756794  0.09239419  0.17703241 13.14359985], Loss = 0.4764\n",
      "Iteration 7038: Weights = [55.30333333  2.3455828   5.61720498  0.09238822  0.17702097 13.14399086], Loss = 0.4763\n",
      "Iteration 7039: Weights = [55.30333333  2.34543125  5.61684204  0.09238225  0.17700953 13.14438184], Loss = 0.4763\n",
      "Iteration 7040: Weights = [55.30333333  2.34527971  5.61647912  0.09237629  0.17699809 13.14477279], Loss = 0.4762\n",
      "Iteration 7041: Weights = [55.30333333  2.34512817  5.61611623  0.09237032  0.17698666 13.14516372], Loss = 0.4761\n",
      "Iteration 7042: Weights = [55.30333333  2.34497665  5.61575336  0.09236435  0.17697522 13.14555462], Loss = 0.4761\n",
      "Iteration 7043: Weights = [55.30333333  2.34482514  5.61539051  0.09235838  0.17696379 13.1459455 ], Loss = 0.4760\n",
      "Iteration 7044: Weights = [55.30333333  2.34467363  5.61502769  0.09235241  0.17695235 13.14633635], Loss = 0.4760\n",
      "Iteration 7045: Weights = [55.30333333  2.34452214  5.6146649   0.09234645  0.17694092 13.14672717], Loss = 0.4759\n",
      "Iteration 7046: Weights = [55.30333333  2.34437065  5.61430212  0.09234048  0.17692949 13.14711798], Loss = 0.4758\n",
      "Iteration 7047: Weights = [55.30333333  2.34421918  5.61393937  0.09233451  0.17691806 13.14750875], Loss = 0.4758\n",
      "Iteration 7048: Weights = [55.30333333  2.34406771  5.61357664  0.09232855  0.17690662 13.1478995 ], Loss = 0.4757\n",
      "Iteration 7049: Weights = [55.30333333  2.34391626  5.61321394  0.09232258  0.17689519 13.14829023], Loss = 0.4756\n",
      "Iteration 7050: Weights = [55.30333333  2.34376481  5.61285126  0.09231662  0.17688376 13.14868093], Loss = 0.4756\n",
      "Iteration 7051: Weights = [55.30333333  2.34361338  5.6124886   0.09231065  0.17687234 13.1490716 ], Loss = 0.4755\n",
      "Iteration 7052: Weights = [55.30333333  2.34346195  5.61212596  0.09230469  0.17686091 13.14946225], Loss = 0.4755\n",
      "Iteration 7053: Weights = [55.30333333  2.34331054  5.61176335  0.09229872  0.17684948 13.14985288], Loss = 0.4754\n",
      "Iteration 7054: Weights = [55.30333333  2.34315913  5.61140077  0.09229276  0.17683805 13.15024348], Loss = 0.4753\n",
      "Iteration 7055: Weights = [55.30333333  2.34300774  5.6110382   0.0922868   0.17682663 13.15063405], Loss = 0.4753\n",
      "Iteration 7056: Weights = [55.30333333  2.34285635  5.61067566  0.09228083  0.1768152  13.1510246 ], Loss = 0.4752\n",
      "Iteration 7057: Weights = [55.30333333  2.34270497  5.61031314  0.09227487  0.17680378 13.15141513], Loss = 0.4752\n",
      "Iteration 7058: Weights = [55.30333333  2.34255361  5.60995065  0.09226891  0.17679235 13.15180562], Loss = 0.4751\n",
      "Iteration 7059: Weights = [55.30333333  2.34240225  5.60958818  0.09226295  0.17678093 13.1521961 ], Loss = 0.4750\n",
      "Iteration 7060: Weights = [55.30333333  2.3422509   5.60922573  0.09225699  0.17676951 13.15258655], Loss = 0.4750\n",
      "Iteration 7061: Weights = [55.30333333  2.34209956  5.60886331  0.09225103  0.17675809 13.15297697], Loss = 0.4749\n",
      "Iteration 7062: Weights = [55.30333333  2.34194824  5.60850091  0.09224506  0.17674667 13.15336737], Loss = 0.4748\n",
      "Iteration 7063: Weights = [55.30333333  2.34179692  5.60813853  0.0922391   0.17673525 13.15375774], Loss = 0.4748\n",
      "Iteration 7064: Weights = [55.30333333  2.34164561  5.60777618  0.09223314  0.17672383 13.15414809], Loss = 0.4747\n",
      "Iteration 7065: Weights = [55.30333333  2.34149431  5.60741385  0.09222719  0.17671241 13.15453841], Loss = 0.4747\n",
      "Iteration 7066: Weights = [55.30333333  2.34134302  5.60705155  0.09222123  0.17670099 13.15492871], Loss = 0.4746\n",
      "Iteration 7067: Weights = [55.30333333  2.34119174  5.60668926  0.09221527  0.17668957 13.15531898], Loss = 0.4745\n",
      "Iteration 7068: Weights = [55.30333333  2.34104047  5.606327    0.09220931  0.17667816 13.15570922], Loss = 0.4745\n",
      "Iteration 7069: Weights = [55.30333333  2.34088922  5.60596477  0.09220335  0.17666674 13.15609944], Loss = 0.4744\n",
      "Iteration 7070: Weights = [55.30333333  2.34073797  5.60560255  0.09219739  0.17665533 13.15648964], Loss = 0.4744\n",
      "Iteration 7071: Weights = [55.30333333  2.34058673  5.60524036  0.09219144  0.17664391 13.15687981], Loss = 0.4743\n",
      "Iteration 7072: Weights = [55.30333333  2.3404355   5.6048782   0.09218548  0.1766325  13.15726996], Loss = 0.4742\n",
      "Iteration 7073: Weights = [55.30333333  2.34028428  5.60451606  0.09217952  0.17662109 13.15766008], Loss = 0.4742\n",
      "Iteration 7074: Weights = [55.30333333  2.34013307  5.60415394  0.09217357  0.17660968 13.15805017], Loss = 0.4741\n",
      "Iteration 7075: Weights = [55.30333333  2.33998186  5.60379184  0.09216761  0.17659827 13.15844024], Loss = 0.4741\n",
      "Iteration 7076: Weights = [55.30333333  2.33983067  5.60342977  0.09216166  0.17658686 13.15883029], Loss = 0.4740\n",
      "Iteration 7077: Weights = [55.30333333  2.33967949  5.60306772  0.0921557   0.17657545 13.15922031], Loss = 0.4739\n",
      "Iteration 7078: Weights = [55.30333333  2.33952832  5.6027057   0.09214975  0.17656404 13.1596103 ], Loss = 0.4739\n",
      "Iteration 7079: Weights = [55.30333333  2.33937716  5.60234369  0.09214379  0.17655263 13.16000027], Loss = 0.4738\n",
      "Iteration 7080: Weights = [55.30333333  2.33922601  5.60198171  0.09213784  0.17654122 13.16039021], Loss = 0.4737\n",
      "Iteration 7081: Weights = [55.30333333  2.33907487  5.60161976  0.09213189  0.17652981 13.16078013], Loss = 0.4737\n",
      "Iteration 7082: Weights = [55.30333333  2.33892373  5.60125783  0.09212594  0.17651841 13.16117002], Loss = 0.4736\n",
      "Iteration 7083: Weights = [55.30333333  2.33877261  5.60089592  0.09211998  0.176507   13.16155989], Loss = 0.4736\n",
      "Iteration 7084: Weights = [55.30333333  2.3386215   5.60053403  0.09211403  0.1764956  13.16194974], Loss = 0.4735\n",
      "Iteration 7085: Weights = [55.30333333  2.3384704   5.60017217  0.09210808  0.17648419 13.16233955], Loss = 0.4734\n",
      "Iteration 7086: Weights = [55.30333333  2.3383193   5.59981033  0.09210213  0.17647279 13.16272935], Loss = 0.4734\n",
      "Iteration 7087: Weights = [55.30333333  2.33816822  5.59944852  0.09209618  0.17646139 13.16311911], Loss = 0.4733\n",
      "Iteration 7088: Weights = [55.30333333  2.33801714  5.59908673  0.09209023  0.17644999 13.16350886], Loss = 0.4733\n",
      "Iteration 7089: Weights = [55.30333333  2.33786608  5.59872496  0.09208428  0.17643859 13.16389857], Loss = 0.4732\n",
      "Iteration 7090: Weights = [55.30333333  2.33771503  5.59836321  0.09207833  0.17642719 13.16428827], Loss = 0.4731\n",
      "Iteration 7091: Weights = [55.30333333  2.33756398  5.59800149  0.09207238  0.17641579 13.16467793], Loss = 0.4731\n",
      "Iteration 7092: Weights = [55.30333333  2.33741295  5.59763979  0.09206643  0.17640439 13.16506757], Loss = 0.4730\n",
      "Iteration 7093: Weights = [55.30333333  2.33726192  5.59727812  0.09206048  0.17639299 13.16545719], Loss = 0.4730\n",
      "Iteration 7094: Weights = [55.30333333  2.33711091  5.59691647  0.09205453  0.17638159 13.16584678], Loss = 0.4729\n",
      "Iteration 7095: Weights = [55.30333333  2.3369599   5.59655484  0.09204858  0.1763702  13.16623635], Loss = 0.4728\n",
      "Iteration 7096: Weights = [55.30333333  2.33680891  5.59619324  0.09204264  0.1763588  13.16662589], Loss = 0.4728\n",
      "Iteration 7097: Weights = [55.30333333  2.33665792  5.59583165  0.09203669  0.17634741 13.16701541], Loss = 0.4727\n",
      "Iteration 7098: Weights = [55.30333333  2.33650694  5.5954701   0.09203074  0.17633601 13.1674049 ], Loss = 0.4726\n",
      "Iteration 7099: Weights = [55.30333333  2.33635598  5.59510856  0.0920248   0.17632462 13.16779436], Loss = 0.4726\n",
      "Iteration 7100: Weights = [55.30333333  2.33620502  5.59474705  0.09201885  0.17631323 13.1681838 ], Loss = 0.4725\n",
      "Iteration 7101: Weights = [55.30333333  2.33605407  5.59438556  0.0920129   0.17630184 13.16857322], Loss = 0.4725\n",
      "Iteration 7102: Weights = [55.30333333  2.33590314  5.5940241   0.09200696  0.17629044 13.16896261], Loss = 0.4724\n",
      "Iteration 7103: Weights = [55.30333333  2.33575221  5.59366266  0.09200101  0.17627905 13.16935197], Loss = 0.4723\n",
      "Iteration 7104: Weights = [55.30333333  2.33560129  5.59330124  0.09199507  0.17626766 13.16974131], Loss = 0.4723\n",
      "Iteration 7105: Weights = [55.30333333  2.33545038  5.59293984  0.09198913  0.17625627 13.17013063], Loss = 0.4722\n",
      "Iteration 7106: Weights = [55.30333333  2.33529949  5.59257847  0.09198318  0.17624489 13.17051991], Loss = 0.4722\n",
      "Iteration 7107: Weights = [55.30333333  2.3351486   5.59221713  0.09197724  0.1762335  13.17090918], Loss = 0.4721\n",
      "Iteration 7108: Weights = [55.30333333  2.33499772  5.5918558   0.0919713   0.17622211 13.17129842], Loss = 0.4720\n",
      "Iteration 7109: Weights = [55.30333333  2.33484685  5.5914945   0.09196535  0.17621073 13.17168763], Loss = 0.4720\n",
      "Iteration 7110: Weights = [55.30333333  2.33469599  5.59113322  0.09195941  0.17619934 13.17207682], Loss = 0.4719\n",
      "Iteration 7111: Weights = [55.30333333  2.33454514  5.59077197  0.09195347  0.17618796 13.17246598], Loss = 0.4719\n",
      "Iteration 7112: Weights = [55.30333333  2.3343943   5.59041074  0.09194753  0.17617657 13.17285512], Loss = 0.4718\n",
      "Iteration 7113: Weights = [55.30333333  2.33424347  5.59004953  0.09194159  0.17616519 13.17324424], Loss = 0.4717\n",
      "Iteration 7114: Weights = [55.30333333  2.33409265  5.58968835  0.09193565  0.17615381 13.17363332], Loss = 0.4717\n",
      "Iteration 7115: Weights = [55.30333333  2.33394184  5.58932718  0.09192971  0.17614243 13.17402239], Loss = 0.4716\n",
      "Iteration 7116: Weights = [55.30333333  2.33379104  5.58896605  0.09192377  0.17613104 13.17441142], Loss = 0.4715\n",
      "Iteration 7117: Weights = [55.30333333  2.33364025  5.58860493  0.09191783  0.17611966 13.17480044], Loss = 0.4715\n",
      "Iteration 7118: Weights = [55.30333333  2.33348947  5.58824384  0.09191189  0.17610828 13.17518943], Loss = 0.4714\n",
      "Iteration 7119: Weights = [55.30333333  2.3333387   5.58788277  0.09190595  0.17609691 13.17557839], Loss = 0.4714\n",
      "Iteration 7120: Weights = [55.30333333  2.33318794  5.58752173  0.09190001  0.17608553 13.17596733], Loss = 0.4713\n",
      "Iteration 7121: Weights = [55.30333333  2.33303718  5.58716071  0.09189407  0.17607415 13.17635624], Loss = 0.4712\n",
      "Iteration 7122: Weights = [55.30333333  2.33288644  5.58679971  0.09188814  0.17606277 13.17674512], Loss = 0.4712\n",
      "Iteration 7123: Weights = [55.30333333  2.33273571  5.58643874  0.0918822   0.1760514  13.17713399], Loss = 0.4711\n",
      "Iteration 7124: Weights = [55.30333333  2.33258499  5.58607779  0.09187626  0.17604002 13.17752282], Loss = 0.4711\n",
      "Iteration 7125: Weights = [55.30333333  2.33243427  5.58571686  0.09187033  0.17602865 13.17791163], Loss = 0.4710\n",
      "Iteration 7126: Weights = [55.30333333  2.33228357  5.58535595  0.09186439  0.17601728 13.17830042], Loss = 0.4709\n",
      "Iteration 7127: Weights = [55.30333333  2.33213288  5.58499507  0.09185846  0.1760059  13.17868918], Loss = 0.4709\n",
      "Iteration 7128: Weights = [55.30333333  2.33198219  5.58463421  0.09185252  0.17599453 13.17907792], Loss = 0.4708\n",
      "Iteration 7129: Weights = [55.30333333  2.33183152  5.58427338  0.09184659  0.17598316 13.17946663], Loss = 0.4708\n",
      "Iteration 7130: Weights = [55.30333333  2.33168085  5.58391257  0.09184065  0.17597179 13.17985532], Loss = 0.4707\n",
      "Iteration 7131: Weights = [55.30333333  2.3315302   5.58355178  0.09183472  0.17596042 13.18024398], Loss = 0.4706\n",
      "Iteration 7132: Weights = [55.30333333  2.33137955  5.58319102  0.09182878  0.17594905 13.18063261], Loss = 0.4706\n",
      "Iteration 7133: Weights = [55.30333333  2.33122892  5.58283028  0.09182285  0.17593768 13.18102122], Loss = 0.4705\n",
      "Iteration 7134: Weights = [55.30333333  2.33107829  5.58246956  0.09181692  0.17592631 13.18140981], Loss = 0.4705\n",
      "Iteration 7135: Weights = [55.30333333  2.33092768  5.58210886  0.09181099  0.17591495 13.18179837], Loss = 0.4704\n",
      "Iteration 7136: Weights = [55.30333333  2.33077707  5.58174819  0.09180505  0.17590358 13.18218691], Loss = 0.4703\n",
      "Iteration 7137: Weights = [55.30333333  2.33062648  5.58138754  0.09179912  0.17589221 13.18257542], Loss = 0.4703\n",
      "Iteration 7138: Weights = [55.30333333  2.33047589  5.58102692  0.09179319  0.17588085 13.1829639 ], Loss = 0.4702\n",
      "Iteration 7139: Weights = [55.30333333  2.33032531  5.58066632  0.09178726  0.17586949 13.18335236], Loss = 0.4701\n",
      "Iteration 7140: Weights = [55.30333333  2.33017475  5.58030574  0.09178133  0.17585812 13.1837408 ], Loss = 0.4701\n",
      "Iteration 7141: Weights = [55.30333333  2.33002419  5.57994518  0.0917754   0.17584676 13.18412921], Loss = 0.4700\n",
      "Iteration 7142: Weights = [55.30333333  2.32987364  5.57958465  0.09176947  0.1758354  13.18451759], Loss = 0.4700\n",
      "Iteration 7143: Weights = [55.30333333  2.3297231   5.57922414  0.09176354  0.17582404 13.18490595], Loss = 0.4699\n",
      "Iteration 7144: Weights = [55.30333333  2.32957258  5.57886366  0.09175761  0.17581268 13.18529429], Loss = 0.4698\n",
      "Iteration 7145: Weights = [55.30333333  2.32942206  5.5785032   0.09175168  0.17580132 13.1856826 ], Loss = 0.4698\n",
      "Iteration 7146: Weights = [55.30333333  2.32927155  5.57814276  0.09174575  0.17578996 13.18607088], Loss = 0.4697\n",
      "Iteration 7147: Weights = [55.30333333  2.32912105  5.57778235  0.09173983  0.1757786  13.18645914], Loss = 0.4697\n",
      "Iteration 7148: Weights = [55.30333333  2.32897056  5.57742195  0.0917339   0.17576724 13.18684737], Loss = 0.4696\n",
      "Iteration 7149: Weights = [55.30333333  2.32882008  5.57706159  0.09172797  0.17575589 13.18723558], Loss = 0.4695\n",
      "Iteration 7150: Weights = [55.30333333  2.32866961  5.57670124  0.09172204  0.17574453 13.18762377], Loss = 0.4695\n",
      "Iteration 7151: Weights = [55.30333333  2.32851915  5.57634092  0.09171612  0.17573318 13.18801193], Loss = 0.4694\n",
      "Iteration 7152: Weights = [55.30333333  2.3283687   5.57598062  0.09171019  0.17572182 13.18840006], Loss = 0.4694\n",
      "Iteration 7153: Weights = [55.30333333  2.32821826  5.57562034  0.09170427  0.17571047 13.18878817], Loss = 0.4693\n",
      "Iteration 7154: Weights = [55.30333333  2.32806783  5.57526009  0.09169834  0.17569911 13.18917625], Loss = 0.4692\n",
      "Iteration 7155: Weights = [55.30333333  2.32791741  5.57489986  0.09169242  0.17568776 13.18956431], Loss = 0.4692\n",
      "Iteration 7156: Weights = [55.30333333  2.327767    5.57453966  0.09168649  0.17567641 13.18995235], Loss = 0.4691\n",
      "Iteration 7157: Weights = [55.30333333  2.32761659  5.57417948  0.09168057  0.17566506 13.19034035], Loss = 0.4691\n",
      "Iteration 7158: Weights = [55.30333333  2.3274662   5.57381932  0.09167464  0.17565371 13.19072834], Loss = 0.4690\n",
      "Iteration 7159: Weights = [55.30333333  2.32731582  5.57345918  0.09166872  0.17564236 13.1911163 ], Loss = 0.4689\n",
      "Iteration 7160: Weights = [55.30333333  2.32716545  5.57309907  0.0916628   0.17563101 13.19150423], Loss = 0.4689\n",
      "Iteration 7161: Weights = [55.30333333  2.32701509  5.57273898  0.09165688  0.17561966 13.19189214], Loss = 0.4688\n",
      "Iteration 7162: Weights = [55.30333333  2.32686473  5.57237891  0.09165095  0.17560832 13.19228002], Loss = 0.4688\n",
      "Iteration 7163: Weights = [55.30333333  2.32671439  5.57201887  0.09164503  0.17559697 13.19266788], Loss = 0.4687\n",
      "Iteration 7164: Weights = [55.30333333  2.32656405  5.57165885  0.09163911  0.17558562 13.19305571], Loss = 0.4686\n",
      "Iteration 7165: Weights = [55.30333333  2.32641373  5.57129886  0.09163319  0.17557428 13.19344352], Loss = 0.4686\n",
      "Iteration 7166: Weights = [55.30333333  2.32626342  5.57093888  0.09162727  0.17556294 13.1938313 ], Loss = 0.4685\n",
      "Iteration 7167: Weights = [55.30333333  2.32611311  5.57057893  0.09162135  0.17555159 13.19421906], Loss = 0.4685\n",
      "Iteration 7168: Weights = [55.30333333  2.32596282  5.57021901  0.09161543  0.17554025 13.19460679], Loss = 0.4684\n",
      "Iteration 7169: Weights = [55.30333333  2.32581253  5.5698591   0.09160951  0.17552891 13.1949945 ], Loss = 0.4683\n",
      "Iteration 7170: Weights = [55.30333333  2.32566226  5.56949922  0.09160359  0.17551757 13.19538218], Loss = 0.4683\n",
      "Iteration 7171: Weights = [55.30333333  2.32551199  5.56913937  0.09159767  0.17550623 13.19576984], Loss = 0.4682\n",
      "Iteration 7172: Weights = [55.30333333  2.32536173  5.56877953  0.09159175  0.17549489 13.19615748], Loss = 0.4681\n",
      "Iteration 7173: Weights = [55.30333333  2.32521149  5.56841972  0.09158583  0.17548355 13.19654508], Loss = 0.4681\n",
      "Iteration 7174: Weights = [55.30333333  2.32506125  5.56805994  0.09157992  0.17547221 13.19693267], Loss = 0.4680\n",
      "Iteration 7175: Weights = [55.30333333  2.32491103  5.56770017  0.091574    0.17546087 13.19732022], Loss = 0.4680\n",
      "Iteration 7176: Weights = [55.30333333  2.32476081  5.56734043  0.09156808  0.17544953 13.19770776], Loss = 0.4679\n",
      "Iteration 7177: Weights = [55.30333333  2.3246106   5.56698072  0.09156217  0.1754382  13.19809526], Loss = 0.4678\n",
      "Iteration 7178: Weights = [55.30333333  2.3244604   5.56662102  0.09155625  0.17542686 13.19848275], Loss = 0.4678\n",
      "Iteration 7179: Weights = [55.30333333  2.32431021  5.56626135  0.09155034  0.17541553 13.1988702 ], Loss = 0.4677\n",
      "Iteration 7180: Weights = [55.30333333  2.32416004  5.5659017   0.09154442  0.17540419 13.19925764], Loss = 0.4677\n",
      "Iteration 7181: Weights = [55.30333333  2.32400987  5.56554208  0.09153851  0.17539286 13.19964504], Loss = 0.4676\n",
      "Iteration 7182: Weights = [55.30333333  2.32385971  5.56518248  0.09153259  0.17538153 13.20003243], Loss = 0.4675\n",
      "Iteration 7183: Weights = [55.30333333  2.32370956  5.5648229   0.09152668  0.1753702  13.20041978], Loss = 0.4675\n",
      "Iteration 7184: Weights = [55.30333333  2.32355942  5.56446335  0.09152076  0.17535886 13.20080712], Loss = 0.4674\n",
      "Iteration 7185: Weights = [55.30333333  2.32340929  5.56410382  0.09151485  0.17534753 13.20119442], Loss = 0.4674\n",
      "Iteration 7186: Weights = [55.30333333  2.32325917  5.56374431  0.09150894  0.1753362  13.2015817 ], Loss = 0.4673\n",
      "Iteration 7187: Weights = [55.30333333  2.32310906  5.56338482  0.09150302  0.17532488 13.20196896], Loss = 0.4672\n",
      "Iteration 7188: Weights = [55.30333333  2.32295896  5.56302536  0.09149711  0.17531355 13.20235619], Loss = 0.4672\n",
      "Iteration 7189: Weights = [55.30333333  2.32280887  5.56266592  0.0914912   0.17530222 13.2027434 ], Loss = 0.4671\n",
      "Iteration 7190: Weights = [55.30333333  2.32265879  5.56230651  0.09148529  0.17529089 13.20313058], Loss = 0.4671\n",
      "Iteration 7191: Weights = [55.30333333  2.32250871  5.56194712  0.09147938  0.17527957 13.20351774], Loss = 0.4670\n",
      "Iteration 7192: Weights = [55.30333333  2.32235865  5.56158775  0.09147347  0.17526824 13.20390487], Loss = 0.4669\n",
      "Iteration 7193: Weights = [55.30333333  2.3222086   5.5612284   0.09146756  0.17525692 13.20429198], Loss = 0.4669\n",
      "Iteration 7194: Weights = [55.30333333  2.32205856  5.56086908  0.09146165  0.17524559 13.20467906], Loss = 0.4668\n",
      "Iteration 7195: Weights = [55.30333333  2.32190853  5.56050978  0.09145574  0.17523427 13.20506612], Loss = 0.4668\n",
      "Iteration 7196: Weights = [55.30333333  2.3217585   5.56015051  0.09144983  0.17522295 13.20545315], Loss = 0.4667\n",
      "Iteration 7197: Weights = [55.30333333  2.32160849  5.55979125  0.09144392  0.17521163 13.20584016], Loss = 0.4666\n",
      "Iteration 7198: Weights = [55.30333333  2.32145848  5.55943203  0.09143801  0.17520031 13.20622714], Loss = 0.4666\n",
      "Iteration 7199: Weights = [55.30333333  2.32130849  5.55907282  0.0914321   0.17518899 13.2066141 ], Loss = 0.4665\n",
      "Iteration 7200: Weights = [55.30333333  2.32115851  5.55871364  0.0914262   0.17517767 13.20700103], Loss = 0.4665\n",
      "Iteration 7201: Weights = [55.30333333  2.32100853  5.55835448  0.09142029  0.17516635 13.20738794], Loss = 0.4664\n",
      "Iteration 7202: Weights = [55.30333333  2.32085857  5.55799534  0.09141438  0.17515503 13.20777482], Loss = 0.4663\n",
      "Iteration 7203: Weights = [55.30333333  2.32070861  5.55763623  0.09140847  0.17514371 13.20816168], Loss = 0.4663\n",
      "Iteration 7204: Weights = [55.30333333  2.32055867  5.55727714  0.09140257  0.1751324  13.20854851], Loss = 0.4662\n",
      "Iteration 7205: Weights = [55.30333333  2.32040873  5.55691807  0.09139666  0.17512108 13.20893531], Loss = 0.4662\n",
      "Iteration 7206: Weights = [55.30333333  2.3202588   5.55655903  0.09139076  0.17510977 13.2093221 ], Loss = 0.4661\n",
      "Iteration 7207: Weights = [55.30333333  2.32010889  5.55620001  0.09138485  0.17509845 13.20970885], Loss = 0.4660\n",
      "Iteration 7208: Weights = [55.30333333  2.31995898  5.55584101  0.09137895  0.17508714 13.21009559], Loss = 0.4660\n",
      "Iteration 7209: Weights = [55.30333333  2.31980908  5.55548203  0.09137304  0.17507583 13.21048229], Loss = 0.4659\n",
      "Iteration 7210: Weights = [55.30333333  2.31965919  5.55512308  0.09136714  0.17506452 13.21086897], Loss = 0.4659\n",
      "Iteration 7211: Weights = [55.30333333  2.31950932  5.55476416  0.09136124  0.1750532  13.21125563], Loss = 0.4658\n",
      "Iteration 7212: Weights = [55.30333333  2.31935945  5.55440525  0.09135533  0.17504189 13.21164226], Loss = 0.4657\n",
      "Iteration 7213: Weights = [55.30333333  2.31920959  5.55404637  0.09134943  0.17503058 13.21202887], Loss = 0.4657\n",
      "Iteration 7214: Weights = [55.30333333  2.31905974  5.55368751  0.09134353  0.17501927 13.21241545], Loss = 0.4656\n",
      "Iteration 7215: Weights = [55.30333333  2.3189099   5.55332868  0.09133763  0.17500797 13.21280201], Loss = 0.4656\n",
      "Iteration 7216: Weights = [55.30333333  2.31876007  5.55296987  0.09133173  0.17499666 13.21318854], Loss = 0.4655\n",
      "Iteration 7217: Weights = [55.30333333  2.31861025  5.55261108  0.09132582  0.17498535 13.21357505], Loss = 0.4654\n",
      "Iteration 7218: Weights = [55.30333333  2.31846044  5.55225231  0.09131992  0.17497405 13.21396153], Loss = 0.4654\n",
      "Iteration 7219: Weights = [55.30333333  2.31831064  5.55189357  0.09131402  0.17496274 13.21434799], Loss = 0.4653\n",
      "Iteration 7220: Weights = [55.30333333  2.31816085  5.55153485  0.09130812  0.17495144 13.21473442], Loss = 0.4653\n",
      "Iteration 7221: Weights = [55.30333333  2.31801107  5.55117615  0.09130222  0.17494013 13.21512083], Loss = 0.4652\n",
      "Iteration 7222: Weights = [55.30333333  2.3178613   5.55081748  0.09129632  0.17492883 13.21550721], Loss = 0.4651\n",
      "Iteration 7223: Weights = [55.30333333  2.31771154  5.55045883  0.09129043  0.17491753 13.21589357], Loss = 0.4651\n",
      "Iteration 7224: Weights = [55.30333333  2.31756179  5.55010021  0.09128453  0.17490622 13.2162799 ], Loss = 0.4650\n",
      "Iteration 7225: Weights = [55.30333333  2.31741204  5.5497416   0.09127863  0.17489492 13.21666621], Loss = 0.4650\n",
      "Iteration 7226: Weights = [55.30333333  2.31726231  5.54938302  0.09127273  0.17488362 13.21705249], Loss = 0.4649\n",
      "Iteration 7227: Weights = [55.30333333  2.31711259  5.54902447  0.09126683  0.17487232 13.21743875], Loss = 0.4648\n",
      "Iteration 7228: Weights = [55.30333333  2.31696287  5.54866593  0.09126094  0.17486102 13.21782498], Loss = 0.4648\n",
      "Iteration 7229: Weights = [55.30333333  2.31681317  5.54830742  0.09125504  0.17484973 13.21821119], Loss = 0.4647\n",
      "Iteration 7230: Weights = [55.30333333  2.31666348  5.54794893  0.09124914  0.17483843 13.21859737], Loss = 0.4647\n",
      "Iteration 7231: Weights = [55.30333333  2.31651379  5.54759047  0.09124325  0.17482713 13.21898353], Loss = 0.4646\n",
      "Iteration 7232: Weights = [55.30333333  2.31636412  5.54723203  0.09123735  0.17481584 13.21936966], Loss = 0.4645\n",
      "Iteration 7233: Weights = [55.30333333  2.31621445  5.54687361  0.09123146  0.17480454 13.21975577], Loss = 0.4645\n",
      "Iteration 7234: Weights = [55.30333333  2.3160648   5.54651522  0.09122556  0.17479325 13.22014185], Loss = 0.4644\n",
      "Iteration 7235: Weights = [55.30333333  2.31591515  5.54615685  0.09121967  0.17478195 13.22052791], Loss = 0.4644\n",
      "Iteration 7236: Weights = [55.30333333  2.31576552  5.5457985   0.09121378  0.17477066 13.22091395], Loss = 0.4643\n",
      "Iteration 7237: Weights = [55.30333333  2.31561589  5.54544017  0.09120788  0.17475937 13.22129995], Loss = 0.4642\n",
      "Iteration 7238: Weights = [55.30333333  2.31546627  5.54508187  0.09120199  0.17474808 13.22168594], Loss = 0.4642\n",
      "Iteration 7239: Weights = [55.30333333  2.31531667  5.54472359  0.0911961   0.17473678 13.22207189], Loss = 0.4641\n",
      "Iteration 7240: Weights = [55.30333333  2.31516707  5.54436534  0.0911902   0.17472549 13.22245783], Loss = 0.4641\n",
      "Iteration 7241: Weights = [55.30333333  2.31501748  5.5440071   0.09118431  0.17471421 13.22284374], Loss = 0.4640\n",
      "Iteration 7242: Weights = [55.30333333  2.3148679   5.54364889  0.09117842  0.17470292 13.22322962], Loss = 0.4639\n",
      "Iteration 7243: Weights = [55.30333333  2.31471834  5.54329071  0.09117253  0.17469163 13.22361548], Loss = 0.4639\n",
      "Iteration 7244: Weights = [55.30333333  2.31456878  5.54293254  0.09116664  0.17468034 13.22400131], Loss = 0.4638\n",
      "Iteration 7245: Weights = [55.30333333  2.31441923  5.5425744   0.09116075  0.17466906 13.22438712], Loss = 0.4638\n",
      "Iteration 7246: Weights = [55.30333333  2.31426969  5.54221629  0.09115486  0.17465777 13.2247729 ], Loss = 0.4637\n",
      "Iteration 7247: Weights = [55.30333333  2.31412016  5.54185819  0.09114897  0.17464648 13.22515866], Loss = 0.4636\n",
      "Iteration 7248: Weights = [55.30333333  2.31397064  5.54150012  0.09114308  0.1746352  13.2255444 ], Loss = 0.4636\n",
      "Iteration 7249: Weights = [55.30333333  2.31382113  5.54114208  0.09113719  0.17462392 13.22593011], Loss = 0.4635\n",
      "Iteration 7250: Weights = [55.30333333  2.31367163  5.54078405  0.0911313   0.17461263 13.22631579], Loss = 0.4635\n",
      "Iteration 7251: Weights = [55.30333333  2.31352214  5.54042605  0.09112541  0.17460135 13.22670145], Loss = 0.4634\n",
      "Iteration 7252: Weights = [55.30333333  2.31337266  5.54006807  0.09111952  0.17459007 13.22708708], Loss = 0.4633\n",
      "Iteration 7253: Weights = [55.30333333  2.31322318  5.53971012  0.09111364  0.17457879 13.22747269], Loss = 0.4633\n",
      "Iteration 7254: Weights = [55.30333333  2.31307372  5.53935219  0.09110775  0.17456751 13.22785828], Loss = 0.4632\n",
      "Iteration 7255: Weights = [55.30333333  2.31292427  5.53899428  0.09110186  0.17455623 13.22824384], Loss = 0.4632\n",
      "Iteration 7256: Weights = [55.30333333  2.31277483  5.53863639  0.09109598  0.17454495 13.22862937], Loss = 0.4631\n",
      "Iteration 7257: Weights = [55.30333333  2.31262539  5.53827853  0.09109009  0.17453367 13.22901488], Loss = 0.4630\n",
      "Iteration 7258: Weights = [55.30333333  2.31247597  5.53792069  0.09108421  0.1745224  13.22940037], Loss = 0.4630\n",
      "Iteration 7259: Weights = [55.30333333  2.31232656  5.53756287  0.09107832  0.17451112 13.22978582], Loss = 0.4629\n",
      "Iteration 7260: Weights = [55.30333333  2.31217715  5.53720508  0.09107244  0.17449985 13.23017126], Loss = 0.4629\n",
      "Iteration 7261: Weights = [55.30333333  2.31202776  5.53684731  0.09106655  0.17448857 13.23055667], Loss = 0.4628\n",
      "Iteration 7262: Weights = [55.30333333  2.31187837  5.53648956  0.09106067  0.1744773  13.23094205], Loss = 0.4627\n",
      "Iteration 7263: Weights = [55.30333333  2.311729    5.53613184  0.09105478  0.17446602 13.23132742], Loss = 0.4627\n",
      "Iteration 7264: Weights = [55.30333333  2.31157963  5.53577414  0.0910489   0.17445475 13.23171275], Loss = 0.4626\n",
      "Iteration 7265: Weights = [55.30333333  2.31143028  5.53541646  0.09104302  0.17444348 13.23209806], Loss = 0.4626\n",
      "Iteration 7266: Weights = [55.30333333  2.31128093  5.53505881  0.09103714  0.17443221 13.23248335], Loss = 0.4625\n",
      "Iteration 7267: Weights = [55.30333333  2.3111316   5.53470118  0.09103125  0.17442094 13.23286861], Loss = 0.4624\n",
      "Iteration 7268: Weights = [55.30333333  2.31098227  5.53434357  0.09102537  0.17440967 13.23325384], Loss = 0.4624\n",
      "Iteration 7269: Weights = [55.30333333  2.31083295  5.53398598  0.09101949  0.1743984  13.23363905], Loss = 0.4623\n",
      "Iteration 7270: Weights = [55.30333333  2.31068364  5.53362842  0.09101361  0.17438713 13.23402424], Loss = 0.4623\n",
      "Iteration 7271: Weights = [55.30333333  2.31053435  5.53327088  0.09100773  0.17437586 13.2344094 ], Loss = 0.4622\n",
      "Iteration 7272: Weights = [55.30333333  2.31038506  5.53291337  0.09100185  0.1743646  13.23479454], Loss = 0.4621\n",
      "Iteration 7273: Weights = [55.30333333  2.31023578  5.53255587  0.09099597  0.17435333 13.23517965], Loss = 0.4621\n",
      "Iteration 7274: Weights = [55.30333333  2.31008651  5.5321984   0.09099009  0.17434207 13.23556473], Loss = 0.4620\n",
      "Iteration 7275: Weights = [55.30333333  2.30993725  5.53184096  0.09098421  0.1743308  13.2359498 ], Loss = 0.4620\n",
      "Iteration 7276: Weights = [55.30333333  2.309788    5.53148353  0.09097833  0.17431954 13.23633483], Loss = 0.4619\n",
      "Iteration 7277: Weights = [55.30333333  2.30963876  5.53112613  0.09097245  0.17430827 13.23671984], Loss = 0.4618\n",
      "Iteration 7278: Weights = [55.30333333  2.30948953  5.53076876  0.09096658  0.17429701 13.23710483], Loss = 0.4618\n",
      "Iteration 7279: Weights = [55.30333333  2.30934031  5.5304114   0.0909607   0.17428575 13.23748979], Loss = 0.4617\n",
      "Iteration 7280: Weights = [55.30333333  2.3091911   5.53005407  0.09095482  0.17427449 13.23787473], Loss = 0.4617\n",
      "Iteration 7281: Weights = [55.30333333  2.3090419   5.52969676  0.09094894  0.17426323 13.23825964], Loss = 0.4616\n",
      "Iteration 7282: Weights = [55.30333333  2.30889271  5.52933948  0.09094307  0.17425197 13.23864453], Loss = 0.4615\n",
      "Iteration 7283: Weights = [55.30333333  2.30874352  5.52898222  0.09093719  0.17424071 13.23902939], Loss = 0.4615\n",
      "Iteration 7284: Weights = [55.30333333  2.30859435  5.52862498  0.09093132  0.17422945 13.23941423], Loss = 0.4614\n",
      "Iteration 7285: Weights = [55.30333333  2.30844519  5.52826776  0.09092544  0.17421819 13.23979904], Loss = 0.4614\n",
      "Iteration 7286: Weights = [55.30333333  2.30829603  5.52791057  0.09091957  0.17420694 13.24018383], Loss = 0.4613\n",
      "Iteration 7287: Weights = [55.30333333  2.30814689  5.5275534   0.09091369  0.17419568 13.24056859], Loss = 0.4612\n",
      "Iteration 7288: Weights = [55.30333333  2.30799776  5.52719625  0.09090782  0.17418443 13.24095333], Loss = 0.4612\n",
      "Iteration 7289: Weights = [55.30333333  2.30784863  5.52683913  0.09090194  0.17417317 13.24133804], Loss = 0.4611\n",
      "Iteration 7290: Weights = [55.30333333  2.30769952  5.52648203  0.09089607  0.17416192 13.24172273], Loss = 0.4611\n",
      "Iteration 7291: Weights = [55.30333333  2.30755041  5.52612495  0.0908902   0.17415067 13.2421074 ], Loss = 0.4610\n",
      "Iteration 7292: Weights = [55.30333333  2.30740132  5.5257679   0.09088432  0.17413941 13.24249204], Loss = 0.4609\n",
      "Iteration 7293: Weights = [55.30333333  2.30725223  5.52541087  0.09087845  0.17412816 13.24287665], Loss = 0.4609\n",
      "Iteration 7294: Weights = [55.30333333  2.30710315  5.52505386  0.09087258  0.17411691 13.24326124], Loss = 0.4608\n",
      "Iteration 7295: Weights = [55.30333333  2.30695409  5.52469688  0.09086671  0.17410566 13.2436458 ], Loss = 0.4608\n",
      "Iteration 7296: Weights = [55.30333333  2.30680503  5.52433991  0.09086084  0.17409441 13.24403034], Loss = 0.4607\n",
      "Iteration 7297: Weights = [55.30333333  2.30665598  5.52398298  0.09085497  0.17408316 13.24441486], Loss = 0.4606\n",
      "Iteration 7298: Weights = [55.30333333  2.30650695  5.52362606  0.0908491   0.17407192 13.24479935], Loss = 0.4606\n",
      "Iteration 7299: Weights = [55.30333333  2.30635792  5.52326917  0.09084323  0.17406067 13.24518381], Loss = 0.4605\n",
      "Iteration 7300: Weights = [55.30333333  2.3062089   5.5229123   0.09083736  0.17404942 13.24556825], Loss = 0.4605\n",
      "Iteration 7301: Weights = [55.30333333  2.30605989  5.52255545  0.09083149  0.17403818 13.24595267], Loss = 0.4604\n",
      "Iteration 7302: Weights = [55.30333333  2.30591089  5.52219863  0.09082562  0.17402693 13.24633706], Loss = 0.4603\n",
      "Iteration 7303: Weights = [55.30333333  2.3057619   5.52184183  0.09081975  0.17401569 13.24672142], Loss = 0.4603\n",
      "Iteration 7304: Weights = [55.30333333  2.30561292  5.52148505  0.09081388  0.17400444 13.24710576], Loss = 0.4602\n",
      "Iteration 7305: Weights = [55.30333333  2.30546395  5.5211283   0.09080802  0.1739932  13.24749008], Loss = 0.4602\n",
      "Iteration 7306: Weights = [55.30333333  2.30531499  5.52077157  0.09080215  0.17398196 13.24787437], Loss = 0.4601\n",
      "Iteration 7307: Weights = [55.30333333  2.30516604  5.52041486  0.09079628  0.17397072 13.24825864], Loss = 0.4601\n",
      "Iteration 7308: Weights = [55.30333333  2.3050171   5.52005817  0.09079041  0.17395948 13.24864288], Loss = 0.4600\n",
      "Iteration 7309: Weights = [55.30333333  2.30486817  5.51970151  0.09078455  0.17394824 13.24902709], Loss = 0.4599\n",
      "Iteration 7310: Weights = [55.30333333  2.30471925  5.51934487  0.09077868  0.173937   13.24941129], Loss = 0.4599\n",
      "Iteration 7311: Weights = [55.30333333  2.30457033  5.51898825  0.09077282  0.17392576 13.24979545], Loss = 0.4598\n",
      "Iteration 7312: Weights = [55.30333333  2.30442143  5.51863166  0.09076695  0.17391452 13.25017959], Loss = 0.4598\n",
      "Iteration 7313: Weights = [55.30333333  2.30427254  5.51827509  0.09076109  0.17390329 13.25056371], Loss = 0.4597\n",
      "Iteration 7314: Weights = [55.30333333  2.30412365  5.51791855  0.09075522  0.17389205 13.2509478 ], Loss = 0.4596\n",
      "Iteration 7315: Weights = [55.30333333  2.30397478  5.51756202  0.09074936  0.17388081 13.25133187], Loss = 0.4596\n",
      "Iteration 7316: Weights = [55.30333333  2.30382591  5.51720552  0.0907435   0.17386958 13.25171591], Loss = 0.4595\n",
      "Iteration 7317: Weights = [55.30333333  2.30367706  5.51684904  0.09073763  0.17385834 13.25209993], Loss = 0.4595\n",
      "Iteration 7318: Weights = [55.30333333  2.30352821  5.51649259  0.09073177  0.17384711 13.25248393], Loss = 0.4594\n",
      "Iteration 7319: Weights = [55.30333333  2.30337938  5.51613616  0.09072591  0.17383588 13.25286789], Loss = 0.4593\n",
      "Iteration 7320: Weights = [55.30333333  2.30323055  5.51577975  0.09072005  0.17382465 13.25325184], Loss = 0.4593\n",
      "Iteration 7321: Weights = [55.30333333  2.30308174  5.51542336  0.09071418  0.17381342 13.25363576], Loss = 0.4592\n",
      "Iteration 7322: Weights = [55.30333333  2.30293293  5.515067    0.09070832  0.17380218 13.25401965], Loss = 0.4592\n",
      "Iteration 7323: Weights = [55.30333333  2.30278413  5.51471066  0.09070246  0.17379096 13.25440352], Loss = 0.4591\n",
      "Iteration 7324: Weights = [55.30333333  2.30263534  5.51435434  0.0906966   0.17377973 13.25478736], Loss = 0.4590\n",
      "Iteration 7325: Weights = [55.30333333  2.30248657  5.51399805  0.09069074  0.1737685  13.25517118], Loss = 0.4590\n",
      "Iteration 7326: Weights = [55.30333333  2.3023378   5.51364178  0.09068488  0.17375727 13.25555498], Loss = 0.4589\n",
      "Iteration 7327: Weights = [55.30333333  2.30218904  5.51328553  0.09067902  0.17374604 13.25593875], Loss = 0.4589\n",
      "Iteration 7328: Weights = [55.30333333  2.30204029  5.51292931  0.09067316  0.17373482 13.25632249], Loss = 0.4588\n",
      "Iteration 7329: Weights = [55.30333333  2.30189155  5.51257311  0.09066731  0.17372359 13.25670621], Loss = 0.4587\n",
      "Iteration 7330: Weights = [55.30333333  2.30174282  5.51221693  0.09066145  0.17371237 13.25708991], Loss = 0.4587\n",
      "Iteration 7331: Weights = [55.30333333  2.3015941   5.51186077  0.09065559  0.17370114 13.25747358], Loss = 0.4586\n",
      "Iteration 7332: Weights = [55.30333333  2.30144539  5.51150464  0.09064973  0.17368992 13.25785722], Loss = 0.4586\n",
      "Iteration 7333: Weights = [55.30333333  2.30129669  5.51114853  0.09064387  0.1736787  13.25824085], Loss = 0.4585\n",
      "Iteration 7334: Weights = [55.30333333  2.301148    5.51079244  0.09063802  0.17366748 13.25862444], Loss = 0.4584\n",
      "Iteration 7335: Weights = [55.30333333  2.30099932  5.51043638  0.09063216  0.17365626 13.25900801], Loss = 0.4584\n",
      "Iteration 7336: Weights = [55.30333333  2.30085064  5.51008034  0.09062631  0.17364504 13.25939156], Loss = 0.4583\n",
      "Iteration 7337: Weights = [55.30333333  2.30070198  5.50972432  0.09062045  0.17363382 13.25977508], Loss = 0.4583\n",
      "Iteration 7338: Weights = [55.30333333  2.30055333  5.50936833  0.09061459  0.1736226  13.26015858], Loss = 0.4582\n",
      "Iteration 7339: Weights = [55.30333333  2.30040469  5.50901236  0.09060874  0.17361138 13.26054205], Loss = 0.4582\n",
      "Iteration 7340: Weights = [55.30333333  2.30025605  5.50865641  0.09060289  0.17360016 13.2609255 ], Loss = 0.4581\n",
      "Iteration 7341: Weights = [55.30333333  2.30010743  5.50830048  0.09059703  0.17358894 13.26130892], Loss = 0.4580\n",
      "Iteration 7342: Weights = [55.30333333  2.29995881  5.50794458  0.09059118  0.17357773 13.26169232], Loss = 0.4580\n",
      "Iteration 7343: Weights = [55.30333333  2.29981021  5.5075887   0.09058532  0.17356651 13.26207569], Loss = 0.4579\n",
      "Iteration 7344: Weights = [55.30333333  2.29966161  5.50723285  0.09057947  0.1735553  13.26245904], Loss = 0.4579\n",
      "Iteration 7345: Weights = [55.30333333  2.29951303  5.50687701  0.09057362  0.17354409 13.26284237], Loss = 0.4578\n",
      "Iteration 7346: Weights = [55.30333333  2.29936445  5.5065212   0.09056777  0.17353287 13.26322567], Loss = 0.4577\n",
      "Iteration 7347: Weights = [55.30333333  2.29921588  5.50616541  0.09056192  0.17352166 13.26360894], Loss = 0.4577\n",
      "Iteration 7348: Weights = [55.30333333  2.29906733  5.50580965  0.09055606  0.17351045 13.26399219], Loss = 0.4576\n",
      "Iteration 7349: Weights = [55.30333333  2.29891878  5.50545391  0.09055021  0.17349924 13.26437541], Loss = 0.4576\n",
      "Iteration 7350: Weights = [55.30333333  2.29877024  5.50509819  0.09054436  0.17348803 13.26475861], Loss = 0.4575\n",
      "Iteration 7351: Weights = [55.30333333  2.29862171  5.50474249  0.09053851  0.17347682 13.26514179], Loss = 0.4574\n",
      "Iteration 7352: Weights = [55.30333333  2.2984732   5.50438682  0.09053266  0.17346561 13.26552494], Loss = 0.4574\n",
      "Iteration 7353: Weights = [55.30333333  2.29832469  5.50403117  0.09052681  0.1734544  13.26590807], Loss = 0.4573\n",
      "Iteration 7354: Weights = [55.30333333  2.29817619  5.50367555  0.09052096  0.17344319 13.26629117], Loss = 0.4573\n",
      "Iteration 7355: Weights = [55.30333333  2.2980277   5.50331994  0.09051511  0.17343199 13.26667424], Loss = 0.4572\n",
      "Iteration 7356: Weights = [55.30333333  2.29787922  5.50296436  0.09050927  0.17342078 13.26705729], Loss = 0.4571\n",
      "Iteration 7357: Weights = [55.30333333  2.29773075  5.5026088   0.09050342  0.17340958 13.26744032], Loss = 0.4571\n",
      "Iteration 7358: Weights = [55.30333333  2.29758229  5.50225327  0.09049757  0.17339837 13.26782332], Loss = 0.4570\n",
      "Iteration 7359: Weights = [55.30333333  2.29743383  5.50189776  0.09049172  0.17338717 13.2682063 ], Loss = 0.4570\n",
      "Iteration 7360: Weights = [55.30333333  2.29728539  5.50154227  0.09048588  0.17337597 13.26858925], Loss = 0.4569\n",
      "Iteration 7361: Weights = [55.30333333  2.29713696  5.5011868   0.09048003  0.17336476 13.26897218], Loss = 0.4569\n",
      "Iteration 7362: Weights = [55.30333333  2.29698854  5.50083136  0.09047418  0.17335356 13.26935508], Loss = 0.4568\n",
      "Iteration 7363: Weights = [55.30333333  2.29684012  5.50047594  0.09046834  0.17334236 13.26973796], Loss = 0.4567\n",
      "Iteration 7364: Weights = [55.30333333  2.29669172  5.50012054  0.09046249  0.17333116 13.27012081], Loss = 0.4567\n",
      "Iteration 7365: Weights = [55.30333333  2.29654333  5.49976517  0.09045665  0.17331996 13.27050364], Loss = 0.4566\n",
      "Iteration 7366: Weights = [55.30333333  2.29639494  5.49940982  0.0904508   0.17330876 13.27088645], Loss = 0.4566\n",
      "Iteration 7367: Weights = [55.30333333  2.29624657  5.49905449  0.09044496  0.17329757 13.27126923], Loss = 0.4565\n",
      "Iteration 7368: Weights = [55.30333333  2.2960982   5.49869919  0.09043912  0.17328637 13.27165198], Loss = 0.4564\n",
      "Iteration 7369: Weights = [55.30333333  2.29594985  5.4983439   0.09043327  0.17327517 13.27203471], Loss = 0.4564\n",
      "Iteration 7370: Weights = [55.30333333  2.2958015   5.49798865  0.09042743  0.17326398 13.27241742], Loss = 0.4563\n",
      "Iteration 7371: Weights = [55.30333333  2.29565317  5.49763341  0.09042159  0.17325278 13.2728001 ], Loss = 0.4563\n",
      "Iteration 7372: Weights = [55.30333333  2.29550484  5.4972782   0.09041574  0.17324159 13.27318275], Loss = 0.4562\n",
      "Iteration 7373: Weights = [55.30333333  2.29535652  5.49692301  0.0904099   0.17323039 13.27356538], Loss = 0.4561\n",
      "Iteration 7374: Weights = [55.30333333  2.29520821  5.49656784  0.09040406  0.1732192  13.27394799], Loss = 0.4561\n",
      "Iteration 7375: Weights = [55.30333333  2.29505992  5.49621269  0.09039822  0.17320801 13.27433057], Loss = 0.4560\n",
      "Iteration 7376: Weights = [55.30333333  2.29491163  5.49585757  0.09039238  0.17319682 13.27471313], Loss = 0.4560\n",
      "Iteration 7377: Weights = [55.30333333  2.29476335  5.49550248  0.09038654  0.17318563 13.27509566], Loss = 0.4559\n",
      "Iteration 7378: Weights = [55.30333333  2.29461508  5.4951474   0.0903807   0.17317444 13.27547817], Loss = 0.4558\n",
      "Iteration 7379: Weights = [55.30333333  2.29446682  5.49479235  0.09037486  0.17316325 13.27586065], Loss = 0.4558\n",
      "Iteration 7380: Weights = [55.30333333  2.29431857  5.49443732  0.09036902  0.17315206 13.27624311], Loss = 0.4557\n",
      "Iteration 7381: Weights = [55.30333333  2.29417033  5.49408231  0.09036318  0.17314087 13.27662554], Loss = 0.4557\n",
      "Iteration 7382: Weights = [55.30333333  2.2940221   5.49372733  0.09035734  0.17312969 13.27700795], Loss = 0.4556\n",
      "Iteration 7383: Weights = [55.30333333  2.29387388  5.49337237  0.0903515   0.1731185  13.27739033], Loss = 0.4556\n",
      "Iteration 7384: Weights = [55.30333333  2.29372566  5.49301743  0.09034567  0.17310731 13.27777269], Loss = 0.4555\n",
      "Iteration 7385: Weights = [55.30333333  2.29357746  5.49266251  0.09033983  0.17309613 13.27815503], Loss = 0.4554\n",
      "Iteration 7386: Weights = [55.30333333  2.29342927  5.49230762  0.09033399  0.17308494 13.27853734], Loss = 0.4554\n",
      "Iteration 7387: Weights = [55.30333333  2.29328109  5.49195275  0.09032815  0.17307376 13.27891962], Loss = 0.4553\n",
      "Iteration 7388: Weights = [55.30333333  2.29313291  5.49159791  0.09032232  0.17306258 13.27930188], Loss = 0.4553\n",
      "Iteration 7389: Weights = [55.30333333  2.29298475  5.49124308  0.09031648  0.1730514  13.27968412], Loss = 0.4552\n",
      "Iteration 7390: Weights = [55.30333333  2.29283659  5.49088828  0.09031065  0.17304022 13.28006633], Loss = 0.4551\n",
      "Iteration 7391: Weights = [55.30333333  2.29268845  5.49053351  0.09030481  0.17302903 13.28044851], Loss = 0.4551\n",
      "Iteration 7392: Weights = [55.30333333  2.29254031  5.49017875  0.09029898  0.17301786 13.28083068], Loss = 0.4550\n",
      "Iteration 7393: Weights = [55.30333333  2.29239219  5.48982402  0.09029314  0.17300668 13.28121281], Loss = 0.4550\n",
      "Iteration 7394: Weights = [55.30333333  2.29224407  5.48946931  0.09028731  0.1729955  13.28159492], Loss = 0.4549\n",
      "Iteration 7395: Weights = [55.30333333  2.29209597  5.48911463  0.09028147  0.17298432 13.28197701], Loss = 0.4548\n",
      "Iteration 7396: Weights = [55.30333333  2.29194787  5.48875996  0.09027564  0.17297314 13.28235907], Loss = 0.4548\n",
      "Iteration 7397: Weights = [55.30333333  2.29179978  5.48840532  0.09026981  0.17296197 13.28274111], Loss = 0.4547\n",
      "Iteration 7398: Weights = [55.30333333  2.2916517   5.48805071  0.09026398  0.17295079 13.28312313], Loss = 0.4547\n",
      "Iteration 7399: Weights = [55.30333333  2.29150364  5.48769611  0.09025814  0.17293962 13.28350511], Loss = 0.4546\n",
      "Iteration 7400: Weights = [55.30333333  2.29135558  5.48734154  0.09025231  0.17292844 13.28388708], Loss = 0.4546\n",
      "Iteration 7401: Weights = [55.30333333  2.29120753  5.48698699  0.09024648  0.17291727 13.28426902], Loss = 0.4545\n",
      "Iteration 7402: Weights = [55.30333333  2.29105949  5.48663247  0.09024065  0.1729061  13.28465093], Loss = 0.4544\n",
      "Iteration 7403: Weights = [55.30333333  2.29091146  5.48627797  0.09023482  0.17289493 13.28503282], Loss = 0.4544\n",
      "Iteration 7404: Weights = [55.30333333  2.29076344  5.48592349  0.09022899  0.17288375 13.28541469], Loss = 0.4543\n",
      "Iteration 7405: Weights = [55.30333333  2.29061543  5.48556903  0.09022316  0.17287258 13.28579653], Loss = 0.4543\n",
      "Iteration 7406: Weights = [55.30333333  2.29046743  5.4852146   0.09021733  0.17286141 13.28617834], Loss = 0.4542\n",
      "Iteration 7407: Weights = [55.30333333  2.29031943  5.48486019  0.0902115   0.17285025 13.28656014], Loss = 0.4541\n",
      "Iteration 7408: Weights = [55.30333333  2.29017145  5.4845058   0.09020567  0.17283908 13.2869419 ], Loss = 0.4541\n",
      "Iteration 7409: Weights = [55.30333333  2.29002348  5.48415143  0.09019984  0.17282791 13.28732364], Loss = 0.4540\n",
      "Iteration 7410: Weights = [55.30333333  2.28987552  5.48379709  0.09019402  0.17281674 13.28770536], Loss = 0.4540\n",
      "Iteration 7411: Weights = [55.30333333  2.28972756  5.48344277  0.09018819  0.17280558 13.28808705], Loss = 0.4539\n",
      "Iteration 7412: Weights = [55.30333333  2.28957962  5.48308848  0.09018236  0.17279441 13.28846872], Loss = 0.4539\n",
      "Iteration 7413: Weights = [55.30333333  2.28943169  5.4827342   0.09017653  0.17278325 13.28885036], Loss = 0.4538\n",
      "Iteration 7414: Weights = [55.30333333  2.28928376  5.48237995  0.09017071  0.17277208 13.28923198], Loss = 0.4537\n",
      "Iteration 7415: Weights = [55.30333333  2.28913585  5.48202572  0.09016488  0.17276092 13.28961358], Loss = 0.4537\n",
      "Iteration 7416: Weights = [55.30333333  2.28898794  5.48167152  0.09015906  0.17274976 13.28999515], Loss = 0.4536\n",
      "Iteration 7417: Weights = [55.30333333  2.28884004  5.48131734  0.09015323  0.1727386  13.29037669], Loss = 0.4536\n",
      "Iteration 7418: Weights = [55.30333333  2.28869216  5.48096318  0.09014741  0.17272743 13.29075821], Loss = 0.4535\n",
      "Iteration 7419: Weights = [55.30333333  2.28854428  5.48060904  0.09014158  0.17271627 13.29113971], Loss = 0.4534\n",
      "Iteration 7420: Weights = [55.30333333  2.28839641  5.48025493  0.09013576  0.17270512 13.29152118], Loss = 0.4534\n",
      "Iteration 7421: Weights = [55.30333333  2.28824855  5.47990084  0.09012993  0.17269396 13.29190262], Loss = 0.4533\n",
      "Iteration 7422: Weights = [55.30333333  2.28810071  5.47954677  0.09012411  0.1726828  13.29228404], Loss = 0.4533\n",
      "Iteration 7423: Weights = [55.30333333  2.28795287  5.47919273  0.09011829  0.17267164 13.29266544], Loss = 0.4532\n",
      "Iteration 7424: Weights = [55.30333333  2.28780504  5.47883871  0.09011246  0.17266048 13.29304681], Loss = 0.4531\n",
      "Iteration 7425: Weights = [55.30333333  2.28765722  5.47848471  0.09010664  0.17264933 13.29342816], Loss = 0.4531\n",
      "Iteration 7426: Weights = [55.30333333  2.28750941  5.47813073  0.09010082  0.17263817 13.29380948], Loss = 0.4530\n",
      "Iteration 7427: Weights = [55.30333333  2.28736161  5.47777678  0.090095    0.17262702 13.29419078], Loss = 0.4530\n",
      "Iteration 7428: Weights = [55.30333333  2.28721382  5.47742285  0.09008918  0.17261586 13.29457206], Loss = 0.4529\n",
      "Iteration 7429: Weights = [55.30333333  2.28706604  5.47706894  0.09008336  0.17260471 13.2949533 ], Loss = 0.4529\n",
      "Iteration 7430: Weights = [55.30333333  2.28691826  5.47671506  0.09007753  0.17259356 13.29533453], Loss = 0.4528\n",
      "Iteration 7431: Weights = [55.30333333  2.2867705   5.47636119  0.09007171  0.17258241 13.29571573], Loss = 0.4527\n",
      "Iteration 7432: Weights = [55.30333333  2.28662275  5.47600736  0.09006589  0.17257126 13.2960969 ], Loss = 0.4527\n",
      "Iteration 7433: Weights = [55.30333333  2.28647501  5.47565354  0.09006008  0.17256011 13.29647805], Loss = 0.4526\n",
      "Iteration 7434: Weights = [55.30333333  2.28632727  5.47529975  0.09005426  0.17254896 13.29685918], Loss = 0.4526\n",
      "Iteration 7435: Weights = [55.30333333  2.28617955  5.47494598  0.09004844  0.17253781 13.29724028], Loss = 0.4525\n",
      "Iteration 7436: Weights = [55.30333333  2.28603183  5.47459223  0.09004262  0.17252666 13.29762136], Loss = 0.4524\n",
      "Iteration 7437: Weights = [55.30333333  2.28588413  5.4742385   0.0900368   0.17251551 13.29800241], Loss = 0.4524\n",
      "Iteration 7438: Weights = [55.30333333  2.28573643  5.4738848   0.09003098  0.17250437 13.29838344], Loss = 0.4523\n",
      "Iteration 7439: Weights = [55.30333333  2.28558875  5.47353112  0.09002517  0.17249322 13.29876444], Loss = 0.4523\n",
      "Iteration 7440: Weights = [55.30333333  2.28544107  5.47317747  0.09001935  0.17248208 13.29914542], Loss = 0.4522\n",
      "Iteration 7441: Weights = [55.30333333  2.2852934   5.47282384  0.09001353  0.17247093 13.29952637], Loss = 0.4522\n",
      "Iteration 7442: Weights = [55.30333333  2.28514574  5.47247023  0.09000772  0.17245979 13.2999073 ], Loss = 0.4521\n",
      "Iteration 7443: Weights = [55.30333333  2.2849981   5.47211664  0.0900019   0.17244864 13.3002882 ], Loss = 0.4520\n",
      "Iteration 7444: Weights = [55.30333333  2.28485046  5.47176307  0.08999609  0.1724375  13.30066908], Loss = 0.4520\n",
      "Iteration 7445: Weights = [55.30333333  2.28470283  5.47140953  0.08999027  0.17242636 13.30104994], Loss = 0.4519\n",
      "Iteration 7446: Weights = [55.30333333  2.28455521  5.47105601  0.08998446  0.17241522 13.30143077], Loss = 0.4519\n",
      "Iteration 7447: Weights = [55.30333333  2.2844076   5.47070252  0.08997864  0.17240408 13.30181157], Loss = 0.4518\n",
      "Iteration 7448: Weights = [55.30333333  2.28426     5.47034904  0.08997283  0.17239294 13.30219236], Loss = 0.4517\n",
      "Iteration 7449: Weights = [55.30333333  2.28411241  5.46999559  0.08996702  0.1723818  13.30257311], Loss = 0.4517\n",
      "Iteration 7450: Weights = [55.30333333  2.28396483  5.46964217  0.0899612   0.17237066 13.30295384], Loss = 0.4516\n",
      "Iteration 7451: Weights = [55.30333333  2.28381726  5.46928876  0.08995539  0.17235953 13.30333455], Loss = 0.4516\n",
      "Iteration 7452: Weights = [55.30333333  2.2836697   5.46893538  0.08994958  0.17234839 13.30371523], Loss = 0.4515\n",
      "Iteration 7453: Weights = [55.30333333  2.28352214  5.46858202  0.08994377  0.17233725 13.30409589], Loss = 0.4515\n",
      "Iteration 7454: Weights = [55.30333333  2.2833746   5.46822869  0.08993796  0.17232612 13.30447653], Loss = 0.4514\n",
      "Iteration 7455: Weights = [55.30333333  2.28322707  5.46787537  0.08993215  0.17231498 13.30485713], Loss = 0.4513\n",
      "Iteration 7456: Weights = [55.30333333  2.28307954  5.46752208  0.08992633  0.17230385 13.30523772], Loss = 0.4513\n",
      "Iteration 7457: Weights = [55.30333333  2.28293203  5.46716881  0.08992052  0.17229272 13.30561828], Loss = 0.4512\n",
      "Iteration 7458: Weights = [55.30333333  2.28278452  5.46681557  0.08991471  0.17228159 13.30599881], Loss = 0.4512\n",
      "Iteration 7459: Weights = [55.30333333  2.28263703  5.46646235  0.0899089   0.17227045 13.30637932], Loss = 0.4511\n",
      "Iteration 7460: Weights = [55.30333333  2.28248954  5.46610915  0.0899031   0.17225932 13.30675981], Loss = 0.4510\n",
      "Iteration 7461: Weights = [55.30333333  2.28234207  5.46575597  0.08989729  0.17224819 13.30714027], Loss = 0.4510\n",
      "Iteration 7462: Weights = [55.30333333  2.2821946   5.46540282  0.08989148  0.17223706 13.30752071], Loss = 0.4509\n",
      "Iteration 7463: Weights = [55.30333333  2.28204714  5.46504969  0.08988567  0.17222594 13.30790112], Loss = 0.4509\n",
      "Iteration 7464: Weights = [55.30333333  2.28189969  5.46469658  0.08987986  0.17221481 13.30828151], Loss = 0.4508\n",
      "Iteration 7465: Weights = [55.30333333  2.28175226  5.46434349  0.08987406  0.17220368 13.30866187], Loss = 0.4508\n",
      "Iteration 7466: Weights = [55.30333333  2.28160483  5.46399043  0.08986825  0.17219255 13.30904221], Loss = 0.4507\n",
      "Iteration 7467: Weights = [55.30333333  2.28145741  5.46363739  0.08986244  0.17218143 13.30942252], Loss = 0.4506\n",
      "Iteration 7468: Weights = [55.30333333  2.28131     5.46328438  0.08985664  0.1721703  13.30980281], Loss = 0.4506\n",
      "Iteration 7469: Weights = [55.30333333  2.2811626   5.46293138  0.08985083  0.17215918 13.31018308], Loss = 0.4505\n",
      "Iteration 7470: Weights = [55.30333333  2.28101521  5.46257841  0.08984502  0.17214806 13.31056332], Loss = 0.4505\n",
      "Iteration 7471: Weights = [55.30333333  2.28086783  5.46222546  0.08983922  0.17213693 13.31094353], Loss = 0.4504\n",
      "Iteration 7472: Weights = [55.30333333  2.28072046  5.46187254  0.08983341  0.17212581 13.31132373], Loss = 0.4503\n",
      "Iteration 7473: Weights = [55.30333333  2.28057309  5.46151964  0.08982761  0.17211469 13.31170389], Loss = 0.4503\n",
      "Iteration 7474: Weights = [55.30333333  2.28042574  5.46116676  0.08982181  0.17210357 13.31208403], Loss = 0.4502\n",
      "Iteration 7475: Weights = [55.30333333  2.2802784   5.4608139   0.089816    0.17209245 13.31246415], Loss = 0.4502\n",
      "Iteration 7476: Weights = [55.30333333  2.28013107  5.46046107  0.0898102   0.17208133 13.31284424], Loss = 0.4501\n",
      "Iteration 7477: Weights = [55.30333333  2.27998374  5.46010825  0.0898044   0.17207021 13.31322431], Loss = 0.4501\n",
      "Iteration 7478: Weights = [55.30333333  2.27983643  5.45975547  0.08979859  0.17205909 13.31360436], Loss = 0.4500\n",
      "Iteration 7479: Weights = [55.30333333  2.27968912  5.4594027   0.08979279  0.17204798 13.31398438], Loss = 0.4499\n",
      "Iteration 7480: Weights = [55.30333333  2.27954183  5.45904996  0.08978699  0.17203686 13.31436437], Loss = 0.4499\n",
      "Iteration 7481: Weights = [55.30333333  2.27939454  5.45869724  0.08978119  0.17202574 13.31474434], Loss = 0.4498\n",
      "Iteration 7482: Weights = [55.30333333  2.27924727  5.45834454  0.08977539  0.17201463 13.31512429], Loss = 0.4498\n",
      "Iteration 7483: Weights = [55.30333333  2.2791      5.45799186  0.08976959  0.17200352 13.31550421], Loss = 0.4497\n",
      "Iteration 7484: Weights = [55.30333333  2.27895274  5.45763921  0.08976379  0.1719924  13.3158841 ], Loss = 0.4496\n",
      "Iteration 7485: Weights = [55.30333333  2.27880549  5.45728658  0.08975799  0.17198129 13.31626398], Loss = 0.4496\n",
      "Iteration 7486: Weights = [55.30333333  2.27865826  5.45693398  0.08975219  0.17197018 13.31664382], Loss = 0.4495\n",
      "Iteration 7487: Weights = [55.30333333  2.27851103  5.45658139  0.08974639  0.17195907 13.31702365], Loss = 0.4495\n",
      "Iteration 7488: Weights = [55.30333333  2.27836381  5.45622883  0.08974059  0.17194796 13.31740344], Loss = 0.4494\n",
      "Iteration 7489: Weights = [55.30333333  2.2782166   5.45587629  0.08973479  0.17193685 13.31778322], Loss = 0.4494\n",
      "Iteration 7490: Weights = [55.30333333  2.2780694   5.45552378  0.08972899  0.17192574 13.31816297], Loss = 0.4493\n",
      "Iteration 7491: Weights = [55.30333333  2.27792221  5.45517129  0.0897232   0.17191463 13.31854269], Loss = 0.4492\n",
      "Iteration 7492: Weights = [55.30333333  2.27777503  5.45481882  0.0897174   0.17190352 13.31892239], Loss = 0.4492\n",
      "Iteration 7493: Weights = [55.30333333  2.27762785  5.45446637  0.0897116   0.17189241 13.31930207], Loss = 0.4491\n",
      "Iteration 7494: Weights = [55.30333333  2.27748069  5.45411395  0.08970581  0.17188131 13.31968172], Loss = 0.4491\n",
      "Iteration 7495: Weights = [55.30333333  2.27733354  5.45376155  0.08970001  0.1718702  13.32006135], Loss = 0.4490\n",
      "Iteration 7496: Weights = [55.30333333  2.2771864   5.45340917  0.08969421  0.1718591  13.32044095], Loss = 0.4490\n",
      "Iteration 7497: Weights = [55.30333333  2.27703926  5.45305681  0.08968842  0.17184799 13.32082052], Loss = 0.4489\n",
      "Iteration 7498: Weights = [55.30333333  2.27689214  5.45270448  0.08968262  0.17183689 13.32120008], Loss = 0.4488\n",
      "Iteration 7499: Weights = [55.30333333  2.27674502  5.45235217  0.08967683  0.17182579 13.32157961], Loss = 0.4488\n",
      "Iteration 7500: Weights = [55.30333333  2.27659792  5.45199988  0.08967104  0.17181468 13.32195911], Loss = 0.4487\n",
      "Iteration 7501: Weights = [55.30333333  2.27645082  5.45164762  0.08966524  0.17180358 13.32233859], Loss = 0.4487\n",
      "Iteration 7502: Weights = [55.30333333  2.27630374  5.45129537  0.08965945  0.17179248 13.32271804], Loss = 0.4486\n",
      "Iteration 7503: Weights = [55.30333333  2.27615666  5.45094316  0.08965365  0.17178138 13.32309747], Loss = 0.4485\n",
      "Iteration 7504: Weights = [55.30333333  2.27600959  5.45059096  0.08964786  0.17177028 13.32347688], Loss = 0.4485\n",
      "Iteration 7505: Weights = [55.30333333  2.27586254  5.45023879  0.08964207  0.17175918 13.32385626], Loss = 0.4484\n",
      "Iteration 7506: Weights = [55.30333333  2.27571549  5.44988663  0.08963628  0.17174809 13.32423562], Loss = 0.4484\n",
      "Iteration 7507: Weights = [55.30333333  2.27556845  5.44953451  0.08963049  0.17173699 13.32461495], Loss = 0.4483\n",
      "Iteration 7508: Weights = [55.30333333  2.27542142  5.4491824   0.08962469  0.17172589 13.32499426], Loss = 0.4483\n",
      "Iteration 7509: Weights = [55.30333333  2.2752744   5.44883032  0.0896189   0.1717148  13.32537354], Loss = 0.4482\n",
      "Iteration 7510: Weights = [55.30333333  2.27512739  5.44847826  0.08961311  0.1717037  13.3257528 ], Loss = 0.4481\n",
      "Iteration 7511: Weights = [55.30333333  2.27498039  5.44812622  0.08960732  0.17169261 13.32613203], Loss = 0.4481\n",
      "Iteration 7512: Weights = [55.30333333  2.2748334   5.44777421  0.08960153  0.17168152 13.32651124], Loss = 0.4480\n",
      "Iteration 7513: Weights = [55.30333333  2.27468642  5.44742222  0.08959574  0.17167042 13.32689043], Loss = 0.4480\n",
      "Iteration 7514: Weights = [55.30333333  2.27453945  5.44707025  0.08958996  0.17165933 13.32726959], Loss = 0.4479\n",
      "Iteration 7515: Weights = [55.30333333  2.27439248  5.4467183   0.08958417  0.17164824 13.32764873], Loss = 0.4479\n",
      "Iteration 7516: Weights = [55.30333333  2.27424553  5.44636638  0.08957838  0.17163715 13.32802784], Loss = 0.4478\n",
      "Iteration 7517: Weights = [55.30333333  2.27409859  5.44601448  0.08957259  0.17162606 13.32840693], Loss = 0.4477\n",
      "Iteration 7518: Weights = [55.30333333  2.27395165  5.4456626   0.0895668   0.17161497 13.32878599], Loss = 0.4477\n",
      "Iteration 7519: Weights = [55.30333333  2.27380473  5.44531074  0.08956102  0.17160388 13.32916503], Loss = 0.4476\n",
      "Iteration 7520: Weights = [55.30333333  2.27365781  5.44495891  0.08955523  0.17159279 13.32954404], Loss = 0.4476\n",
      "Iteration 7521: Weights = [55.30333333  2.27351091  5.4446071   0.08954944  0.17158171 13.32992303], Loss = 0.4475\n",
      "Iteration 7522: Weights = [55.30333333  2.27336401  5.44425531  0.08954366  0.17157062 13.33030199], Loss = 0.4474\n",
      "Iteration 7523: Weights = [55.30333333  2.27321712  5.44390355  0.08953787  0.17155954 13.33068094], Loss = 0.4474\n",
      "Iteration 7524: Weights = [55.30333333  2.27307025  5.44355181  0.08953209  0.17154845 13.33105985], Loss = 0.4473\n",
      "Iteration 7525: Weights = [55.30333333  2.27292338  5.44320009  0.0895263   0.17153737 13.33143874], Loss = 0.4473\n",
      "Iteration 7526: Weights = [55.30333333  2.27277652  5.44284839  0.08952052  0.17152628 13.33181761], Loss = 0.4472\n",
      "Iteration 7527: Weights = [55.30333333  2.27262967  5.44249672  0.08951473  0.1715152  13.33219645], Loss = 0.4472\n",
      "Iteration 7528: Weights = [55.30333333  2.27248283  5.44214507  0.08950895  0.17150412 13.33257527], Loss = 0.4471\n",
      "Iteration 7529: Weights = [55.30333333  2.272336    5.44179344  0.08950317  0.17149304 13.33295406], Loss = 0.4470\n",
      "Iteration 7530: Weights = [55.30333333  2.27218918  5.44144184  0.08949738  0.17148196 13.33333283], Loss = 0.4470\n",
      "Iteration 7531: Weights = [55.30333333  2.27204237  5.44109025  0.0894916   0.17147088 13.33371158], Loss = 0.4469\n",
      "Iteration 7532: Weights = [55.30333333  2.27189557  5.44073869  0.08948582  0.1714598  13.3340903 ], Loss = 0.4469\n",
      "Iteration 7533: Weights = [55.30333333  2.27174878  5.44038716  0.08948004  0.17144872 13.33446899], Loss = 0.4468\n",
      "Iteration 7534: Weights = [55.30333333  2.271602    5.44003564  0.08947425  0.17143764 13.33484766], Loss = 0.4468\n",
      "Iteration 7535: Weights = [55.30333333  2.27145523  5.43968415  0.08946847  0.17142656 13.33522631], Loss = 0.4467\n",
      "Iteration 7536: Weights = [55.30333333  2.27130846  5.43933268  0.08946269  0.17141549 13.33560493], Loss = 0.4466\n",
      "Iteration 7537: Weights = [55.30333333  2.27116171  5.43898124  0.08945691  0.17140441 13.33598353], Loss = 0.4466\n",
      "Iteration 7538: Weights = [55.30333333  2.27101496  5.43862981  0.08945113  0.17139334 13.3363621 ], Loss = 0.4465\n",
      "Iteration 7539: Weights = [55.30333333  2.27086823  5.43827841  0.08944535  0.17138226 13.33674065], Loss = 0.4465\n",
      "Iteration 7540: Weights = [55.30333333  2.2707215   5.43792703  0.08943957  0.17137119 13.33711918], Loss = 0.4464\n",
      "Iteration 7541: Weights = [55.30333333  2.27057479  5.43757568  0.08943379  0.17136012 13.33749768], Loss = 0.4463\n",
      "Iteration 7542: Weights = [55.30333333  2.27042808  5.43722434  0.08942802  0.17134905 13.33787615], Loss = 0.4463\n",
      "Iteration 7543: Weights = [55.30333333  2.27028138  5.43687303  0.08942224  0.17133798 13.3382546 ], Loss = 0.4462\n",
      "Iteration 7544: Weights = [55.30333333  2.2701347   5.43652175  0.08941646  0.1713269  13.33863303], Loss = 0.4462\n",
      "Iteration 7545: Weights = [55.30333333  2.26998802  5.43617048  0.08941068  0.17131583 13.33901143], Loss = 0.4461\n",
      "Iteration 7546: Weights = [55.30333333  2.26984135  5.43581924  0.08940491  0.17130477 13.33938981], Loss = 0.4461\n",
      "Iteration 7547: Weights = [55.30333333  2.26969469  5.43546802  0.08939913  0.1712937  13.33976816], Loss = 0.4460\n",
      "Iteration 7548: Weights = [55.30333333  2.26954804  5.43511683  0.08939335  0.17128263 13.34014649], Loss = 0.4459\n",
      "Iteration 7549: Weights = [55.30333333  2.2694014   5.43476565  0.08938758  0.17127156 13.34052479], Loss = 0.4459\n",
      "Iteration 7550: Weights = [55.30333333  2.26925477  5.4344145   0.0893818   0.1712605  13.34090307], Loss = 0.4458\n",
      "Iteration 7551: Weights = [55.30333333  2.26910815  5.43406337  0.08937603  0.17124943 13.34128133], Loss = 0.4458\n",
      "Iteration 7552: Weights = [55.30333333  2.26896154  5.43371227  0.08937025  0.17123837 13.34165956], Loss = 0.4457\n",
      "Iteration 7553: Weights = [55.30333333  2.26881494  5.43336118  0.08936448  0.1712273  13.34203777], Loss = 0.4457\n",
      "Iteration 7554: Weights = [55.30333333  2.26866834  5.43301012  0.0893587   0.17121624 13.34241595], Loss = 0.4456\n",
      "Iteration 7555: Weights = [55.30333333  2.26852176  5.43265909  0.08935293  0.17120518 13.34279411], Loss = 0.4455\n",
      "Iteration 7556: Weights = [55.30333333  2.26837519  5.43230807  0.08934716  0.17119411 13.34317224], Loss = 0.4455\n",
      "Iteration 7557: Weights = [55.30333333  2.26822862  5.43195708  0.08934138  0.17118305 13.34355035], Loss = 0.4454\n",
      "Iteration 7558: Weights = [55.30333333  2.26808207  5.43160611  0.08933561  0.17117199 13.34392843], Loss = 0.4454\n",
      "Iteration 7559: Weights = [55.30333333  2.26793552  5.43125516  0.08932984  0.17116093 13.34430649], Loss = 0.4453\n",
      "Iteration 7560: Weights = [55.30333333  2.26778899  5.43090424  0.08932407  0.17114987 13.34468453], Loss = 0.4453\n",
      "Iteration 7561: Weights = [55.30333333  2.26764246  5.43055334  0.0893183   0.17113882 13.34506254], Loss = 0.4452\n",
      "Iteration 7562: Weights = [55.30333333  2.26749594  5.43020246  0.08931252  0.17112776 13.34544053], Loss = 0.4451\n",
      "Iteration 7563: Weights = [55.30333333  2.26734944  5.4298516   0.08930675  0.1711167  13.34581849], Loss = 0.4451\n",
      "Iteration 7564: Weights = [55.30333333  2.26720294  5.42950077  0.08930098  0.17110565 13.34619643], Loss = 0.4450\n",
      "Iteration 7565: Weights = [55.30333333  2.26705645  5.42914996  0.08929521  0.17109459 13.34657434], Loss = 0.4450\n",
      "Iteration 7566: Weights = [55.30333333  2.26690997  5.42879917  0.08928944  0.17108353 13.34695223], Loss = 0.4449\n",
      "Iteration 7567: Weights = [55.30333333  2.2667635   5.4284484   0.08928368  0.17107248 13.34733009], Loss = 0.4449\n",
      "Iteration 7568: Weights = [55.30333333  2.26661704  5.42809766  0.08927791  0.17106143 13.34770793], Loss = 0.4448\n",
      "Iteration 7569: Weights = [55.30333333  2.26647059  5.42774694  0.08927214  0.17105037 13.34808575], Loss = 0.4447\n",
      "Iteration 7570: Weights = [55.30333333  2.26632415  5.42739624  0.08926637  0.17103932 13.34846354], Loss = 0.4447\n",
      "Iteration 7571: Weights = [55.30333333  2.26617772  5.42704557  0.0892606   0.17102827 13.34884131], Loss = 0.4446\n",
      "Iteration 7572: Weights = [55.30333333  2.26603129  5.42669491  0.08925484  0.17101722 13.34921905], Loss = 0.4446\n",
      "Iteration 7573: Weights = [55.30333333  2.26588488  5.42634428  0.08924907  0.17100617 13.34959677], Loss = 0.4445\n",
      "Iteration 7574: Weights = [55.30333333  2.26573848  5.42599368  0.0892433   0.17099512 13.34997446], Loss = 0.4444\n",
      "Iteration 7575: Weights = [55.30333333  2.26559208  5.42564309  0.08923754  0.17098407 13.35035213], Loss = 0.4444\n",
      "Iteration 7576: Weights = [55.30333333  2.2654457   5.42529253  0.08923177  0.17097303 13.35072978], Loss = 0.4443\n",
      "Iteration 7577: Weights = [55.30333333  2.26529933  5.42494199  0.089226    0.17096198 13.3511074 ], Loss = 0.4443\n",
      "Iteration 7578: Weights = [55.30333333  2.26515296  5.42459148  0.08922024  0.17095093 13.35148499], Loss = 0.4442\n",
      "Iteration 7579: Weights = [55.30333333  2.2650066   5.42424098  0.08921447  0.17093989 13.35186256], Loss = 0.4442\n",
      "Iteration 7580: Weights = [55.30333333  2.26486026  5.42389051  0.08920871  0.17092884 13.35224011], Loss = 0.4441\n",
      "Iteration 7581: Weights = [55.30333333  2.26471392  5.42354006  0.08920295  0.1709178  13.35261763], Loss = 0.4440\n",
      "Iteration 7582: Weights = [55.30333333  2.26456759  5.42318964  0.08919718  0.17090676 13.35299513], Loss = 0.4440\n",
      "Iteration 7583: Weights = [55.30333333  2.26442127  5.42283923  0.08919142  0.17089571 13.35337261], Loss = 0.4439\n",
      "Iteration 7584: Weights = [55.30333333  2.26427496  5.42248885  0.08918566  0.17088467 13.35375006], Loss = 0.4439\n",
      "Iteration 7585: Weights = [55.30333333  2.26412867  5.42213849  0.08917989  0.17087363 13.35412748], Loss = 0.4438\n",
      "Iteration 7586: Weights = [55.30333333  2.26398238  5.42178816  0.08917413  0.17086259 13.35450488], Loss = 0.4438\n",
      "Iteration 7587: Weights = [55.30333333  2.2638361   5.42143785  0.08916837  0.17085155 13.35488226], Loss = 0.4437\n",
      "Iteration 7588: Weights = [55.30333333  2.26368982  5.42108756  0.08916261  0.17084051 13.35525961], Loss = 0.4436\n",
      "Iteration 7589: Weights = [55.30333333  2.26354356  5.42073729  0.08915685  0.17082947 13.35563694], Loss = 0.4436\n",
      "Iteration 7590: Weights = [55.30333333  2.26339731  5.42038704  0.08915109  0.17081843 13.35601424], Loss = 0.4435\n",
      "Iteration 7591: Weights = [55.30333333  2.26325107  5.42003682  0.08914533  0.1708074  13.35639152], Loss = 0.4435\n",
      "Iteration 7592: Weights = [55.30333333  2.26310483  5.41968662  0.08913957  0.17079636 13.35676878], Loss = 0.4434\n",
      "Iteration 7593: Weights = [55.30333333  2.26295861  5.41933645  0.08913381  0.17078533 13.35714601], Loss = 0.4434\n",
      "Iteration 7594: Weights = [55.30333333  2.2628124   5.41898629  0.08912805  0.17077429 13.35752321], Loss = 0.4433\n",
      "Iteration 7595: Weights = [55.30333333  2.26266619  5.41863616  0.08912229  0.17076326 13.35790039], Loss = 0.4432\n",
      "Iteration 7596: Weights = [55.30333333  2.26252     5.41828605  0.08911653  0.17075222 13.35827755], Loss = 0.4432\n",
      "Iteration 7597: Weights = [55.30333333  2.26237381  5.41793596  0.08911077  0.17074119 13.35865468], Loss = 0.4431\n",
      "Iteration 7598: Weights = [55.30333333  2.26222763  5.4175859   0.08910502  0.17073016 13.35903179], Loss = 0.4431\n",
      "Iteration 7599: Weights = [55.30333333  2.26208147  5.41723586  0.08909926  0.17071913 13.35940888], Loss = 0.4430\n",
      "Iteration 7600: Weights = [55.30333333  2.26193531  5.41688584  0.0890935   0.1707081  13.35978594], Loss = 0.4430\n",
      "Iteration 7601: Weights = [55.30333333  2.26178916  5.41653585  0.08908775  0.17069707 13.36016297], Loss = 0.4429\n",
      "Iteration 7602: Weights = [55.30333333  2.26164302  5.41618587  0.08908199  0.17068604 13.36053998], Loss = 0.4428\n",
      "Iteration 7603: Weights = [55.30333333  2.26149689  5.41583592  0.08907623  0.17067501 13.36091697], Loss = 0.4428\n",
      "Iteration 7604: Weights = [55.30333333  2.26135077  5.41548599  0.08907048  0.17066398 13.36129393], Loss = 0.4427\n",
      "Iteration 7605: Weights = [55.30333333  2.26120466  5.41513609  0.08906472  0.17065296 13.36167087], Loss = 0.4427\n",
      "Iteration 7606: Weights = [55.30333333  2.26105856  5.41478621  0.08905897  0.17064193 13.36204778], Loss = 0.4426\n",
      "Iteration 7607: Weights = [55.30333333  2.26091247  5.41443635  0.08905321  0.1706309  13.36242467], Loss = 0.4426\n",
      "Iteration 7608: Weights = [55.30333333  2.26076639  5.41408651  0.08904746  0.17061988 13.36280154], Loss = 0.4425\n",
      "Iteration 7609: Weights = [55.30333333  2.26062032  5.41373669  0.08904171  0.17060885 13.36317838], Loss = 0.4424\n",
      "Iteration 7610: Weights = [55.30333333  2.26047425  5.4133869   0.08903595  0.17059783 13.36355519], Loss = 0.4424\n",
      "Iteration 7611: Weights = [55.30333333  2.2603282   5.41303713  0.0890302   0.17058681 13.36393198], Loss = 0.4423\n",
      "Iteration 7612: Weights = [55.30333333  2.26018215  5.41268738  0.08902445  0.17057579 13.36430875], Loss = 0.4423\n",
      "Iteration 7613: Weights = [55.30333333  2.26003612  5.41233766  0.0890187   0.17056477 13.36468549], Loss = 0.4422\n",
      "Iteration 7614: Weights = [55.30333333  2.25989009  5.41198796  0.08901294  0.17055375 13.36506221], Loss = 0.4422\n",
      "Iteration 7615: Weights = [55.30333333  2.25974408  5.41163828  0.08900719  0.17054273 13.36543891], Loss = 0.4421\n",
      "Iteration 7616: Weights = [55.30333333  2.25959807  5.41128862  0.08900144  0.17053171 13.36581558], Loss = 0.4420\n",
      "Iteration 7617: Weights = [55.30333333  2.25945207  5.41093899  0.08899569  0.17052069 13.36619222], Loss = 0.4420\n",
      "Iteration 7618: Weights = [55.30333333  2.25930609  5.41058937  0.08898994  0.17050967 13.36656885], Loss = 0.4419\n",
      "Iteration 7619: Weights = [55.30333333  2.25916011  5.41023979  0.08898419  0.17049865 13.36694544], Loss = 0.4419\n",
      "Iteration 7620: Weights = [55.30333333  2.25901414  5.40989022  0.08897844  0.17048764 13.36732202], Loss = 0.4418\n",
      "Iteration 7621: Weights = [55.30333333  2.25886818  5.40954067  0.08897269  0.17047662 13.36769856], Loss = 0.4418\n",
      "Iteration 7622: Weights = [55.30333333  2.25872223  5.40919115  0.08896694  0.17046561 13.36807509], Loss = 0.4417\n",
      "Iteration 7623: Weights = [55.30333333  2.25857629  5.40884165  0.0889612   0.17045459 13.36845159], Loss = 0.4416\n",
      "Iteration 7624: Weights = [55.30333333  2.25843036  5.40849218  0.08895545  0.17044358 13.36882806], Loss = 0.4416\n",
      "Iteration 7625: Weights = [55.30333333  2.25828444  5.40814273  0.0889497   0.17043257 13.36920451], Loss = 0.4415\n",
      "Iteration 7626: Weights = [55.30333333  2.25813852  5.40779329  0.08894395  0.17042155 13.36958094], Loss = 0.4415\n",
      "Iteration 7627: Weights = [55.30333333  2.25799262  5.40744389  0.08893821  0.17041054 13.36995734], Loss = 0.4414\n",
      "Iteration 7628: Weights = [55.30333333  2.25784673  5.4070945   0.08893246  0.17039953 13.37033372], Loss = 0.4414\n",
      "Iteration 7629: Weights = [55.30333333  2.25770084  5.40674514  0.08892671  0.17038852 13.37071007], Loss = 0.4413\n",
      "Iteration 7630: Weights = [55.30333333  2.25755497  5.4063958   0.08892097  0.17037751 13.3710864 ], Loss = 0.4412\n",
      "Iteration 7631: Weights = [55.30333333  2.2574091   5.40604648  0.08891522  0.17036651 13.37146271], Loss = 0.4412\n",
      "Iteration 7632: Weights = [55.30333333  2.25726325  5.40569718  0.08890948  0.1703555  13.37183899], Loss = 0.4411\n",
      "Iteration 7633: Weights = [55.30333333  2.2571174   5.40534791  0.08890373  0.17034449 13.37221525], Loss = 0.4411\n",
      "Iteration 7634: Weights = [55.30333333  2.25697156  5.40499866  0.08889799  0.17033348 13.37259148], Loss = 0.4410\n",
      "Iteration 7635: Weights = [55.30333333  2.25682574  5.40464943  0.08889224  0.17032248 13.37296769], Loss = 0.4410\n",
      "Iteration 7636: Weights = [55.30333333  2.25667992  5.40430023  0.0888865   0.17031147 13.37334387], Loss = 0.4409\n",
      "Iteration 7637: Weights = [55.30333333  2.25653411  5.40395104  0.08888076  0.17030047 13.37372003], Loss = 0.4408\n",
      "Iteration 7638: Weights = [55.30333333  2.25638831  5.40360188  0.08887502  0.17028947 13.37409617], Loss = 0.4408\n",
      "Iteration 7639: Weights = [55.30333333  2.25624252  5.40325275  0.08886927  0.17027846 13.37447228], Loss = 0.4407\n",
      "Iteration 7640: Weights = [55.30333333  2.25609674  5.40290363  0.08886353  0.17026746 13.37484836], Loss = 0.4407\n",
      "Iteration 7641: Weights = [55.30333333  2.25595097  5.40255454  0.08885779  0.17025646 13.37522442], Loss = 0.4406\n",
      "Iteration 7642: Weights = [55.30333333  2.25580521  5.40220547  0.08885205  0.17024546 13.37560046], Loss = 0.4406\n",
      "Iteration 7643: Weights = [55.30333333  2.25565946  5.40185642  0.08884631  0.17023446 13.37597648], Loss = 0.4405\n",
      "Iteration 7644: Weights = [55.30333333  2.25551371  5.4015074   0.08884057  0.17022346 13.37635246], Loss = 0.4404\n",
      "Iteration 7645: Weights = [55.30333333  2.25536798  5.40115839  0.08883483  0.17021246 13.37672843], Loss = 0.4404\n",
      "Iteration 7646: Weights = [55.30333333  2.25522226  5.40080941  0.08882909  0.17020146 13.37710437], Loss = 0.4403\n",
      "Iteration 7647: Weights = [55.30333333  2.25507654  5.40046046  0.08882335  0.17019047 13.37748029], Loss = 0.4403\n",
      "Iteration 7648: Weights = [55.30333333  2.25493084  5.40011152  0.08881761  0.17017947 13.37785618], Loss = 0.4402\n",
      "Iteration 7649: Weights = [55.30333333  2.25478514  5.39976261  0.08881187  0.17016847 13.37823205], Loss = 0.4402\n",
      "Iteration 7650: Weights = [55.30333333  2.25463946  5.39941372  0.08880613  0.17015748 13.37860789], Loss = 0.4401\n",
      "Iteration 7651: Weights = [55.30333333  2.25449378  5.39906485  0.08880039  0.17014649 13.37898371], Loss = 0.4400\n",
      "Iteration 7652: Weights = [55.30333333  2.25434811  5.39871601  0.08879466  0.17013549 13.3793595 ], Loss = 0.4400\n",
      "Iteration 7653: Weights = [55.30333333  2.25420245  5.39836719  0.08878892  0.1701245  13.37973527], Loss = 0.4399\n",
      "Iteration 7654: Weights = [55.30333333  2.2540568   5.39801839  0.08878318  0.17011351 13.38011102], Loss = 0.4399\n",
      "Iteration 7655: Weights = [55.30333333  2.25391117  5.39766961  0.08877745  0.17010252 13.38048674], Loss = 0.4398\n",
      "Iteration 7656: Weights = [55.30333333  2.25376554  5.39732086  0.08877171  0.17009153 13.38086244], Loss = 0.4398\n",
      "Iteration 7657: Weights = [55.30333333  2.25361992  5.39697213  0.08876597  0.17008054 13.38123811], Loss = 0.4397\n",
      "Iteration 7658: Weights = [55.30333333  2.2534743   5.39662342  0.08876024  0.17006955 13.38161376], Loss = 0.4396\n",
      "Iteration 7659: Weights = [55.30333333  2.2533287   5.39627473  0.0887545   0.17005856 13.38198939], Loss = 0.4396\n",
      "Iteration 7660: Weights = [55.30333333  2.25318311  5.39592607  0.08874877  0.17004757 13.38236499], Loss = 0.4395\n",
      "Iteration 7661: Weights = [55.30333333  2.25303753  5.39557742  0.08874303  0.17003658 13.38274057], Loss = 0.4395\n",
      "Iteration 7662: Weights = [55.30333333  2.25289196  5.3952288   0.0887373   0.1700256  13.38311612], Loss = 0.4394\n",
      "Iteration 7663: Weights = [55.30333333  2.25274639  5.39488021  0.08873157  0.17001461 13.38349165], Loss = 0.4394\n",
      "Iteration 7664: Weights = [55.30333333  2.25260084  5.39453163  0.08872583  0.17000363 13.38386715], Loss = 0.4393\n",
      "Iteration 7665: Weights = [55.30333333  2.25245529  5.39418308  0.0887201   0.16999264 13.38424263], Loss = 0.4393\n",
      "Iteration 7666: Weights = [55.30333333  2.25230976  5.39383455  0.08871437  0.16998166 13.38461808], Loss = 0.4392\n",
      "Iteration 7667: Weights = [55.30333333  2.25216423  5.39348605  0.08870864  0.16997067 13.38499351], Loss = 0.4391\n",
      "Iteration 7668: Weights = [55.30333333  2.25201871  5.39313756  0.08870291  0.16995969 13.38536892], Loss = 0.4391\n",
      "Iteration 7669: Weights = [55.30333333  2.25187321  5.3927891   0.08869717  0.16994871 13.3857443 ], Loss = 0.4390\n",
      "Iteration 7670: Weights = [55.30333333  2.25172771  5.39244066  0.08869144  0.16993773 13.38611966], Loss = 0.4390\n",
      "Iteration 7671: Weights = [55.30333333  2.25158222  5.39209225  0.08868571  0.16992675 13.386495  ], Loss = 0.4389\n",
      "Iteration 7672: Weights = [55.30333333  2.25143674  5.39174385  0.08867998  0.16991577 13.38687031], Loss = 0.4389\n",
      "Iteration 7673: Weights = [55.30333333  2.25129127  5.39139548  0.08867425  0.16990479 13.38724559], Loss = 0.4388\n",
      "Iteration 7674: Weights = [55.30333333  2.25114581  5.39104713  0.08866852  0.16989381 13.38762085], Loss = 0.4387\n",
      "Iteration 7675: Weights = [55.30333333  2.25100036  5.39069881  0.08866279  0.16988284 13.38799609], Loss = 0.4387\n",
      "Iteration 7676: Weights = [55.30333333  2.25085492  5.3903505   0.08865707  0.16987186 13.3883713 ], Loss = 0.4386\n",
      "Iteration 7677: Weights = [55.30333333  2.25070948  5.39000222  0.08865134  0.16986089 13.38874649], Loss = 0.4386\n",
      "Iteration 7678: Weights = [55.30333333  2.25056406  5.38965396  0.08864561  0.16984991 13.38912165], Loss = 0.4385\n",
      "Iteration 7679: Weights = [55.30333333  2.25041865  5.38930573  0.08863988  0.16983894 13.38949679], Loss = 0.4385\n",
      "Iteration 7680: Weights = [55.30333333  2.25027324  5.38895751  0.08863415  0.16982796 13.38987191], Loss = 0.4384\n",
      "Iteration 7681: Weights = [55.30333333  2.25012785  5.38860932  0.08862843  0.16981699 13.390247  ], Loss = 0.4383\n",
      "Iteration 7682: Weights = [55.30333333  2.24998246  5.38826115  0.0886227   0.16980602 13.39062207], Loss = 0.4383\n",
      "Iteration 7683: Weights = [55.30333333  2.24983709  5.387913    0.08861697  0.16979505 13.39099711], Loss = 0.4382\n",
      "Iteration 7684: Weights = [55.30333333  2.24969172  5.38756488  0.08861125  0.16978407 13.39137213], Loss = 0.4382\n",
      "Iteration 7685: Weights = [55.30333333  2.24954636  5.38721678  0.08860552  0.1697731  13.39174712], Loss = 0.4381\n",
      "Iteration 7686: Weights = [55.30333333  2.24940102  5.3868687   0.0885998   0.16976214 13.39212209], Loss = 0.4381\n",
      "Iteration 7687: Weights = [55.30333333  2.24925568  5.38652064  0.08859407  0.16975117 13.39249704], Loss = 0.4380\n",
      "Iteration 7688: Weights = [55.30333333  2.24911035  5.38617261  0.08858835  0.1697402  13.39287196], Loss = 0.4379\n",
      "Iteration 7689: Weights = [55.30333333  2.24896503  5.3858246   0.08858263  0.16972923 13.39324686], Loss = 0.4379\n",
      "Iteration 7690: Weights = [55.30333333  2.24881972  5.38547661  0.0885769   0.16971826 13.39362173], Loss = 0.4378\n",
      "Iteration 7691: Weights = [55.30333333  2.24867442  5.38512864  0.08857118  0.1697073  13.39399658], Loss = 0.4378\n",
      "Iteration 7692: Weights = [55.30333333  2.24852913  5.3847807   0.08856546  0.16969633 13.39437141], Loss = 0.4377\n",
      "Iteration 7693: Weights = [55.30333333  2.24838385  5.38443278  0.08855973  0.16968537 13.39474621], Loss = 0.4377\n",
      "Iteration 7694: Weights = [55.30333333  2.24823857  5.38408488  0.08855401  0.16967441 13.39512098], Loss = 0.4376\n",
      "Iteration 7695: Weights = [55.30333333  2.24809331  5.383737    0.08854829  0.16966344 13.39549574], Loss = 0.4376\n",
      "Iteration 7696: Weights = [55.30333333  2.24794806  5.38338915  0.08854257  0.16965248 13.39587046], Loss = 0.4375\n",
      "Iteration 7697: Weights = [55.30333333  2.24780281  5.38304132  0.08853685  0.16964152 13.39624517], Loss = 0.4374\n",
      "Iteration 7698: Weights = [55.30333333  2.24765758  5.38269351  0.08853113  0.16963056 13.39661985], Loss = 0.4374\n",
      "Iteration 7699: Weights = [55.30333333  2.24751235  5.38234572  0.08852541  0.1696196  13.3969945 ], Loss = 0.4373\n",
      "Iteration 7700: Weights = [55.30333333  2.24736713  5.38199796  0.08851969  0.16960864 13.39736913], Loss = 0.4373\n",
      "Iteration 7701: Weights = [55.30333333  2.24722193  5.38165021  0.08851397  0.16959768 13.39774374], Loss = 0.4372\n",
      "Iteration 7702: Weights = [55.30333333  2.24707673  5.3813025   0.08850825  0.16958672 13.39811832], Loss = 0.4372\n",
      "Iteration 7703: Weights = [55.30333333  2.24693154  5.3809548   0.08850253  0.16957576 13.39849288], Loss = 0.4371\n",
      "Iteration 7704: Weights = [55.30333333  2.24678636  5.38060712  0.08849681  0.16956481 13.39886742], Loss = 0.4370\n",
      "Iteration 7705: Weights = [55.30333333  2.24664119  5.38025947  0.08849109  0.16955385 13.39924193], Loss = 0.4370\n",
      "Iteration 7706: Weights = [55.30333333  2.24649603  5.37991184  0.08848538  0.1695429  13.39961641], Loss = 0.4369\n",
      "Iteration 7707: Weights = [55.30333333  2.24635088  5.37956424  0.08847966  0.16953194 13.39999088], Loss = 0.4369\n",
      "Iteration 7708: Weights = [55.30333333  2.24620574  5.37921665  0.08847394  0.16952099 13.40036531], Loss = 0.4368\n",
      "Iteration 7709: Weights = [55.30333333  2.24606061  5.37886909  0.08846823  0.16951004 13.40073973], Loss = 0.4368\n",
      "Iteration 7710: Weights = [55.30333333  2.24591549  5.37852155  0.08846251  0.16949908 13.40111412], Loss = 0.4367\n",
      "Iteration 7711: Weights = [55.30333333  2.24577037  5.37817403  0.08845679  0.16948813 13.40148848], Loss = 0.4366\n",
      "Iteration 7712: Weights = [55.30333333  2.24562527  5.37782654  0.08845108  0.16947718 13.40186282], Loss = 0.4366\n",
      "Iteration 7713: Weights = [55.30333333  2.24548018  5.37747907  0.08844536  0.16946623 13.40223714], Loss = 0.4365\n",
      "Iteration 7714: Weights = [55.30333333  2.24533509  5.37713162  0.08843965  0.16945528 13.40261143], Loss = 0.4365\n",
      "Iteration 7715: Weights = [55.30333333  2.24519002  5.37678419  0.08843394  0.16944433 13.4029857 ], Loss = 0.4364\n",
      "Iteration 7716: Weights = [55.30333333  2.24504495  5.37643678  0.08842822  0.16943338 13.40335994], Loss = 0.4364\n",
      "Iteration 7717: Weights = [55.30333333  2.24489989  5.3760894   0.08842251  0.16942244 13.40373416], Loss = 0.4363\n",
      "Iteration 7718: Weights = [55.30333333  2.24475485  5.37574204  0.08841679  0.16941149 13.40410836], Loss = 0.4363\n",
      "Iteration 7719: Weights = [55.30333333  2.24460981  5.3753947   0.08841108  0.16940054 13.40448253], Loss = 0.4362\n",
      "Iteration 7720: Weights = [55.30333333  2.24446478  5.37504739  0.08840537  0.1693896  13.40485668], Loss = 0.4361\n",
      "Iteration 7721: Weights = [55.30333333  2.24431976  5.3747001   0.08839966  0.16937865 13.4052308 ], Loss = 0.4361\n",
      "Iteration 7722: Weights = [55.30333333  2.24417475  5.37435283  0.08839395  0.16936771 13.4056049 ], Loss = 0.4360\n",
      "Iteration 7723: Weights = [55.30333333  2.24402975  5.37400558  0.08838823  0.16935677 13.40597897], Loss = 0.4360\n",
      "Iteration 7724: Weights = [55.30333333  2.24388476  5.37365835  0.08838252  0.16934582 13.40635303], Loss = 0.4359\n",
      "Iteration 7725: Weights = [55.30333333  2.24373978  5.37331115  0.08837681  0.16933488 13.40672705], Loss = 0.4359\n",
      "Iteration 7726: Weights = [55.30333333  2.2435948   5.37296397  0.0883711   0.16932394 13.40710105], Loss = 0.4358\n",
      "Iteration 7727: Weights = [55.30333333  2.24344984  5.37261681  0.08836539  0.169313   13.40747503], Loss = 0.4357\n",
      "Iteration 7728: Weights = [55.30333333  2.24330489  5.37226968  0.08835968  0.16930206 13.40784899], Loss = 0.4357\n",
      "Iteration 7729: Weights = [55.30333333  2.24315994  5.37192256  0.08835397  0.16929112 13.40822292], Loss = 0.4356\n",
      "Iteration 7730: Weights = [55.30333333  2.24301501  5.37157547  0.08834827  0.16928018 13.40859682], Loss = 0.4356\n",
      "Iteration 7731: Weights = [55.30333333  2.24287008  5.3712284   0.08834256  0.16926925 13.4089707 ], Loss = 0.4355\n",
      "Iteration 7732: Weights = [55.30333333  2.24272516  5.37088136  0.08833685  0.16925831 13.40934456], Loss = 0.4355\n",
      "Iteration 7733: Weights = [55.30333333  2.24258026  5.37053434  0.08833114  0.16924737 13.40971839], Loss = 0.4354\n",
      "Iteration 7734: Weights = [55.30333333  2.24243536  5.37018733  0.08832543  0.16923644 13.4100922 ], Loss = 0.4354\n",
      "Iteration 7735: Weights = [55.30333333  2.24229047  5.36984036  0.08831973  0.1692255  13.41046599], Loss = 0.4353\n",
      "Iteration 7736: Weights = [55.30333333  2.24214559  5.3694934   0.08831402  0.16921457 13.41083975], Loss = 0.4352\n",
      "Iteration 7737: Weights = [55.30333333  2.24200072  5.36914647  0.08830831  0.16920364 13.41121349], Loss = 0.4352\n",
      "Iteration 7738: Weights = [55.30333333  2.24185586  5.36879955  0.08830261  0.1691927  13.4115872 ], Loss = 0.4351\n",
      "Iteration 7739: Weights = [55.30333333  2.24171101  5.36845267  0.0882969   0.16918177 13.41196089], Loss = 0.4351\n",
      "Iteration 7740: Weights = [55.30333333  2.24156617  5.3681058   0.0882912   0.16917084 13.41233455], Loss = 0.4350\n",
      "Iteration 7741: Weights = [55.30333333  2.24142134  5.36775896  0.08828549  0.16915991 13.41270819], Loss = 0.4350\n",
      "Iteration 7742: Weights = [55.30333333  2.24127652  5.36741213  0.08827979  0.16914898 13.41308181], Loss = 0.4349\n",
      "Iteration 7743: Weights = [55.30333333  2.2411317   5.36706533  0.08827409  0.16913805 13.4134554 ], Loss = 0.4348\n",
      "Iteration 7744: Weights = [55.30333333  2.2409869   5.36671856  0.08826838  0.16912712 13.41382897], Loss = 0.4348\n",
      "Iteration 7745: Weights = [55.30333333  2.2408421   5.3663718   0.08826268  0.16911619 13.41420251], Loss = 0.4347\n",
      "Iteration 7746: Weights = [55.30333333  2.24069732  5.36602507  0.08825698  0.16910527 13.41457603], Loss = 0.4347\n",
      "Iteration 7747: Weights = [55.30333333  2.24055254  5.36567836  0.08825127  0.16909434 13.41494952], Loss = 0.4346\n",
      "Iteration 7748: Weights = [55.30333333  2.24040778  5.36533167  0.08824557  0.16908342 13.41532299], Loss = 0.4346\n",
      "Iteration 7749: Weights = [55.30333333  2.24026302  5.36498501  0.08823987  0.16907249 13.41569644], Loss = 0.4345\n",
      "Iteration 7750: Weights = [55.30333333  2.24011827  5.36463837  0.08823417  0.16906157 13.41606986], Loss = 0.4345\n",
      "Iteration 7751: Weights = [55.30333333  2.23997353  5.36429175  0.08822847  0.16905064 13.41644326], Loss = 0.4344\n",
      "Iteration 7752: Weights = [55.30333333  2.2398288   5.36394515  0.08822277  0.16903972 13.41681664], Loss = 0.4343\n",
      "Iteration 7753: Weights = [55.30333333  2.23968408  5.36359857  0.08821707  0.1690288  13.41718999], Loss = 0.4343\n",
      "Iteration 7754: Weights = [55.30333333  2.23953937  5.36325202  0.08821137  0.16901788 13.41756331], Loss = 0.4342\n",
      "Iteration 7755: Weights = [55.30333333  2.23939467  5.36290549  0.08820567  0.16900696 13.41793662], Loss = 0.4342\n",
      "Iteration 7756: Weights = [55.30333333  2.23924998  5.36255898  0.08819997  0.16899604 13.41830989], Loss = 0.4341\n",
      "Iteration 7757: Weights = [55.30333333  2.2391053   5.3622125   0.08819427  0.16898512 13.41868315], Loss = 0.4341\n",
      "Iteration 7758: Weights = [55.30333333  2.23896062  5.36186603  0.08818857  0.1689742  13.41905638], Loss = 0.4340\n",
      "Iteration 7759: Weights = [55.30333333  2.23881596  5.36151959  0.08818287  0.16896328 13.41942958], Loss = 0.4339\n",
      "Iteration 7760: Weights = [55.30333333  2.23867131  5.36117317  0.08817718  0.16895236 13.41980277], Loss = 0.4339\n",
      "Iteration 7761: Weights = [55.30333333  2.23852666  5.36082678  0.08817148  0.16894145 13.42017592], Loss = 0.4338\n",
      "Iteration 7762: Weights = [55.30333333  2.23838203  5.3604804   0.08816578  0.16893053 13.42054906], Loss = 0.4338\n",
      "Iteration 7763: Weights = [55.30333333  2.2382374   5.36013405  0.08816008  0.16891962 13.42092217], Loss = 0.4337\n",
      "Iteration 7764: Weights = [55.30333333  2.23809278  5.35978772  0.08815439  0.1689087  13.42129525], Loss = 0.4337\n",
      "Iteration 7765: Weights = [55.30333333  2.23794817  5.35944142  0.08814869  0.16889779 13.42166831], Loss = 0.4336\n",
      "Iteration 7766: Weights = [55.30333333  2.23780358  5.35909513  0.088143    0.16888688 13.42204135], Loss = 0.4336\n",
      "Iteration 7767: Weights = [55.30333333  2.23765899  5.35874887  0.0881373   0.16887597 13.42241436], Loss = 0.4335\n",
      "Iteration 7768: Weights = [55.30333333  2.23751441  5.35840263  0.08813161  0.16886505 13.42278735], Loss = 0.4334\n",
      "Iteration 7769: Weights = [55.30333333  2.23736984  5.35805641  0.08812591  0.16885414 13.42316032], Loss = 0.4334\n",
      "Iteration 7770: Weights = [55.30333333  2.23722528  5.35771022  0.08812022  0.16884323 13.42353326], Loss = 0.4333\n",
      "Iteration 7771: Weights = [55.30333333  2.23708072  5.35736405  0.08811452  0.16883232 13.42390617], Loss = 0.4333\n",
      "Iteration 7772: Weights = [55.30333333  2.23693618  5.3570179   0.08810883  0.16882142 13.42427907], Loss = 0.4332\n",
      "Iteration 7773: Weights = [55.30333333  2.23679165  5.35667177  0.08810314  0.16881051 13.42465193], Loss = 0.4332\n",
      "Iteration 7774: Weights = [55.30333333  2.23664713  5.35632566  0.08809745  0.1687996  13.42502478], Loss = 0.4331\n",
      "Iteration 7775: Weights = [55.30333333  2.23650261  5.35597958  0.08809175  0.16878869 13.4253976 ], Loss = 0.4331\n",
      "Iteration 7776: Weights = [55.30333333  2.23635811  5.35563352  0.08808606  0.16877779 13.42577039], Loss = 0.4330\n",
      "Iteration 7777: Weights = [55.30333333  2.23621361  5.35528748  0.08808037  0.16876688 13.42614317], Loss = 0.4329\n",
      "Iteration 7778: Weights = [55.30333333  2.23606912  5.35494147  0.08807468  0.16875598 13.42651591], Loss = 0.4329\n",
      "Iteration 7779: Weights = [55.30333333  2.23592465  5.35459547  0.08806899  0.16874507 13.42688864], Loss = 0.4328\n",
      "Iteration 7780: Weights = [55.30333333  2.23578018  5.3542495   0.0880633   0.16873417 13.42726134], Loss = 0.4328\n",
      "Iteration 7781: Weights = [55.30333333  2.23563572  5.35390355  0.08805761  0.16872327 13.42763401], Loss = 0.4327\n",
      "Iteration 7782: Weights = [55.30333333  2.23549127  5.35355763  0.08805192  0.16871237 13.42800666], Loss = 0.4327\n",
      "Iteration 7783: Weights = [55.30333333  2.23534683  5.35321172  0.08804623  0.16870147 13.42837929], Loss = 0.4326\n",
      "Iteration 7784: Weights = [55.30333333  2.2352024   5.35286584  0.08804054  0.16869057 13.4287519 ], Loss = 0.4325\n",
      "Iteration 7785: Weights = [55.30333333  2.23505798  5.35251998  0.08803485  0.16867967 13.42912447], Loss = 0.4325\n",
      "Iteration 7786: Weights = [55.30333333  2.23491357  5.35217414  0.08802916  0.16866877 13.42949703], Loss = 0.4324\n",
      "Iteration 7787: Weights = [55.30333333  2.23476917  5.35182833  0.08802348  0.16865787 13.42986956], Loss = 0.4324\n",
      "Iteration 7788: Weights = [55.30333333  2.23462478  5.35148254  0.08801779  0.16864697 13.43024207], Loss = 0.4323\n",
      "Iteration 7789: Weights = [55.30333333  2.23448039  5.35113677  0.0880121   0.16863608 13.43061455], Loss = 0.4323\n",
      "Iteration 7790: Weights = [55.30333333  2.23433602  5.35079102  0.08800642  0.16862518 13.43098701], Loss = 0.4322\n",
      "Iteration 7791: Weights = [55.30333333  2.23419165  5.35044529  0.08800073  0.16861429 13.43135944], Loss = 0.4322\n",
      "Iteration 7792: Weights = [55.30333333  2.2340473   5.35009959  0.08799504  0.16860339 13.43173186], Loss = 0.4321\n",
      "Iteration 7793: Weights = [55.30333333  2.23390295  5.34975391  0.08798936  0.1685925  13.43210424], Loss = 0.4320\n",
      "Iteration 7794: Weights = [55.30333333  2.23375861  5.34940825  0.08798367  0.1685816  13.4324766 ], Loss = 0.4320\n",
      "Iteration 7795: Weights = [55.30333333  2.23361429  5.34906262  0.08797799  0.16857071 13.43284894], Loss = 0.4319\n",
      "Iteration 7796: Weights = [55.30333333  2.23346997  5.348717    0.0879723   0.16855982 13.43322126], Loss = 0.4319\n",
      "Iteration 7797: Weights = [55.30333333  2.23332566  5.34837141  0.08796662  0.16854893 13.43359355], Loss = 0.4318\n",
      "Iteration 7798: Weights = [55.30333333  2.23318136  5.34802584  0.08796094  0.16853804 13.43396582], Loss = 0.4318\n",
      "Iteration 7799: Weights = [55.30333333  2.23303707  5.34768029  0.08795525  0.16852715 13.43433806], Loss = 0.4317\n",
      "Iteration 7800: Weights = [55.30333333  2.23289279  5.34733477  0.08794957  0.16851626 13.43471028], Loss = 0.4317\n",
      "Iteration 7801: Weights = [55.30333333  2.23274852  5.34698927  0.08794389  0.16850537 13.43508247], Loss = 0.4316\n",
      "Iteration 7802: Weights = [55.30333333  2.23260425  5.34664379  0.0879382   0.16849448 13.43545464], Loss = 0.4315\n",
      "Iteration 7803: Weights = [55.30333333  2.23246     5.34629833  0.08793252  0.1684836  13.43582679], Loss = 0.4315\n",
      "Iteration 7804: Weights = [55.30333333  2.23231576  5.3459529   0.08792684  0.16847271 13.43619891], Loss = 0.4314\n",
      "Iteration 7805: Weights = [55.30333333  2.23217152  5.34560748  0.08792116  0.16846183 13.43657101], Loss = 0.4314\n",
      "Iteration 7806: Weights = [55.30333333  2.2320273   5.34526209  0.08791548  0.16845094 13.43694308], Loss = 0.4313\n",
      "Iteration 7807: Weights = [55.30333333  2.23188308  5.34491672  0.0879098   0.16844006 13.43731513], Loss = 0.4313\n",
      "Iteration 7808: Weights = [55.30333333  2.23173888  5.34457138  0.08790412  0.16842917 13.43768716], Loss = 0.4312\n",
      "Iteration 7809: Weights = [55.30333333  2.23159468  5.34422606  0.08789844  0.16841829 13.43805916], Loss = 0.4312\n",
      "Iteration 7810: Weights = [55.30333333  2.23145049  5.34388075  0.08789276  0.16840741 13.43843114], Loss = 0.4311\n",
      "Iteration 7811: Weights = [55.30333333  2.23130631  5.34353547  0.08788708  0.16839653 13.43880309], Loss = 0.4310\n",
      "Iteration 7812: Weights = [55.30333333  2.23116214  5.34319022  0.0878814   0.16838565 13.43917502], Loss = 0.4310\n",
      "Iteration 7813: Weights = [55.30333333  2.23101798  5.34284498  0.08787572  0.16837477 13.43954693], Loss = 0.4309\n",
      "Iteration 7814: Weights = [55.30333333  2.23087383  5.34249977  0.08787005  0.16836389 13.43991881], Loss = 0.4309\n",
      "Iteration 7815: Weights = [55.30333333  2.23072969  5.34215458  0.08786437  0.16835301 13.44029067], Loss = 0.4308\n",
      "Iteration 7816: Weights = [55.30333333  2.23058556  5.34180942  0.08785869  0.16834213 13.4406625 ], Loss = 0.4308\n",
      "Iteration 7817: Weights = [55.30333333  2.23044144  5.34146427  0.08785302  0.16833126 13.44103431], Loss = 0.4307\n",
      "Iteration 7818: Weights = [55.30333333  2.23029732  5.34111915  0.08784734  0.16832038 13.4414061 ], Loss = 0.4307\n",
      "Iteration 7819: Weights = [55.30333333  2.23015322  5.34077405  0.08784166  0.16830951 13.44177786], Loss = 0.4306\n",
      "Iteration 7820: Weights = [55.30333333  2.23000912  5.34042897  0.08783599  0.16829863 13.44214959], Loss = 0.4305\n",
      "Iteration 7821: Weights = [55.30333333  2.22986504  5.34008391  0.08783031  0.16828776 13.44252131], Loss = 0.4305\n",
      "Iteration 7822: Weights = [55.30333333  2.22972096  5.33973888  0.08782464  0.16827688 13.442893  ], Loss = 0.4304\n",
      "Iteration 7823: Weights = [55.30333333  2.2295769   5.33939387  0.08781896  0.16826601 13.44326466], Loss = 0.4304\n",
      "Iteration 7824: Weights = [55.30333333  2.22943284  5.33904888  0.08781329  0.16825514 13.4436363 ], Loss = 0.4303\n",
      "Iteration 7825: Weights = [55.30333333  2.22928879  5.33870391  0.08780761  0.16824427 13.44400792], Loss = 0.4303\n",
      "Iteration 7826: Weights = [55.30333333  2.22914475  5.33835897  0.08780194  0.1682334  13.44437952], Loss = 0.4302\n",
      "Iteration 7827: Weights = [55.30333333  2.22900072  5.33801405  0.08779627  0.16822253 13.44475109], Loss = 0.4302\n",
      "Iteration 7828: Weights = [55.30333333  2.2288567   5.33766915  0.0877906   0.16821166 13.44512263], Loss = 0.4301\n",
      "Iteration 7829: Weights = [55.30333333  2.22871269  5.33732427  0.08778492  0.16820079 13.44549415], Loss = 0.4300\n",
      "Iteration 7830: Weights = [55.30333333  2.22856869  5.33697941  0.08777925  0.16818992 13.44586565], Loss = 0.4300\n",
      "Iteration 7831: Weights = [55.30333333  2.2284247   5.33663458  0.08777358  0.16817905 13.44623712], Loss = 0.4299\n",
      "Iteration 7832: Weights = [55.30333333  2.22828071  5.33628977  0.08776791  0.16816819 13.44660857], Loss = 0.4299\n",
      "Iteration 7833: Weights = [55.30333333  2.22813674  5.33594498  0.08776224  0.16815732 13.44698   ], Loss = 0.4298\n",
      "Iteration 7834: Weights = [55.30333333  2.22799278  5.33560022  0.08775657  0.16814646 13.4473514 ], Loss = 0.4298\n",
      "Iteration 7835: Weights = [55.30333333  2.22784882  5.33525547  0.0877509   0.16813559 13.44772278], Loss = 0.4297\n",
      "Iteration 7836: Weights = [55.30333333  2.22770488  5.33491075  0.08774523  0.16812473 13.44809413], Loss = 0.4297\n",
      "Iteration 7837: Weights = [55.30333333  2.22756094  5.33456605  0.08773956  0.16811387 13.44846546], Loss = 0.4296\n",
      "Iteration 7838: Weights = [55.30333333  2.22741701  5.33422137  0.08773389  0.168103   13.44883677], Loss = 0.4295\n",
      "Iteration 7839: Weights = [55.30333333  2.22727309  5.33387672  0.08772822  0.16809214 13.44920805], Loss = 0.4295\n",
      "Iteration 7840: Weights = [55.30333333  2.22712918  5.33353209  0.08772255  0.16808128 13.44957931], Loss = 0.4294\n",
      "Iteration 7841: Weights = [55.30333333  2.22698529  5.33318748  0.08771688  0.16807042 13.44995054], Loss = 0.4294\n",
      "Iteration 7842: Weights = [55.30333333  2.2268414   5.33284289  0.08771122  0.16805956 13.45032175], Loss = 0.4293\n",
      "Iteration 7843: Weights = [55.30333333  2.22669751  5.33249832  0.08770555  0.1680487  13.45069293], Loss = 0.4293\n",
      "Iteration 7844: Weights = [55.30333333  2.22655364  5.33215378  0.08769988  0.16803785 13.4510641 ], Loss = 0.4292\n",
      "Iteration 7845: Weights = [55.30333333  2.22640978  5.33180926  0.08769422  0.16802699 13.45143523], Loss = 0.4292\n",
      "Iteration 7846: Weights = [55.30333333  2.22626593  5.33146476  0.08768855  0.16801613 13.45180635], Loss = 0.4291\n",
      "Iteration 7847: Weights = [55.30333333  2.22612209  5.33112028  0.08768288  0.16800528 13.45217744], Loss = 0.4290\n",
      "Iteration 7848: Weights = [55.30333333  2.22597825  5.33077583  0.08767722  0.16799442 13.4525485 ], Loss = 0.4290\n",
      "Iteration 7849: Weights = [55.30333333  2.22583443  5.3304314   0.08767155  0.16798357 13.45291954], Loss = 0.4289\n",
      "Iteration 7850: Weights = [55.30333333  2.22569061  5.33008699  0.08766589  0.16797271 13.45329056], Loss = 0.4289\n",
      "Iteration 7851: Weights = [55.30333333  2.2255468   5.3297426   0.08766022  0.16796186 13.45366156], Loss = 0.4288\n",
      "Iteration 7852: Weights = [55.30333333  2.22540301  5.32939823  0.08765456  0.16795101 13.45403253], Loss = 0.4288\n",
      "Iteration 7853: Weights = [55.30333333  2.22525922  5.32905389  0.0876489   0.16794016 13.45440347], Loss = 0.4287\n",
      "Iteration 7854: Weights = [55.30333333  2.22511544  5.32870957  0.08764323  0.1679293  13.45477439], Loss = 0.4287\n",
      "Iteration 7855: Weights = [55.30333333  2.22497167  5.32836527  0.08763757  0.16791845 13.45514529], Loss = 0.4286\n",
      "Iteration 7856: Weights = [55.30333333  2.22482791  5.32802099  0.08763191  0.1679076  13.45551617], Loss = 0.4285\n",
      "Iteration 7857: Weights = [55.30333333  2.22468416  5.32767674  0.08762625  0.16789676 13.45588702], Loss = 0.4285\n",
      "Iteration 7858: Weights = [55.30333333  2.22454042  5.32733251  0.08762059  0.16788591 13.45625784], Loss = 0.4284\n",
      "Iteration 7859: Weights = [55.30333333  2.22439669  5.3269883   0.08761492  0.16787506 13.45662864], Loss = 0.4284\n",
      "Iteration 7860: Weights = [55.30333333  2.22425296  5.32664411  0.08760926  0.16786421 13.45699942], Loss = 0.4283\n",
      "Iteration 7861: Weights = [55.30333333  2.22410925  5.32629995  0.0876036   0.16785337 13.45737018], Loss = 0.4283\n",
      "Iteration 7862: Weights = [55.30333333  2.22396555  5.3259558   0.08759794  0.16784252 13.45774091], Loss = 0.4282\n",
      "Iteration 7863: Weights = [55.30333333  2.22382185  5.32561168  0.08759228  0.16783168 13.45811161], Loss = 0.4282\n",
      "Iteration 7864: Weights = [55.30333333  2.22367817  5.32526758  0.08758662  0.16782083 13.45848229], Loss = 0.4281\n",
      "Iteration 7865: Weights = [55.30333333  2.22353449  5.32492351  0.08758096  0.16780999 13.45885295], Loss = 0.4280\n",
      "Iteration 7866: Weights = [55.30333333  2.22339082  5.32457945  0.0875753   0.16779915 13.45922359], Loss = 0.4280\n",
      "Iteration 7867: Weights = [55.30333333  2.22324717  5.32423542  0.08756965  0.16778831 13.4595942 ], Loss = 0.4279\n",
      "Iteration 7868: Weights = [55.30333333  2.22310352  5.32389141  0.08756399  0.16777746 13.45996478], Loss = 0.4279\n",
      "Iteration 7869: Weights = [55.30333333  2.22295988  5.32354742  0.08755833  0.16776662 13.46033535], Loss = 0.4278\n",
      "Iteration 7870: Weights = [55.30333333  2.22281625  5.32320346  0.08755267  0.16775578 13.46070589], Loss = 0.4278\n",
      "Iteration 7871: Weights = [55.30333333  2.22267263  5.32285952  0.08754702  0.16774495 13.4610764 ], Loss = 0.4277\n",
      "Iteration 7872: Weights = [55.30333333  2.22252902  5.3225156   0.08754136  0.16773411 13.46144689], Loss = 0.4277\n",
      "Iteration 7873: Weights = [55.30333333  2.22238541  5.3221717   0.0875357   0.16772327 13.46181736], Loss = 0.4276\n",
      "Iteration 7874: Weights = [55.30333333  2.22224182  5.32182782  0.08753005  0.16771243 13.4621878 ], Loss = 0.4275\n",
      "Iteration 7875: Weights = [55.30333333  2.22209824  5.32148397  0.08752439  0.1677016  13.46255822], Loss = 0.4275\n",
      "Iteration 7876: Weights = [55.30333333  2.22195466  5.32114014  0.08751874  0.16769076 13.46292861], Loss = 0.4274\n",
      "Iteration 7877: Weights = [55.30333333  2.2218111   5.32079633  0.08751308  0.16767993 13.46329899], Loss = 0.4274\n",
      "Iteration 7878: Weights = [55.30333333  2.22166754  5.32045254  0.08750743  0.16766909 13.46366933], Loss = 0.4273\n",
      "Iteration 7879: Weights = [55.30333333  2.221524    5.32010877  0.08750177  0.16765826 13.46403966], Loss = 0.4273\n",
      "Iteration 7880: Weights = [55.30333333  2.22138046  5.31976503  0.08749612  0.16764743 13.46440996], Loss = 0.4272\n",
      "Iteration 7881: Weights = [55.30333333  2.22123693  5.31942131  0.08749047  0.16763659 13.46478023], Loss = 0.4272\n",
      "Iteration 7882: Weights = [55.30333333  2.22109341  5.31907761  0.08748481  0.16762576 13.46515048], Loss = 0.4271\n",
      "Iteration 7883: Weights = [55.30333333  2.2209499   5.31873393  0.08747916  0.16761493 13.46552071], Loss = 0.4271\n",
      "Iteration 7884: Weights = [55.30333333  2.2208064   5.31839028  0.08747351  0.1676041  13.46589091], Loss = 0.4270\n",
      "Iteration 7885: Weights = [55.30333333  2.22066291  5.31804665  0.08746786  0.16759327 13.46626109], Loss = 0.4269\n",
      "Iteration 7886: Weights = [55.30333333  2.22051943  5.31770304  0.08746221  0.16758244 13.46663125], Loss = 0.4269\n",
      "Iteration 7887: Weights = [55.30333333  2.22037596  5.31735945  0.08745655  0.16757162 13.46700138], Loss = 0.4268\n",
      "Iteration 7888: Weights = [55.30333333  2.2202325   5.31701589  0.0874509   0.16756079 13.46737149], Loss = 0.4268\n",
      "Iteration 7889: Weights = [55.30333333  2.22008904  5.31667234  0.08744525  0.16754996 13.46774157], Loss = 0.4267\n",
      "Iteration 7890: Weights = [55.30333333  2.2199456   5.31632882  0.0874396   0.16753914 13.46811163], Loss = 0.4267\n",
      "Iteration 7891: Weights = [55.30333333  2.21980216  5.31598532  0.08743395  0.16752831 13.46848167], Loss = 0.4266\n",
      "Iteration 7892: Weights = [55.30333333  2.21965874  5.31564185  0.0874283   0.16751749 13.46885168], Loss = 0.4266\n",
      "Iteration 7893: Weights = [55.30333333  2.21951532  5.31529839  0.08742266  0.16750666 13.46922167], Loss = 0.4265\n",
      "Iteration 7894: Weights = [55.30333333  2.21937191  5.31495496  0.08741701  0.16749584 13.46959163], Loss = 0.4264\n",
      "Iteration 7895: Weights = [55.30333333  2.21922852  5.31461155  0.08741136  0.16748502 13.46996157], Loss = 0.4264\n",
      "Iteration 7896: Weights = [55.30333333  2.21908513  5.31426816  0.08740571  0.1674742  13.47033149], Loss = 0.4263\n",
      "Iteration 7897: Weights = [55.30333333  2.21894175  5.3139248   0.08740006  0.16746338 13.47070138], Loss = 0.4263\n",
      "Iteration 7898: Weights = [55.30333333  2.21879838  5.31358145  0.08739442  0.16745256 13.47107125], Loss = 0.4262\n",
      "Iteration 7899: Weights = [55.30333333  2.21865502  5.31323813  0.08738877  0.16744174 13.4714411 ], Loss = 0.4262\n",
      "Iteration 7900: Weights = [55.30333333  2.21851166  5.31289483  0.08738312  0.16743092 13.47181092], Loss = 0.4261\n",
      "Iteration 7901: Weights = [55.30333333  2.21836832  5.31255156  0.08737748  0.1674201  13.47218072], Loss = 0.4261\n",
      "Iteration 7902: Weights = [55.30333333  2.21822499  5.3122083   0.08737183  0.16740928 13.47255049], Loss = 0.4260\n",
      "Iteration 7903: Weights = [55.30333333  2.21808166  5.31186507  0.08736619  0.16739847 13.47292024], Loss = 0.4259\n",
      "Iteration 7904: Weights = [55.30333333  2.21793835  5.31152186  0.08736054  0.16738765 13.47328996], Loss = 0.4259\n",
      "Iteration 7905: Weights = [55.30333333  2.21779504  5.31117867  0.0873549   0.16737683 13.47365967], Loss = 0.4258\n",
      "Iteration 7906: Weights = [55.30333333  2.21765175  5.31083551  0.08734925  0.16736602 13.47402934], Loss = 0.4258\n",
      "Iteration 7907: Weights = [55.30333333  2.21750846  5.31049236  0.08734361  0.16735521 13.474399  ], Loss = 0.4257\n",
      "Iteration 7908: Weights = [55.30333333  2.21736518  5.31014924  0.08733797  0.16734439 13.47476863], Loss = 0.4257\n",
      "Iteration 7909: Weights = [55.30333333  2.21722191  5.30980614  0.08733232  0.16733358 13.47513823], Loss = 0.4256\n",
      "Iteration 7910: Weights = [55.30333333  2.21707866  5.30946306  0.08732668  0.16732277 13.47550782], Loss = 0.4256\n",
      "Iteration 7911: Weights = [55.30333333  2.21693541  5.30912001  0.08732104  0.16731196 13.47587737], Loss = 0.4255\n",
      "Iteration 7912: Weights = [55.30333333  2.21679216  5.30877698  0.0873154   0.16730115 13.47624691], Loss = 0.4255\n",
      "Iteration 7913: Weights = [55.30333333  2.21664893  5.30843397  0.08730975  0.16729034 13.47661642], Loss = 0.4254\n",
      "Iteration 7914: Weights = [55.30333333  2.21650571  5.30809098  0.08730411  0.16727953 13.4769859 ], Loss = 0.4253\n",
      "Iteration 7915: Weights = [55.30333333  2.2163625   5.30774801  0.08729847  0.16726872 13.47735537], Loss = 0.4253\n",
      "Iteration 7916: Weights = [55.30333333  2.21621929  5.30740507  0.08729283  0.16725791 13.47772481], Loss = 0.4252\n",
      "Iteration 7917: Weights = [55.30333333  2.2160761   5.30706214  0.08728719  0.16724711 13.47809422], Loss = 0.4252\n",
      "Iteration 7918: Weights = [55.30333333  2.21593292  5.30671924  0.08728155  0.1672363  13.47846361], Loss = 0.4251\n",
      "Iteration 7919: Weights = [55.30333333  2.21578974  5.30637637  0.08727591  0.16722549 13.47883298], Loss = 0.4251\n",
      "Iteration 7920: Weights = [55.30333333  2.21564657  5.30603351  0.08727027  0.16721469 13.47920232], Loss = 0.4250\n",
      "Iteration 7921: Weights = [55.30333333  2.21550342  5.30569068  0.08726463  0.16720389 13.47957164], Loss = 0.4250\n",
      "Iteration 7922: Weights = [55.30333333  2.21536027  5.30534787  0.087259    0.16719308 13.47994094], Loss = 0.4249\n",
      "Iteration 7923: Weights = [55.30333333  2.21521713  5.30500508  0.08725336  0.16718228 13.48031021], Loss = 0.4248\n",
      "Iteration 7924: Weights = [55.30333333  2.215074    5.30466231  0.08724772  0.16717148 13.48067946], Loss = 0.4248\n",
      "Iteration 7925: Weights = [55.30333333  2.21493088  5.30431956  0.08724208  0.16716068 13.48104868], Loss = 0.4247\n",
      "Iteration 7926: Weights = [55.30333333  2.21478777  5.30397684  0.08723645  0.16714988 13.48141788], Loss = 0.4247\n",
      "Iteration 7927: Weights = [55.30333333  2.21464467  5.30363414  0.08723081  0.16713908 13.48178706], Loss = 0.4246\n",
      "Iteration 7928: Weights = [55.30333333  2.21450157  5.30329146  0.08722517  0.16712828 13.48215621], Loss = 0.4246\n",
      "Iteration 7929: Weights = [55.30333333  2.21435849  5.30294881  0.08721954  0.16711748 13.48252534], Loss = 0.4245\n",
      "Iteration 7930: Weights = [55.30333333  2.21421542  5.30260617  0.0872139   0.16710668 13.48289445], Loss = 0.4245\n",
      "Iteration 7931: Weights = [55.30333333  2.21407235  5.30226356  0.08720827  0.16709588 13.48326353], Loss = 0.4244\n",
      "Iteration 7932: Weights = [55.30333333  2.21392929  5.30192097  0.08720263  0.16708509 13.48363258], Loss = 0.4244\n",
      "Iteration 7933: Weights = [55.30333333  2.21378625  5.3015784   0.087197    0.16707429 13.48400162], Loss = 0.4243\n",
      "Iteration 7934: Weights = [55.30333333  2.21364321  5.30123586  0.08719136  0.1670635  13.48437063], Loss = 0.4242\n",
      "Iteration 7935: Weights = [55.30333333  2.21350018  5.30089333  0.08718573  0.1670527  13.48473961], Loss = 0.4242\n",
      "Iteration 7936: Weights = [55.30333333  2.21335716  5.30055083  0.0871801   0.16704191 13.48510857], Loss = 0.4241\n",
      "Iteration 7937: Weights = [55.30333333  2.21321415  5.30020835  0.08717446  0.16703112 13.48547751], Loss = 0.4241\n",
      "Iteration 7938: Weights = [55.30333333  2.21307115  5.2998659   0.08716883  0.16702032 13.48584643], Loss = 0.4240\n",
      "Iteration 7939: Weights = [55.30333333  2.21292816  5.29952346  0.0871632   0.16700953 13.48621532], Loss = 0.4240\n",
      "Iteration 7940: Weights = [55.30333333  2.21278518  5.29918105  0.08715757  0.16699874 13.48658418], Loss = 0.4239\n",
      "Iteration 7941: Weights = [55.30333333  2.21264221  5.29883866  0.08715194  0.16698795 13.48695303], Loss = 0.4239\n",
      "Iteration 7942: Weights = [55.30333333  2.21249925  5.29849629  0.08714631  0.16697716 13.48732184], Loss = 0.4238\n",
      "Iteration 7943: Weights = [55.30333333  2.21235629  5.29815394  0.08714067  0.16696637 13.48769064], Loss = 0.4238\n",
      "Iteration 7944: Weights = [55.30333333  2.21221335  5.29781162  0.08713504  0.16695558 13.48805941], Loss = 0.4237\n",
      "Iteration 7945: Weights = [55.30333333  2.21207041  5.29746932  0.08712941  0.1669448  13.48842816], Loss = 0.4236\n",
      "Iteration 7946: Weights = [55.30333333  2.21192749  5.29712704  0.08712378  0.16693401 13.48879688], Loss = 0.4236\n",
      "Iteration 7947: Weights = [55.30333333  2.21178457  5.29678478  0.08711816  0.16692322 13.48916558], Loss = 0.4235\n",
      "Iteration 7948: Weights = [55.30333333  2.21164166  5.29644254  0.08711253  0.16691244 13.48953426], Loss = 0.4235\n",
      "Iteration 7949: Weights = [55.30333333  2.21149876  5.29610033  0.0871069   0.16690165 13.48990291], Loss = 0.4234\n",
      "Iteration 7950: Weights = [55.30333333  2.21135587  5.29575814  0.08710127  0.16689087 13.49027154], Loss = 0.4234\n",
      "Iteration 7951: Weights = [55.30333333  2.21121299  5.29541597  0.08709564  0.16688009 13.49064014], Loss = 0.4233\n",
      "Iteration 7952: Weights = [55.30333333  2.21107012  5.29507382  0.08709001  0.16686931 13.49100872], Loss = 0.4233\n",
      "Iteration 7953: Weights = [55.30333333  2.21092726  5.29473169  0.08708439  0.16685852 13.49137728], Loss = 0.4232\n",
      "Iteration 7954: Weights = [55.30333333  2.21078441  5.29438959  0.08707876  0.16684774 13.49174581], Loss = 0.4232\n",
      "Iteration 7955: Weights = [55.30333333  2.21064156  5.29404751  0.08707313  0.16683696 13.49211432], Loss = 0.4231\n",
      "Iteration 7956: Weights = [55.30333333  2.21049873  5.29370545  0.08706751  0.16682618 13.49248281], Loss = 0.4230\n",
      "Iteration 7957: Weights = [55.30333333  2.21035591  5.29336341  0.08706188  0.1668154  13.49285127], Loss = 0.4230\n",
      "Iteration 7958: Weights = [55.30333333  2.21021309  5.2930214   0.08705626  0.16680463 13.49321971], Loss = 0.4229\n",
      "Iteration 7959: Weights = [55.30333333  2.21007028  5.29267941  0.08705063  0.16679385 13.49358812], Loss = 0.4229\n",
      "Iteration 7960: Weights = [55.30333333  2.20992749  5.29233743  0.08704501  0.16678307 13.49395651], Loss = 0.4228\n",
      "Iteration 7961: Weights = [55.30333333  2.2097847   5.29199549  0.08703938  0.16677229 13.49432488], Loss = 0.4228\n",
      "Iteration 7962: Weights = [55.30333333  2.20964192  5.29165356  0.08703376  0.16676152 13.49469322], Loss = 0.4227\n",
      "Iteration 7963: Weights = [55.30333333  2.20949915  5.29131166  0.08702814  0.16675074 13.49506154], Loss = 0.4227\n",
      "Iteration 7964: Weights = [55.30333333  2.20935639  5.29096977  0.08702251  0.16673997 13.49542983], Loss = 0.4226\n",
      "Iteration 7965: Weights = [55.30333333  2.20921364  5.29062791  0.08701689  0.1667292  13.4957981 ], Loss = 0.4225\n",
      "Iteration 7966: Weights = [55.30333333  2.2090709   5.29028607  0.08701127  0.16671842 13.49616635], Loss = 0.4225\n",
      "Iteration 7967: Weights = [55.30333333  2.20892817  5.28994426  0.08700565  0.16670765 13.49653457], Loss = 0.4224\n",
      "Iteration 7968: Weights = [55.30333333  2.20878544  5.28960246  0.08700003  0.16669688 13.49690277], Loss = 0.4224\n",
      "Iteration 7969: Weights = [55.30333333  2.20864273  5.28926069  0.0869944   0.16668611 13.49727095], Loss = 0.4223\n",
      "Iteration 7970: Weights = [55.30333333  2.20850002  5.28891894  0.08698878  0.16667534 13.4976391 ], Loss = 0.4223\n",
      "Iteration 7971: Weights = [55.30333333  2.20835733  5.28857722  0.08698316  0.16666457 13.49800723], Loss = 0.4222\n",
      "Iteration 7972: Weights = [55.30333333  2.20821464  5.28823551  0.08697754  0.1666538  13.49837533], Loss = 0.4222\n",
      "Iteration 7973: Weights = [55.30333333  2.20807196  5.28789383  0.08697192  0.16664303 13.49874342], Loss = 0.4221\n",
      "Iteration 7974: Weights = [55.30333333  2.2079293   5.28755217  0.0869663   0.16663227 13.49911147], Loss = 0.4221\n",
      "Iteration 7975: Weights = [55.30333333  2.20778664  5.28721053  0.08696068  0.1666215  13.49947951], Loss = 0.4220\n",
      "Iteration 7976: Weights = [55.30333333  2.20764399  5.28686891  0.08695507  0.16661074 13.49984751], Loss = 0.4219\n",
      "Iteration 7977: Weights = [55.30333333  2.20750135  5.28652731  0.08694945  0.16659997 13.5002155 ], Loss = 0.4219\n",
      "Iteration 7978: Weights = [55.30333333  2.20735872  5.28618574  0.08694383  0.16658921 13.50058346], Loss = 0.4218\n",
      "Iteration 7979: Weights = [55.30333333  2.20721609  5.28584419  0.08693821  0.16657844 13.5009514 ], Loss = 0.4218\n",
      "Iteration 7980: Weights = [55.30333333  2.20707348  5.28550266  0.08693259  0.16656768 13.50131931], Loss = 0.4217\n",
      "Iteration 7981: Weights = [55.30333333  2.20693088  5.28516115  0.08692698  0.16655692 13.50168721], Loss = 0.4217\n",
      "Iteration 7982: Weights = [55.30333333  2.20678828  5.28481967  0.08692136  0.16654616 13.50205507], Loss = 0.4216\n",
      "Iteration 7983: Weights = [55.30333333  2.2066457   5.28447821  0.08691574  0.16653539 13.50242292], Loss = 0.4216\n",
      "Iteration 7984: Weights = [55.30333333  2.20650312  5.28413676  0.08691013  0.16652463 13.50279073], Loss = 0.4215\n",
      "Iteration 7985: Weights = [55.30333333  2.20636056  5.28379535  0.08690451  0.16651387 13.50315853], Loss = 0.4215\n",
      "Iteration 7986: Weights = [55.30333333  2.206218    5.28345395  0.0868989   0.16650312 13.5035263 ], Loss = 0.4214\n",
      "Iteration 7987: Weights = [55.30333333  2.20607545  5.28311257  0.08689328  0.16649236 13.50389405], Loss = 0.4213\n",
      "Iteration 7988: Weights = [55.30333333  2.20593291  5.28277122  0.08688767  0.1664816  13.50426177], Loss = 0.4213\n",
      "Iteration 7989: Weights = [55.30333333  2.20579038  5.28242989  0.08688206  0.16647084 13.50462947], Loss = 0.4212\n",
      "Iteration 7990: Weights = [55.30333333  2.20564786  5.28208858  0.08687644  0.16646009 13.50499715], Loss = 0.4212\n",
      "Iteration 7991: Weights = [55.30333333  2.20550535  5.2817473   0.08687083  0.16644933 13.5053648 ], Loss = 0.4211\n",
      "Iteration 7992: Weights = [55.30333333  2.20536285  5.28140603  0.08686522  0.16643858 13.50573243], Loss = 0.4211\n",
      "Iteration 7993: Weights = [55.30333333  2.20522036  5.28106479  0.0868596   0.16642782 13.50610004], Loss = 0.4210\n",
      "Iteration 7994: Weights = [55.30333333  2.20507787  5.28072357  0.08685399  0.16641707 13.50646762], Loss = 0.4210\n",
      "Iteration 7995: Weights = [55.30333333  2.2049354   5.28038237  0.08684838  0.16640632 13.50683518], Loss = 0.4209\n",
      "Iteration 7996: Weights = [55.30333333  2.20479293  5.2800412   0.08684277  0.16639557 13.50720271], Loss = 0.4209\n",
      "Iteration 7997: Weights = [55.30333333  2.20465048  5.27970004  0.08683716  0.16638482 13.50757022], Loss = 0.4208\n",
      "Iteration 7998: Weights = [55.30333333  2.20450803  5.27935891  0.08683155  0.16637406 13.50793771], Loss = 0.4208\n",
      "Iteration 7999: Weights = [55.30333333  2.20436559  5.2790178   0.08682594  0.16636331 13.50830517], Loss = 0.4207\n",
      "Iteration 8000: Weights = [55.30333333  2.20422316  5.27867671  0.08682033  0.16635257 13.50867261], Loss = 0.4206\n",
      "Iteration 8001: Weights = [55.30333333  2.20408074  5.27833565  0.08681472  0.16634182 13.50904003], Loss = 0.4206\n",
      "Iteration 8002: Weights = [55.30333333  2.20393833  5.2779946   0.08680911  0.16633107 13.50940742], Loss = 0.4205\n",
      "Iteration 8003: Weights = [55.30333333  2.20379593  5.27765358  0.0868035   0.16632032 13.50977479], Loss = 0.4205\n",
      "Iteration 8004: Weights = [55.30333333  2.20365354  5.27731258  0.08679789  0.16630958 13.51014213], Loss = 0.4204\n",
      "Iteration 8005: Weights = [55.30333333  2.20351116  5.2769716   0.08679228  0.16629883 13.51050945], Loss = 0.4204\n",
      "Iteration 8006: Weights = [55.30333333  2.20336878  5.27663065  0.08678667  0.16628809 13.51087675], Loss = 0.4203\n",
      "Iteration 8007: Weights = [55.30333333  2.20322642  5.27628971  0.08678107  0.16627734 13.51124402], Loss = 0.4203\n",
      "Iteration 8008: Weights = [55.30333333  2.20308407  5.2759488   0.08677546  0.1662666  13.51161127], Loss = 0.4202\n",
      "Iteration 8009: Weights = [55.30333333  2.20294172  5.27560791  0.08676985  0.16625586 13.5119785 ], Loss = 0.4202\n",
      "Iteration 8010: Weights = [55.30333333  2.20279938  5.27526704  0.08676425  0.16624511 13.5123457 ], Loss = 0.4201\n",
      "Iteration 8011: Weights = [55.30333333  2.20265706  5.2749262   0.08675864  0.16623437 13.51271288], Loss = 0.4200\n",
      "Iteration 8012: Weights = [55.30333333  2.20251474  5.27458538  0.08675303  0.16622363 13.51308003], Loss = 0.4200\n",
      "Iteration 8013: Weights = [55.30333333  2.20237243  5.27424457  0.08674743  0.16621289 13.51344716], Loss = 0.4199\n",
      "Iteration 8014: Weights = [55.30333333  2.20223013  5.27390379  0.08674182  0.16620215 13.51381427], Loss = 0.4199\n",
      "Iteration 8015: Weights = [55.30333333  2.20208784  5.27356304  0.08673622  0.16619141 13.51418135], Loss = 0.4198\n",
      "Iteration 8016: Weights = [55.30333333  2.20194556  5.2732223   0.08673061  0.16618068 13.51454841], Loss = 0.4198\n",
      "Iteration 8017: Weights = [55.30333333  2.20180329  5.27288159  0.08672501  0.16616994 13.51491545], Loss = 0.4197\n",
      "Iteration 8018: Weights = [55.30333333  2.20166102  5.2725409   0.08671941  0.1661592  13.51528246], Loss = 0.4197\n",
      "Iteration 8019: Weights = [55.30333333  2.20151877  5.27220023  0.0867138   0.16614847 13.51564945], Loss = 0.4196\n",
      "Iteration 8020: Weights = [55.30333333  2.20137652  5.27185958  0.0867082   0.16613773 13.51601641], Loss = 0.4196\n",
      "Iteration 8021: Weights = [55.30333333  2.20123429  5.27151895  0.0867026   0.166127   13.51638335], Loss = 0.4195\n",
      "Iteration 8022: Weights = [55.30333333  2.20109206  5.27117835  0.086697    0.16611626 13.51675027], Loss = 0.4194\n",
      "Iteration 8023: Weights = [55.30333333  2.20094985  5.27083777  0.0866914   0.16610553 13.51711716], Loss = 0.4194\n",
      "Iteration 8024: Weights = [55.30333333  2.20080764  5.27049721  0.08668579  0.1660948  13.51748403], Loss = 0.4193\n",
      "Iteration 8025: Weights = [55.30333333  2.20066544  5.27015667  0.08668019  0.16608406 13.51785088], Loss = 0.4193\n",
      "Iteration 8026: Weights = [55.30333333  2.20052325  5.26981616  0.08667459  0.16607333 13.5182177 ], Loss = 0.4192\n",
      "Iteration 8027: Weights = [55.30333333  2.20038107  5.26947566  0.08666899  0.1660626  13.5185845 ], Loss = 0.4192\n",
      "Iteration 8028: Weights = [55.30333333  2.2002389   5.26913519  0.08666339  0.16605187 13.51895128], Loss = 0.4191\n",
      "Iteration 8029: Weights = [55.30333333  2.20009674  5.26879474  0.08665779  0.16604114 13.51931803], Loss = 0.4191\n",
      "Iteration 8030: Weights = [55.30333333  2.19995458  5.26845431  0.08665219  0.16603042 13.51968475], Loss = 0.4190\n",
      "Iteration 8031: Weights = [55.30333333  2.19981244  5.26811391  0.0866466   0.16601969 13.52005146], Loss = 0.4190\n",
      "Iteration 8032: Weights = [55.30333333  2.19967031  5.26777353  0.086641    0.16600896 13.52041814], Loss = 0.4189\n",
      "Iteration 8033: Weights = [55.30333333  2.19952818  5.26743316  0.0866354   0.16599824 13.5207848 ], Loss = 0.4189\n",
      "Iteration 8034: Weights = [55.30333333  2.19938607  5.26709283  0.0866298   0.16598751 13.52115143], Loss = 0.4188\n",
      "Iteration 8035: Weights = [55.30333333  2.19924396  5.26675251  0.0866242   0.16597679 13.52151804], Loss = 0.4187\n",
      "Iteration 8036: Weights = [55.30333333  2.19910186  5.26641221  0.08661861  0.16596606 13.52188462], Loss = 0.4187\n",
      "Iteration 8037: Weights = [55.30333333  2.19895977  5.26607194  0.08661301  0.16595534 13.52225118], Loss = 0.4186\n",
      "Iteration 8038: Weights = [55.30333333  2.19881769  5.26573169  0.08660741  0.16594462 13.52261772], Loss = 0.4186\n",
      "Iteration 8039: Weights = [55.30333333  2.19867562  5.26539146  0.08660182  0.16593389 13.52298424], Loss = 0.4185\n",
      "Iteration 8040: Weights = [55.30333333  2.19853356  5.26505125  0.08659622  0.16592317 13.52335073], Loss = 0.4185\n",
      "Iteration 8041: Weights = [55.30333333  2.19839151  5.26471106  0.08659063  0.16591245 13.5237172 ], Loss = 0.4184\n",
      "Iteration 8042: Weights = [55.30333333  2.19824947  5.2643709   0.08658503  0.16590173 13.52408364], Loss = 0.4184\n",
      "Iteration 8043: Weights = [55.30333333  2.19810744  5.26403076  0.08657944  0.16589101 13.52445006], Loss = 0.4183\n",
      "Iteration 8044: Weights = [55.30333333  2.19796541  5.26369064  0.08657384  0.16588029 13.52481645], Loss = 0.4183\n",
      "Iteration 8045: Weights = [55.30333333  2.1978234   5.26335054  0.08656825  0.16586958 13.52518283], Loss = 0.4182\n",
      "Iteration 8046: Weights = [55.30333333  2.19768139  5.26301047  0.08656266  0.16585886 13.52554918], Loss = 0.4181\n",
      "Iteration 8047: Weights = [55.30333333  2.19753939  5.26267041  0.08655706  0.16584814 13.5259155 ], Loss = 0.4181\n",
      "Iteration 8048: Weights = [55.30333333  2.19739741  5.26233038  0.08655147  0.16583743 13.5262818 ], Loss = 0.4180\n",
      "Iteration 8049: Weights = [55.30333333  2.19725543  5.26199037  0.08654588  0.16582671 13.52664808], Loss = 0.4180\n",
      "Iteration 8050: Weights = [55.30333333  2.19711346  5.26165038  0.08654029  0.165816   13.52701433], Loss = 0.4179\n",
      "Iteration 8051: Weights = [55.30333333  2.1969715   5.26131042  0.0865347   0.16580528 13.52738056], Loss = 0.4179\n",
      "Iteration 8052: Weights = [55.30333333  2.19682955  5.26097047  0.0865291   0.16579457 13.52774677], Loss = 0.4178\n",
      "Iteration 8053: Weights = [55.30333333  2.19668761  5.26063055  0.08652351  0.16578386 13.52811295], Loss = 0.4178\n",
      "Iteration 8054: Weights = [55.30333333  2.19654567  5.26029065  0.08651792  0.16577315 13.52847911], Loss = 0.4177\n",
      "Iteration 8055: Weights = [55.30333333  2.19640375  5.25995077  0.08651233  0.16576244 13.52884525], Loss = 0.4177\n",
      "Iteration 8056: Weights = [55.30333333  2.19626184  5.25961092  0.08650674  0.16575173 13.52921136], Loss = 0.4176\n",
      "Iteration 8057: Weights = [55.30333333  2.19611993  5.25927108  0.08650115  0.16574102 13.52957745], Loss = 0.4176\n",
      "Iteration 8058: Weights = [55.30333333  2.19597804  5.25893127  0.08649557  0.16573031 13.52994351], Loss = 0.4175\n",
      "Iteration 8059: Weights = [55.30333333  2.19583615  5.25859148  0.08648998  0.1657196  13.53030955], Loss = 0.4174\n",
      "Iteration 8060: Weights = [55.30333333  2.19569427  5.25825171  0.08648439  0.16570889 13.53067557], Loss = 0.4174\n",
      "Iteration 8061: Weights = [55.30333333  2.1955524   5.25791197  0.0864788   0.16569818 13.53104157], Loss = 0.4173\n",
      "Iteration 8062: Weights = [55.30333333  2.19541055  5.25757224  0.08647321  0.16568748 13.53140754], Loss = 0.4173\n",
      "Iteration 8063: Weights = [55.30333333  2.1952687   5.25723254  0.08646763  0.16567677 13.53177348], Loss = 0.4172\n",
      "Iteration 8064: Weights = [55.30333333  2.19512685  5.25689286  0.08646204  0.16566607 13.53213941], Loss = 0.4172\n",
      "Iteration 8065: Weights = [55.30333333  2.19498502  5.2565532   0.08645645  0.16565536 13.5325053 ], Loss = 0.4171\n",
      "Iteration 8066: Weights = [55.30333333  2.1948432   5.25621357  0.08645087  0.16564466 13.53287118], Loss = 0.4171\n",
      "Iteration 8067: Weights = [55.30333333  2.19470139  5.25587395  0.08644528  0.16563396 13.53323703], Loss = 0.4170\n",
      "Iteration 8068: Weights = [55.30333333  2.19455958  5.25553436  0.08643969  0.16562326 13.53360286], Loss = 0.4170\n",
      "Iteration 8069: Weights = [55.30333333  2.19441779  5.25519479  0.08643411  0.16561255 13.53396867], Loss = 0.4169\n",
      "Iteration 8070: Weights = [55.30333333  2.194276    5.25485524  0.08642853  0.16560185 13.53433445], Loss = 0.4169\n",
      "Iteration 8071: Weights = [55.30333333  2.19413423  5.25451571  0.08642294  0.16559115 13.5347002 ], Loss = 0.4168\n",
      "Iteration 8072: Weights = [55.30333333  2.19399246  5.25417621  0.08641736  0.16558046 13.53506594], Loss = 0.4167\n",
      "Iteration 8073: Weights = [55.30333333  2.1938507   5.25383673  0.08641177  0.16556976 13.53543165], Loss = 0.4167\n",
      "Iteration 8074: Weights = [55.30333333  2.19370895  5.25349726  0.08640619  0.16555906 13.53579733], Loss = 0.4166\n",
      "Iteration 8075: Weights = [55.30333333  2.19356721  5.25315783  0.08640061  0.16554836 13.536163  ], Loss = 0.4166\n",
      "Iteration 8076: Weights = [55.30333333  2.19342548  5.25281841  0.08639502  0.16553767 13.53652864], Loss = 0.4165\n",
      "Iteration 8077: Weights = [55.30333333  2.19328376  5.25247901  0.08638944  0.16552697 13.53689425], Loss = 0.4165\n",
      "Iteration 8078: Weights = [55.30333333  2.19314205  5.25213964  0.08638386  0.16551627 13.53725984], Loss = 0.4164\n",
      "Iteration 8079: Weights = [55.30333333  2.19300034  5.25180029  0.08637828  0.16550558 13.53762541], Loss = 0.4164\n",
      "Iteration 8080: Weights = [55.30333333  2.19285865  5.25146096  0.0863727   0.16549489 13.53799096], Loss = 0.4163\n",
      "Iteration 8081: Weights = [55.30333333  2.19271696  5.25112165  0.08636712  0.16548419 13.53835648], Loss = 0.4163\n",
      "Iteration 8082: Weights = [55.30333333  2.19257529  5.25078237  0.08636154  0.1654735  13.53872198], Loss = 0.4162\n",
      "Iteration 8083: Weights = [55.30333333  2.19243362  5.2504431   0.08635596  0.16546281 13.53908745], Loss = 0.4162\n",
      "Iteration 8084: Weights = [55.30333333  2.19229196  5.25010386  0.08635038  0.16545212 13.5394529 ], Loss = 0.4161\n",
      "Iteration 8085: Weights = [55.30333333  2.19215032  5.24976464  0.0863448   0.16544143 13.53981833], Loss = 0.4160\n",
      "Iteration 8086: Weights = [55.30333333  2.19200868  5.24942544  0.08633922  0.16543074 13.54018373], Loss = 0.4160\n",
      "Iteration 8087: Weights = [55.30333333  2.19186705  5.24908627  0.08633364  0.16542005 13.54054911], Loss = 0.4159\n",
      "Iteration 8088: Weights = [55.30333333  2.19172543  5.24874711  0.08632806  0.16540936 13.54091446], Loss = 0.4159\n",
      "Iteration 8089: Weights = [55.30333333  2.19158381  5.24840798  0.08632248  0.16539868 13.5412798 ], Loss = 0.4158\n",
      "Iteration 8090: Weights = [55.30333333  2.19144221  5.24806887  0.08631691  0.16538799 13.54164511], Loss = 0.4158\n",
      "Iteration 8091: Weights = [55.30333333  2.19130062  5.24772978  0.08631133  0.1653773  13.54201039], Loss = 0.4157\n",
      "Iteration 8092: Weights = [55.30333333  2.19115903  5.24739072  0.08630575  0.16536662 13.54237565], Loss = 0.4157\n",
      "Iteration 8093: Weights = [55.30333333  2.19101746  5.24705167  0.08630018  0.16535593 13.54274089], Loss = 0.4156\n",
      "Iteration 8094: Weights = [55.30333333  2.19087589  5.24671265  0.0862946   0.16534525 13.5431061 ], Loss = 0.4156\n",
      "Iteration 8095: Weights = [55.30333333  2.19073433  5.24637365  0.08628903  0.16533456 13.54347129], Loss = 0.4155\n",
      "Iteration 8096: Weights = [55.30333333  2.19059279  5.24603467  0.08628345  0.16532388 13.54383646], Loss = 0.4155\n",
      "Iteration 8097: Weights = [55.30333333  2.19045125  5.24569571  0.08627788  0.1653132  13.54420161], Loss = 0.4154\n",
      "Iteration 8098: Weights = [55.30333333  2.19030972  5.24535678  0.0862723   0.16530252 13.54456673], Loss = 0.4153\n",
      "Iteration 8099: Weights = [55.30333333  2.1901682   5.24501787  0.08626673  0.16529184 13.54493182], Loss = 0.4153\n",
      "Iteration 8100: Weights = [55.30333333  2.19002669  5.24467897  0.08626115  0.16528116 13.54529689], Loss = 0.4152\n",
      "Iteration 8101: Weights = [55.30333333  2.18988519  5.2443401   0.08625558  0.16527048 13.54566194], Loss = 0.4152\n",
      "Iteration 8102: Weights = [55.30333333  2.18974369  5.24400126  0.08625001  0.1652598  13.54602697], Loss = 0.4151\n",
      "Iteration 8103: Weights = [55.30333333  2.18960221  5.24366243  0.08624443  0.16524912 13.54639197], Loss = 0.4151\n",
      "Iteration 8104: Weights = [55.30333333  2.18946073  5.24332363  0.08623886  0.16523845 13.54675695], Loss = 0.4150\n",
      "Iteration 8105: Weights = [55.30333333  2.18931927  5.24298485  0.08623329  0.16522777 13.5471219 ], Loss = 0.4150\n",
      "Iteration 8106: Weights = [55.30333333  2.18917781  5.24264609  0.08622772  0.16521709 13.54748683], Loss = 0.4149\n",
      "Iteration 8107: Weights = [55.30333333  2.18903637  5.24230735  0.08622215  0.16520642 13.54785174], Loss = 0.4149\n",
      "Iteration 8108: Weights = [55.30333333  2.18889493  5.24196863  0.08621657  0.16519575 13.54821663], Loss = 0.4148\n",
      "Iteration 8109: Weights = [55.30333333  2.1887535   5.24162994  0.086211    0.16518507 13.54858149], Loss = 0.4148\n",
      "Iteration 8110: Weights = [55.30333333  2.18861208  5.24129127  0.08620543  0.1651744  13.54894632], Loss = 0.4147\n",
      "Iteration 8111: Weights = [55.30333333  2.18847067  5.24095262  0.08619986  0.16516373 13.54931114], Loss = 0.4147\n",
      "Iteration 8112: Weights = [55.30333333  2.18832927  5.24061399  0.08619429  0.16515305 13.54967593], Loss = 0.4146\n",
      "Iteration 8113: Weights = [55.30333333  2.18818787  5.24027538  0.08618872  0.16514238 13.55004069], Loss = 0.4145\n",
      "Iteration 8114: Weights = [55.30333333  2.18804649  5.2399368   0.08618316  0.16513171 13.55040544], Loss = 0.4145\n",
      "Iteration 8115: Weights = [55.30333333  2.18790512  5.23959823  0.08617759  0.16512104 13.55077015], Loss = 0.4144\n",
      "Iteration 8116: Weights = [55.30333333  2.18776375  5.23925969  0.08617202  0.16511038 13.55113485], Loss = 0.4144\n",
      "Iteration 8117: Weights = [55.30333333  2.1876224   5.23892117  0.08616645  0.16509971 13.55149952], Loss = 0.4143\n",
      "Iteration 8118: Weights = [55.30333333  2.18748105  5.23858268  0.08616088  0.16508904 13.55186417], Loss = 0.4143\n",
      "Iteration 8119: Weights = [55.30333333  2.18733971  5.2382442   0.08615532  0.16507837 13.55222879], Loss = 0.4142\n",
      "Iteration 8120: Weights = [55.30333333  2.18719838  5.23790575  0.08614975  0.16506771 13.5525934 ], Loss = 0.4142\n",
      "Iteration 8121: Weights = [55.30333333  2.18705706  5.23756732  0.08614418  0.16505704 13.55295797], Loss = 0.4141\n",
      "Iteration 8122: Weights = [55.30333333  2.18691575  5.23722891  0.08613862  0.16504638 13.55332253], Loss = 0.4141\n",
      "Iteration 8123: Weights = [55.30333333  2.18677445  5.23689052  0.08613305  0.16503571 13.55368706], Loss = 0.4140\n",
      "Iteration 8124: Weights = [55.30333333  2.18663316  5.23655215  0.08612749  0.16502505 13.55405157], Loss = 0.4140\n",
      "Iteration 8125: Weights = [55.30333333  2.18649188  5.23621381  0.08612192  0.16501439 13.55441605], Loss = 0.4139\n",
      "Iteration 8126: Weights = [55.30333333  2.1863506   5.23587549  0.08611636  0.16500373 13.55478051], Loss = 0.4138\n",
      "Iteration 8127: Weights = [55.30333333  2.18620934  5.23553719  0.08611079  0.16499306 13.55514494], Loss = 0.4138\n",
      "Iteration 8128: Weights = [55.30333333  2.18606808  5.23519891  0.08610523  0.1649824  13.55550936], Loss = 0.4137\n",
      "Iteration 8129: Weights = [55.30333333  2.18592684  5.23486065  0.08609967  0.16497174 13.55587375], Loss = 0.4137\n",
      "Iteration 8130: Weights = [55.30333333  2.1857856   5.23452242  0.0860941   0.16496108 13.55623811], Loss = 0.4136\n",
      "Iteration 8131: Weights = [55.30333333  2.18564437  5.2341842   0.08608854  0.16495043 13.55660245], Loss = 0.4136\n",
      "Iteration 8132: Weights = [55.30333333  2.18550315  5.23384601  0.08608298  0.16493977 13.55696677], Loss = 0.4135\n",
      "Iteration 8133: Weights = [55.30333333  2.18536194  5.23350784  0.08607742  0.16492911 13.55733107], Loss = 0.4135\n",
      "Iteration 8134: Weights = [55.30333333  2.18522074  5.23316969  0.08607185  0.16491845 13.55769534], Loss = 0.4134\n",
      "Iteration 8135: Weights = [55.30333333  2.18507955  5.23283157  0.08606629  0.1649078  13.55805959], Loss = 0.4134\n",
      "Iteration 8136: Weights = [55.30333333  2.18493837  5.23249346  0.08606073  0.16489714 13.55842381], Loss = 0.4133\n",
      "Iteration 8137: Weights = [55.30333333  2.1847972   5.23215538  0.08605517  0.16488649 13.55878801], Loss = 0.4133\n",
      "Iteration 8138: Weights = [55.30333333  2.18465603  5.23181732  0.08604961  0.16487584 13.55915219], Loss = 0.4132\n",
      "Iteration 8139: Weights = [55.30333333  2.18451488  5.23147928  0.08604405  0.16486518 13.55951634], Loss = 0.4132\n",
      "Iteration 8140: Weights = [55.30333333  2.18437373  5.23114127  0.08603849  0.16485453 13.55988047], Loss = 0.4131\n",
      "Iteration 8141: Weights = [55.30333333  2.1842326   5.23080327  0.08603293  0.16484388 13.56024458], Loss = 0.4130\n",
      "Iteration 8142: Weights = [55.30333333  2.18409147  5.2304653   0.08602737  0.16483323 13.56060866], Loss = 0.4130\n",
      "Iteration 8143: Weights = [55.30333333  2.18395035  5.23012735  0.08602182  0.16482258 13.56097272], Loss = 0.4129\n",
      "Iteration 8144: Weights = [55.30333333  2.18380924  5.22978942  0.08601626  0.16481193 13.56133676], Loss = 0.4129\n",
      "Iteration 8145: Weights = [55.30333333  2.18366814  5.22945151  0.0860107   0.16480128 13.56170077], Loss = 0.4128\n",
      "Iteration 8146: Weights = [55.30333333  2.18352705  5.22911363  0.08600514  0.16479063 13.56206476], Loss = 0.4128\n",
      "Iteration 8147: Weights = [55.30333333  2.18338597  5.22877576  0.08599959  0.16477998 13.56242873], Loss = 0.4127\n",
      "Iteration 8148: Weights = [55.30333333  2.18324489  5.22843792  0.08599403  0.16476934 13.56279267], Loss = 0.4127\n",
      "Iteration 8149: Weights = [55.30333333  2.18310383  5.2281001   0.08598847  0.16475869 13.56315659], Loss = 0.4126\n",
      "Iteration 8150: Weights = [55.30333333  2.18296277  5.22776231  0.08598292  0.16474805 13.56352048], Loss = 0.4126\n",
      "Iteration 8151: Weights = [55.30333333  2.18282173  5.22742453  0.08597736  0.1647374  13.56388436], Loss = 0.4125\n",
      "Iteration 8152: Weights = [55.30333333  2.18268069  5.22708677  0.08597181  0.16472676 13.5642482 ], Loss = 0.4125\n",
      "Iteration 8153: Weights = [55.30333333  2.18253967  5.22674904  0.08596625  0.16471611 13.56461203], Loss = 0.4124\n",
      "Iteration 8154: Weights = [55.30333333  2.18239865  5.22641133  0.0859607   0.16470547 13.56497583], Loss = 0.4124\n",
      "Iteration 8155: Weights = [55.30333333  2.18225764  5.22607364  0.08595514  0.16469483 13.56533961], Loss = 0.4123\n",
      "Iteration 8156: Weights = [55.30333333  2.18211664  5.22573598  0.08594959  0.16468419 13.56570336], Loss = 0.4122\n",
      "Iteration 8157: Weights = [55.30333333  2.18197565  5.22539833  0.08594404  0.16467355 13.56606709], Loss = 0.4122\n",
      "Iteration 8158: Weights = [55.30333333  2.18183466  5.22506071  0.08593848  0.16466291 13.5664308 ], Loss = 0.4121\n",
      "Iteration 8159: Weights = [55.30333333  2.18169369  5.2247231   0.08593293  0.16465227 13.56679448], Loss = 0.4121\n",
      "Iteration 8160: Weights = [55.30333333  2.18155273  5.22438552  0.08592738  0.16464163 13.56715814], Loss = 0.4120\n",
      "Iteration 8161: Weights = [55.30333333  2.18141177  5.22404797  0.08592183  0.16463099 13.56752178], Loss = 0.4120\n",
      "Iteration 8162: Weights = [55.30333333  2.18127083  5.22371043  0.08591628  0.16462035 13.56788539], Loss = 0.4119\n",
      "Iteration 8163: Weights = [55.30333333  2.18112989  5.22337292  0.08591072  0.16460972 13.56824898], Loss = 0.4119\n",
      "Iteration 8164: Weights = [55.30333333  2.18098897  5.22303542  0.08590517  0.16459908 13.56861255], Loss = 0.4118\n",
      "Iteration 8165: Weights = [55.30333333  2.18084805  5.22269795  0.08589962  0.16458845 13.56897609], Loss = 0.4118\n",
      "Iteration 8166: Weights = [55.30333333  2.18070714  5.2223605   0.08589407  0.16457781 13.56933961], Loss = 0.4117\n",
      "Iteration 8167: Weights = [55.30333333  2.18056624  5.22202308  0.08588852  0.16456718 13.56970311], Loss = 0.4117\n",
      "Iteration 8168: Weights = [55.30333333  2.18042535  5.22168567  0.08588297  0.16455655 13.57006658], Loss = 0.4116\n",
      "Iteration 8169: Weights = [55.30333333  2.18028447  5.22134829  0.08587742  0.16454591 13.57043003], Loss = 0.4116\n",
      "Iteration 8170: Weights = [55.30333333  2.18014359  5.22101093  0.08587188  0.16453528 13.57079345], Loss = 0.4115\n",
      "Iteration 8171: Weights = [55.30333333  2.18000273  5.22067359  0.08586633  0.16452465 13.57115685], Loss = 0.4114\n",
      "Iteration 8172: Weights = [55.30333333  2.17986188  5.22033627  0.08586078  0.16451402 13.57152023], Loss = 0.4114\n",
      "Iteration 8173: Weights = [55.30333333  2.17972103  5.21999897  0.08585523  0.16450339 13.57188359], Loss = 0.4113\n",
      "Iteration 8174: Weights = [55.30333333  2.17958019  5.2196617   0.08584968  0.16449276 13.57224692], Loss = 0.4113\n",
      "Iteration 8175: Weights = [55.30333333  2.17943937  5.21932444  0.08584414  0.16448213 13.57261023], Loss = 0.4112\n",
      "Iteration 8176: Weights = [55.30333333  2.17929855  5.21898721  0.08583859  0.16447151 13.57297351], Loss = 0.4112\n",
      "Iteration 8177: Weights = [55.30333333  2.17915774  5.21865     0.08583304  0.16446088 13.57333677], Loss = 0.4111\n",
      "Iteration 8178: Weights = [55.30333333  2.17901694  5.21831282  0.0858275   0.16445025 13.57370001], Loss = 0.4111\n",
      "Iteration 8179: Weights = [55.30333333  2.17887615  5.21797565  0.08582195  0.16443963 13.57406322], Loss = 0.4110\n",
      "Iteration 8180: Weights = [55.30333333  2.17873537  5.21763851  0.08581641  0.164429   13.57442641], Loss = 0.4110\n",
      "Iteration 8181: Weights = [55.30333333  2.1785946   5.21730138  0.08581086  0.16441838 13.57478958], Loss = 0.4109\n",
      "Iteration 8182: Weights = [55.30333333  2.17845383  5.21696428  0.08580532  0.16440776 13.57515272], Loss = 0.4109\n",
      "Iteration 8183: Weights = [55.30333333  2.17831308  5.21662721  0.08579977  0.16439713 13.57551584], Loss = 0.4108\n",
      "Iteration 8184: Weights = [55.30333333  2.17817233  5.21629015  0.08579423  0.16438651 13.57587894], Loss = 0.4108\n",
      "Iteration 8185: Weights = [55.30333333  2.1780316   5.21595311  0.08578869  0.16437589 13.57624201], Loss = 0.4107\n",
      "Iteration 8186: Weights = [55.30333333  2.17789087  5.2156161   0.08578314  0.16436527 13.57660506], Loss = 0.4107\n",
      "Iteration 8187: Weights = [55.30333333  2.17775015  5.21527911  0.0857776   0.16435465 13.57696809], Loss = 0.4106\n",
      "Iteration 8188: Weights = [55.30333333  2.17760944  5.21494214  0.08577206  0.16434403 13.57733109], Loss = 0.4105\n",
      "Iteration 8189: Weights = [55.30333333  2.17746874  5.21460519  0.08576652  0.16433341 13.57769407], Loss = 0.4105\n",
      "Iteration 8190: Weights = [55.30333333  2.17732805  5.21426827  0.08576098  0.16432279 13.57805703], Loss = 0.4104\n",
      "Iteration 8191: Weights = [55.30333333  2.17718737  5.21393136  0.08575544  0.16431218 13.57841996], Loss = 0.4104\n",
      "Iteration 8192: Weights = [55.30333333  2.1770467   5.21359448  0.08574989  0.16430156 13.57878287], Loss = 0.4103\n",
      "Iteration 8193: Weights = [55.30333333  2.17690604  5.21325762  0.08574435  0.16429094 13.57914575], Loss = 0.4103\n",
      "Iteration 8194: Weights = [55.30333333  2.17676538  5.21292078  0.08573881  0.16428033 13.57950861], Loss = 0.4102\n",
      "Iteration 8195: Weights = [55.30333333  2.17662474  5.21258396  0.08573327  0.16426971 13.57987145], Loss = 0.4102\n",
      "Iteration 8196: Weights = [55.30333333  2.1764841   5.21224717  0.08572773  0.1642591  13.58023427], Loss = 0.4101\n",
      "Iteration 8197: Weights = [55.30333333  2.17634347  5.21191039  0.0857222   0.16424849 13.58059706], Loss = 0.4101\n",
      "Iteration 8198: Weights = [55.30333333  2.17620285  5.21157364  0.08571666  0.16423788 13.58095983], Loss = 0.4100\n",
      "Iteration 8199: Weights = [55.30333333  2.17606225  5.21123691  0.08571112  0.16422726 13.58132257], Loss = 0.4100\n",
      "Iteration 8200: Weights = [55.30333333  2.17592165  5.2109002   0.08570558  0.16421665 13.58168529], Loss = 0.4099\n",
      "Iteration 8201: Weights = [55.30333333  2.17578106  5.21056351  0.08570004  0.16420604 13.58204799], Loss = 0.4099\n",
      "Iteration 8202: Weights = [55.30333333  2.17564047  5.21022685  0.08569451  0.16419543 13.58241067], Loss = 0.4098\n",
      "Iteration 8203: Weights = [55.30333333  2.1754999   5.20989021  0.08568897  0.16418482 13.58277332], Loss = 0.4098\n",
      "Iteration 8204: Weights = [55.30333333  2.17535934  5.20955358  0.08568343  0.16417422 13.58313595], Loss = 0.4097\n",
      "Iteration 8205: Weights = [55.30333333  2.17521878  5.20921698  0.0856779   0.16416361 13.58349855], Loss = 0.4096\n",
      "Iteration 8206: Weights = [55.30333333  2.17507824  5.20888041  0.08567236  0.164153   13.58386113], Loss = 0.4096\n",
      "Iteration 8207: Weights = [55.30333333  2.1749377   5.20854385  0.08566682  0.16414239 13.58422369], Loss = 0.4095\n",
      "Iteration 8208: Weights = [55.30333333  2.17479718  5.20820732  0.08566129  0.16413179 13.58458622], Loss = 0.4095\n",
      "Iteration 8209: Weights = [55.30333333  2.17465666  5.2078708   0.08565575  0.16412118 13.58494873], Loss = 0.4094\n",
      "Iteration 8210: Weights = [55.30333333  2.17451615  5.20753431  0.08565022  0.16411058 13.58531122], Loss = 0.4094\n",
      "Iteration 8211: Weights = [55.30333333  2.17437565  5.20719784  0.08564469  0.16409998 13.58567368], Loss = 0.4093\n",
      "Iteration 8212: Weights = [55.30333333  2.17423516  5.2068614   0.08563915  0.16408937 13.58603612], Loss = 0.4093\n",
      "Iteration 8213: Weights = [55.30333333  2.17409468  5.20652497  0.08563362  0.16407877 13.58639854], Loss = 0.4092\n",
      "Iteration 8214: Weights = [55.30333333  2.1739542   5.20618857  0.08562809  0.16406817 13.58676093], Loss = 0.4092\n",
      "Iteration 8215: Weights = [55.30333333  2.17381374  5.20585218  0.08562255  0.16405757 13.5871233 ], Loss = 0.4091\n",
      "Iteration 8216: Weights = [55.30333333  2.17367329  5.20551582  0.08561702  0.16404697 13.58748565], Loss = 0.4091\n",
      "Iteration 8217: Weights = [55.30333333  2.17353284  5.20517948  0.08561149  0.16403637 13.58784797], Loss = 0.4090\n",
      "Iteration 8218: Weights = [55.30333333  2.1733924   5.20484317  0.08560596  0.16402577 13.58821027], Loss = 0.4090\n",
      "Iteration 8219: Weights = [55.30333333  2.17325198  5.20450687  0.08560043  0.16401517 13.58857255], Loss = 0.4089\n",
      "Iteration 8220: Weights = [55.30333333  2.17311156  5.2041706   0.0855949   0.16400458 13.5889348 ], Loss = 0.4089\n",
      "Iteration 8221: Weights = [55.30333333  2.17297115  5.20383435  0.08558937  0.16399398 13.58929703], Loss = 0.4088\n",
      "Iteration 8222: Weights = [55.30333333  2.17283075  5.20349812  0.08558384  0.16398338 13.58965924], Loss = 0.4087\n",
      "Iteration 8223: Weights = [55.30333333  2.17269036  5.20316191  0.08557831  0.16397279 13.59002142], Loss = 0.4087\n",
      "Iteration 8224: Weights = [55.30333333  2.17254998  5.20282572  0.08557278  0.16396219 13.59038358], Loss = 0.4086\n",
      "Iteration 8225: Weights = [55.30333333  2.1724096   5.20248956  0.08556725  0.1639516  13.59074572], Loss = 0.4086\n",
      "Iteration 8226: Weights = [55.30333333  2.17226924  5.20215341  0.08556172  0.16394101 13.59110783], Loss = 0.4085\n",
      "Iteration 8227: Weights = [55.30333333  2.17212888  5.20181729  0.08555619  0.16393041 13.59146992], Loss = 0.4085\n",
      "Iteration 8228: Weights = [55.30333333  2.17198854  5.20148119  0.08555066  0.16391982 13.59183198], Loss = 0.4084\n",
      "Iteration 8229: Weights = [55.30333333  2.1718482   5.20114511  0.08554513  0.16390923 13.59219403], Loss = 0.4084\n",
      "Iteration 8230: Weights = [55.30333333  2.17170788  5.20080906  0.08553961  0.16389864 13.59255605], Loss = 0.4083\n",
      "Iteration 8231: Weights = [55.30333333  2.17156756  5.20047302  0.08553408  0.16388805 13.59291804], Loss = 0.4083\n",
      "Iteration 8232: Weights = [55.30333333  2.17142725  5.20013701  0.08552855  0.16387746 13.59328001], Loss = 0.4082\n",
      "Iteration 8233: Weights = [55.30333333  2.17128695  5.19980102  0.08552303  0.16386687 13.59364196], Loss = 0.4082\n",
      "Iteration 8234: Weights = [55.30333333  2.17114666  5.19946505  0.0855175   0.16385628 13.59400389], Loss = 0.4081\n",
      "Iteration 8235: Weights = [55.30333333  2.17100637  5.1991291   0.08551198  0.1638457  13.59436579], Loss = 0.4081\n",
      "Iteration 8236: Weights = [55.30333333  2.1708661   5.19879317  0.08550645  0.16383511 13.59472767], Loss = 0.4080\n",
      "Iteration 8237: Weights = [55.30333333  2.17072584  5.19845727  0.08550093  0.16382453 13.59508952], Loss = 0.4080\n",
      "Iteration 8238: Weights = [55.30333333  2.17058558  5.19812139  0.0854954   0.16381394 13.59545136], Loss = 0.4079\n",
      "Iteration 8239: Weights = [55.30333333  2.17044534  5.19778553  0.08548988  0.16380336 13.59581316], Loss = 0.4078\n",
      "Iteration 8240: Weights = [55.30333333  2.1703051   5.19744969  0.08548435  0.16379277 13.59617495], Loss = 0.4078\n",
      "Iteration 8241: Weights = [55.30333333  2.17016487  5.19711387  0.08547883  0.16378219 13.59653671], Loss = 0.4077\n",
      "Iteration 8242: Weights = [55.30333333  2.17002465  5.19677807  0.08547331  0.16377161 13.59689845], Loss = 0.4077\n",
      "Iteration 8243: Weights = [55.30333333  2.16988444  5.1964423   0.08546779  0.16376103 13.59726016], Loss = 0.4076\n",
      "Iteration 8244: Weights = [55.30333333  2.16974424  5.19610655  0.08546226  0.16375044 13.59762186], Loss = 0.4076\n",
      "Iteration 8245: Weights = [55.30333333  2.16960405  5.19577081  0.08545674  0.16373986 13.59798352], Loss = 0.4075\n",
      "Iteration 8246: Weights = [55.30333333  2.16946387  5.19543511  0.08545122  0.16372928 13.59834517], Loss = 0.4075\n",
      "Iteration 8247: Weights = [55.30333333  2.16932369  5.19509942  0.0854457   0.16371871 13.59870679], Loss = 0.4074\n",
      "Iteration 8248: Weights = [55.30333333  2.16918353  5.19476375  0.08544018  0.16370813 13.59906839], Loss = 0.4074\n",
      "Iteration 8249: Weights = [55.30333333  2.16904338  5.19442811  0.08543466  0.16369755 13.59942996], Loss = 0.4073\n",
      "Iteration 8250: Weights = [55.30333333  2.16890323  5.19409249  0.08542914  0.16368697 13.59979151], Loss = 0.4073\n",
      "Iteration 8251: Weights = [55.30333333  2.16876309  5.19375688  0.08542362  0.1636764  13.60015304], Loss = 0.4072\n",
      "Iteration 8252: Weights = [55.30333333  2.16862296  5.19342131  0.0854181   0.16366582 13.60051455], Loss = 0.4072\n",
      "Iteration 8253: Weights = [55.30333333  2.16848284  5.19308575  0.08541258  0.16365525 13.60087603], Loss = 0.4071\n",
      "Iteration 8254: Weights = [55.30333333  2.16834273  5.19275021  0.08540706  0.16364467 13.60123749], Loss = 0.4071\n",
      "Iteration 8255: Weights = [55.30333333  2.16820263  5.1924147   0.08540154  0.1636341  13.60159892], Loss = 0.4070\n",
      "Iteration 8256: Weights = [55.30333333  2.16806254  5.19207921  0.08539602  0.16362353 13.60196033], Loss = 0.4070\n",
      "Iteration 8257: Weights = [55.30333333  2.16792246  5.19174374  0.08539051  0.16361295 13.60232172], Loss = 0.4069\n",
      "Iteration 8258: Weights = [55.30333333  2.16778238  5.19140829  0.08538499  0.16360238 13.60268309], Loss = 0.4068\n",
      "Iteration 8259: Weights = [55.30333333  2.16764232  5.19107286  0.08537947  0.16359181 13.60304443], Loss = 0.4068\n",
      "Iteration 8260: Weights = [55.30333333  2.16750226  5.19073745  0.08537396  0.16358124 13.60340574], Loss = 0.4067\n",
      "Iteration 8261: Weights = [55.30333333  2.16736222  5.19040207  0.08536844  0.16357067 13.60376704], Loss = 0.4067\n",
      "Iteration 8262: Weights = [55.30333333  2.16722218  5.19006671  0.08536292  0.1635601  13.60412831], Loss = 0.4066\n",
      "Iteration 8263: Weights = [55.30333333  2.16708215  5.18973137  0.08535741  0.16354954 13.60448956], Loss = 0.4066\n",
      "Iteration 8264: Weights = [55.30333333  2.16694213  5.18939605  0.08535189  0.16353897 13.60485078], Loss = 0.4065\n",
      "Iteration 8265: Weights = [55.30333333  2.16680212  5.18906075  0.08534638  0.1635284  13.60521198], Loss = 0.4065\n",
      "Iteration 8266: Weights = [55.30333333  2.16666212  5.18872547  0.08534086  0.16351784 13.60557316], Loss = 0.4064\n",
      "Iteration 8267: Weights = [55.30333333  2.16652213  5.18839022  0.08533535  0.16350727 13.60593432], Loss = 0.4064\n",
      "Iteration 8268: Weights = [55.30333333  2.16638214  5.18805499  0.08532984  0.16349671 13.60629545], Loss = 0.4063\n",
      "Iteration 8269: Weights = [55.30333333  2.16624217  5.18771978  0.08532432  0.16348614 13.60665656], Loss = 0.4063\n",
      "Iteration 8270: Weights = [55.30333333  2.1661022   5.18738459  0.08531881  0.16347558 13.60701764], Loss = 0.4062\n",
      "Iteration 8271: Weights = [55.30333333  2.16596225  5.18704942  0.0853133   0.16346502 13.6073787 ], Loss = 0.4062\n",
      "Iteration 8272: Weights = [55.30333333  2.1658223   5.18671428  0.08530779  0.16345446 13.60773974], Loss = 0.4061\n",
      "Iteration 8273: Weights = [55.30333333  2.16568236  5.18637915  0.08530227  0.16344389 13.60810075], Loss = 0.4061\n",
      "Iteration 8274: Weights = [55.30333333  2.16554243  5.18604405  0.08529676  0.16343333 13.60846174], Loss = 0.4060\n",
      "Iteration 8275: Weights = [55.30333333  2.16540251  5.18570897  0.08529125  0.16342277 13.60882271], Loss = 0.4060\n",
      "Iteration 8276: Weights = [55.30333333  2.1652626   5.18537391  0.08528574  0.16341222 13.60918366], Loss = 0.4059\n",
      "Iteration 8277: Weights = [55.30333333  2.1651227   5.18503887  0.08528023  0.16340166 13.60954458], Loss = 0.4059\n",
      "Iteration 8278: Weights = [55.30333333  2.16498281  5.18470386  0.08527472  0.1633911  13.60990548], Loss = 0.4058\n",
      "Iteration 8279: Weights = [55.30333333  2.16484292  5.18436886  0.08526921  0.16338054 13.61026635], Loss = 0.4057\n",
      "Iteration 8280: Weights = [55.30333333  2.16470305  5.18403389  0.0852637   0.16336999 13.6106272 ], Loss = 0.4057\n",
      "Iteration 8281: Weights = [55.30333333  2.16456318  5.18369894  0.08525819  0.16335943 13.61098803], Loss = 0.4056\n",
      "Iteration 8282: Weights = [55.30333333  2.16442333  5.18336401  0.08525268  0.16334888 13.61134883], Loss = 0.4056\n",
      "Iteration 8283: Weights = [55.30333333  2.16428348  5.1830291   0.08524717  0.16333832 13.61170962], Loss = 0.4055\n",
      "Iteration 8284: Weights = [55.30333333  2.16414364  5.18269422  0.08524167  0.16332777 13.61207037], Loss = 0.4055\n",
      "Iteration 8285: Weights = [55.30333333  2.16400381  5.18235935  0.08523616  0.16331721 13.61243111], Loss = 0.4054\n",
      "Iteration 8286: Weights = [55.30333333  2.16386399  5.18202451  0.08523065  0.16330666 13.61279182], Loss = 0.4054\n",
      "Iteration 8287: Weights = [55.30333333  2.16372418  5.18168969  0.08522514  0.16329611 13.61315251], Loss = 0.4053\n",
      "Iteration 8288: Weights = [55.30333333  2.16358438  5.18135489  0.08521964  0.16328556 13.61351317], Loss = 0.4053\n",
      "Iteration 8289: Weights = [55.30333333  2.16344458  5.18102011  0.08521413  0.16327501 13.61387381], Loss = 0.4052\n",
      "Iteration 8290: Weights = [55.30333333  2.1633048   5.18068536  0.08520863  0.16326446 13.61423443], Loss = 0.4052\n",
      "Iteration 8291: Weights = [55.30333333  2.16316502  5.18035062  0.08520312  0.16325391 13.61459503], Loss = 0.4051\n",
      "Iteration 8292: Weights = [55.30333333  2.16302526  5.18001591  0.08519762  0.16324336 13.6149556 ], Loss = 0.4051\n",
      "Iteration 8293: Weights = [55.30333333  2.1628855   5.17968122  0.08519211  0.16323282 13.61531615], Loss = 0.4050\n",
      "Iteration 8294: Weights = [55.30333333  2.16274575  5.17934655  0.08518661  0.16322227 13.61567667], Loss = 0.4050\n",
      "Iteration 8295: Weights = [55.30333333  2.16260601  5.1790119   0.0851811   0.16321172 13.61603717], Loss = 0.4049\n",
      "Iteration 8296: Weights = [55.30333333  2.16246628  5.17867727  0.0851756   0.16320118 13.61639765], Loss = 0.4049\n",
      "Iteration 8297: Weights = [55.30333333  2.16232656  5.17834267  0.08517009  0.16319063 13.61675811], Loss = 0.4048\n",
      "Iteration 8298: Weights = [55.30333333  2.16218685  5.17800808  0.08516459  0.16318009 13.61711854], Loss = 0.4048\n",
      "Iteration 8299: Weights = [55.30333333  2.16204714  5.17767352  0.08515909  0.16316954 13.61747895], Loss = 0.4047\n",
      "Iteration 8300: Weights = [55.30333333  2.16190745  5.17733898  0.08515359  0.163159   13.61783933], Loss = 0.4046\n",
      "Iteration 8301: Weights = [55.30333333  2.16176776  5.17700446  0.08514808  0.16314846 13.6181997 ], Loss = 0.4046\n",
      "Iteration 8302: Weights = [55.30333333  2.16162809  5.17666997  0.08514258  0.16313792 13.61856003], Loss = 0.4045\n",
      "Iteration 8303: Weights = [55.30333333  2.16148842  5.17633549  0.08513708  0.16312738 13.61892035], Loss = 0.4045\n",
      "Iteration 8304: Weights = [55.30333333  2.16134876  5.17600104  0.08513158  0.16311684 13.61928064], Loss = 0.4044\n",
      "Iteration 8305: Weights = [55.30333333  2.16120911  5.17566661  0.08512608  0.1631063  13.61964091], Loss = 0.4044\n",
      "Iteration 8306: Weights = [55.30333333  2.16106947  5.1753322   0.08512058  0.16309576 13.62000116], Loss = 0.4043\n",
      "Iteration 8307: Weights = [55.30333333  2.16092984  5.17499781  0.08511508  0.16308522 13.62036138], Loss = 0.4043\n",
      "Iteration 8308: Weights = [55.30333333  2.16079022  5.17466344  0.08510958  0.16307468 13.62072158], Loss = 0.4042\n",
      "Iteration 8309: Weights = [55.30333333  2.16065061  5.1743291   0.08510408  0.16306415 13.62108175], Loss = 0.4042\n",
      "Iteration 8310: Weights = [55.30333333  2.160511    5.17399477  0.08509858  0.16305361 13.62144191], Loss = 0.4041\n",
      "Iteration 8311: Weights = [55.30333333  2.16037141  5.17366047  0.08509309  0.16304308 13.62180203], Loss = 0.4041\n",
      "Iteration 8312: Weights = [55.30333333  2.16023182  5.17332619  0.08508759  0.16303254 13.62216214], Loss = 0.4040\n",
      "Iteration 8313: Weights = [55.30333333  2.16009225  5.17299193  0.08508209  0.16302201 13.62252222], Loss = 0.4040\n",
      "Iteration 8314: Weights = [55.30333333  2.15995268  5.17265769  0.08507659  0.16301148 13.62288228], Loss = 0.4039\n",
      "Iteration 8315: Weights = [55.30333333  2.15981312  5.17232348  0.0850711   0.16300094 13.62324232], Loss = 0.4039\n",
      "Iteration 8316: Weights = [55.30333333  2.15967357  5.17198928  0.0850656   0.16299041 13.62360233], Loss = 0.4038\n",
      "Iteration 8317: Weights = [55.30333333  2.15953403  5.17165511  0.0850601   0.16297988 13.62396232], Loss = 0.4038\n",
      "Iteration 8318: Weights = [55.30333333  2.1593945   5.17132096  0.08505461  0.16296935 13.62432229], Loss = 0.4037\n",
      "Iteration 8319: Weights = [55.30333333  2.15925497  5.17098683  0.08504911  0.16295882 13.62468223], Loss = 0.4037\n",
      "Iteration 8320: Weights = [55.30333333  2.15911546  5.17065272  0.08504362  0.16294829 13.62504215], Loss = 0.4036\n",
      "Iteration 8321: Weights = [55.30333333  2.15897596  5.17031864  0.08503812  0.16293776 13.62540205], Loss = 0.4036\n",
      "Iteration 8322: Weights = [55.30333333  2.15883646  5.16998457  0.08503263  0.16292723 13.62576192], Loss = 0.4035\n",
      "Iteration 8323: Weights = [55.30333333  2.15869697  5.16965053  0.08502713  0.16291671 13.62612177], Loss = 0.4034\n",
      "Iteration 8324: Weights = [55.30333333  2.15855749  5.16931651  0.08502164  0.16290618 13.6264816 ], Loss = 0.4034\n",
      "Iteration 8325: Weights = [55.30333333  2.15841803  5.16898251  0.08501614  0.16289566 13.6268414 ], Loss = 0.4033\n",
      "Iteration 8326: Weights = [55.30333333  2.15827857  5.16864853  0.08501065  0.16288513 13.62720118], Loss = 0.4033\n",
      "Iteration 8327: Weights = [55.30333333  2.15813912  5.16831457  0.08500516  0.16287461 13.62756094], Loss = 0.4032\n",
      "Iteration 8328: Weights = [55.30333333  2.15799967  5.16798064  0.08499967  0.16286408 13.62792067], Loss = 0.4032\n",
      "Iteration 8329: Weights = [55.30333333  2.15786024  5.16764672  0.08499417  0.16285356 13.62828038], Loss = 0.4031\n",
      "Iteration 8330: Weights = [55.30333333  2.15772082  5.16731283  0.08498868  0.16284304 13.62864007], Loss = 0.4031\n",
      "Iteration 8331: Weights = [55.30333333  2.1575814   5.16697896  0.08498319  0.16283252 13.62899973], Loss = 0.4030\n",
      "Iteration 8332: Weights = [55.30333333  2.157442    5.16664511  0.0849777   0.16282199 13.62935937], Loss = 0.4030\n",
      "Iteration 8333: Weights = [55.30333333  2.1573026   5.16631128  0.08497221  0.16281147 13.62971899], Loss = 0.4029\n",
      "Iteration 8334: Weights = [55.30333333  2.15716321  5.16597748  0.08496672  0.16280095 13.63007859], Loss = 0.4029\n",
      "Iteration 8335: Weights = [55.30333333  2.15702383  5.16564369  0.08496123  0.16279044 13.63043816], Loss = 0.4028\n",
      "Iteration 8336: Weights = [55.30333333  2.15688446  5.16530993  0.08495574  0.16277992 13.63079771], Loss = 0.4028\n",
      "Iteration 8337: Weights = [55.30333333  2.1567451   5.16497619  0.08495025  0.1627694  13.63115723], Loss = 0.4027\n",
      "Iteration 8338: Weights = [55.30333333  2.15660575  5.16464247  0.08494476  0.16275888 13.63151673], Loss = 0.4027\n",
      "Iteration 8339: Weights = [55.30333333  2.15646641  5.16430877  0.08493927  0.16274837 13.63187621], Loss = 0.4026\n",
      "Iteration 8340: Weights = [55.30333333  2.15632708  5.1639751   0.08493379  0.16273785 13.63223567], Loss = 0.4026\n",
      "Iteration 8341: Weights = [55.30333333  2.15618775  5.16364144  0.0849283   0.16272734 13.6325951 ], Loss = 0.4025\n",
      "Iteration 8342: Weights = [55.30333333  2.15604844  5.16330781  0.08492281  0.16271682 13.63295451], Loss = 0.4025\n",
      "Iteration 8343: Weights = [55.30333333  2.15590913  5.1629742   0.08491732  0.16270631 13.63331389], Loss = 0.4024\n",
      "Iteration 8344: Weights = [55.30333333  2.15576983  5.16264061  0.08491184  0.1626958  13.63367325], Loss = 0.4024\n",
      "Iteration 8345: Weights = [55.30333333  2.15563054  5.16230704  0.08490635  0.16268528 13.63403259], Loss = 0.4023\n",
      "Iteration 8346: Weights = [55.30333333  2.15549126  5.16197349  0.08490087  0.16267477 13.63439191], Loss = 0.4022\n",
      "Iteration 8347: Weights = [55.30333333  2.15535199  5.16163997  0.08489538  0.16266426 13.6347512 ], Loss = 0.4022\n",
      "Iteration 8348: Weights = [55.30333333  2.15521273  5.16130646  0.08488989  0.16265375 13.63511047], Loss = 0.4021\n",
      "Iteration 8349: Weights = [55.30333333  2.15507348  5.16097298  0.08488441  0.16264324 13.63546972], Loss = 0.4021\n",
      "Iteration 8350: Weights = [55.30333333  2.15493423  5.16063952  0.08487892  0.16263273 13.63582894], Loss = 0.4020\n",
      "Iteration 8351: Weights = [55.30333333  2.154795    5.16030608  0.08487344  0.16262223 13.63618814], Loss = 0.4020\n",
      "Iteration 8352: Weights = [55.30333333  2.15465577  5.15997266  0.08486796  0.16261172 13.63654731], Loss = 0.4019\n",
      "Iteration 8353: Weights = [55.30333333  2.15451656  5.15963927  0.08486247  0.16260121 13.63690647], Loss = 0.4019\n",
      "Iteration 8354: Weights = [55.30333333  2.15437735  5.15930589  0.08485699  0.16259071 13.6372656 ], Loss = 0.4018\n",
      "Iteration 8355: Weights = [55.30333333  2.15423815  5.15897254  0.08485151  0.1625802  13.6376247 ], Loss = 0.4018\n",
      "Iteration 8356: Weights = [55.30333333  2.15409896  5.15863921  0.08484602  0.1625697  13.63798379], Loss = 0.4017\n",
      "Iteration 8357: Weights = [55.30333333  2.15395978  5.1583059   0.08484054  0.16255919 13.63834285], Loss = 0.4017\n",
      "Iteration 8358: Weights = [55.30333333  2.15382061  5.15797261  0.08483506  0.16254869 13.63870189], Loss = 0.4016\n",
      "Iteration 8359: Weights = [55.30333333  2.15368145  5.15763934  0.08482958  0.16253819 13.6390609 ], Loss = 0.4016\n",
      "Iteration 8360: Weights = [55.30333333  2.15354229  5.1573061   0.0848241   0.16252768 13.63941989], Loss = 0.4015\n",
      "Iteration 8361: Weights = [55.30333333  2.15340315  5.15697287  0.08481862  0.16251718 13.63977886], Loss = 0.4015\n",
      "Iteration 8362: Weights = [55.30333333  2.15326401  5.15663967  0.08481314  0.16250668 13.6401378 ], Loss = 0.4014\n",
      "Iteration 8363: Weights = [55.30333333  2.15312489  5.15630649  0.08480766  0.16249618 13.64049672], Loss = 0.4014\n",
      "Iteration 8364: Weights = [55.30333333  2.15298577  5.15597333  0.08480218  0.16248568 13.64085562], Loss = 0.4013\n",
      "Iteration 8365: Weights = [55.30333333  2.15284666  5.15564019  0.0847967   0.16247518 13.6412145 ], Loss = 0.4013\n",
      "Iteration 8366: Weights = [55.30333333  2.15270756  5.15530707  0.08479122  0.16246469 13.64157335], Loss = 0.4012\n",
      "Iteration 8367: Weights = [55.30333333  2.15256847  5.15497398  0.08478574  0.16245419 13.64193218], Loss = 0.4012\n",
      "Iteration 8368: Weights = [55.30333333  2.15242939  5.15464091  0.08478026  0.16244369 13.64229098], Loss = 0.4011\n",
      "Iteration 8369: Weights = [55.30333333  2.15229031  5.15430786  0.08477479  0.1624332  13.64264976], Loss = 0.4011\n",
      "Iteration 8370: Weights = [55.30333333  2.15215125  5.15397483  0.08476931  0.1624227  13.64300852], Loss = 0.4010\n",
      "Iteration 8371: Weights = [55.30333333  2.1520122   5.15364182  0.08476383  0.16241221 13.64336726], Loss = 0.4010\n",
      "Iteration 8372: Weights = [55.30333333  2.15187315  5.15330883  0.08475835  0.16240171 13.64372597], Loss = 0.4009\n",
      "Iteration 8373: Weights = [55.30333333  2.15173411  5.15297586  0.08475288  0.16239122 13.64408466], Loss = 0.4008\n",
      "Iteration 8374: Weights = [55.30333333  2.15159509  5.15264292  0.0847474   0.16238073 13.64444333], Loss = 0.4008\n",
      "Iteration 8375: Weights = [55.30333333  2.15145607  5.15231     0.08474193  0.16237024 13.64480197], Loss = 0.4007\n",
      "Iteration 8376: Weights = [55.30333333  2.15131706  5.1519771   0.08473645  0.16235975 13.64516059], Loss = 0.4007\n",
      "Iteration 8377: Weights = [55.30333333  2.15117806  5.15164422  0.08473098  0.16234926 13.64551919], Loss = 0.4006\n",
      "Iteration 8378: Weights = [55.30333333  2.15103906  5.15131136  0.0847255   0.16233877 13.64587776], Loss = 0.4006\n",
      "Iteration 8379: Weights = [55.30333333  2.15090008  5.15097852  0.08472003  0.16232828 13.64623631], Loss = 0.4005\n",
      "Iteration 8380: Weights = [55.30333333  2.15076111  5.15064571  0.08471455  0.16231779 13.64659484], Loss = 0.4005\n",
      "Iteration 8381: Weights = [55.30333333  2.15062214  5.15031291  0.08470908  0.1623073  13.64695334], Loss = 0.4004\n",
      "Iteration 8382: Weights = [55.30333333  2.15048319  5.14998014  0.08470361  0.16229681 13.64731182], Loss = 0.4004\n",
      "Iteration 8383: Weights = [55.30333333  2.15034424  5.14964739  0.08469813  0.16228633 13.64767028], Loss = 0.4003\n",
      "Iteration 8384: Weights = [55.30333333  2.1502053   5.14931466  0.08469266  0.16227584 13.64802871], Loss = 0.4003\n",
      "Iteration 8385: Weights = [55.30333333  2.15006637  5.14898195  0.08468719  0.16226536 13.64838713], Loss = 0.4002\n",
      "Iteration 8386: Weights = [55.30333333  2.14992745  5.14864927  0.08468172  0.16225487 13.64874551], Loss = 0.4002\n",
      "Iteration 8387: Weights = [55.30333333  2.14978854  5.1483166   0.08467625  0.16224439 13.64910388], Loss = 0.4001\n",
      "Iteration 8388: Weights = [55.30333333  2.14964964  5.14798396  0.08467077  0.16223391 13.64946222], Loss = 0.4001\n",
      "Iteration 8389: Weights = [55.30333333  2.14951075  5.14765134  0.0846653   0.16222342 13.64982054], Loss = 0.4000\n",
      "Iteration 8390: Weights = [55.30333333  2.14937186  5.14731874  0.08465983  0.16221294 13.65017884], Loss = 0.4000\n",
      "Iteration 8391: Weights = [55.30333333  2.14923299  5.14698616  0.08465436  0.16220246 13.65053711], Loss = 0.3999\n",
      "Iteration 8392: Weights = [55.30333333  2.14909412  5.1466536   0.08464889  0.16219198 13.65089536], Loss = 0.3999\n",
      "Iteration 8393: Weights = [55.30333333  2.14895526  5.14632107  0.08464342  0.1621815  13.65125358], Loss = 0.3998\n",
      "Iteration 8394: Weights = [55.30333333  2.14881641  5.14598855  0.08463795  0.16217102 13.65161179], Loss = 0.3998\n",
      "Iteration 8395: Weights = [55.30333333  2.14867757  5.14565606  0.08463249  0.16216054 13.65196997], Loss = 0.3997\n",
      "Iteration 8396: Weights = [55.30333333  2.14853874  5.14532359  0.08462702  0.16215007 13.65232812], Loss = 0.3997\n",
      "Iteration 8397: Weights = [55.30333333  2.14839992  5.14499114  0.08462155  0.16213959 13.65268626], Loss = 0.3996\n",
      "Iteration 8398: Weights = [55.30333333  2.14826111  5.14465871  0.08461608  0.16212911 13.65304437], Loss = 0.3996\n",
      "Iteration 8399: Weights = [55.30333333  2.14812231  5.1443263   0.08461062  0.16211864 13.65340245], Loss = 0.3995\n",
      "Iteration 8400: Weights = [55.30333333  2.14798351  5.14399392  0.08460515  0.16210816 13.65376052], Loss = 0.3995\n",
      "Iteration 8401: Weights = [55.30333333  2.14784473  5.14366156  0.08459968  0.16209769 13.65411856], Loss = 0.3994\n",
      "Iteration 8402: Weights = [55.30333333  2.14770595  5.14332921  0.08459422  0.16208722 13.65447658], Loss = 0.3993\n",
      "Iteration 8403: Weights = [55.30333333  2.14756718  5.14299689  0.08458875  0.16207674 13.65483457], Loss = 0.3993\n",
      "Iteration 8404: Weights = [55.30333333  2.14742842  5.14266459  0.08458328  0.16206627 13.65519254], Loss = 0.3992\n",
      "Iteration 8405: Weights = [55.30333333  2.14728967  5.14233231  0.08457782  0.1620558  13.65555049], Loss = 0.3992\n",
      "Iteration 8406: Weights = [55.30333333  2.14715093  5.14200006  0.08457235  0.16204533 13.65590842], Loss = 0.3991\n",
      "Iteration 8407: Weights = [55.30333333  2.1470122   5.14166782  0.08456689  0.16203486 13.65626632], Loss = 0.3991\n",
      "Iteration 8408: Weights = [55.30333333  2.14687348  5.14133561  0.08456143  0.16202439 13.6566242 ], Loss = 0.3990\n",
      "Iteration 8409: Weights = [55.30333333  2.14673477  5.14100342  0.08455596  0.16201392 13.65698206], Loss = 0.3990\n",
      "Iteration 8410: Weights = [55.30333333  2.14659606  5.14067125  0.0845505   0.16200345 13.65733989], Loss = 0.3989\n",
      "Iteration 8411: Weights = [55.30333333  2.14645736  5.1403391   0.08454504  0.16199299 13.6576977 ], Loss = 0.3989\n",
      "Iteration 8412: Weights = [55.30333333  2.14631868  5.14000697  0.08453957  0.16198252 13.65805548], Loss = 0.3988\n",
      "Iteration 8413: Weights = [55.30333333  2.14618     5.13967486  0.08453411  0.16197205 13.65841325], Loss = 0.3988\n",
      "Iteration 8414: Weights = [55.30333333  2.14604133  5.13934278  0.08452865  0.16196159 13.65877099], Loss = 0.3987\n",
      "Iteration 8415: Weights = [55.30333333  2.14590267  5.13901072  0.08452319  0.16195112 13.65912871], Loss = 0.3987\n",
      "Iteration 8416: Weights = [55.30333333  2.14576402  5.13867867  0.08451773  0.16194066 13.6594864 ], Loss = 0.3986\n",
      "Iteration 8417: Weights = [55.30333333  2.14562538  5.13834665  0.08451227  0.1619302  13.65984407], Loss = 0.3986\n",
      "Iteration 8418: Weights = [55.30333333  2.14548674  5.13801466  0.08450681  0.16191973 13.66020172], Loss = 0.3985\n",
      "Iteration 8419: Weights = [55.30333333  2.14534812  5.13768268  0.08450135  0.16190927 13.66055934], Loss = 0.3985\n",
      "Iteration 8420: Weights = [55.30333333  2.1452095   5.13735072  0.08449589  0.16189881 13.66091695], Loss = 0.3984\n",
      "Iteration 8421: Weights = [55.30333333  2.1450709   5.13701879  0.08449043  0.16188835 13.66127452], Loss = 0.3984\n",
      "Iteration 8422: Weights = [55.30333333  2.1449323   5.13668687  0.08448497  0.16187789 13.66163208], Loss = 0.3983\n",
      "Iteration 8423: Weights = [55.30333333  2.14479371  5.13635498  0.08447951  0.16186743 13.66198961], Loss = 0.3983\n",
      "Iteration 8424: Weights = [55.30333333  2.14465513  5.13602311  0.08447405  0.16185697 13.66234712], Loss = 0.3982\n",
      "Iteration 8425: Weights = [55.30333333  2.14451656  5.13569126  0.08446859  0.16184651 13.66270461], Loss = 0.3982\n",
      "Iteration 8426: Weights = [55.30333333  2.144378    5.13535944  0.08446313  0.16183606 13.66306207], Loss = 0.3981\n",
      "Iteration 8427: Weights = [55.30333333  2.14423945  5.13502763  0.08445768  0.1618256  13.66341951], Loss = 0.3981\n",
      "Iteration 8428: Weights = [55.30333333  2.14410091  5.13469585  0.08445222  0.16181514 13.66377693], Loss = 0.3980\n",
      "Iteration 8429: Weights = [55.30333333  2.14396237  5.13436408  0.08444676  0.16180469 13.66413432], Loss = 0.3980\n",
      "Iteration 8430: Weights = [55.30333333  2.14382384  5.13403234  0.08444131  0.16179423 13.66449169], Loss = 0.3979\n",
      "Iteration 8431: Weights = [55.30333333  2.14368533  5.13370062  0.08443585  0.16178378 13.66484904], Loss = 0.3979\n",
      "Iteration 8432: Weights = [55.30333333  2.14354682  5.13336892  0.0844304   0.16177333 13.66520636], Loss = 0.3978\n",
      "Iteration 8433: Weights = [55.30333333  2.14340832  5.13303725  0.08442494  0.16176287 13.66556367], Loss = 0.3978\n",
      "Iteration 8434: Weights = [55.30333333  2.14326983  5.13270559  0.08441949  0.16175242 13.66592094], Loss = 0.3977\n",
      "Iteration 8435: Weights = [55.30333333  2.14313135  5.13237395  0.08441403  0.16174197 13.6662782 ], Loss = 0.3976\n",
      "Iteration 8436: Weights = [55.30333333  2.14299288  5.13204234  0.08440858  0.16173152 13.66663543], Loss = 0.3976\n",
      "Iteration 8437: Weights = [55.30333333  2.14285442  5.13171075  0.08440312  0.16172107 13.66699264], Loss = 0.3975\n",
      "Iteration 8438: Weights = [55.30333333  2.14271596  5.13137918  0.08439767  0.16171062 13.66734983], Loss = 0.3975\n",
      "Iteration 8439: Weights = [55.30333333  2.14257752  5.13104763  0.08439222  0.16170017 13.66770699], Loss = 0.3974\n",
      "Iteration 8440: Weights = [55.30333333  2.14243908  5.1307161   0.08438676  0.16168973 13.66806413], Loss = 0.3974\n",
      "Iteration 8441: Weights = [55.30333333  2.14230065  5.1303846   0.08438131  0.16167928 13.66842125], Loss = 0.3973\n",
      "Iteration 8442: Weights = [55.30333333  2.14216223  5.13005311  0.08437586  0.16166883 13.66877834], Loss = 0.3973\n",
      "Iteration 8443: Weights = [55.30333333  2.14202382  5.12972165  0.08437041  0.16165839 13.66913541], Loss = 0.3972\n",
      "Iteration 8444: Weights = [55.30333333  2.14188542  5.12939021  0.08436496  0.16164794 13.66949246], Loss = 0.3972\n",
      "Iteration 8445: Weights = [55.30333333  2.14174703  5.12905879  0.0843595   0.1616375  13.66984948], Loss = 0.3971\n",
      "Iteration 8446: Weights = [55.30333333  2.14160865  5.12872739  0.08435405  0.16162705 13.67020649], Loss = 0.3971\n",
      "Iteration 8447: Weights = [55.30333333  2.14147028  5.12839601  0.0843486   0.16161661 13.67056346], Loss = 0.3970\n",
      "Iteration 8448: Weights = [55.30333333  2.14133191  5.12806466  0.08434315  0.16160617 13.67092042], Loss = 0.3970\n",
      "Iteration 8449: Weights = [55.30333333  2.14119356  5.12773332  0.0843377   0.16159573 13.67127735], Loss = 0.3969\n",
      "Iteration 8450: Weights = [55.30333333  2.14105521  5.12740201  0.08433226  0.16158528 13.67163426], Loss = 0.3969\n",
      "Iteration 8451: Weights = [55.30333333  2.14091687  5.12707072  0.08432681  0.16157484 13.67199115], Loss = 0.3968\n",
      "Iteration 8452: Weights = [55.30333333  2.14077854  5.12673945  0.08432136  0.1615644  13.67234801], Loss = 0.3968\n",
      "Iteration 8453: Weights = [55.30333333  2.14064022  5.1264082   0.08431591  0.16155397 13.67270485], Loss = 0.3967\n",
      "Iteration 8454: Weights = [55.30333333  2.14050191  5.12607697  0.08431046  0.16154353 13.67306167], Loss = 0.3967\n",
      "Iteration 8455: Weights = [55.30333333  2.14036361  5.12574576  0.08430501  0.16153309 13.67341846], Loss = 0.3966\n",
      "Iteration 8456: Weights = [55.30333333  2.14022532  5.12541458  0.08429957  0.16152265 13.67377523], Loss = 0.3966\n",
      "Iteration 8457: Weights = [55.30333333  2.14008703  5.12508342  0.08429412  0.16151222 13.67413198], Loss = 0.3965\n",
      "Iteration 8458: Weights = [55.30333333  2.13994876  5.12475227  0.08428867  0.16150178 13.6744887 ], Loss = 0.3965\n",
      "Iteration 8459: Weights = [55.30333333  2.13981049  5.12442115  0.08428323  0.16149135 13.67484541], Loss = 0.3964\n",
      "Iteration 8460: Weights = [55.30333333  2.13967223  5.12409005  0.08427778  0.16148091 13.67520208], Loss = 0.3964\n",
      "Iteration 8461: Weights = [55.30333333  2.13953398  5.12375898  0.08427234  0.16147048 13.67555874], Loss = 0.3963\n",
      "Iteration 8462: Weights = [55.30333333  2.13939574  5.12342792  0.08426689  0.16146005 13.67591537], Loss = 0.3963\n",
      "Iteration 8463: Weights = [55.30333333  2.13925751  5.12309689  0.08426145  0.16144961 13.67627198], Loss = 0.3962\n",
      "Iteration 8464: Weights = [55.30333333  2.13911929  5.12276587  0.084256    0.16143918 13.67662857], Loss = 0.3962\n",
      "Iteration 8465: Weights = [55.30333333  2.13898108  5.12243488  0.08425056  0.16142875 13.67698513], Loss = 0.3961\n",
      "Iteration 8466: Weights = [55.30333333  2.13884288  5.12210391  0.08424512  0.16141832 13.67734167], Loss = 0.3961\n",
      "Iteration 8467: Weights = [55.30333333  2.13870468  5.12177296  0.08423967  0.16140789 13.67769819], Loss = 0.3960\n",
      "Iteration 8468: Weights = [55.30333333  2.13856649  5.12144203  0.08423423  0.16139746 13.67805468], Loss = 0.3960\n",
      "Iteration 8469: Weights = [55.30333333  2.13842832  5.12111112  0.08422879  0.16138703 13.67841116], Loss = 0.3959\n",
      "Iteration 8470: Weights = [55.30333333  2.13829015  5.12078024  0.08422334  0.16137661 13.6787676 ], Loss = 0.3959\n",
      "Iteration 8471: Weights = [55.30333333  2.13815199  5.12044938  0.0842179   0.16136618 13.67912403], Loss = 0.3958\n",
      "Iteration 8472: Weights = [55.30333333  2.13801384  5.12011853  0.08421246  0.16135575 13.67948043], Loss = 0.3958\n",
      "Iteration 8473: Weights = [55.30333333  2.1378757   5.11978771  0.08420702  0.16134533 13.67983681], Loss = 0.3957\n",
      "Iteration 8474: Weights = [55.30333333  2.13773757  5.11945691  0.08420158  0.1613349  13.68019317], Loss = 0.3956\n",
      "Iteration 8475: Weights = [55.30333333  2.13759944  5.11912613  0.08419614  0.16132448 13.6805495 ], Loss = 0.3956\n",
      "Iteration 8476: Weights = [55.30333333  2.13746133  5.11879538  0.0841907   0.16131405 13.68090581], Loss = 0.3955\n",
      "Iteration 8477: Weights = [55.30333333  2.13732322  5.11846464  0.08418526  0.16130363 13.6812621 ], Loss = 0.3955\n",
      "Iteration 8478: Weights = [55.30333333  2.13718513  5.11813393  0.08417982  0.16129321 13.68161836], Loss = 0.3954\n",
      "Iteration 8479: Weights = [55.30333333  2.13704704  5.11780323  0.08417438  0.16128279 13.6819746 ], Loss = 0.3954\n",
      "Iteration 8480: Weights = [55.30333333  2.13690896  5.11747256  0.08416894  0.16127237 13.68233082], Loss = 0.3953\n",
      "Iteration 8481: Weights = [55.30333333  2.13677089  5.11714191  0.0841635   0.16126195 13.68268702], Loss = 0.3953\n",
      "Iteration 8482: Weights = [55.30333333  2.13663283  5.11681128  0.08415807  0.16125153 13.68304319], Loss = 0.3952\n",
      "Iteration 8483: Weights = [55.30333333  2.13649478  5.11648068  0.08415263  0.16124111 13.68339934], Loss = 0.3952\n",
      "Iteration 8484: Weights = [55.30333333  2.13635673  5.11615009  0.08414719  0.16123069 13.68375546], Loss = 0.3951\n",
      "Iteration 8485: Weights = [55.30333333  2.1362187   5.11581953  0.08414175  0.16122027 13.68411157], Loss = 0.3951\n",
      "Iteration 8486: Weights = [55.30333333  2.13608067  5.11548898  0.08413632  0.16120986 13.68446765], Loss = 0.3950\n",
      "Iteration 8487: Weights = [55.30333333  2.13594266  5.11515846  0.08413088  0.16119944 13.6848237 ], Loss = 0.3950\n",
      "Iteration 8488: Weights = [55.30333333  2.13580465  5.11482796  0.08412545  0.16118903 13.68517974], Loss = 0.3949\n",
      "Iteration 8489: Weights = [55.30333333  2.13566665  5.11449748  0.08412001  0.16117861 13.68553575], Loss = 0.3949\n",
      "Iteration 8490: Weights = [55.30333333  2.13552866  5.11416702  0.08411457  0.1611682  13.68589174], Loss = 0.3948\n",
      "Iteration 8491: Weights = [55.30333333  2.13539068  5.11383659  0.08410914  0.16115778 13.6862477 ], Loss = 0.3948\n",
      "Iteration 8492: Weights = [55.30333333  2.13525271  5.11350617  0.08410371  0.16114737 13.68660364], Loss = 0.3947\n",
      "Iteration 8493: Weights = [55.30333333  2.13511474  5.11317578  0.08409827  0.16113696 13.68695956], Loss = 0.3947\n",
      "Iteration 8494: Weights = [55.30333333  2.13497679  5.1128454   0.08409284  0.16112655 13.68731546], Loss = 0.3946\n",
      "Iteration 8495: Weights = [55.30333333  2.13483885  5.11251505  0.0840874   0.16111614 13.68767133], Loss = 0.3946\n",
      "Iteration 8496: Weights = [55.30333333  2.13470091  5.11218472  0.08408197  0.16110573 13.68802718], Loss = 0.3945\n",
      "Iteration 8497: Weights = [55.30333333  2.13456298  5.11185441  0.08407654  0.16109532 13.68838301], Loss = 0.3945\n",
      "Iteration 8498: Weights = [55.30333333  2.13442506  5.11152413  0.08407111  0.16108491 13.68873881], Loss = 0.3944\n",
      "Iteration 8499: Weights = [55.30333333  2.13428715  5.11119386  0.08406567  0.1610745  13.68909459], Loss = 0.3944\n",
      "Iteration 8500: Weights = [55.30333333  2.13414925  5.11086362  0.08406024  0.16106409 13.68945035], Loss = 0.3943\n",
      "Iteration 8501: Weights = [55.30333333  2.13401136  5.11053339  0.08405481  0.16105369 13.68980609], Loss = 0.3943\n",
      "Iteration 8502: Weights = [55.30333333  2.13387348  5.11020319  0.08404938  0.16104328 13.6901618 ], Loss = 0.3942\n",
      "Iteration 8503: Weights = [55.30333333  2.13373561  5.10987301  0.08404395  0.16103287 13.69051749], Loss = 0.3942\n",
      "Iteration 8504: Weights = [55.30333333  2.13359774  5.10954285  0.08403852  0.16102247 13.69087315], Loss = 0.3941\n",
      "Iteration 8505: Weights = [55.30333333  2.13345988  5.10921271  0.08403309  0.16101207 13.6912288 ], Loss = 0.3941\n",
      "Iteration 8506: Weights = [55.30333333  2.13332204  5.1088826   0.08402766  0.16100166 13.69158442], Loss = 0.3940\n",
      "Iteration 8507: Weights = [55.30333333  2.1331842   5.1085525   0.08402223  0.16099126 13.69194001], Loss = 0.3940\n",
      "Iteration 8508: Weights = [55.30333333  2.13304637  5.10822243  0.0840168   0.16098086 13.69229559], Loss = 0.3939\n",
      "Iteration 8509: Weights = [55.30333333  2.13290855  5.10789238  0.08401137  0.16097046 13.69265114], Loss = 0.3939\n",
      "Iteration 8510: Weights = [55.30333333  2.13277074  5.10756234  0.08400594  0.16096006 13.69300667], Loss = 0.3938\n",
      "Iteration 8511: Weights = [55.30333333  2.13263293  5.10723233  0.08400052  0.16094966 13.69336217], Loss = 0.3938\n",
      "Iteration 8512: Weights = [55.30333333  2.13249514  5.10690235  0.08399509  0.16093926 13.69371765], Loss = 0.3937\n",
      "Iteration 8513: Weights = [55.30333333  2.13235736  5.10657238  0.08398966  0.16092886 13.69407311], Loss = 0.3937\n",
      "Iteration 8514: Weights = [55.30333333  2.13221958  5.10624243  0.08398424  0.16091846 13.69442855], Loss = 0.3936\n",
      "Iteration 8515: Weights = [55.30333333  2.13208181  5.10591251  0.08397881  0.16090806 13.69478396], Loss = 0.3936\n",
      "Iteration 8516: Weights = [55.30333333  2.13194406  5.10558261  0.08397338  0.16089767 13.69513935], Loss = 0.3935\n",
      "Iteration 8517: Weights = [55.30333333  2.13180631  5.10525272  0.08396796  0.16088727 13.69549472], Loss = 0.3935\n",
      "Iteration 8518: Weights = [55.30333333  2.13166857  5.10492286  0.08396253  0.16087688 13.69585007], Loss = 0.3934\n",
      "Iteration 8519: Weights = [55.30333333  2.13153083  5.10459302  0.08395711  0.16086648 13.69620539], Loss = 0.3934\n",
      "Iteration 8520: Weights = [55.30333333  2.13139311  5.10426321  0.08395168  0.16085609 13.69656069], Loss = 0.3933\n",
      "Iteration 8521: Weights = [55.30333333  2.1312554   5.10393341  0.08394626  0.16084569 13.69691596], Loss = 0.3933\n",
      "Iteration 8522: Weights = [55.30333333  2.13111769  5.10360363  0.08394083  0.1608353  13.69727121], Loss = 0.3932\n",
      "Iteration 8523: Weights = [55.30333333  2.13098     5.10327388  0.08393541  0.16082491 13.69762644], Loss = 0.3932\n",
      "Iteration 8524: Weights = [55.30333333  2.13084231  5.10294415  0.08392999  0.16081452 13.69798165], Loss = 0.3931\n",
      "Iteration 8525: Weights = [55.30333333  2.13070463  5.10261443  0.08392456  0.16080413 13.69833683], Loss = 0.3931\n",
      "Iteration 8526: Weights = [55.30333333  2.13056696  5.10228474  0.08391914  0.16079374 13.698692  ], Loss = 0.3930\n",
      "Iteration 8527: Weights = [55.30333333  2.1304293   5.10195508  0.08391372  0.16078335 13.69904713], Loss = 0.3929\n",
      "Iteration 8528: Weights = [55.30333333  2.13029165  5.10162543  0.0839083   0.16077296 13.69940225], Loss = 0.3929\n",
      "Iteration 8529: Weights = [55.30333333  2.13015401  5.1012958   0.08390288  0.16076257 13.69975734], Loss = 0.3928\n",
      "Iteration 8530: Weights = [55.30333333  2.13001638  5.1009662   0.08389746  0.16075218 13.70011241], Loss = 0.3928\n",
      "Iteration 8531: Weights = [55.30333333  2.12987875  5.10063661  0.08389203  0.1607418  13.70046746], Loss = 0.3927\n",
      "Iteration 8532: Weights = [55.30333333  2.12974114  5.10030705  0.08388661  0.16073141 13.70082248], Loss = 0.3927\n",
      "Iteration 8533: Weights = [55.30333333  2.12960353  5.09997751  0.08388119  0.16072103 13.70117748], Loss = 0.3926\n",
      "Iteration 8534: Weights = [55.30333333  2.12946593  5.09964799  0.08387577  0.16071064 13.70153246], Loss = 0.3926\n",
      "Iteration 8535: Weights = [55.30333333  2.12932834  5.09931849  0.08387036  0.16070026 13.70188741], Loss = 0.3925\n",
      "Iteration 8536: Weights = [55.30333333  2.12919076  5.09898901  0.08386494  0.16068988 13.70224234], Loss = 0.3925\n",
      "Iteration 8537: Weights = [55.30333333  2.12905319  5.09865956  0.08385952  0.16067949 13.70259725], Loss = 0.3924\n",
      "Iteration 8538: Weights = [55.30333333  2.12891563  5.09833012  0.0838541   0.16066911 13.70295214], Loss = 0.3924\n",
      "Iteration 8539: Weights = [55.30333333  2.12877807  5.09800071  0.08384868  0.16065873 13.703307  ], Loss = 0.3923\n",
      "Iteration 8540: Weights = [55.30333333  2.12864053  5.09767132  0.08384326  0.16064835 13.70366184], Loss = 0.3923\n",
      "Iteration 8541: Weights = [55.30333333  2.12850299  5.09734195  0.08383785  0.16063797 13.70401666], Loss = 0.3922\n",
      "Iteration 8542: Weights = [55.30333333  2.12836547  5.0970126   0.08383243  0.16062759 13.70437145], Loss = 0.3922\n",
      "Iteration 8543: Weights = [55.30333333  2.12822795  5.09668327  0.08382701  0.16061721 13.70472622], Loss = 0.3921\n",
      "Iteration 8544: Weights = [55.30333333  2.12809044  5.09635396  0.0838216   0.16060683 13.70508097], Loss = 0.3921\n",
      "Iteration 8545: Weights = [55.30333333  2.12795294  5.09602468  0.08381618  0.16059646 13.70543569], Loss = 0.3920\n",
      "Iteration 8546: Weights = [55.30333333  2.12781545  5.09569541  0.08381077  0.16058608 13.7057904 ], Loss = 0.3920\n",
      "Iteration 8547: Weights = [55.30333333  2.12767797  5.09536617  0.08380535  0.16057571 13.70614508], Loss = 0.3919\n",
      "Iteration 8548: Weights = [55.30333333  2.12754049  5.09503695  0.08379994  0.16056533 13.70649973], Loss = 0.3919\n",
      "Iteration 8549: Weights = [55.30333333  2.12740303  5.09470775  0.08379452  0.16055496 13.70685436], Loss = 0.3918\n",
      "Iteration 8550: Weights = [55.30333333  2.12726557  5.09437857  0.08378911  0.16054458 13.70720898], Loss = 0.3918\n",
      "Iteration 8551: Weights = [55.30333333  2.12712813  5.09404941  0.08378369  0.16053421 13.70756356], Loss = 0.3917\n",
      "Iteration 8552: Weights = [55.30333333  2.12699069  5.09372027  0.08377828  0.16052384 13.70791813], Loss = 0.3917\n",
      "Iteration 8553: Weights = [55.30333333  2.12685326  5.09339116  0.08377287  0.16051346 13.70827267], Loss = 0.3916\n",
      "Iteration 8554: Weights = [55.30333333  2.12671584  5.09306206  0.08376745  0.16050309 13.70862719], Loss = 0.3916\n",
      "Iteration 8555: Weights = [55.30333333  2.12657843  5.09273299  0.08376204  0.16049272 13.70898168], Loss = 0.3915\n",
      "Iteration 8556: Weights = [55.30333333  2.12644102  5.09240394  0.08375663  0.16048235 13.70933616], Loss = 0.3915\n",
      "Iteration 8557: Weights = [55.30333333  2.12630363  5.09207491  0.08375122  0.16047198 13.70969061], Loss = 0.3914\n",
      "Iteration 8558: Weights = [55.30333333  2.12616625  5.0917459   0.08374581  0.16046162 13.71004504], Loss = 0.3914\n",
      "Iteration 8559: Weights = [55.30333333  2.12602887  5.09141691  0.0837404   0.16045125 13.71039944], Loss = 0.3913\n",
      "Iteration 8560: Weights = [55.30333333  2.1258915   5.09108794  0.08373498  0.16044088 13.71075382], Loss = 0.3913\n",
      "Iteration 8561: Weights = [55.30333333  2.12575415  5.090759    0.08372957  0.16043051 13.71110818], Loss = 0.3912\n",
      "Iteration 8562: Weights = [55.30333333  2.1256168   5.09043007  0.08372416  0.16042015 13.71146252], Loss = 0.3912\n",
      "Iteration 8563: Weights = [55.30333333  2.12547946  5.09010117  0.08371875  0.16040978 13.71181683], Loss = 0.3911\n",
      "Iteration 8564: Weights = [55.30333333  2.12534212  5.08977229  0.08371335  0.16039942 13.71217112], Loss = 0.3911\n",
      "Iteration 8565: Weights = [55.30333333  2.1252048   5.08944343  0.08370794  0.16038906 13.71252538], Loss = 0.3910\n",
      "Iteration 8566: Weights = [55.30333333  2.12506749  5.08911459  0.08370253  0.16037869 13.71287963], Loss = 0.3910\n",
      "Iteration 8567: Weights = [55.30333333  2.12493018  5.08878577  0.08369712  0.16036833 13.71323385], Loss = 0.3909\n",
      "Iteration 8568: Weights = [55.30333333  2.12479289  5.08845698  0.08369171  0.16035797 13.71358805], Loss = 0.3909\n",
      "Iteration 8569: Weights = [55.30333333  2.1246556   5.0881282   0.0836863   0.16034761 13.71394222], Loss = 0.3908\n",
      "Iteration 8570: Weights = [55.30333333  2.12451832  5.08779945  0.0836809   0.16033725 13.71429638], Loss = 0.3908\n",
      "Iteration 8571: Weights = [55.30333333  2.12438105  5.08747071  0.08367549  0.16032689 13.71465051], Loss = 0.3907\n",
      "Iteration 8572: Weights = [55.30333333  2.12424379  5.087142    0.08367008  0.16031653 13.71500461], Loss = 0.3907\n",
      "Iteration 8573: Weights = [55.30333333  2.12410654  5.08681331  0.08366468  0.16030617 13.7153587 ], Loss = 0.3906\n",
      "Iteration 8574: Weights = [55.30333333  2.1239693   5.08648464  0.08365927  0.16029581 13.71571276], Loss = 0.3906\n",
      "Iteration 8575: Weights = [55.30333333  2.12383206  5.08615599  0.08365387  0.16028546 13.7160668 ], Loss = 0.3905\n",
      "Iteration 8576: Weights = [55.30333333  2.12369484  5.08582737  0.08364846  0.1602751  13.71642081], Loss = 0.3905\n",
      "Iteration 8577: Weights = [55.30333333  2.12355762  5.08549876  0.08364306  0.16026474 13.7167748 ], Loss = 0.3904\n",
      "Iteration 8578: Weights = [55.30333333  2.12342041  5.08517018  0.08363765  0.16025439 13.71712877], Loss = 0.3904\n",
      "Iteration 8579: Weights = [55.30333333  2.12328322  5.08484161  0.08363225  0.16024403 13.71748272], Loss = 0.3903\n",
      "Iteration 8580: Weights = [55.30333333  2.12314603  5.08451307  0.08362685  0.16023368 13.71783664], Loss = 0.3903\n",
      "Iteration 8581: Weights = [55.30333333  2.12300885  5.08418455  0.08362144  0.16022333 13.71819054], Loss = 0.3902\n",
      "Iteration 8582: Weights = [55.30333333  2.12287167  5.08385605  0.08361604  0.16021297 13.71854442], Loss = 0.3902\n",
      "Iteration 8583: Weights = [55.30333333  2.12273451  5.08352757  0.08361064  0.16020262 13.71889828], Loss = 0.3901\n",
      "Iteration 8584: Weights = [55.30333333  2.12259736  5.08319912  0.08360523  0.16019227 13.71925211], Loss = 0.3901\n",
      "Iteration 8585: Weights = [55.30333333  2.12246021  5.08287068  0.08359983  0.16018192 13.71960592], Loss = 0.3900\n",
      "Iteration 8586: Weights = [55.30333333  2.12232307  5.08254227  0.08359443  0.16017157 13.71995971], Loss = 0.3900\n",
      "Iteration 8587: Weights = [55.30333333  2.12218595  5.08221387  0.08358903  0.16016122 13.72031347], Loss = 0.3899\n",
      "Iteration 8588: Weights = [55.30333333  2.12204883  5.0818855   0.08358363  0.16015087 13.72066721], Loss = 0.3899\n",
      "Iteration 8589: Weights = [55.30333333  2.12191172  5.08155715  0.08357823  0.16014053 13.72102093], Loss = 0.3898\n",
      "Iteration 8590: Weights = [55.30333333  2.12177462  5.08122882  0.08357283  0.16013018 13.72137462], Loss = 0.3898\n",
      "Iteration 8591: Weights = [55.30333333  2.12163753  5.08090051  0.08356743  0.16011983 13.7217283 ], Loss = 0.3897\n",
      "Iteration 8592: Weights = [55.30333333  2.12150044  5.08057223  0.08356203  0.16010949 13.72208195], Loss = 0.3897\n",
      "Iteration 8593: Weights = [55.30333333  2.12136337  5.08024396  0.08355663  0.16009914 13.72243557], Loss = 0.3896\n",
      "Iteration 8594: Weights = [55.30333333  2.1212263   5.07991572  0.08355123  0.1600888  13.72278918], Loss = 0.3896\n",
      "Iteration 8595: Weights = [55.30333333  2.12108925  5.07958749  0.08354583  0.16007846 13.72314276], Loss = 0.3895\n",
      "Iteration 8596: Weights = [55.30333333  2.1209522   5.07925929  0.08354043  0.16006811 13.72349632], Loss = 0.3895\n",
      "Iteration 8597: Weights = [55.30333333  2.12081516  5.07893111  0.08353504  0.16005777 13.72384985], Loss = 0.3894\n",
      "Iteration 8598: Weights = [55.30333333  2.12067813  5.07860295  0.08352964  0.16004743 13.72420336], Loss = 0.3894\n",
      "Iteration 8599: Weights = [55.30333333  2.12054111  5.07827481  0.08352424  0.16003709 13.72455685], Loss = 0.3893\n",
      "Iteration 8600: Weights = [55.30333333  2.1204041   5.07794669  0.08351885  0.16002675 13.72491032], Loss = 0.3893\n",
      "Iteration 8601: Weights = [55.30333333  2.12026709  5.0776186   0.08351345  0.16001641 13.72526376], Loss = 0.3892\n",
      "Iteration 8602: Weights = [55.30333333  2.1201301   5.07729052  0.08350805  0.16000607 13.72561718], Loss = 0.3892\n",
      "Iteration 8603: Weights = [55.30333333  2.11999311  5.07696247  0.08350266  0.15999573 13.72597058], Loss = 0.3891\n",
      "Iteration 8604: Weights = [55.30333333  2.11985613  5.07663443  0.08349726  0.15998539 13.72632396], Loss = 0.3891\n",
      "Iteration 8605: Weights = [55.30333333  2.11971917  5.07630642  0.08349187  0.15997506 13.72667731], Loss = 0.3890\n",
      "Iteration 8606: Weights = [55.30333333  2.11958221  5.07597843  0.08348647  0.15996472 13.72703064], Loss = 0.3890\n",
      "Iteration 8607: Weights = [55.30333333  2.11944526  5.07565046  0.08348108  0.15995438 13.72738395], Loss = 0.3889\n",
      "Iteration 8608: Weights = [55.30333333  2.11930831  5.07532251  0.08347568  0.15994405 13.72773723], Loss = 0.3889\n",
      "Iteration 8609: Weights = [55.30333333  2.11917138  5.07499459  0.08347029  0.15993371 13.72809049], Loss = 0.3888\n",
      "Iteration 8610: Weights = [55.30333333  2.11903446  5.07466668  0.0834649   0.15992338 13.72844373], Loss = 0.3888\n",
      "Iteration 8611: Weights = [55.30333333  2.11889754  5.0743388   0.08345951  0.15991305 13.72879695], Loss = 0.3887\n",
      "Iteration 8612: Weights = [55.30333333  2.11876064  5.07401093  0.08345411  0.15990272 13.72915014], Loss = 0.3887\n",
      "Iteration 8613: Weights = [55.30333333  2.11862374  5.07368309  0.08344872  0.15989238 13.72950331], Loss = 0.3886\n",
      "Iteration 8614: Weights = [55.30333333  2.11848685  5.07335527  0.08344333  0.15988205 13.72985646], Loss = 0.3886\n",
      "Iteration 8615: Weights = [55.30333333  2.11834997  5.07302747  0.08343794  0.15987172 13.73020958], Loss = 0.3885\n",
      "Iteration 8616: Weights = [55.30333333  2.1182131   5.07269969  0.08343255  0.15986139 13.73056268], Loss = 0.3885\n",
      "Iteration 8617: Weights = [55.30333333  2.11807624  5.07237194  0.08342716  0.15985106 13.73091576], Loss = 0.3884\n",
      "Iteration 8618: Weights = [55.30333333  2.11793938  5.0720442   0.08342177  0.15984074 13.73126882], Loss = 0.3884\n",
      "Iteration 8619: Weights = [55.30333333  2.11780254  5.07171649  0.08341638  0.15983041 13.73162185], Loss = 0.3883\n",
      "Iteration 8620: Weights = [55.30333333  2.11766571  5.07138879  0.08341099  0.15982008 13.73197486], Loss = 0.3883\n",
      "Iteration 8621: Weights = [55.30333333  2.11752888  5.07106112  0.0834056   0.15980975 13.73232785], Loss = 0.3882\n",
      "Iteration 8622: Weights = [55.30333333  2.11739206  5.07073347  0.08340021  0.15979943 13.73268081], Loss = 0.3882\n",
      "Iteration 8623: Weights = [55.30333333  2.11725525  5.07040584  0.08339482  0.1597891  13.73303375], Loss = 0.3881\n",
      "Iteration 8624: Weights = [55.30333333  2.11711845  5.07007823  0.08338943  0.15977878 13.73338667], Loss = 0.3881\n",
      "Iteration 8625: Weights = [55.30333333  2.11698166  5.06975064  0.08338404  0.15976846 13.73373957], Loss = 0.3880\n",
      "Iteration 8626: Weights = [55.30333333  2.11684488  5.06942307  0.08337865  0.15975813 13.73409244], Loss = 0.3880\n",
      "Iteration 8627: Weights = [55.30333333  2.1167081   5.06909553  0.08337327  0.15974781 13.73444529], Loss = 0.3879\n",
      "Iteration 8628: Weights = [55.30333333  2.11657134  5.068768    0.08336788  0.15973749 13.73479812], Loss = 0.3879\n",
      "Iteration 8629: Weights = [55.30333333  2.11643458  5.0684405   0.08336249  0.15972717 13.73515093], Loss = 0.3878\n",
      "Iteration 8630: Weights = [55.30333333  2.11629784  5.06811302  0.08335711  0.15971685 13.73550371], Loss = 0.3878\n",
      "Iteration 8631: Weights = [55.30333333  2.1161611   5.06778556  0.08335172  0.15970653 13.73585647], Loss = 0.3877\n",
      "Iteration 8632: Weights = [55.30333333  2.11602437  5.06745812  0.08334634  0.15969621 13.7362092 ], Loss = 0.3877\n",
      "Iteration 8633: Weights = [55.30333333  2.11588765  5.0671307   0.08334095  0.15968589 13.73656192], Loss = 0.3876\n",
      "Iteration 8634: Weights = [55.30333333  2.11575094  5.0668033   0.08333557  0.15967557 13.73691461], Loss = 0.3876\n",
      "Iteration 8635: Weights = [55.30333333  2.11561423  5.06647592  0.08333018  0.15966526 13.73726728], Loss = 0.3875\n",
      "Iteration 8636: Weights = [55.30333333  2.11547754  5.06614857  0.0833248   0.15965494 13.73761992], Loss = 0.3875\n",
      "Iteration 8637: Weights = [55.30333333  2.11534085  5.06582124  0.08331941  0.15964462 13.73797255], Loss = 0.3874\n",
      "Iteration 8638: Weights = [55.30333333  2.11520418  5.06549392  0.08331403  0.15963431 13.73832515], Loss = 0.3874\n",
      "Iteration 8639: Weights = [55.30333333  2.11506751  5.06516663  0.08330865  0.159624   13.73867772], Loss = 0.3873\n",
      "Iteration 8640: Weights = [55.30333333  2.11493085  5.06483936  0.08330326  0.15961368 13.73903028], Loss = 0.3873\n",
      "Iteration 8641: Weights = [55.30333333  2.1147942   5.06451211  0.08329788  0.15960337 13.73938281], Loss = 0.3872\n",
      "Iteration 8642: Weights = [55.30333333  2.11465756  5.06418488  0.0832925   0.15959306 13.73973532], Loss = 0.3872\n",
      "Iteration 8643: Weights = [55.30333333  2.11452093  5.06385768  0.08328712  0.15958274 13.7400878 ], Loss = 0.3871\n",
      "Iteration 8644: Weights = [55.30333333  2.1143843   5.06353049  0.08328174  0.15957243 13.74044027], Loss = 0.3871\n",
      "Iteration 8645: Weights = [55.30333333  2.11424769  5.06320332  0.08327636  0.15956212 13.74079271], Loss = 0.3870\n",
      "Iteration 8646: Weights = [55.30333333  2.11411108  5.06287618  0.08327098  0.15955181 13.74114513], Loss = 0.3870\n",
      "Iteration 8647: Weights = [55.30333333  2.11397449  5.06254906  0.0832656   0.1595415  13.74149752], Loss = 0.3869\n",
      "Iteration 8648: Weights = [55.30333333  2.1138379   5.06222196  0.08326022  0.1595312  13.74184989], Loss = 0.3869\n",
      "Iteration 8649: Weights = [55.30333333  2.11370132  5.06189488  0.08325484  0.15952089 13.74220224], Loss = 0.3868\n",
      "Iteration 8650: Weights = [55.30333333  2.11356475  5.06156782  0.08324946  0.15951058 13.74255457], Loss = 0.3868\n",
      "Iteration 8651: Weights = [55.30333333  2.11342819  5.06124078  0.08324408  0.15950028 13.74290687], Loss = 0.3867\n",
      "Iteration 8652: Weights = [55.30333333  2.11329163  5.06091376  0.0832387   0.15948997 13.74325915], Loss = 0.3867\n",
      "Iteration 8653: Weights = [55.30333333  2.11315509  5.06058677  0.08323332  0.15947966 13.74361141], Loss = 0.3866\n",
      "Iteration 8654: Weights = [55.30333333  2.11301856  5.06025979  0.08322794  0.15946936 13.74396365], Loss = 0.3866\n",
      "Iteration 8655: Weights = [55.30333333  2.11288203  5.05993284  0.08322257  0.15945906 13.74431586], Loss = 0.3865\n",
      "Iteration 8656: Weights = [55.30333333  2.11274551  5.05960591  0.08321719  0.15944875 13.74466805], Loss = 0.3865\n",
      "Iteration 8657: Weights = [55.30333333  2.112609    5.05927899  0.08321181  0.15943845 13.74502022], Loss = 0.3864\n",
      "Iteration 8658: Weights = [55.30333333  2.1124725   5.0589521   0.08320643  0.15942815 13.74537236], Loss = 0.3864\n",
      "Iteration 8659: Weights = [55.30333333  2.11233601  5.05862523  0.08320106  0.15941785 13.74572448], Loss = 0.3863\n",
      "Iteration 8660: Weights = [55.30333333  2.11219953  5.05829839  0.08319568  0.15940755 13.74607658], Loss = 0.3863\n",
      "Iteration 8661: Weights = [55.30333333  2.11206306  5.05797156  0.08319031  0.15939725 13.74642866], Loss = 0.3862\n",
      "Iteration 8662: Weights = [55.30333333  2.11192659  5.05764475  0.08318493  0.15938695 13.74678071], Loss = 0.3862\n",
      "Iteration 8663: Weights = [55.30333333  2.11179014  5.05731797  0.08317956  0.15937665 13.74713274], Loss = 0.3861\n",
      "Iteration 8664: Weights = [55.30333333  2.11165369  5.05699121  0.08317418  0.15936635 13.74748475], Loss = 0.3861\n",
      "Iteration 8665: Weights = [55.30333333  2.11151725  5.05666446  0.08316881  0.15935606 13.74783674], Loss = 0.3860\n",
      "Iteration 8666: Weights = [55.30333333  2.11138082  5.05633774  0.08316344  0.15934576 13.7481887 ], Loss = 0.3860\n",
      "Iteration 8667: Weights = [55.30333333  2.1112444   5.05601104  0.08315806  0.15933547 13.74854064], Loss = 0.3859\n",
      "Iteration 8668: Weights = [55.30333333  2.11110799  5.05568436  0.08315269  0.15932517 13.74889256], Loss = 0.3859\n",
      "Iteration 8669: Weights = [55.30333333  2.11097159  5.05535771  0.08314732  0.15931488 13.74924445], Loss = 0.3858\n",
      "Iteration 8670: Weights = [55.30333333  2.11083519  5.05503107  0.08314194  0.15930458 13.74959632], Loss = 0.3858\n",
      "Iteration 8671: Weights = [55.30333333  2.11069881  5.05470445  0.08313657  0.15929429 13.74994817], Loss = 0.3857\n",
      "Iteration 8672: Weights = [55.30333333  2.11056243  5.05437786  0.0831312   0.159284   13.7503    ], Loss = 0.3857\n",
      "Iteration 8673: Weights = [55.30333333  2.11042606  5.05405128  0.08312583  0.15927371 13.7506518 ], Loss = 0.3856\n",
      "Iteration 8674: Weights = [55.30333333  2.1102897   5.05372473  0.08312046  0.15926341 13.75100358], Loss = 0.3856\n",
      "Iteration 8675: Weights = [55.30333333  2.11015335  5.0533982   0.08311509  0.15925312 13.75135534], Loss = 0.3855\n",
      "Iteration 8676: Weights = [55.30333333  2.11001701  5.05307169  0.08310972  0.15924283 13.75170708], Loss = 0.3855\n",
      "Iteration 8677: Weights = [55.30333333  2.10988068  5.0527452   0.08310435  0.15923255 13.75205879], Loss = 0.3854\n",
      "Iteration 8678: Weights = [55.30333333  2.10974436  5.05241873  0.08309898  0.15922226 13.75241048], Loss = 0.3854\n",
      "Iteration 8679: Weights = [55.30333333  2.10960804  5.05209229  0.08309361  0.15921197 13.75276214], Loss = 0.3853\n",
      "Iteration 8680: Weights = [55.30333333  2.10947174  5.05176586  0.08308824  0.15920168 13.75311379], Loss = 0.3853\n",
      "Iteration 8681: Weights = [55.30333333  2.10933544  5.05143946  0.08308287  0.1591914  13.75346541], Loss = 0.3852\n",
      "Iteration 8682: Weights = [55.30333333  2.10919915  5.05111307  0.0830775   0.15918111 13.75381701], Loss = 0.3852\n",
      "Iteration 8683: Weights = [55.30333333  2.10906287  5.05078671  0.08307214  0.15917083 13.75416859], Loss = 0.3851\n",
      "Iteration 8684: Weights = [55.30333333  2.1089266   5.05046037  0.08306677  0.15916054 13.75452014], Loss = 0.3851\n",
      "Iteration 8685: Weights = [55.30333333  2.10879034  5.05013405  0.0830614   0.15915026 13.75487167], Loss = 0.3850\n",
      "Iteration 8686: Weights = [55.30333333  2.10865408  5.04980775  0.08305603  0.15913997 13.75522318], Loss = 0.3850\n",
      "Iteration 8687: Weights = [55.30333333  2.10851784  5.04948147  0.08305067  0.15912969 13.75557466], Loss = 0.3849\n",
      "Iteration 8688: Weights = [55.30333333  2.1083816   5.04915521  0.0830453   0.15911941 13.75592613], Loss = 0.3849\n",
      "Iteration 8689: Weights = [55.30333333  2.10824538  5.04882898  0.08303994  0.15910913 13.75627757], Loss = 0.3848\n",
      "Iteration 8690: Weights = [55.30333333  2.10810916  5.04850276  0.08303457  0.15909885 13.75662898], Loss = 0.3848\n",
      "Iteration 8691: Weights = [55.30333333  2.10797295  5.04817657  0.08302921  0.15908857 13.75698038], Loss = 0.3847\n",
      "Iteration 8692: Weights = [55.30333333  2.10783675  5.0478504   0.08302384  0.15907829 13.75733175], Loss = 0.3847\n",
      "Iteration 8693: Weights = [55.30333333  2.10770056  5.04752424  0.08301848  0.15906801 13.7576831 ], Loss = 0.3846\n",
      "Iteration 8694: Weights = [55.30333333  2.10756438  5.04719811  0.08301311  0.15905773 13.75803442], Loss = 0.3846\n",
      "Iteration 8695: Weights = [55.30333333  2.1074282   5.046872    0.08300775  0.15904746 13.75838573], Loss = 0.3845\n",
      "Iteration 8696: Weights = [55.30333333  2.10729204  5.04654592  0.08300239  0.15903718 13.75873701], Loss = 0.3845\n",
      "Iteration 8697: Weights = [55.30333333  2.10715588  5.04621985  0.08299702  0.1590269  13.75908827], Loss = 0.3844\n",
      "Iteration 8698: Weights = [55.30333333  2.10701973  5.0458938   0.08299166  0.15901663 13.7594395 ], Loss = 0.3844\n",
      "Iteration 8699: Weights = [55.30333333  2.10688359  5.04556778  0.0829863   0.15900636 13.75979072], Loss = 0.3843\n",
      "Iteration 8700: Weights = [55.30333333  2.10674746  5.04524177  0.08298094  0.15899608 13.76014191], Loss = 0.3843\n",
      "Iteration 8701: Weights = [55.30333333  2.10661134  5.04491579  0.08297557  0.15898581 13.76049307], Loss = 0.3842\n",
      "Iteration 8702: Weights = [55.30333333  2.10647523  5.04458983  0.08297021  0.15897554 13.76084422], Loss = 0.3842\n",
      "Iteration 8703: Weights = [55.30333333  2.10633913  5.04426389  0.08296485  0.15896526 13.76119534], Loss = 0.3841\n",
      "Iteration 8704: Weights = [55.30333333  2.10620303  5.04393797  0.08295949  0.15895499 13.76154644], Loss = 0.3841\n",
      "Iteration 8705: Weights = [55.30333333  2.10606695  5.04361207  0.08295413  0.15894472 13.76189752], Loss = 0.3840\n",
      "Iteration 8706: Weights = [55.30333333  2.10593087  5.04328619  0.08294877  0.15893445 13.76224857], Loss = 0.3840\n",
      "Iteration 8707: Weights = [55.30333333  2.1057948   5.04296033  0.08294341  0.15892418 13.7625996 ], Loss = 0.3839\n",
      "Iteration 8708: Weights = [55.30333333  2.10565874  5.0426345   0.08293805  0.15891392 13.76295061], Loss = 0.3839\n",
      "Iteration 8709: Weights = [55.30333333  2.10552269  5.04230868  0.08293269  0.15890365 13.7633016 ], Loss = 0.3838\n",
      "Iteration 8710: Weights = [55.30333333  2.10538665  5.04198289  0.08292734  0.15889338 13.76365256], Loss = 0.3838\n",
      "Iteration 8711: Weights = [55.30333333  2.10525061  5.04165711  0.08292198  0.15888311 13.7640035 ], Loss = 0.3837\n",
      "Iteration 8712: Weights = [55.30333333  2.10511459  5.04133136  0.08291662  0.15887285 13.76435442], Loss = 0.3837\n",
      "Iteration 8713: Weights = [55.30333333  2.10497857  5.04100563  0.08291126  0.15886258 13.76470531], Loss = 0.3836\n",
      "Iteration 8714: Weights = [55.30333333  2.10484257  5.04067992  0.08290591  0.15885232 13.76505619], Loss = 0.3836\n",
      "Iteration 8715: Weights = [55.30333333  2.10470657  5.04035423  0.08290055  0.15884206 13.76540704], Loss = 0.3835\n",
      "Iteration 8716: Weights = [55.30333333  2.10457058  5.04002857  0.08289519  0.15883179 13.76575786], Loss = 0.3835\n",
      "Iteration 8717: Weights = [55.30333333  2.1044346   5.03970292  0.08288984  0.15882153 13.76610867], Loss = 0.3834\n",
      "Iteration 8718: Weights = [55.30333333  2.10429863  5.0393773   0.08288448  0.15881127 13.76645945], Loss = 0.3834\n",
      "Iteration 8719: Weights = [55.30333333  2.10416266  5.03905169  0.08287913  0.15880101 13.76681021], Loss = 0.3833\n",
      "Iteration 8720: Weights = [55.30333333  2.10402671  5.03872611  0.08287377  0.15879075 13.76716095], Loss = 0.3833\n",
      "Iteration 8721: Weights = [55.30333333  2.10389076  5.03840055  0.08286842  0.15878049 13.76751166], Loss = 0.3832\n",
      "Iteration 8722: Weights = [55.30333333  2.10375483  5.038075    0.08286306  0.15877023 13.76786235], Loss = 0.3832\n",
      "Iteration 8723: Weights = [55.30333333  2.1036189   5.03774948  0.08285771  0.15875997 13.76821302], Loss = 0.3831\n",
      "Iteration 8724: Weights = [55.30333333  2.10348298  5.03742399  0.08285235  0.15874971 13.76856367], Loss = 0.3831\n",
      "Iteration 8725: Weights = [55.30333333  2.10334707  5.03709851  0.082847    0.15873945 13.76891429], Loss = 0.3830\n",
      "Iteration 8726: Weights = [55.30333333  2.10321117  5.03677305  0.08284165  0.1587292  13.76926489], Loss = 0.3830\n",
      "Iteration 8727: Weights = [55.30333333  2.10307528  5.03644761  0.0828363   0.15871894 13.76961547], Loss = 0.3829\n",
      "Iteration 8728: Weights = [55.30333333  2.10293939  5.0361222   0.08283094  0.15870869 13.76996602], Loss = 0.3829\n",
      "Iteration 8729: Weights = [55.30333333  2.10280352  5.03579681  0.08282559  0.15869843 13.77031656], Loss = 0.3828\n",
      "Iteration 8730: Weights = [55.30333333  2.10266765  5.03547143  0.08282024  0.15868818 13.77066707], Loss = 0.3828\n",
      "Iteration 8731: Weights = [55.30333333  2.10253179  5.03514608  0.08281489  0.15867793 13.77101755], Loss = 0.3827\n",
      "Iteration 8732: Weights = [55.30333333  2.10239595  5.03482075  0.08280954  0.15866767 13.77136802], Loss = 0.3827\n",
      "Iteration 8733: Weights = [55.30333333  2.10226011  5.03449544  0.08280419  0.15865742 13.77171846], Loss = 0.3826\n",
      "Iteration 8734: Weights = [55.30333333  2.10212427  5.03417015  0.08279884  0.15864717 13.77206888], Loss = 0.3826\n",
      "Iteration 8735: Weights = [55.30333333  2.10198845  5.03384488  0.08279349  0.15863692 13.77241928], Loss = 0.3825\n",
      "Iteration 8736: Weights = [55.30333333  2.10185264  5.03351964  0.08278814  0.15862667 13.77276965], Loss = 0.3825\n",
      "Iteration 8737: Weights = [55.30333333  2.10171683  5.03319441  0.08278279  0.15861642 13.77312   ], Loss = 0.3824\n",
      "Iteration 8738: Weights = [55.30333333  2.10158104  5.03286921  0.08277744  0.15860617 13.77347033], Loss = 0.3824\n",
      "Iteration 8739: Weights = [55.30333333  2.10144525  5.03254402  0.08277209  0.15859592 13.77382064], Loss = 0.3823\n",
      "Iteration 8740: Weights = [55.30333333  2.10130947  5.03221886  0.08276674  0.15858568 13.77417092], Loss = 0.3823\n",
      "Iteration 8741: Weights = [55.30333333  2.1011737   5.03189372  0.0827614   0.15857543 13.77452118], Loss = 0.3822\n",
      "Iteration 8742: Weights = [55.30333333  2.10103794  5.0315686   0.08275605  0.15856518 13.77487142], Loss = 0.3822\n",
      "Iteration 8743: Weights = [55.30333333  2.10090219  5.0312435   0.0827507   0.15855494 13.77522164], Loss = 0.3821\n",
      "Iteration 8744: Weights = [55.30333333  2.10076644  5.03091842  0.08274535  0.15854469 13.77557183], Loss = 0.3821\n",
      "Iteration 8745: Weights = [55.30333333  2.10063071  5.03059336  0.08274001  0.15853445 13.775922  ], Loss = 0.3820\n",
      "Iteration 8746: Weights = [55.30333333  2.10049498  5.03026832  0.08273466  0.15852421 13.77627215], Loss = 0.3820\n",
      "Iteration 8747: Weights = [55.30333333  2.10035927  5.02994331  0.08272932  0.15851396 13.77662228], Loss = 0.3819\n",
      "Iteration 8748: Weights = [55.30333333  2.10022356  5.02961831  0.08272397  0.15850372 13.77697238], Loss = 0.3819\n",
      "Iteration 8749: Weights = [55.30333333  2.10008786  5.02929334  0.08271863  0.15849348 13.77732246], Loss = 0.3818\n",
      "Iteration 8750: Weights = [55.30333333  2.09995217  5.02896839  0.08271328  0.15848324 13.77767252], Loss = 0.3818\n",
      "Iteration 8751: Weights = [55.30333333  2.09981648  5.02864345  0.08270794  0.158473   13.77802255], Loss = 0.3817\n",
      "Iteration 8752: Weights = [55.30333333  2.09968081  5.02831854  0.08270259  0.15846276 13.77837256], Loss = 0.3817\n",
      "Iteration 8753: Weights = [55.30333333  2.09954515  5.02799365  0.08269725  0.15845252 13.77872255], Loss = 0.3816\n",
      "Iteration 8754: Weights = [55.30333333  2.09940949  5.02766879  0.08269191  0.15844229 13.77907252], Loss = 0.3816\n",
      "Iteration 8755: Weights = [55.30333333  2.09927384  5.02734394  0.08268656  0.15843205 13.77942246], Loss = 0.3815\n",
      "Iteration 8756: Weights = [55.30333333  2.09913821  5.02701911  0.08268122  0.15842181 13.77977239], Loss = 0.3815\n",
      "Iteration 8757: Weights = [55.30333333  2.09900258  5.0266943   0.08267588  0.15841158 13.78012228], Loss = 0.3814\n",
      "Iteration 8758: Weights = [55.30333333  2.09886696  5.02636952  0.08267054  0.15840134 13.78047216], Loss = 0.3814\n",
      "Iteration 8759: Weights = [55.30333333  2.09873134  5.02604476  0.0826652   0.15839111 13.78082202], Loss = 0.3813\n",
      "Iteration 8760: Weights = [55.30333333  2.09859574  5.02572001  0.08265985  0.15838087 13.78117185], Loss = 0.3813\n",
      "Iteration 8761: Weights = [55.30333333  2.09846015  5.02539529  0.08265451  0.15837064 13.78152166], Loss = 0.3812\n",
      "Iteration 8762: Weights = [55.30333333  2.09832456  5.02507059  0.08264917  0.15836041 13.78187144], Loss = 0.3812\n",
      "Iteration 8763: Weights = [55.30333333  2.09818898  5.02474591  0.08264383  0.15835017 13.7822212 ], Loss = 0.3811\n",
      "Iteration 8764: Weights = [55.30333333  2.09805341  5.02442125  0.08263849  0.15833994 13.78257095], Loss = 0.3811\n",
      "Iteration 8765: Weights = [55.30333333  2.09791786  5.02409661  0.08263315  0.15832971 13.78292066], Loss = 0.3810\n",
      "Iteration 8766: Weights = [55.30333333  2.0977823   5.023772    0.08262781  0.15831948 13.78327036], Loss = 0.3810\n",
      "Iteration 8767: Weights = [55.30333333  2.09764676  5.0234474   0.08262248  0.15830925 13.78362003], Loss = 0.3809\n",
      "Iteration 8768: Weights = [55.30333333  2.09751123  5.02312283  0.08261714  0.15829902 13.78396968], Loss = 0.3809\n",
      "Iteration 8769: Weights = [55.30333333  2.09737571  5.02279827  0.0826118   0.1582888  13.78431931], Loss = 0.3809\n",
      "Iteration 8770: Weights = [55.30333333  2.09724019  5.02247374  0.08260646  0.15827857 13.78466892], Loss = 0.3808\n",
      "Iteration 8771: Weights = [55.30333333  2.09710468  5.02214923  0.08260112  0.15826834 13.7850185 ], Loss = 0.3808\n",
      "Iteration 8772: Weights = [55.30333333  2.09696918  5.02182474  0.08259579  0.15825812 13.78536806], Loss = 0.3807\n",
      "Iteration 8773: Weights = [55.30333333  2.0968337   5.02150026  0.08259045  0.15824789 13.7857176 ], Loss = 0.3807\n",
      "Iteration 8774: Weights = [55.30333333  2.09669821  5.02117582  0.08258511  0.15823767 13.78606711], Loss = 0.3806\n",
      "Iteration 8775: Weights = [55.30333333  2.09656274  5.02085139  0.08257978  0.15822744 13.7864166 ], Loss = 0.3806\n",
      "Iteration 8776: Weights = [55.30333333  2.09642728  5.02052698  0.08257444  0.15821722 13.78676607], Loss = 0.3805\n",
      "Iteration 8777: Weights = [55.30333333  2.09629183  5.02020259  0.08256911  0.158207   13.78711552], Loss = 0.3805\n",
      "Iteration 8778: Weights = [55.30333333  2.09615638  5.01987823  0.08256377  0.15819677 13.78746495], Loss = 0.3804\n",
      "Iteration 8779: Weights = [55.30333333  2.09602094  5.01955388  0.08255844  0.15818655 13.78781435], Loss = 0.3804\n",
      "Iteration 8780: Weights = [55.30333333  2.09588551  5.01922956  0.0825531   0.15817633 13.78816373], Loss = 0.3803\n",
      "Iteration 8781: Weights = [55.30333333  2.0957501   5.01890526  0.08254777  0.15816611 13.78851309], Loss = 0.3803\n",
      "Iteration 8782: Weights = [55.30333333  2.09561468  5.01858098  0.08254244  0.15815589 13.78886242], Loss = 0.3802\n",
      "Iteration 8783: Weights = [55.30333333  2.09547928  5.01825672  0.0825371   0.15814567 13.78921173], Loss = 0.3802\n",
      "Iteration 8784: Weights = [55.30333333  2.09534389  5.01793248  0.08253177  0.15813545 13.78956102], Loss = 0.3801\n",
      "Iteration 8785: Weights = [55.30333333  2.09520851  5.01760826  0.08252644  0.15812524 13.78991029], Loss = 0.3801\n",
      "Iteration 8786: Weights = [55.30333333  2.09507313  5.01728406  0.08252111  0.15811502 13.79025953], Loss = 0.3800\n",
      "Iteration 8787: Weights = [55.30333333  2.09493776  5.01695988  0.08251577  0.1581048  13.79060875], Loss = 0.3800\n",
      "Iteration 8788: Weights = [55.30333333  2.0948024   5.01663573  0.08251044  0.15809459 13.79095795], Loss = 0.3799\n",
      "Iteration 8789: Weights = [55.30333333  2.09466706  5.01631159  0.08250511  0.15808437 13.79130713], Loss = 0.3799\n",
      "Iteration 8790: Weights = [55.30333333  2.09453171  5.01598748  0.08249978  0.15807416 13.79165628], Loss = 0.3798\n",
      "Iteration 8791: Weights = [55.30333333  2.09439638  5.01566339  0.08249445  0.15806395 13.79200541], Loss = 0.3798\n",
      "Iteration 8792: Weights = [55.30333333  2.09426106  5.01533931  0.08248912  0.15805373 13.79235452], Loss = 0.3797\n",
      "Iteration 8793: Weights = [55.30333333  2.09412574  5.01501526  0.08248379  0.15804352 13.79270361], Loss = 0.3797\n",
      "Iteration 8794: Weights = [55.30333333  2.09399044  5.01469123  0.08247846  0.15803331 13.79305267], Loss = 0.3796\n",
      "Iteration 8795: Weights = [55.30333333  2.09385514  5.01436722  0.08247313  0.1580231  13.79340171], Loss = 0.3796\n",
      "Iteration 8796: Weights = [55.30333333  2.09371985  5.01404323  0.0824678   0.15801289 13.79375073], Loss = 0.3795\n",
      "Iteration 8797: Weights = [55.30333333  2.09358457  5.01371927  0.08246247  0.15800268 13.79409973], Loss = 0.3795\n",
      "Iteration 8798: Weights = [55.30333333  2.0934493   5.01339532  0.08245715  0.15799247 13.7944487 ], Loss = 0.3794\n",
      "Iteration 8799: Weights = [55.30333333  2.09331404  5.01307139  0.08245182  0.15798226 13.79479765], Loss = 0.3794\n",
      "Iteration 8800: Weights = [55.30333333  2.09317879  5.01274749  0.08244649  0.15797205 13.79514658], Loss = 0.3793\n",
      "Iteration 8801: Weights = [55.30333333  2.09304354  5.01242361  0.08244116  0.15796185 13.79549549], Loss = 0.3793\n",
      "Iteration 8802: Weights = [55.30333333  2.09290831  5.01209974  0.08243584  0.15795164 13.79584437], Loss = 0.3792\n",
      "Iteration 8803: Weights = [55.30333333  2.09277308  5.0117759   0.08243051  0.15794144 13.79619323], Loss = 0.3792\n",
      "Iteration 8804: Weights = [55.30333333  2.09263786  5.01145208  0.08242518  0.15793123 13.79654207], Loss = 0.3791\n",
      "Iteration 8805: Weights = [55.30333333  2.09250265  5.01112828  0.08241986  0.15792103 13.79689088], Loss = 0.3791\n",
      "Iteration 8806: Weights = [55.30333333  2.09236745  5.0108045   0.08241453  0.15791082 13.79723968], Loss = 0.3790\n",
      "Iteration 8807: Weights = [55.30333333  2.09223226  5.01048074  0.08240921  0.15790062 13.79758845], Loss = 0.3790\n",
      "Iteration 8808: Weights = [55.30333333  2.09209708  5.01015701  0.08240388  0.15789042 13.7979372 ], Loss = 0.3789\n",
      "Iteration 8809: Weights = [55.30333333  2.0919619   5.00983329  0.08239856  0.15788022 13.79828592], Loss = 0.3789\n",
      "Iteration 8810: Weights = [55.30333333  2.09182674  5.00950959  0.08239324  0.15787002 13.79863462], Loss = 0.3788\n",
      "Iteration 8811: Weights = [55.30333333  2.09169158  5.00918592  0.08238791  0.15785982 13.7989833 ], Loss = 0.3788\n",
      "Iteration 8812: Weights = [55.30333333  2.09155643  5.00886227  0.08238259  0.15784962 13.79933196], Loss = 0.3787\n",
      "Iteration 8813: Weights = [55.30333333  2.09142129  5.00853863  0.08237727  0.15783942 13.7996806 ], Loss = 0.3787\n",
      "Iteration 8814: Weights = [55.30333333  2.09128616  5.00821502  0.08237194  0.15782922 13.80002921], Loss = 0.3786\n",
      "Iteration 8815: Weights = [55.30333333  2.09115104  5.00789143  0.08236662  0.15781902 13.8003778 ], Loss = 0.3786\n",
      "Iteration 8816: Weights = [55.30333333  2.09101593  5.00756786  0.0823613   0.15780882 13.80072637], Loss = 0.3785\n",
      "Iteration 8817: Weights = [55.30333333  2.09088082  5.00724431  0.08235598  0.15779863 13.80107491], Loss = 0.3785\n",
      "Iteration 8818: Weights = [55.30333333  2.09074572  5.00692078  0.08235066  0.15778843 13.80142344], Loss = 0.3784\n",
      "Iteration 8819: Weights = [55.30333333  2.09061064  5.00659728  0.08234534  0.15777824 13.80177194], Loss = 0.3784\n",
      "Iteration 8820: Weights = [55.30333333  2.09047556  5.00627379  0.08234002  0.15776804 13.80212041], Loss = 0.3783\n",
      "Iteration 8821: Weights = [55.30333333  2.09034049  5.00595033  0.08233469  0.15775785 13.80246887], Loss = 0.3783\n",
      "Iteration 8822: Weights = [55.30333333  2.09020543  5.00562688  0.08232938  0.15774766 13.8028173 ], Loss = 0.3783\n",
      "Iteration 8823: Weights = [55.30333333  2.09007038  5.00530346  0.08232406  0.15773746 13.80316571], Loss = 0.3782\n",
      "Iteration 8824: Weights = [55.30333333  2.08993533  5.00498005  0.08231874  0.15772727 13.8035141 ], Loss = 0.3782\n",
      "Iteration 8825: Weights = [55.30333333  2.0898003   5.00465667  0.08231342  0.15771708 13.80386247], Loss = 0.3781\n",
      "Iteration 8826: Weights = [55.30333333  2.08966527  5.00433331  0.0823081   0.15770689 13.80421081], Loss = 0.3781\n",
      "Iteration 8827: Weights = [55.30333333  2.08953025  5.00400997  0.08230278  0.1576967  13.80455913], Loss = 0.3780\n",
      "Iteration 8828: Weights = [55.30333333  2.08939524  5.00368665  0.08229746  0.15768651 13.80490743], Loss = 0.3780\n",
      "Iteration 8829: Weights = [55.30333333  2.08926024  5.00336335  0.08229215  0.15767632 13.8052557 ], Loss = 0.3779\n",
      "Iteration 8830: Weights = [55.30333333  2.08912525  5.00304008  0.08228683  0.15766613 13.80560395], Loss = 0.3779\n",
      "Iteration 8831: Weights = [55.30333333  2.08899027  5.00271682  0.08228151  0.15765595 13.80595218], Loss = 0.3778\n",
      "Iteration 8832: Weights = [55.30333333  2.0888553   5.00239359  0.0822762   0.15764576 13.80630039], Loss = 0.3778\n",
      "Iteration 8833: Weights = [55.30333333  2.08872033  5.00207037  0.08227088  0.15763558 13.80664858], Loss = 0.3777\n",
      "Iteration 8834: Weights = [55.30333333  2.08858538  5.00174718  0.08226556  0.15762539 13.80699674], Loss = 0.3777\n",
      "Iteration 8835: Weights = [55.30333333  2.08845043  5.001424    0.08226025  0.15761521 13.80734488], Loss = 0.3776\n",
      "Iteration 8836: Weights = [55.30333333  2.08831549  5.00110085  0.08225493  0.15760502 13.807693  ], Loss = 0.3776\n",
      "Iteration 8837: Weights = [55.30333333  2.08818056  5.00077772  0.08224962  0.15759484 13.80804109], Loss = 0.3775\n",
      "Iteration 8838: Weights = [55.30333333  2.08804564  5.00045461  0.0822443   0.15758466 13.80838916], Loss = 0.3775\n",
      "Iteration 8839: Weights = [55.30333333  2.08791072  5.00013152  0.08223899  0.15757447 13.80873721], Loss = 0.3774\n",
      "Iteration 8840: Weights = [55.30333333  2.08777582  4.99980845  0.08223368  0.15756429 13.80908524], Loss = 0.3774\n",
      "Iteration 8841: Weights = [55.30333333  2.08764092  4.9994854   0.08222836  0.15755411 13.80943325], Loss = 0.3773\n",
      "Iteration 8842: Weights = [55.30333333  2.08750604  4.99916238  0.08222305  0.15754393 13.80978123], Loss = 0.3773\n",
      "Iteration 8843: Weights = [55.30333333  2.08737116  4.99883937  0.08221774  0.15753375 13.81012919], Loss = 0.3772\n",
      "Iteration 8844: Weights = [55.30333333  2.08723629  4.99851639  0.08221243  0.15752357 13.81047713], Loss = 0.3772\n",
      "Iteration 8845: Weights = [55.30333333  2.08710143  4.99819342  0.08220711  0.1575134  13.81082504], Loss = 0.3771\n",
      "Iteration 8846: Weights = [55.30333333  2.08696658  4.99787048  0.0822018   0.15750322 13.81117294], Loss = 0.3771\n",
      "Iteration 8847: Weights = [55.30333333  2.08683173  4.99754756  0.08219649  0.15749304 13.81152081], Loss = 0.3770\n",
      "Iteration 8848: Weights = [55.30333333  2.0866969   4.99722466  0.08219118  0.15748287 13.81186865], Loss = 0.3770\n",
      "Iteration 8849: Weights = [55.30333333  2.08656207  4.99690177  0.08218587  0.15747269 13.81221648], Loss = 0.3769\n",
      "Iteration 8850: Weights = [55.30333333  2.08642726  4.99657891  0.08218056  0.15746252 13.81256428], Loss = 0.3769\n",
      "Iteration 8851: Weights = [55.30333333  2.08629245  4.99625608  0.08217525  0.15745234 13.81291206], Loss = 0.3768\n",
      "Iteration 8852: Weights = [55.30333333  2.08615765  4.99593326  0.08216994  0.15744217 13.81325982], Loss = 0.3768\n",
      "Iteration 8853: Weights = [55.30333333  2.08602286  4.99561046  0.08216463  0.157432   13.81360756], Loss = 0.3767\n",
      "Iteration 8854: Weights = [55.30333333  2.08588808  4.99528768  0.08215932  0.15742183 13.81395527], Loss = 0.3767\n",
      "Iteration 8855: Weights = [55.30333333  2.0857533   4.99496493  0.08215401  0.15741165 13.81430296], Loss = 0.3766\n",
      "Iteration 8856: Weights = [55.30333333  2.08561854  4.99464219  0.08214871  0.15740148 13.81465063], Loss = 0.3766\n",
      "Iteration 8857: Weights = [55.30333333  2.08548378  4.99431948  0.0821434   0.15739131 13.81499827], Loss = 0.3765\n",
      "Iteration 8858: Weights = [55.30333333  2.08534904  4.99399679  0.08213809  0.15738114 13.8153459 ], Loss = 0.3765\n",
      "Iteration 8859: Weights = [55.30333333  2.0852143   4.99367411  0.08213278  0.15737098 13.8156935 ], Loss = 0.3764\n",
      "Iteration 8860: Weights = [55.30333333  2.08507957  4.99335146  0.08212748  0.15736081 13.81604108], Loss = 0.3764\n",
      "Iteration 8861: Weights = [55.30333333  2.08494485  4.99302883  0.08212217  0.15735064 13.81638863], Loss = 0.3763\n",
      "Iteration 8862: Weights = [55.30333333  2.08481013  4.99270622  0.08211686  0.15734047 13.81673616], Loss = 0.3763\n",
      "Iteration 8863: Weights = [55.30333333  2.08467543  4.99238363  0.08211156  0.15733031 13.81708368], Loss = 0.3763\n",
      "Iteration 8864: Weights = [55.30333333  2.08454073  4.99206107  0.08210625  0.15732014 13.81743116], Loss = 0.3762\n",
      "Iteration 8865: Weights = [55.30333333  2.08440605  4.99173852  0.08210095  0.15730998 13.81777863], Loss = 0.3762\n",
      "Iteration 8866: Weights = [55.30333333  2.08427137  4.99141599  0.08209564  0.15729981 13.81812607], Loss = 0.3761\n",
      "Iteration 8867: Weights = [55.30333333  2.0841367   4.99109349  0.08209034  0.15728965 13.81847349], Loss = 0.3761\n",
      "Iteration 8868: Weights = [55.30333333  2.08400204  4.990771    0.08208504  0.15727949 13.81882089], Loss = 0.3760\n",
      "Iteration 8869: Weights = [55.30333333  2.08386739  4.99044854  0.08207973  0.15726932 13.81916827], Loss = 0.3760\n",
      "Iteration 8870: Weights = [55.30333333  2.08373275  4.9901261   0.08207443  0.15725916 13.81951562], Loss = 0.3759\n",
      "Iteration 8871: Weights = [55.30333333  2.08359811  4.98980367  0.08206913  0.157249   13.81986295], Loss = 0.3759\n",
      "Iteration 8872: Weights = [55.30333333  2.08346349  4.98948127  0.08206382  0.15723884 13.82021026], Loss = 0.3758\n",
      "Iteration 8873: Weights = [55.30333333  2.08332887  4.98915889  0.08205852  0.15722868 13.82055755], Loss = 0.3758\n",
      "Iteration 8874: Weights = [55.30333333  2.08319426  4.98883653  0.08205322  0.15721852 13.82090481], Loss = 0.3757\n",
      "Iteration 8875: Weights = [55.30333333  2.08305966  4.98851419  0.08204792  0.15720837 13.82125205], Loss = 0.3757\n",
      "Iteration 8876: Weights = [55.30333333  2.08292507  4.98819188  0.08204262  0.15719821 13.82159927], Loss = 0.3756\n",
      "Iteration 8877: Weights = [55.30333333  2.08279049  4.98786958  0.08203731  0.15718805 13.82194647], Loss = 0.3756\n",
      "Iteration 8878: Weights = [55.30333333  2.08265592  4.9875473   0.08203201  0.15717789 13.82229364], Loss = 0.3755\n",
      "Iteration 8879: Weights = [55.30333333  2.08252135  4.98722505  0.08202671  0.15716774 13.8226408 ], Loss = 0.3755\n",
      "Iteration 8880: Weights = [55.30333333  2.0823868   4.98690281  0.08202141  0.15715758 13.82298793], Loss = 0.3754\n",
      "Iteration 8881: Weights = [55.30333333  2.08225225  4.9865806   0.08201611  0.15714743 13.82333503], Loss = 0.3754\n",
      "Iteration 8882: Weights = [55.30333333  2.08211771  4.98625841  0.08201081  0.15713728 13.82368212], Loss = 0.3753\n",
      "Iteration 8883: Weights = [55.30333333  2.08198318  4.98593623  0.08200552  0.15712712 13.82402918], Loss = 0.3753\n",
      "Iteration 8884: Weights = [55.30333333  2.08184866  4.98561408  0.08200022  0.15711697 13.82437622], Loss = 0.3752\n",
      "Iteration 8885: Weights = [55.30333333  2.08171415  4.98529195  0.08199492  0.15710682 13.82472324], Loss = 0.3752\n",
      "Iteration 8886: Weights = [55.30333333  2.08157964  4.98496984  0.08198962  0.15709667 13.82507023], Loss = 0.3751\n",
      "Iteration 8887: Weights = [55.30333333  2.08144515  4.98464775  0.08198432  0.15708652 13.8254172 ], Loss = 0.3751\n",
      "Iteration 8888: Weights = [55.30333333  2.08131066  4.98432569  0.08197903  0.15707637 13.82576415], Loss = 0.3750\n",
      "Iteration 8889: Weights = [55.30333333  2.08117618  4.98400364  0.08197373  0.15706622 13.82611108], Loss = 0.3750\n",
      "Iteration 8890: Weights = [55.30333333  2.08104172  4.98368161  0.08196843  0.15705607 13.82645799], Loss = 0.3749\n",
      "Iteration 8891: Weights = [55.30333333  2.08090726  4.98335961  0.08196314  0.15704592 13.82680487], Loss = 0.3749\n",
      "Iteration 8892: Weights = [55.30333333  2.0807728   4.98303762  0.08195784  0.15703578 13.82715173], Loss = 0.3748\n",
      "Iteration 8893: Weights = [55.30333333  2.08063836  4.98271566  0.08195255  0.15702563 13.82749857], Loss = 0.3748\n",
      "Iteration 8894: Weights = [55.30333333  2.08050393  4.98239371  0.08194725  0.15701548 13.82784538], Loss = 0.3747\n",
      "Iteration 8895: Weights = [55.30333333  2.0803695   4.98207179  0.08194196  0.15700534 13.82819218], Loss = 0.3747\n",
      "Iteration 8896: Weights = [55.30333333  2.08023508  4.98174989  0.08193666  0.15699519 13.82853895], Loss = 0.3747\n",
      "Iteration 8897: Weights = [55.30333333  2.08010068  4.98142801  0.08193137  0.15698505 13.82888569], Loss = 0.3746\n",
      "Iteration 8898: Weights = [55.30333333  2.07996628  4.98110615  0.08192607  0.15697491 13.82923242], Loss = 0.3746\n",
      "Iteration 8899: Weights = [55.30333333  2.07983189  4.98078431  0.08192078  0.15696477 13.82957912], Loss = 0.3745\n",
      "Iteration 8900: Weights = [55.30333333  2.0796975   4.98046249  0.08191549  0.15695462 13.8299258 ], Loss = 0.3745\n",
      "Iteration 8901: Weights = [55.30333333  2.07956313  4.98014069  0.08191019  0.15694448 13.83027246], Loss = 0.3744\n",
      "Iteration 8902: Weights = [55.30333333  2.07942877  4.97981892  0.0819049   0.15693434 13.8306191 ], Loss = 0.3744\n",
      "Iteration 8903: Weights = [55.30333333  2.07929441  4.97949716  0.08189961  0.1569242  13.83096571], Loss = 0.3743\n",
      "Iteration 8904: Weights = [55.30333333  2.07916006  4.97917542  0.08189432  0.15691406 13.8313123 ], Loss = 0.3743\n",
      "Iteration 8905: Weights = [55.30333333  2.07902572  4.97885371  0.08188903  0.15690392 13.83165887], Loss = 0.3742\n",
      "Iteration 8906: Weights = [55.30333333  2.07889139  4.97853202  0.08188374  0.15689379 13.83200542], Loss = 0.3742\n",
      "Iteration 8907: Weights = [55.30333333  2.07875707  4.97821034  0.08187845  0.15688365 13.83235194], Loss = 0.3741\n",
      "Iteration 8908: Weights = [55.30333333  2.07862276  4.97788869  0.08187316  0.15687351 13.83269845], Loss = 0.3741\n",
      "Iteration 8909: Weights = [55.30333333  2.07848846  4.97756706  0.08186787  0.15686338 13.83304493], Loss = 0.3740\n",
      "Iteration 8910: Weights = [55.30333333  2.07835416  4.97724545  0.08186258  0.15685324 13.83339138], Loss = 0.3740\n",
      "Iteration 8911: Weights = [55.30333333  2.07821987  4.97692386  0.08185729  0.15684311 13.83373782], Loss = 0.3739\n",
      "Iteration 8912: Weights = [55.30333333  2.0780856   4.97660229  0.081852    0.15683297 13.83408423], Loss = 0.3739\n",
      "Iteration 8913: Weights = [55.30333333  2.07795133  4.97628074  0.08184671  0.15682284 13.83443062], Loss = 0.3738\n",
      "Iteration 8914: Weights = [55.30333333  2.07781707  4.97595921  0.08184142  0.15681271 13.83477699], Loss = 0.3738\n",
      "Iteration 8915: Weights = [55.30333333  2.07768281  4.97563771  0.08183613  0.15680257 13.83512333], Loss = 0.3737\n",
      "Iteration 8916: Weights = [55.30333333  2.07754857  4.97531622  0.08183084  0.15679244 13.83546966], Loss = 0.3737\n",
      "Iteration 8917: Weights = [55.30333333  2.07741434  4.97499476  0.08182556  0.15678231 13.83581596], Loss = 0.3736\n",
      "Iteration 8918: Weights = [55.30333333  2.07728011  4.97467331  0.08182027  0.15677218 13.83616223], Loss = 0.3736\n",
      "Iteration 8919: Weights = [55.30333333  2.07714589  4.97435189  0.08181498  0.15676205 13.83650849], Loss = 0.3735\n",
      "Iteration 8920: Weights = [55.30333333  2.07701168  4.97403049  0.0818097   0.15675192 13.83685472], Loss = 0.3735\n",
      "Iteration 8921: Weights = [55.30333333  2.07687748  4.9737091   0.08180441  0.1567418  13.83720093], Loss = 0.3734\n",
      "Iteration 8922: Weights = [55.30333333  2.07674329  4.97338774  0.08179913  0.15673167 13.83754712], Loss = 0.3734\n",
      "Iteration 8923: Weights = [55.30333333  2.07660911  4.9730664   0.08179384  0.15672154 13.83789329], Loss = 0.3733\n",
      "Iteration 8924: Weights = [55.30333333  2.07647494  4.97274508  0.08178856  0.15671142 13.83823943], Loss = 0.3733\n",
      "Iteration 8925: Weights = [55.30333333  2.07634077  4.97242378  0.08178327  0.15670129 13.83858555], Loss = 0.3732\n",
      "Iteration 8926: Weights = [55.30333333  2.07620661  4.9721025   0.08177799  0.15669117 13.83893165], Loss = 0.3732\n",
      "Iteration 8927: Weights = [55.30333333  2.07607247  4.97178125  0.0817727   0.15668104 13.83927773], Loss = 0.3732\n",
      "Iteration 8928: Weights = [55.30333333  2.07593833  4.97146001  0.08176742  0.15667092 13.83962378], Loss = 0.3731\n",
      "Iteration 8929: Weights = [55.30333333  2.0758042   4.97113879  0.08176214  0.1566608  13.83996982], Loss = 0.3731\n",
      "Iteration 8930: Weights = [55.30333333  2.07567008  4.9708176   0.08175685  0.15665067 13.84031583], Loss = 0.3730\n",
      "Iteration 8931: Weights = [55.30333333  2.07553596  4.97049642  0.08175157  0.15664055 13.84066181], Loss = 0.3730\n",
      "Iteration 8932: Weights = [55.30333333  2.07540186  4.97017527  0.08174629  0.15663043 13.84100778], Loss = 0.3729\n",
      "Iteration 8933: Weights = [55.30333333  2.07526776  4.96985414  0.08174101  0.15662031 13.84135372], Loss = 0.3729\n",
      "Iteration 8934: Weights = [55.30333333  2.07513367  4.96953303  0.08173573  0.15661019 13.84169964], Loss = 0.3728\n",
      "Iteration 8935: Weights = [55.30333333  2.0749996   4.96921193  0.08173045  0.15660007 13.84204554], Loss = 0.3728\n",
      "Iteration 8936: Weights = [55.30333333  2.07486553  4.96889086  0.08172516  0.15658995 13.84239141], Loss = 0.3727\n",
      "Iteration 8937: Weights = [55.30333333  2.07473146  4.96856981  0.08171988  0.15657984 13.84273727], Loss = 0.3727\n",
      "Iteration 8938: Weights = [55.30333333  2.07459741  4.96824878  0.0817146   0.15656972 13.8430831 ], Loss = 0.3726\n",
      "Iteration 8939: Weights = [55.30333333  2.07446337  4.96792778  0.08170932  0.1565596  13.84342891], Loss = 0.3726\n",
      "Iteration 8940: Weights = [55.30333333  2.07432933  4.96760679  0.08170404  0.15654949 13.84377469], Loss = 0.3725\n",
      "Iteration 8941: Weights = [55.30333333  2.07419531  4.96728582  0.08169877  0.15653937 13.84412046], Loss = 0.3725\n",
      "Iteration 8942: Weights = [55.30333333  2.07406129  4.96696487  0.08169349  0.15652926 13.8444662 ], Loss = 0.3724\n",
      "Iteration 8943: Weights = [55.30333333  2.07392728  4.96664395  0.08168821  0.15651914 13.84481192], Loss = 0.3724\n",
      "Iteration 8944: Weights = [55.30333333  2.07379328  4.96632304  0.08168293  0.15650903 13.84515761], Loss = 0.3723\n",
      "Iteration 8945: Weights = [55.30333333  2.07365929  4.96600216  0.08167765  0.15649892 13.84550329], Loss = 0.3723\n",
      "Iteration 8946: Weights = [55.30333333  2.0735253   4.9656813   0.08167238  0.15648881 13.84584894], Loss = 0.3722\n",
      "Iteration 8947: Weights = [55.30333333  2.07339133  4.96536045  0.0816671   0.1564787  13.84619457], Loss = 0.3722\n",
      "Iteration 8948: Weights = [55.30333333  2.07325736  4.96503963  0.08166182  0.15646859 13.84654018], Loss = 0.3721\n",
      "Iteration 8949: Weights = [55.30333333  2.07312341  4.96471883  0.08165655  0.15645848 13.84688576], Loss = 0.3721\n",
      "Iteration 8950: Weights = [55.30333333  2.07298946  4.96439805  0.08165127  0.15644837 13.84723133], Loss = 0.3720\n",
      "Iteration 8951: Weights = [55.30333333  2.07285552  4.96407729  0.08164599  0.15643826 13.84757687], Loss = 0.3720\n",
      "Iteration 8952: Weights = [55.30333333  2.07272159  4.96375655  0.08164072  0.15642815 13.84792239], Loss = 0.3719\n",
      "Iteration 8953: Weights = [55.30333333  2.07258766  4.96343583  0.08163544  0.15641804 13.84826788], Loss = 0.3719\n",
      "Iteration 8954: Weights = [55.30333333  2.07245375  4.96311514  0.08163017  0.15640794 13.84861335], Loss = 0.3719\n",
      "Iteration 8955: Weights = [55.30333333  2.07231984  4.96279446  0.08162489  0.15639783 13.84895881], Loss = 0.3718\n",
      "Iteration 8956: Weights = [55.30333333  2.07218595  4.9624738   0.08161962  0.15638773 13.84930423], Loss = 0.3718\n",
      "Iteration 8957: Weights = [55.30333333  2.07205206  4.96215317  0.08161435  0.15637762 13.84964964], Loss = 0.3717\n",
      "Iteration 8958: Weights = [55.30333333  2.07191818  4.96183255  0.08160907  0.15636752 13.84999503], Loss = 0.3717\n",
      "Iteration 8959: Weights = [55.30333333  2.07178431  4.96151196  0.0816038   0.15635741 13.85034039], Loss = 0.3716\n",
      "Iteration 8960: Weights = [55.30333333  2.07165045  4.96119139  0.08159853  0.15634731 13.85068573], Loss = 0.3716\n",
      "Iteration 8961: Weights = [55.30333333  2.07151659  4.96087083  0.08159326  0.15633721 13.85103104], Loss = 0.3715\n",
      "Iteration 8962: Weights = [55.30333333  2.07138275  4.9605503   0.08158798  0.15632711 13.85137634], Loss = 0.3715\n",
      "Iteration 8963: Weights = [55.30333333  2.07124891  4.96022979  0.08158271  0.15631701 13.85172161], Loss = 0.3714\n",
      "Iteration 8964: Weights = [55.30333333  2.07111508  4.9599093   0.08157744  0.15630691 13.85206686], Loss = 0.3714\n",
      "Iteration 8965: Weights = [55.30333333  2.07098127  4.95958883  0.08157217  0.15629681 13.85241209], Loss = 0.3713\n",
      "Iteration 8966: Weights = [55.30333333  2.07084745  4.95926838  0.0815669   0.15628671 13.8527573 ], Loss = 0.3713\n",
      "Iteration 8967: Weights = [55.30333333  2.07071365  4.95894795  0.08156163  0.15627661 13.85310248], Loss = 0.3712\n",
      "Iteration 8968: Weights = [55.30333333  2.07057986  4.95862754  0.08155636  0.15626652 13.85344764], Loss = 0.3712\n",
      "Iteration 8969: Weights = [55.30333333  2.07044608  4.95830716  0.08155109  0.15625642 13.85379278], Loss = 0.3711\n",
      "Iteration 8970: Weights = [55.30333333  2.0703123   4.95798679  0.08154582  0.15624632 13.8541379 ], Loss = 0.3711\n",
      "Iteration 8971: Weights = [55.30333333  2.07017853  4.95766645  0.08154055  0.15623623 13.85448299], Loss = 0.3710\n",
      "Iteration 8972: Weights = [55.30333333  2.07004477  4.95734612  0.08153528  0.15622613 13.85482806], Loss = 0.3710\n",
      "Iteration 8973: Weights = [55.30333333  2.06991103  4.95702582  0.08153002  0.15621604 13.85517311], Loss = 0.3709\n",
      "Iteration 8974: Weights = [55.30333333  2.06977728  4.95670553  0.08152475  0.15620594 13.85551814], Loss = 0.3709\n",
      "Iteration 8975: Weights = [55.30333333  2.06964355  4.95638527  0.08151948  0.15619585 13.85586314], Loss = 0.3708\n",
      "Iteration 8976: Weights = [55.30333333  2.06950983  4.95606503  0.08151421  0.15618576 13.85620813], Loss = 0.3708\n",
      "Iteration 8977: Weights = [55.30333333  2.06937611  4.95574481  0.08150895  0.15617567 13.85655309], Loss = 0.3707\n",
      "Iteration 8978: Weights = [55.30333333  2.06924241  4.95542461  0.08150368  0.15616558 13.85689803], Loss = 0.3707\n",
      "Iteration 8979: Weights = [55.30333333  2.06910871  4.95510443  0.08149841  0.15615549 13.85724294], Loss = 0.3707\n",
      "Iteration 8980: Weights = [55.30333333  2.06897502  4.95478427  0.08149315  0.1561454  13.85758784], Loss = 0.3706\n",
      "Iteration 8981: Weights = [55.30333333  2.06884134  4.95446413  0.08148788  0.15613531 13.85793271], Loss = 0.3706\n",
      "Iteration 8982: Weights = [55.30333333  2.06870767  4.95414401  0.08148262  0.15612522 13.85827756], Loss = 0.3705\n",
      "Iteration 8983: Weights = [55.30333333  2.068574    4.95382391  0.08147735  0.15611513 13.85862238], Loss = 0.3705\n",
      "Iteration 8984: Weights = [55.30333333  2.06844035  4.95350384  0.08147209  0.15610505 13.85896719], Loss = 0.3704\n",
      "Iteration 8985: Weights = [55.30333333  2.0683067   4.95318378  0.08146682  0.15609496 13.85931197], Loss = 0.3704\n",
      "Iteration 8986: Weights = [55.30333333  2.06817306  4.95286375  0.08146156  0.15608487 13.85965673], Loss = 0.3703\n",
      "Iteration 8987: Weights = [55.30333333  2.06803944  4.95254373  0.0814563   0.15607479 13.86000147], Loss = 0.3703\n",
      "Iteration 8988: Weights = [55.30333333  2.06790582  4.95222374  0.08145103  0.15606471 13.86034618], Loss = 0.3702\n",
      "Iteration 8989: Weights = [55.30333333  2.0677722   4.95190377  0.08144577  0.15605462 13.86069088], Loss = 0.3702\n",
      "Iteration 8990: Weights = [55.30333333  2.0676386   4.95158381  0.08144051  0.15604454 13.86103555], Loss = 0.3701\n",
      "Iteration 8991: Weights = [55.30333333  2.06750501  4.95126388  0.08143525  0.15603446 13.86138019], Loss = 0.3701\n",
      "Iteration 8992: Weights = [55.30333333  2.06737142  4.95094397  0.08142999  0.15602437 13.86172482], Loss = 0.3700\n",
      "Iteration 8993: Weights = [55.30333333  2.06723784  4.95062408  0.08142472  0.15601429 13.86206943], Loss = 0.3700\n",
      "Iteration 8994: Weights = [55.30333333  2.06710428  4.95030421  0.08141946  0.15600421 13.86241401], Loss = 0.3699\n",
      "Iteration 8995: Weights = [55.30333333  2.06697072  4.94998436  0.0814142   0.15599413 13.86275857], Loss = 0.3699\n",
      "Iteration 8996: Weights = [55.30333333  2.06683716  4.94966453  0.08140894  0.15598405 13.8631031 ], Loss = 0.3698\n",
      "Iteration 8997: Weights = [55.30333333  2.06670362  4.94934472  0.08140368  0.15597398 13.86344762], Loss = 0.3698\n",
      "Iteration 8998: Weights = [55.30333333  2.06657009  4.94902494  0.08139842  0.1559639  13.86379211], Loss = 0.3697\n",
      "Iteration 8999: Weights = [55.30333333  2.06643656  4.94870517  0.08139316  0.15595382 13.86413658], Loss = 0.3697\n",
      "Iteration 9000: Weights = [55.30333333  2.06630305  4.94838543  0.0813879   0.15594374 13.86448103], Loss = 0.3696\n",
      "Iteration 9001: Weights = [55.30333333  2.06616954  4.9480657   0.08138265  0.15593367 13.86482546], Loss = 0.3696\n",
      "Iteration 9002: Weights = [55.30333333  2.06603604  4.947746    0.08137739  0.15592359 13.86516986], Loss = 0.3696\n",
      "Iteration 9003: Weights = [55.30333333  2.06590255  4.94742631  0.08137213  0.15591352 13.86551424], Loss = 0.3695\n",
      "Iteration 9004: Weights = [55.30333333  2.06576907  4.94710665  0.08136687  0.15590345 13.8658586 ], Loss = 0.3695\n",
      "Iteration 9005: Weights = [55.30333333  2.06563559  4.94678701  0.08136161  0.15589337 13.86620294], Loss = 0.3694\n",
      "Iteration 9006: Weights = [55.30333333  2.06550213  4.94646738  0.08135636  0.1558833  13.86654725], Loss = 0.3694\n",
      "Iteration 9007: Weights = [55.30333333  2.06536867  4.94614778  0.0813511   0.15587323 13.86689154], Loss = 0.3693\n",
      "Iteration 9008: Weights = [55.30333333  2.06523522  4.9458282   0.08134584  0.15586316 13.86723581], Loss = 0.3693\n",
      "Iteration 9009: Weights = [55.30333333  2.06510179  4.94550864  0.08134059  0.15585309 13.86758006], Loss = 0.3692\n",
      "Iteration 9010: Weights = [55.30333333  2.06496835  4.9451891   0.08133533  0.15584302 13.86792429], Loss = 0.3692\n",
      "Iteration 9011: Weights = [55.30333333  2.06483493  4.94486958  0.08133008  0.15583295 13.86826849], Loss = 0.3691\n",
      "Iteration 9012: Weights = [55.30333333  2.06470152  4.94455009  0.08132482  0.15582288 13.86861267], Loss = 0.3691\n",
      "Iteration 9013: Weights = [55.30333333  2.06456812  4.94423061  0.08131957  0.15581281 13.86895683], Loss = 0.3690\n",
      "Iteration 9014: Weights = [55.30333333  2.06443472  4.94391115  0.08131431  0.15580274 13.86930097], Loss = 0.3690\n",
      "Iteration 9015: Weights = [55.30333333  2.06430133  4.94359172  0.08130906  0.15579268 13.86964508], Loss = 0.3689\n",
      "Iteration 9016: Weights = [55.30333333  2.06416795  4.9432723   0.08130381  0.15578261 13.86998918], Loss = 0.3689\n",
      "Iteration 9017: Weights = [55.30333333  2.06403458  4.94295291  0.08129855  0.15577254 13.87033325], Loss = 0.3688\n",
      "Iteration 9018: Weights = [55.30333333  2.06390122  4.94263353  0.0812933   0.15576248 13.87067729], Loss = 0.3688\n",
      "Iteration 9019: Weights = [55.30333333  2.06376787  4.94231418  0.08128805  0.15575241 13.87102132], Loss = 0.3687\n",
      "Iteration 9020: Weights = [55.30333333  2.06363453  4.94199485  0.0812828   0.15574235 13.87136532], Loss = 0.3687\n",
      "Iteration 9021: Weights = [55.30333333  2.06350119  4.94167553  0.08127754  0.15573229 13.8717093 ], Loss = 0.3686\n",
      "Iteration 9022: Weights = [55.30333333  2.06336786  4.94135624  0.08127229  0.15572223 13.87205326], Loss = 0.3686\n",
      "Iteration 9023: Weights = [55.30333333  2.06323454  4.94103697  0.08126704  0.15571216 13.8723972 ], Loss = 0.3686\n",
      "Iteration 9024: Weights = [55.30333333  2.06310123  4.94071772  0.08126179  0.1557021  13.87274111], Loss = 0.3685\n",
      "Iteration 9025: Weights = [55.30333333  2.06296793  4.94039849  0.08125654  0.15569204 13.87308501], Loss = 0.3685\n",
      "Iteration 9026: Weights = [55.30333333  2.06283464  4.94007928  0.08125129  0.15568198 13.87342888], Loss = 0.3684\n",
      "Iteration 9027: Weights = [55.30333333  2.06270136  4.93976009  0.08124604  0.15567193 13.87377272], Loss = 0.3684\n",
      "Iteration 9028: Weights = [55.30333333  2.06256808  4.93944093  0.08124079  0.15566187 13.87411655], Loss = 0.3683\n",
      "Iteration 9029: Weights = [55.30333333  2.06243481  4.93912178  0.08123554  0.15565181 13.87446035], Loss = 0.3683\n",
      "Iteration 9030: Weights = [55.30333333  2.06230156  4.93880265  0.08123029  0.15564175 13.87480413], Loss = 0.3682\n",
      "Iteration 9031: Weights = [55.30333333  2.06216831  4.93848355  0.08122504  0.1556317  13.87514789], Loss = 0.3682\n",
      "Iteration 9032: Weights = [55.30333333  2.06203507  4.93816446  0.0812198   0.15562164 13.87549163], Loss = 0.3681\n",
      "Iteration 9033: Weights = [55.30333333  2.06190183  4.9378454   0.08121455  0.15561159 13.87583534], Loss = 0.3681\n",
      "Iteration 9034: Weights = [55.30333333  2.06176861  4.93752635  0.0812093   0.15560153 13.87617904], Loss = 0.3680\n",
      "Iteration 9035: Weights = [55.30333333  2.0616354   4.93720733  0.08120405  0.15559148 13.87652271], Loss = 0.3680\n",
      "Iteration 9036: Weights = [55.30333333  2.06150219  4.93688833  0.08119881  0.15558142 13.87686635], Loss = 0.3679\n",
      "Iteration 9037: Weights = [55.30333333  2.06136899  4.93656934  0.08119356  0.15557137 13.87720998], Loss = 0.3679\n",
      "Iteration 9038: Weights = [55.30333333  2.0612358   4.93625038  0.08118831  0.15556132 13.87755358], Loss = 0.3678\n",
      "Iteration 9039: Weights = [55.30333333  2.06110262  4.93593144  0.08118307  0.15555127 13.87789717], Loss = 0.3678\n",
      "Iteration 9040: Weights = [55.30333333  2.06096945  4.93561252  0.08117782  0.15554122 13.87824072], Loss = 0.3677\n",
      "Iteration 9041: Weights = [55.30333333  2.06083629  4.93529362  0.08117258  0.15553117 13.87858426], Loss = 0.3677\n",
      "Iteration 9042: Weights = [55.30333333  2.06070313  4.93497474  0.08116733  0.15552112 13.87892778], Loss = 0.3676\n",
      "Iteration 9043: Weights = [55.30333333  2.06056999  4.93465588  0.08116209  0.15551107 13.87927127], Loss = 0.3676\n",
      "Iteration 9044: Weights = [55.30333333  2.06043685  4.93433704  0.08115685  0.15550102 13.87961474], Loss = 0.3676\n",
      "Iteration 9045: Weights = [55.30333333  2.06030372  4.93401823  0.0811516   0.15549098 13.87995819], Loss = 0.3675\n",
      "Iteration 9046: Weights = [55.30333333  2.0601706   4.93369943  0.08114636  0.15548093 13.88030161], Loss = 0.3675\n",
      "Iteration 9047: Weights = [55.30333333  2.06003749  4.93338065  0.08114112  0.15547088 13.88064502], Loss = 0.3674\n",
      "Iteration 9048: Weights = [55.30333333  2.05990438  4.9330619   0.08113587  0.15546084 13.8809884 ], Loss = 0.3674\n",
      "Iteration 9049: Weights = [55.30333333  2.05977129  4.93274316  0.08113063  0.15545079 13.88133176], Loss = 0.3673\n",
      "Iteration 9050: Weights = [55.30333333  2.0596382   4.93242445  0.08112539  0.15544075 13.88167509], Loss = 0.3673\n",
      "Iteration 9051: Weights = [55.30333333  2.05950513  4.93210575  0.08112015  0.15543071 13.88201841], Loss = 0.3672\n",
      "Iteration 9052: Weights = [55.30333333  2.05937206  4.93178708  0.0811149   0.15542066 13.8823617 ], Loss = 0.3672\n",
      "Iteration 9053: Weights = [55.30333333  2.059239    4.93146843  0.08110966  0.15541062 13.88270497], Loss = 0.3671\n",
      "Iteration 9054: Weights = [55.30333333  2.05910595  4.9311498   0.08110442  0.15540058 13.88304822], Loss = 0.3671\n",
      "Iteration 9055: Weights = [55.30333333  2.0589729   4.93083119  0.08109918  0.15539054 13.88339145], Loss = 0.3670\n",
      "Iteration 9056: Weights = [55.30333333  2.05883987  4.93051259  0.08109394  0.1553805  13.88373465], Loss = 0.3670\n",
      "Iteration 9057: Weights = [55.30333333  2.05870684  4.93019402  0.0810887   0.15537046 13.88407783], Loss = 0.3669\n",
      "Iteration 9058: Weights = [55.30333333  2.05857382  4.92987547  0.08108346  0.15536042 13.88442099], Loss = 0.3669\n",
      "Iteration 9059: Weights = [55.30333333  2.05844082  4.92955695  0.08107823  0.15535038 13.88476413], Loss = 0.3668\n",
      "Iteration 9060: Weights = [55.30333333  2.05830782  4.92923844  0.08107299  0.15534035 13.88510725], Loss = 0.3668\n",
      "Iteration 9061: Weights = [55.30333333  2.05817482  4.92891995  0.08106775  0.15533031 13.88545034], Loss = 0.3667\n",
      "Iteration 9062: Weights = [55.30333333  2.05804184  4.92860148  0.08106251  0.15532027 13.88579341], Loss = 0.3667\n",
      "Iteration 9063: Weights = [55.30333333  2.05790887  4.92828303  0.08105727  0.15531024 13.88613646], Loss = 0.3667\n",
      "Iteration 9064: Weights = [55.30333333  2.0577759   4.92796461  0.08105204  0.1553002  13.88647949], Loss = 0.3666\n",
      "Iteration 9065: Weights = [55.30333333  2.05764295  4.9276462   0.0810468   0.15529017 13.88682249], Loss = 0.3666\n",
      "Iteration 9066: Weights = [55.30333333  2.05751     4.92732782  0.08104156  0.15528013 13.88716547], Loss = 0.3665\n",
      "Iteration 9067: Weights = [55.30333333  2.05737706  4.92700945  0.08103633  0.1552701  13.88750843], Loss = 0.3665\n",
      "Iteration 9068: Weights = [55.30333333  2.05724413  4.92669111  0.08103109  0.15526007 13.88785137], Loss = 0.3664\n",
      "Iteration 9069: Weights = [55.30333333  2.0571112   4.92637279  0.08102585  0.15525004 13.88819429], Loss = 0.3664\n",
      "Iteration 9070: Weights = [55.30333333  2.05697829  4.92605448  0.08102062  0.15524001 13.88853718], Loss = 0.3663\n",
      "Iteration 9071: Weights = [55.30333333  2.05684538  4.9257362   0.08101538  0.15522998 13.88888005], Loss = 0.3663\n",
      "Iteration 9072: Weights = [55.30333333  2.05671249  4.92541794  0.08101015  0.15521995 13.8892229 ], Loss = 0.3662\n",
      "Iteration 9073: Weights = [55.30333333  2.0565796   4.9250997   0.08100492  0.15520992 13.88956573], Loss = 0.3662\n",
      "Iteration 9074: Weights = [55.30333333  2.05644672  4.92478148  0.08099968  0.15519989 13.88990854], Loss = 0.3661\n",
      "Iteration 9075: Weights = [55.30333333  2.05631385  4.92446328  0.08099445  0.15518986 13.89025132], Loss = 0.3661\n",
      "Iteration 9076: Weights = [55.30333333  2.05618098  4.9241451   0.08098921  0.15517983 13.89059408], Loss = 0.3660\n",
      "Iteration 9077: Weights = [55.30333333  2.05604813  4.92382694  0.08098398  0.15516981 13.89093682], Loss = 0.3660\n",
      "Iteration 9078: Weights = [55.30333333  2.05591529  4.9235088   0.08097875  0.15515978 13.89127953], Loss = 0.3659\n",
      "Iteration 9079: Weights = [55.30333333  2.05578245  4.92319068  0.08097352  0.15514976 13.89162223], Loss = 0.3659\n",
      "Iteration 9080: Weights = [55.30333333  2.05564962  4.92287259  0.08096829  0.15513973 13.8919649 ], Loss = 0.3658\n",
      "Iteration 9081: Weights = [55.30333333  2.0555168   4.92255451  0.08096305  0.15512971 13.89230755], Loss = 0.3658\n",
      "Iteration 9082: Weights = [55.30333333  2.05538399  4.92223645  0.08095782  0.15511968 13.89265018], Loss = 0.3658\n",
      "Iteration 9083: Weights = [55.30333333  2.05525119  4.92191842  0.08095259  0.15510966 13.89299279], Loss = 0.3657\n",
      "Iteration 9084: Weights = [55.30333333  2.05511839  4.9216004   0.08094736  0.15509964 13.89333537], Loss = 0.3657\n",
      "Iteration 9085: Weights = [55.30333333  2.05498561  4.92128241  0.08094213  0.15508962 13.89367793], Loss = 0.3656\n",
      "Iteration 9086: Weights = [55.30333333  2.05485283  4.92096443  0.0809369   0.1550796  13.89402047], Loss = 0.3656\n",
      "Iteration 9087: Weights = [55.30333333  2.05472006  4.92064648  0.08093167  0.15506958 13.89436299], Loss = 0.3655\n",
      "Iteration 9088: Weights = [55.30333333  2.0545873   4.92032855  0.08092644  0.15505956 13.89470548], Loss = 0.3655\n",
      "Iteration 9089: Weights = [55.30333333  2.05445455  4.92001063  0.08092121  0.15504954 13.89504796], Loss = 0.3654\n",
      "Iteration 9090: Weights = [55.30333333  2.05432181  4.91969274  0.08091598  0.15503952 13.89539041], Loss = 0.3654\n",
      "Iteration 9091: Weights = [55.30333333  2.05418908  4.91937487  0.08091076  0.1550295  13.89573284], Loss = 0.3653\n",
      "Iteration 9092: Weights = [55.30333333  2.05405635  4.91905702  0.08090553  0.15501949 13.89607524], Loss = 0.3653\n",
      "Iteration 9093: Weights = [55.30333333  2.05392363  4.91873919  0.0809003   0.15500947 13.89641763], Loss = 0.3652\n",
      "Iteration 9094: Weights = [55.30333333  2.05379093  4.91842138  0.08089507  0.15499946 13.89675999], Loss = 0.3652\n",
      "Iteration 9095: Weights = [55.30333333  2.05365823  4.91810359  0.08088985  0.15498944 13.89710233], Loss = 0.3651\n",
      "Iteration 9096: Weights = [55.30333333  2.05352554  4.91778582  0.08088462  0.15497943 13.89744465], Loss = 0.3651\n",
      "Iteration 9097: Weights = [55.30333333  2.05339285  4.91746808  0.0808794   0.15496941 13.89778694], Loss = 0.3650\n",
      "Iteration 9098: Weights = [55.30333333  2.05326018  4.91715035  0.08087417  0.1549594  13.89812922], Loss = 0.3650\n",
      "Iteration 9099: Weights = [55.30333333  2.05312751  4.91683264  0.08086894  0.15494939 13.89847147], Loss = 0.3650\n",
      "Iteration 9100: Weights = [55.30333333  2.05299486  4.91651495  0.08086372  0.15493938 13.8988137 ], Loss = 0.3649\n",
      "Iteration 9101: Weights = [55.30333333  2.05286221  4.91619729  0.08085849  0.15492937 13.89915591], Loss = 0.3649\n",
      "Iteration 9102: Weights = [55.30333333  2.05272957  4.91587964  0.08085327  0.15491936 13.89949809], Loss = 0.3648\n",
      "Iteration 9103: Weights = [55.30333333  2.05259694  4.91556202  0.08084805  0.15490935 13.89984026], Loss = 0.3648\n",
      "Iteration 9104: Weights = [55.30333333  2.05246432  4.91524441  0.08084282  0.15489934 13.9001824 ], Loss = 0.3647\n",
      "Iteration 9105: Weights = [55.30333333  2.0523317   4.91492683  0.0808376   0.15488933 13.90052452], Loss = 0.3647\n",
      "Iteration 9106: Weights = [55.30333333  2.0521991   4.91460927  0.08083238  0.15487932 13.90086661], Loss = 0.3646\n",
      "Iteration 9107: Weights = [55.30333333  2.0520665   4.91429172  0.08082715  0.15486931 13.90120869], Loss = 0.3646\n",
      "Iteration 9108: Weights = [55.30333333  2.05193391  4.9139742   0.08082193  0.15485931 13.90155074], Loss = 0.3645\n",
      "Iteration 9109: Weights = [55.30333333  2.05180133  4.9136567   0.08081671  0.1548493  13.90189277], Loss = 0.3645\n",
      "Iteration 9110: Weights = [55.30333333  2.05166876  4.91333922  0.08081149  0.1548393  13.90223478], Loss = 0.3644\n",
      "Iteration 9111: Weights = [55.30333333  2.0515362   4.91302176  0.08080626  0.15482929 13.90257677], Loss = 0.3644\n",
      "Iteration 9112: Weights = [55.30333333  2.05140365  4.91270432  0.08080104  0.15481929 13.90291873], Loss = 0.3643\n",
      "Iteration 9113: Weights = [55.30333333  2.0512711   4.9123869   0.08079582  0.15480928 13.90326068], Loss = 0.3643\n",
      "Iteration 9114: Weights = [55.30333333  2.05113856  4.9120695   0.0807906   0.15479928 13.9036026 ], Loss = 0.3642\n",
      "Iteration 9115: Weights = [55.30333333  2.05100603  4.91175212  0.08078538  0.15478928 13.90394449], Loss = 0.3642\n",
      "Iteration 9116: Weights = [55.30333333  2.05087352  4.91143476  0.08078016  0.15477928 13.90428637], Loss = 0.3641\n",
      "Iteration 9117: Weights = [55.30333333  2.050741    4.91111742  0.08077494  0.15476928 13.90462822], Loss = 0.3641\n",
      "Iteration 9118: Weights = [55.30333333  2.0506085   4.91080011  0.08076972  0.15475928 13.90497006], Loss = 0.3641\n",
      "Iteration 9119: Weights = [55.30333333  2.05047601  4.91048281  0.08076451  0.15474928 13.90531187], Loss = 0.3640\n",
      "Iteration 9120: Weights = [55.30333333  2.05034352  4.91016553  0.08075929  0.15473928 13.90565365], Loss = 0.3640\n",
      "Iteration 9121: Weights = [55.30333333  2.05021105  4.90984828  0.08075407  0.15472928 13.90599542], Loss = 0.3639\n",
      "Iteration 9122: Weights = [55.30333333  2.05007858  4.90953104  0.08074885  0.15471928 13.90633716], Loss = 0.3639\n",
      "Iteration 9123: Weights = [55.30333333  2.04994612  4.90921383  0.08074363  0.15470929 13.90667888], Loss = 0.3638\n",
      "Iteration 9124: Weights = [55.30333333  2.04981367  4.90889663  0.08073842  0.15469929 13.90702058], Loss = 0.3638\n",
      "Iteration 9125: Weights = [55.30333333  2.04968122  4.90857946  0.0807332   0.1546893  13.90736226], Loss = 0.3637\n",
      "Iteration 9126: Weights = [55.30333333  2.04954879  4.90826231  0.08072798  0.1546793  13.90770392], Loss = 0.3637\n",
      "Iteration 9127: Weights = [55.30333333  2.04941636  4.90794518  0.08072277  0.15466931 13.90804555], Loss = 0.3636\n",
      "Iteration 9128: Weights = [55.30333333  2.04928395  4.90762806  0.08071755  0.15465931 13.90838716], Loss = 0.3636\n",
      "Iteration 9129: Weights = [55.30333333  2.04915154  4.90731097  0.08071234  0.15464932 13.90872875], Loss = 0.3635\n",
      "Iteration 9130: Weights = [55.30333333  2.04901914  4.9069939   0.08070712  0.15463933 13.90907032], Loss = 0.3635\n",
      "Iteration 9131: Weights = [55.30333333  2.04888675  4.90667685  0.08070191  0.15462934 13.90941186], Loss = 0.3634\n",
      "Iteration 9132: Weights = [55.30333333  2.04875437  4.90635982  0.08069669  0.15461935 13.90975338], Loss = 0.3634\n",
      "Iteration 9133: Weights = [55.30333333  2.04862199  4.90604281  0.08069148  0.15460936 13.91009489], Loss = 0.3634\n",
      "Iteration 9134: Weights = [55.30333333  2.04848963  4.90572582  0.08068627  0.15459937 13.91043636], Loss = 0.3633\n",
      "Iteration 9135: Weights = [55.30333333  2.04835727  4.90540885  0.08068105  0.15458938 13.91077782], Loss = 0.3633\n",
      "Iteration 9136: Weights = [55.30333333  2.04822492  4.9050919   0.08067584  0.15457939 13.91111926], Loss = 0.3632\n",
      "Iteration 9137: Weights = [55.30333333  2.04809258  4.90477498  0.08067063  0.1545694  13.91146067], Loss = 0.3632\n",
      "Iteration 9138: Weights = [55.30333333  2.04796025  4.90445807  0.08066541  0.15455941 13.91180206], Loss = 0.3631\n",
      "Iteration 9139: Weights = [55.30333333  2.04782793  4.90414118  0.0806602   0.15454943 13.91214343], Loss = 0.3631\n",
      "Iteration 9140: Weights = [55.30333333  2.04769561  4.90382431  0.08065499  0.15453944 13.91248477], Loss = 0.3630\n",
      "Iteration 9141: Weights = [55.30333333  2.04756331  4.90350747  0.08064978  0.15452946 13.9128261 ], Loss = 0.3630\n",
      "Iteration 9142: Weights = [55.30333333  2.04743101  4.90319064  0.08064457  0.15451947 13.9131674 ], Loss = 0.3629\n",
      "Iteration 9143: Weights = [55.30333333  2.04729872  4.90287384  0.08063936  0.15450949 13.91350868], Loss = 0.3629\n",
      "Iteration 9144: Weights = [55.30333333  2.04716644  4.90255705  0.08063415  0.15449951 13.91384994], Loss = 0.3628\n",
      "Iteration 9145: Weights = [55.30333333  2.04703417  4.90224029  0.08062894  0.15448952 13.91419117], Loss = 0.3628\n",
      "Iteration 9146: Weights = [55.30333333  2.04690191  4.90192355  0.08062373  0.15447954 13.91453239], Loss = 0.3627\n",
      "Iteration 9147: Weights = [55.30333333  2.04676965  4.90160682  0.08061852  0.15446956 13.91487358], Loss = 0.3627\n",
      "Iteration 9148: Weights = [55.30333333  2.04663741  4.90129012  0.08061331  0.15445958 13.91521475], Loss = 0.3626\n",
      "Iteration 9149: Weights = [55.30333333  2.04650517  4.90097344  0.0806081   0.1544496  13.9155559 ], Loss = 0.3626\n",
      "Iteration 9150: Weights = [55.30333333  2.04637294  4.90065678  0.08060289  0.15443962 13.91589702], Loss = 0.3626\n",
      "Iteration 9151: Weights = [55.30333333  2.04624072  4.90034013  0.08059769  0.15442964 13.91623813], Loss = 0.3625\n",
      "Iteration 9152: Weights = [55.30333333  2.04610851  4.90002351  0.08059248  0.15441966 13.91657921], Loss = 0.3625\n",
      "Iteration 9153: Weights = [55.30333333  2.0459763   4.89970691  0.08058727  0.15440969 13.91692027], Loss = 0.3624\n",
      "Iteration 9154: Weights = [55.30333333  2.04584411  4.89939033  0.08058206  0.15439971 13.91726131], Loss = 0.3624\n",
      "Iteration 9155: Weights = [55.30333333  2.04571192  4.89907377  0.08057686  0.15438973 13.91760232], Loss = 0.3623\n",
      "Iteration 9156: Weights = [55.30333333  2.04557975  4.89875724  0.08057165  0.15437976 13.91794332], Loss = 0.3623\n",
      "Iteration 9157: Weights = [55.30333333  2.04544758  4.89844072  0.08056645  0.15436978 13.91828429], Loss = 0.3622\n",
      "Iteration 9158: Weights = [55.30333333  2.04531542  4.89812422  0.08056124  0.15435981 13.91862524], Loss = 0.3622\n",
      "Iteration 9159: Weights = [55.30333333  2.04518326  4.89780774  0.08055603  0.15434984 13.91896617], Loss = 0.3621\n",
      "Iteration 9160: Weights = [55.30333333  2.04505112  4.89749128  0.08055083  0.15433986 13.91930707], Loss = 0.3621\n",
      "Iteration 9161: Weights = [55.30333333  2.04491899  4.89717485  0.08054562  0.15432989 13.91964796], Loss = 0.3620\n",
      "Iteration 9162: Weights = [55.30333333  2.04478686  4.89685843  0.08054042  0.15431992 13.91998882], Loss = 0.3620\n",
      "Iteration 9163: Weights = [55.30333333  2.04465474  4.89654203  0.08053522  0.15430995 13.92032966], Loss = 0.3619\n",
      "Iteration 9164: Weights = [55.30333333  2.04452263  4.89622566  0.08053001  0.15429998 13.92067047], Loss = 0.3619\n",
      "Iteration 9165: Weights = [55.30333333  2.04439053  4.8959093   0.08052481  0.15429001 13.92101127], Loss = 0.3619\n",
      "Iteration 9166: Weights = [55.30333333  2.04425844  4.89559297  0.08051961  0.15428004 13.92135204], Loss = 0.3618\n",
      "Iteration 9167: Weights = [55.30333333  2.04412636  4.89527666  0.0805144   0.15427007 13.92169279], Loss = 0.3618\n",
      "Iteration 9168: Weights = [55.30333333  2.04399428  4.89496036  0.0805092   0.1542601  13.92203352], Loss = 0.3617\n",
      "Iteration 9169: Weights = [55.30333333  2.04386221  4.89464409  0.080504    0.15425014 13.92237423], Loss = 0.3617\n",
      "Iteration 9170: Weights = [55.30333333  2.04373016  4.89432784  0.0804988   0.15424017 13.92271492], Loss = 0.3616\n",
      "Iteration 9171: Weights = [55.30333333  2.04359811  4.8940116   0.0804936   0.1542302  13.92305558], Loss = 0.3616\n",
      "Iteration 9172: Weights = [55.30333333  2.04346607  4.89369539  0.0804884   0.15422024 13.92339622], Loss = 0.3615\n",
      "Iteration 9173: Weights = [55.30333333  2.04333403  4.8933792   0.0804832   0.15421027 13.92373684], Loss = 0.3615\n",
      "Iteration 9174: Weights = [55.30333333  2.04320201  4.89306303  0.080478    0.15420031 13.92407744], Loss = 0.3614\n",
      "Iteration 9175: Weights = [55.30333333  2.04306999  4.89274688  0.0804728   0.15419035 13.92441801], Loss = 0.3614\n",
      "Iteration 9176: Weights = [55.30333333  2.04293799  4.89243075  0.0804676   0.15418039 13.92475857], Loss = 0.3613\n",
      "Iteration 9177: Weights = [55.30333333  2.04280599  4.89211464  0.0804624   0.15417042 13.9250991 ], Loss = 0.3613\n",
      "Iteration 9178: Weights = [55.30333333  2.042674    4.89179855  0.0804572   0.15416046 13.92543961], Loss = 0.3612\n",
      "Iteration 9179: Weights = [55.30333333  2.04254202  4.89148248  0.080452    0.1541505  13.92578009], Loss = 0.3612\n",
      "Iteration 9180: Weights = [55.30333333  2.04241005  4.89116643  0.0804468   0.15414054 13.92612056], Loss = 0.3612\n",
      "Iteration 9181: Weights = [55.30333333  2.04227808  4.8908504   0.0804416   0.15413058 13.926461  ], Loss = 0.3611\n",
      "Iteration 9182: Weights = [55.30333333  2.04214613  4.8905344   0.08043641  0.15412062 13.92680142], Loss = 0.3611\n",
      "Iteration 9183: Weights = [55.30333333  2.04201418  4.89021841  0.08043121  0.15411067 13.92714182], Loss = 0.3610\n",
      "Iteration 9184: Weights = [55.30333333  2.04188224  4.88990244  0.08042601  0.15410071 13.9274822 ], Loss = 0.3610\n",
      "Iteration 9185: Weights = [55.30333333  2.04175031  4.8895865   0.08042082  0.15409075 13.92782256], Loss = 0.3609\n",
      "Iteration 9186: Weights = [55.30333333  2.04161839  4.88927057  0.08041562  0.1540808  13.92816289], Loss = 0.3609\n",
      "Iteration 9187: Weights = [55.30333333  2.04148647  4.88895466  0.08041042  0.15407084 13.9285032 ], Loss = 0.3608\n",
      "Iteration 9188: Weights = [55.30333333  2.04135457  4.88863878  0.08040523  0.15406088 13.92884349], Loss = 0.3608\n",
      "Iteration 9189: Weights = [55.30333333  2.04122267  4.88832291  0.08040003  0.15405093 13.92918376], Loss = 0.3607\n",
      "Iteration 9190: Weights = [55.30333333  2.04109079  4.88800707  0.08039484  0.15404098 13.929524  ], Loss = 0.3607\n",
      "Iteration 9191: Weights = [55.30333333  2.04095891  4.88769125  0.08038964  0.15403102 13.92986423], Loss = 0.3606\n",
      "Iteration 9192: Weights = [55.30333333  2.04082704  4.88737544  0.08038445  0.15402107 13.93020443], Loss = 0.3606\n",
      "Iteration 9193: Weights = [55.30333333  2.04069518  4.88705966  0.08037926  0.15401112 13.93054461], Loss = 0.3605\n",
      "Iteration 9194: Weights = [55.30333333  2.04056332  4.8867439   0.08037406  0.15400117 13.93088476], Loss = 0.3605\n",
      "Iteration 9195: Weights = [55.30333333  2.04043148  4.88642815  0.08036887  0.15399122 13.9312249 ], Loss = 0.3605\n",
      "Iteration 9196: Weights = [55.30333333  2.04029964  4.88611243  0.08036368  0.15398127 13.93156501], Loss = 0.3604\n",
      "Iteration 9197: Weights = [55.30333333  2.04016781  4.88579673  0.08035848  0.15397132 13.9319051 ], Loss = 0.3604\n",
      "Iteration 9198: Weights = [55.30333333  2.04003599  4.88548105  0.08035329  0.15396137 13.93224517], Loss = 0.3603\n",
      "Iteration 9199: Weights = [55.30333333  2.03990418  4.88516539  0.0803481   0.15395142 13.93258522], Loss = 0.3603\n",
      "Iteration 9200: Weights = [55.30333333  2.03977238  4.88484975  0.08034291  0.15394148 13.93292525], Loss = 0.3602\n",
      "Iteration 9201: Weights = [55.30333333  2.03964059  4.88453413  0.08033772  0.15393153 13.93326525], Loss = 0.3602\n",
      "Iteration 9202: Weights = [55.30333333  2.0395088   4.88421853  0.08033253  0.15392158 13.93360523], Loss = 0.3601\n",
      "Iteration 9203: Weights = [55.30333333  2.03937702  4.88390295  0.08032734  0.15391164 13.93394519], Loss = 0.3601\n",
      "Iteration 9204: Weights = [55.30333333  2.03924526  4.88358739  0.08032215  0.15390169 13.93428513], Loss = 0.3600\n",
      "Iteration 9205: Weights = [55.30333333  2.0391135   4.88327185  0.08031696  0.15389175 13.93462505], Loss = 0.3600\n",
      "Iteration 9206: Weights = [55.30333333  2.03898175  4.88295633  0.08031177  0.15388181 13.93496494], Loss = 0.3599\n",
      "Iteration 9207: Weights = [55.30333333  2.03885     4.88264084  0.08030658  0.15387187 13.93530481], Loss = 0.3599\n",
      "Iteration 9208: Weights = [55.30333333  2.03871827  4.88232536  0.08030139  0.15386192 13.93564466], Loss = 0.3598\n",
      "Iteration 9209: Weights = [55.30333333  2.03858654  4.8820099   0.0802962   0.15385198 13.93598449], Loss = 0.3598\n",
      "Iteration 9210: Weights = [55.30333333  2.03845483  4.88169447  0.08029101  0.15384204 13.9363243 ], Loss = 0.3598\n",
      "Iteration 9211: Weights = [55.30333333  2.03832312  4.88137905  0.08028583  0.1538321  13.93666408], Loss = 0.3597\n",
      "Iteration 9212: Weights = [55.30333333  2.03819142  4.88106365  0.08028064  0.15382216 13.93700384], Loss = 0.3597\n",
      "Iteration 9213: Weights = [55.30333333  2.03805972  4.88074828  0.08027545  0.15381222 13.93734358], Loss = 0.3596\n",
      "Iteration 9214: Weights = [55.30333333  2.03792804  4.88043292  0.08027026  0.15380228 13.9376833 ], Loss = 0.3596\n",
      "Iteration 9215: Weights = [55.30333333  2.03779637  4.88011759  0.08026508  0.15379235 13.938023  ], Loss = 0.3595\n",
      "Iteration 9216: Weights = [55.30333333  2.0376647   4.87980228  0.08025989  0.15378241 13.93836267], Loss = 0.3595\n",
      "Iteration 9217: Weights = [55.30333333  2.03753304  4.87948698  0.08025471  0.15377247 13.93870232], Loss = 0.3594\n",
      "Iteration 9218: Weights = [55.30333333  2.03740139  4.87917171  0.08024952  0.15376254 13.93904195], Loss = 0.3594\n",
      "Iteration 9219: Weights = [55.30333333  2.03726975  4.87885645  0.08024434  0.1537526  13.93938156], Loss = 0.3593\n",
      "Iteration 9220: Weights = [55.30333333  2.03713812  4.87854122  0.08023915  0.15374267 13.93972115], Loss = 0.3593\n",
      "Iteration 9221: Weights = [55.30333333  2.0370065   4.87822601  0.08023397  0.15373274 13.94006071], Loss = 0.3592\n",
      "Iteration 9222: Weights = [55.30333333  2.03687488  4.87791082  0.08022878  0.1537228  13.94040026], Loss = 0.3592\n",
      "Iteration 9223: Weights = [55.30333333  2.03674328  4.87759565  0.0802236   0.15371287 13.94073978], Loss = 0.3591\n",
      "Iteration 9224: Weights = [55.30333333  2.03661168  4.87728049  0.08021842  0.15370294 13.94107927], Loss = 0.3591\n",
      "Iteration 9225: Weights = [55.30333333  2.03648009  4.87696536  0.08021323  0.15369301 13.94141875], Loss = 0.3591\n",
      "Iteration 9226: Weights = [55.30333333  2.03634851  4.87665025  0.08020805  0.15368308 13.94175821], Loss = 0.3590\n",
      "Iteration 9227: Weights = [55.30333333  2.03621694  4.87633516  0.08020287  0.15367315 13.94209764], Loss = 0.3590\n",
      "Iteration 9228: Weights = [55.30333333  2.03608537  4.87602009  0.08019768  0.15366322 13.94243705], Loss = 0.3589\n",
      "Iteration 9229: Weights = [55.30333333  2.03595382  4.87570504  0.0801925   0.15365329 13.94277644], Loss = 0.3589\n",
      "Iteration 9230: Weights = [55.30333333  2.03582227  4.87539002  0.08018732  0.15364336 13.94311581], Loss = 0.3588\n",
      "Iteration 9231: Weights = [55.30333333  2.03569073  4.87507501  0.08018214  0.15363344 13.94345515], Loss = 0.3588\n",
      "Iteration 9232: Weights = [55.30333333  2.0355592   4.87476002  0.08017696  0.15362351 13.94379447], Loss = 0.3587\n",
      "Iteration 9233: Weights = [55.30333333  2.03542768  4.87444505  0.08017178  0.15361358 13.94413378], Loss = 0.3587\n",
      "Iteration 9234: Weights = [55.30333333  2.03529617  4.8741301   0.0801666   0.15360366 13.94447306], Loss = 0.3586\n",
      "Iteration 9235: Weights = [55.30333333  2.03516466  4.87381517  0.08016142  0.15359373 13.94481231], Loss = 0.3586\n",
      "Iteration 9236: Weights = [55.30333333  2.03503316  4.87350027  0.08015624  0.15358381 13.94515155], Loss = 0.3585\n",
      "Iteration 9237: Weights = [55.30333333  2.03490168  4.87318538  0.08015106  0.15357389 13.94549076], Loss = 0.3585\n",
      "Iteration 9238: Weights = [55.30333333  2.0347702   4.87287051  0.08014588  0.15356396 13.94582995], Loss = 0.3585\n",
      "Iteration 9239: Weights = [55.30333333  2.03463873  4.87255567  0.0801407   0.15355404 13.94616912], Loss = 0.3584\n",
      "Iteration 9240: Weights = [55.30333333  2.03450727  4.87224084  0.08013553  0.15354412 13.94650827], Loss = 0.3584\n",
      "Iteration 9241: Weights = [55.30333333  2.03437581  4.87192604  0.08013035  0.1535342  13.9468474 ], Loss = 0.3583\n",
      "Iteration 9242: Weights = [55.30333333  2.03424437  4.87161125  0.08012517  0.15352428 13.9471865 ], Loss = 0.3583\n",
      "Iteration 9243: Weights = [55.30333333  2.03411293  4.87129649  0.08011999  0.15351436 13.94752558], Loss = 0.3582\n",
      "Iteration 9244: Weights = [55.30333333  2.0339815   4.87098174  0.08011482  0.15350444 13.94786464], Loss = 0.3582\n",
      "Iteration 9245: Weights = [55.30333333  2.03385008  4.87066702  0.08010964  0.15349452 13.94820368], Loss = 0.3581\n",
      "Iteration 9246: Weights = [55.30333333  2.03371867  4.87035232  0.08010446  0.1534846  13.9485427 ], Loss = 0.3581\n",
      "Iteration 9247: Weights = [55.30333333  2.03358727  4.87003763  0.08009929  0.15347469 13.94888169], Loss = 0.3580\n",
      "Iteration 9248: Weights = [55.30333333  2.03345587  4.86972297  0.08009411  0.15346477 13.94922067], Loss = 0.3580\n",
      "Iteration 9249: Weights = [55.30333333  2.03332449  4.86940833  0.08008894  0.15345486 13.94955962], Loss = 0.3579\n",
      "Iteration 9250: Weights = [55.30333333  2.03319311  4.86909371  0.08008376  0.15344494 13.94989855], Loss = 0.3579\n",
      "Iteration 9251: Weights = [55.30333333  2.03306174  4.8687791   0.08007859  0.15343503 13.95023745], Loss = 0.3579\n",
      "Iteration 9252: Weights = [55.30333333  2.03293038  4.86846452  0.08007342  0.15342511 13.95057634], Loss = 0.3578\n",
      "Iteration 9253: Weights = [55.30333333  2.03279903  4.86814996  0.08006824  0.1534152  13.9509152 ], Loss = 0.3578\n",
      "Iteration 9254: Weights = [55.30333333  2.03266769  4.86783542  0.08006307  0.15340529 13.95125404], Loss = 0.3577\n",
      "Iteration 9255: Weights = [55.30333333  2.03253635  4.8675209   0.0800579   0.15339537 13.95159286], Loss = 0.3577\n",
      "Iteration 9256: Weights = [55.30333333  2.03240503  4.8672064   0.08005272  0.15338546 13.95193166], Loss = 0.3576\n",
      "Iteration 9257: Weights = [55.30333333  2.03227371  4.86689192  0.08004755  0.15337555 13.95227043], Loss = 0.3576\n",
      "Iteration 9258: Weights = [55.30333333  2.0321424   4.86657746  0.08004238  0.15336564 13.95260919], Loss = 0.3575\n",
      "Iteration 9259: Weights = [55.30333333  2.0320111   4.86626302  0.08003721  0.15335573 13.95294792], Loss = 0.3575\n",
      "Iteration 9260: Weights = [55.30333333  2.03187981  4.8659486   0.08003204  0.15334582 13.95328663], Loss = 0.3574\n",
      "Iteration 9261: Weights = [55.30333333  2.03174852  4.8656342   0.08002686  0.15333592 13.95362532], Loss = 0.3574\n",
      "Iteration 9262: Weights = [55.30333333  2.03161725  4.86531982  0.08002169  0.15332601 13.95396398], Loss = 0.3573\n",
      "Iteration 9263: Weights = [55.30333333  2.03148598  4.86500547  0.08001652  0.1533161  13.95430263], Loss = 0.3573\n",
      "Iteration 9264: Weights = [55.30333333  2.03135472  4.86469113  0.08001135  0.1533062  13.95464125], Loss = 0.3573\n",
      "Iteration 9265: Weights = [55.30333333  2.03122347  4.86437681  0.08000618  0.15329629 13.95497985], Loss = 0.3572\n",
      "Iteration 9266: Weights = [55.30333333  2.03109223  4.86406251  0.08000101  0.15328639 13.95531843], Loss = 0.3572\n",
      "Iteration 9267: Weights = [55.30333333  2.030961    4.86374824  0.07999585  0.15327648 13.95565699], Loss = 0.3571\n",
      "Iteration 9268: Weights = [55.30333333  2.03082977  4.86343398  0.07999068  0.15326658 13.95599552], Loss = 0.3571\n",
      "Iteration 9269: Weights = [55.30333333  2.03069856  4.86311974  0.07998551  0.15325668 13.95633404], Loss = 0.3570\n",
      "Iteration 9270: Weights = [55.30333333  2.03056735  4.86280553  0.07998034  0.15324677 13.95667253], Loss = 0.3570\n",
      "Iteration 9271: Weights = [55.30333333  2.03043615  4.86249133  0.07997517  0.15323687 13.957011  ], Loss = 0.3569\n",
      "Iteration 9272: Weights = [55.30333333  2.03030496  4.86217716  0.07997001  0.15322697 13.95734944], Loss = 0.3569\n",
      "Iteration 9273: Weights = [55.30333333  2.03017378  4.861863    0.07996484  0.15321707 13.95768787], Loss = 0.3568\n",
      "Iteration 9274: Weights = [55.30333333  2.0300426   4.86154887  0.07995967  0.15320717 13.95802627], Loss = 0.3568\n",
      "Iteration 9275: Weights = [55.30333333  2.02991144  4.86123475  0.07995451  0.15319727 13.95836465], Loss = 0.3567\n",
      "Iteration 9276: Weights = [55.30333333  2.02978028  4.86092066  0.07994934  0.15318737 13.95870301], Loss = 0.3567\n",
      "Iteration 9277: Weights = [55.30333333  2.02964913  4.86060658  0.07994417  0.15317748 13.95904135], Loss = 0.3567\n",
      "Iteration 9278: Weights = [55.30333333  2.02951799  4.86029253  0.07993901  0.15316758 13.95937967], Loss = 0.3566\n",
      "Iteration 9279: Weights = [55.30333333  2.02938686  4.8599785   0.07993384  0.15315768 13.95971796], Loss = 0.3566\n",
      "Iteration 9280: Weights = [55.30333333  2.02925574  4.85966448  0.07992868  0.15314779 13.96005624], Loss = 0.3565\n",
      "Iteration 9281: Weights = [55.30333333  2.02912463  4.85935049  0.07992351  0.15313789 13.96039449], Loss = 0.3565\n",
      "Iteration 9282: Weights = [55.30333333  2.02899352  4.85903652  0.07991835  0.153128   13.96073271], Loss = 0.3564\n",
      "Iteration 9283: Weights = [55.30333333  2.02886242  4.85872257  0.07991319  0.1531181  13.96107092], Loss = 0.3564\n",
      "Iteration 9284: Weights = [55.30333333  2.02873133  4.85840864  0.07990802  0.15310821 13.96140911], Loss = 0.3563\n",
      "Iteration 9285: Weights = [55.30333333  2.02860025  4.85809472  0.07990286  0.15309832 13.96174727], Loss = 0.3563\n",
      "Iteration 9286: Weights = [55.30333333  2.02846918  4.85778083  0.0798977   0.15308843 13.96208541], Loss = 0.3562\n",
      "Iteration 9287: Weights = [55.30333333  2.02833812  4.85746696  0.07989253  0.15307853 13.96242353], Loss = 0.3562\n",
      "Iteration 9288: Weights = [55.30333333  2.02820706  4.85715311  0.07988737  0.15306864 13.96276163], Loss = 0.3561\n",
      "Iteration 9289: Weights = [55.30333333  2.02807602  4.85683928  0.07988221  0.15305875 13.9630997 ], Loss = 0.3561\n",
      "Iteration 9290: Weights = [55.30333333  2.02794498  4.85652547  0.07987705  0.15304886 13.96343776], Loss = 0.3561\n",
      "Iteration 9291: Weights = [55.30333333  2.02781395  4.85621168  0.07987189  0.15303898 13.96377579], Loss = 0.3560\n",
      "Iteration 9292: Weights = [55.30333333  2.02768293  4.85589791  0.07986673  0.15302909 13.9641138 ], Loss = 0.3560\n",
      "Iteration 9293: Weights = [55.30333333  2.02755192  4.85558416  0.07986157  0.1530192  13.96445179], Loss = 0.3559\n",
      "Iteration 9294: Weights = [55.30333333  2.02742091  4.85527043  0.07985641  0.15300931 13.96478976], Loss = 0.3559\n",
      "Iteration 9295: Weights = [55.30333333  2.02728992  4.85495672  0.07985125  0.15299943 13.9651277 ], Loss = 0.3558\n",
      "Iteration 9296: Weights = [55.30333333  2.02715893  4.85464304  0.07984609  0.15298954 13.96546562], Loss = 0.3558\n",
      "Iteration 9297: Weights = [55.30333333  2.02702795  4.85432937  0.07984093  0.15297966 13.96580352], Loss = 0.3557\n",
      "Iteration 9298: Weights = [55.30333333  2.02689698  4.85401572  0.07983577  0.15296977 13.9661414 ], Loss = 0.3557\n",
      "Iteration 9299: Weights = [55.30333333  2.02676602  4.85370209  0.07983061  0.15295989 13.96647926], Loss = 0.3556\n",
      "Iteration 9300: Weights = [55.30333333  2.02663506  4.85338848  0.07982545  0.15295    13.9668171 ], Loss = 0.3556\n",
      "Iteration 9301: Weights = [55.30333333  2.02650412  4.8530749   0.0798203   0.15294012 13.96715491], Loss = 0.3555\n",
      "Iteration 9302: Weights = [55.30333333  2.02637318  4.85276133  0.07981514  0.15293024 13.9674927 ], Loss = 0.3555\n",
      "Iteration 9303: Weights = [55.30333333  2.02624225  4.85244778  0.07980998  0.15292036 13.96783047], Loss = 0.3555\n",
      "Iteration 9304: Weights = [55.30333333  2.02611133  4.85213426  0.07980483  0.15291048 13.96816822], Loss = 0.3554\n",
      "Iteration 9305: Weights = [55.30333333  2.02598042  4.85182075  0.07979967  0.1529006  13.96850595], Loss = 0.3554\n",
      "Iteration 9306: Weights = [55.30333333  2.02584952  4.85150727  0.07979451  0.15289072 13.96884365], Loss = 0.3553\n",
      "Iteration 9307: Weights = [55.30333333  2.02571863  4.8511938   0.07978936  0.15288084 13.96918134], Loss = 0.3553\n",
      "Iteration 9308: Weights = [55.30333333  2.02558774  4.85088035  0.0797842   0.15287096 13.969519  ], Loss = 0.3552\n",
      "Iteration 9309: Weights = [55.30333333  2.02545686  4.85056693  0.07977905  0.15286109 13.96985664], Loss = 0.3552\n",
      "Iteration 9310: Weights = [55.30333333  2.02532599  4.85025352  0.07977389  0.15285121 13.97019425], Loss = 0.3551\n",
      "Iteration 9311: Weights = [55.30333333  2.02519513  4.84994014  0.07976874  0.15284133 13.97053185], Loss = 0.3551\n",
      "Iteration 9312: Weights = [55.30333333  2.02506428  4.84962677  0.07976358  0.15283146 13.97086942], Loss = 0.3550\n",
      "Iteration 9313: Weights = [55.30333333  2.02493344  4.84931343  0.07975843  0.15282158 13.97120697], Loss = 0.3550\n",
      "Iteration 9314: Weights = [55.30333333  2.0248026   4.84900011  0.07975328  0.15281171 13.9715445 ], Loss = 0.3550\n",
      "Iteration 9315: Weights = [55.30333333  2.02467178  4.8486868   0.07974812  0.15280184 13.97188201], Loss = 0.3549\n",
      "Iteration 9316: Weights = [55.30333333  2.02454096  4.84837352  0.07974297  0.15279196 13.9722195 ], Loss = 0.3549\n",
      "Iteration 9317: Weights = [55.30333333  2.02441015  4.84806026  0.07973782  0.15278209 13.97255696], Loss = 0.3548\n",
      "Iteration 9318: Weights = [55.30333333  2.02427935  4.84774701  0.07973267  0.15277222 13.97289441], Loss = 0.3548\n",
      "Iteration 9319: Weights = [55.30333333  2.02414856  4.84743379  0.07972752  0.15276235 13.97323183], Loss = 0.3547\n",
      "Iteration 9320: Weights = [55.30333333  2.02401777  4.84712059  0.07972236  0.15275248 13.97356923], Loss = 0.3547\n",
      "Iteration 9321: Weights = [55.30333333  2.023887    4.84680741  0.07971721  0.15274261 13.97390661], Loss = 0.3546\n",
      "Iteration 9322: Weights = [55.30333333  2.02375623  4.84649424  0.07971206  0.15273274 13.97424396], Loss = 0.3546\n",
      "Iteration 9323: Weights = [55.30333333  2.02362547  4.8461811   0.07970691  0.15272287 13.9745813 ], Loss = 0.3545\n",
      "Iteration 9324: Weights = [55.30333333  2.02349472  4.84586798  0.07970176  0.152713   13.97491861], Loss = 0.3545\n",
      "Iteration 9325: Weights = [55.30333333  2.02336398  4.84555488  0.07969661  0.15270314 13.9752559 ], Loss = 0.3544\n",
      "Iteration 9326: Weights = [55.30333333  2.02323324  4.8452418   0.07969146  0.15269327 13.97559317], Loss = 0.3544\n",
      "Iteration 9327: Weights = [55.30333333  2.02310252  4.84492874  0.07968631  0.1526834  13.97593041], Loss = 0.3544\n",
      "Iteration 9328: Weights = [55.30333333  2.0229718   4.8446157   0.07968117  0.15267354 13.97626764], Loss = 0.3543\n",
      "Iteration 9329: Weights = [55.30333333  2.02284109  4.84430268  0.07967602  0.15266367 13.97660484], Loss = 0.3543\n",
      "Iteration 9330: Weights = [55.30333333  2.02271039  4.84398968  0.07967087  0.15265381 13.97694202], Loss = 0.3542\n",
      "Iteration 9331: Weights = [55.30333333  2.0225797   4.8436767   0.07966572  0.15264395 13.97727918], Loss = 0.3542\n",
      "Iteration 9332: Weights = [55.30333333  2.02244902  4.84336374  0.07966057  0.15263408 13.97761632], Loss = 0.3541\n",
      "Iteration 9333: Weights = [55.30333333  2.02231834  4.8430508   0.07965543  0.15262422 13.97795344], Loss = 0.3541\n",
      "Iteration 9334: Weights = [55.30333333  2.02218768  4.84273788  0.07965028  0.15261436 13.97829053], Loss = 0.3540\n",
      "Iteration 9335: Weights = [55.30333333  2.02205702  4.84242498  0.07964513  0.1526045  13.9786276 ], Loss = 0.3540\n",
      "Iteration 9336: Weights = [55.30333333  2.02192637  4.8421121   0.07963999  0.15259464 13.97896465], Loss = 0.3539\n",
      "Iteration 9337: Weights = [55.30333333  2.02179573  4.84179924  0.07963484  0.15258478 13.97930168], Loss = 0.3539\n",
      "Iteration 9338: Weights = [55.30333333  2.0216651   4.8414864   0.0796297   0.15257492 13.97963869], Loss = 0.3539\n",
      "Iteration 9339: Weights = [55.30333333  2.02153447  4.84117359  0.07962455  0.15256506 13.97997568], Loss = 0.3538\n",
      "Iteration 9340: Weights = [55.30333333  2.02140386  4.84086079  0.07961941  0.15255521 13.98031264], Loss = 0.3538\n",
      "Iteration 9341: Weights = [55.30333333  2.02127325  4.84054801  0.07961426  0.15254535 13.98064958], Loss = 0.3537\n",
      "Iteration 9342: Weights = [55.30333333  2.02114265  4.84023525  0.07960912  0.15253549 13.9809865 ], Loss = 0.3537\n",
      "Iteration 9343: Weights = [55.30333333  2.02101206  4.83992252  0.07960397  0.15252564 13.9813234 ], Loss = 0.3536\n",
      "Iteration 9344: Weights = [55.30333333  2.02088148  4.8396098   0.07959883  0.15251578 13.98166028], Loss = 0.3536\n",
      "Iteration 9345: Weights = [55.30333333  2.02075091  4.8392971   0.07959369  0.15250593 13.98199713], Loss = 0.3535\n",
      "Iteration 9346: Weights = [55.30333333  2.02062034  4.83898442  0.07958855  0.15249607 13.98233396], Loss = 0.3535\n",
      "Iteration 9347: Weights = [55.30333333  2.02048979  4.83867177  0.0795834   0.15248622 13.98267077], Loss = 0.3534\n",
      "Iteration 9348: Weights = [55.30333333  2.02035924  4.83835913  0.07957826  0.15247637 13.98300756], Loss = 0.3534\n",
      "Iteration 9349: Weights = [55.30333333  2.0202287   4.83804652  0.07957312  0.15246652 13.98334433], Loss = 0.3533\n",
      "Iteration 9350: Weights = [55.30333333  2.02009817  4.83773392  0.07956798  0.15245667 13.98368108], Loss = 0.3533\n",
      "Iteration 9351: Weights = [55.30333333  2.01996765  4.83742134  0.07956284  0.15244682 13.9840178 ], Loss = 0.3533\n",
      "Iteration 9352: Weights = [55.30333333  2.01983713  4.83710879  0.0795577   0.15243697 13.9843545 ], Loss = 0.3532\n",
      "Iteration 9353: Weights = [55.30333333  2.01970663  4.83679625  0.07955256  0.15242712 13.98469118], Loss = 0.3532\n",
      "Iteration 9354: Weights = [55.30333333  2.01957613  4.83648374  0.07954742  0.15241727 13.98502784], Loss = 0.3531\n",
      "Iteration 9355: Weights = [55.30333333  2.01944564  4.83617124  0.07954228  0.15240742 13.98536448], Loss = 0.3531\n",
      "Iteration 9356: Weights = [55.30333333  2.01931516  4.83585877  0.07953714  0.15239757 13.9857011 ], Loss = 0.3530\n",
      "Iteration 9357: Weights = [55.30333333  2.01918469  4.83554631  0.079532    0.15238773 13.98603769], Loss = 0.3530\n",
      "Iteration 9358: Weights = [55.30333333  2.01905422  4.83523388  0.07952686  0.15237788 13.98637426], Loss = 0.3529\n",
      "Iteration 9359: Weights = [55.30333333  2.01892377  4.83492147  0.07952172  0.15236803 13.98671081], Loss = 0.3529\n",
      "Iteration 9360: Weights = [55.30333333  2.01879332  4.83460907  0.07951658  0.15235819 13.98704734], Loss = 0.3528\n",
      "Iteration 9361: Weights = [55.30333333  2.01866288  4.8342967   0.07951145  0.15234834 13.98738385], Loss = 0.3528\n",
      "Iteration 9362: Weights = [55.30333333  2.01853245  4.83398434  0.07950631  0.1523385  13.98772033], Loss = 0.3528\n",
      "Iteration 9363: Weights = [55.30333333  2.01840203  4.83367201  0.07950117  0.15232866 13.9880568 ], Loss = 0.3527\n",
      "Iteration 9364: Weights = [55.30333333  2.01827162  4.8333597   0.07949603  0.15231882 13.98839324], Loss = 0.3527\n",
      "Iteration 9365: Weights = [55.30333333  2.01814122  4.8330474   0.0794909   0.15230897 13.98872966], Loss = 0.3526\n",
      "Iteration 9366: Weights = [55.30333333  2.01801082  4.83273513  0.07948576  0.15229913 13.98906605], Loss = 0.3526\n",
      "Iteration 9367: Weights = [55.30333333  2.01788043  4.83242288  0.07948063  0.15228929 13.98940243], Loss = 0.3525\n",
      "Iteration 9368: Weights = [55.30333333  2.01775005  4.83211065  0.07947549  0.15227945 13.98973879], Loss = 0.3525\n",
      "Iteration 9369: Weights = [55.30333333  2.01761968  4.83179843  0.07947036  0.15226961 13.99007512], Loss = 0.3524\n",
      "Iteration 9370: Weights = [55.30333333  2.01748932  4.83148624  0.07946522  0.15225978 13.99041143], Loss = 0.3524\n",
      "Iteration 9371: Weights = [55.30333333  2.01735896  4.83117407  0.07946009  0.15224994 13.99074772], Loss = 0.3523\n",
      "Iteration 9372: Weights = [55.30333333  2.01722862  4.83086192  0.07945495  0.1522401  13.99108399], Loss = 0.3523\n",
      "Iteration 9373: Weights = [55.30333333  2.01709828  4.83054979  0.07944982  0.15223026 13.99142023], Loss = 0.3523\n",
      "Iteration 9374: Weights = [55.30333333  2.01696795  4.83023767  0.07944468  0.15222043 13.99175646], Loss = 0.3522\n",
      "Iteration 9375: Weights = [55.30333333  2.01683763  4.82992558  0.07943955  0.15221059 13.99209266], Loss = 0.3522\n",
      "Iteration 9376: Weights = [55.30333333  2.01670732  4.82961351  0.07943442  0.15220076 13.99242884], Loss = 0.3521\n",
      "Iteration 9377: Weights = [55.30333333  2.01657702  4.82930146  0.07942929  0.15219092 13.992765  ], Loss = 0.3521\n",
      "Iteration 9378: Weights = [55.30333333  2.01644672  4.82898943  0.07942415  0.15218109 13.99310114], Loss = 0.3520\n",
      "Iteration 9379: Weights = [55.30333333  2.01631644  4.82867742  0.07941902  0.15217126 13.99343725], Loss = 0.3520\n",
      "Iteration 9380: Weights = [55.30333333  2.01618616  4.82836543  0.07941389  0.15216143 13.99377335], Loss = 0.3519\n",
      "Iteration 9381: Weights = [55.30333333  2.01605589  4.82805346  0.07940876  0.15215159 13.99410942], Loss = 0.3519\n",
      "Iteration 9382: Weights = [55.30333333  2.01592563  4.82774151  0.07940363  0.15214176 13.99444547], Loss = 0.3518\n",
      "Iteration 9383: Weights = [55.30333333  2.01579537  4.82742958  0.0793985   0.15213193 13.9947815 ], Loss = 0.3518\n",
      "Iteration 9384: Weights = [55.30333333  2.01566513  4.82711767  0.07939337  0.1521221  13.9951175 ], Loss = 0.3518\n",
      "Iteration 9385: Weights = [55.30333333  2.01553489  4.82680578  0.07938824  0.15211228 13.99545349], Loss = 0.3517\n",
      "Iteration 9386: Weights = [55.30333333  2.01540466  4.82649391  0.07938311  0.15210245 13.99578945], Loss = 0.3517\n",
      "Iteration 9387: Weights = [55.30333333  2.01527445  4.82618206  0.07937798  0.15209262 13.99612539], Loss = 0.3516\n",
      "Iteration 9388: Weights = [55.30333333  2.01514423  4.82587023  0.07937285  0.15208279 13.99646131], Loss = 0.3516\n",
      "Iteration 9389: Weights = [55.30333333  2.01501403  4.82555842  0.07936772  0.15207297 13.99679721], Loss = 0.3515\n",
      "Iteration 9390: Weights = [55.30333333  2.01488384  4.82524663  0.0793626   0.15206314 13.99713309], Loss = 0.3515\n",
      "Iteration 9391: Weights = [55.30333333  2.01475365  4.82493486  0.07935747  0.15205332 13.99746894], Loss = 0.3514\n",
      "Iteration 9392: Weights = [55.30333333  2.01462347  4.82462311  0.07935234  0.15204349 13.99780478], Loss = 0.3514\n",
      "Iteration 9393: Weights = [55.30333333  2.01449331  4.82431139  0.07934721  0.15203367 13.99814059], Loss = 0.3513\n",
      "Iteration 9394: Weights = [55.30333333  2.01436315  4.82399968  0.07934209  0.15202384 13.99847638], Loss = 0.3513\n",
      "Iteration 9395: Weights = [55.30333333  2.01423299  4.82368799  0.07933696  0.15201402 13.99881215], Loss = 0.3513\n",
      "Iteration 9396: Weights = [55.30333333  2.01410285  4.82337632  0.07933183  0.1520042  13.99914789], Loss = 0.3512\n",
      "Iteration 9397: Weights = [55.30333333  2.01397271  4.82306467  0.07932671  0.15199438 13.99948362], Loss = 0.3512\n",
      "Iteration 9398: Weights = [55.30333333  2.01384259  4.82275304  0.07932158  0.15198456 13.99981932], Loss = 0.3511\n",
      "Iteration 9399: Weights = [55.30333333  2.01371247  4.82244144  0.07931646  0.15197474 14.000155  ], Loss = 0.3511\n",
      "Iteration 9400: Weights = [55.30333333  2.01358236  4.82212985  0.07931133  0.15196492 14.00049066], Loss = 0.3510\n",
      "Iteration 9401: Weights = [55.30333333  2.01345226  4.82181828  0.07930621  0.1519551  14.0008263 ], Loss = 0.3510\n",
      "Iteration 9402: Weights = [55.30333333  2.01332216  4.82150673  0.07930108  0.15194528 14.00116192], Loss = 0.3509\n",
      "Iteration 9403: Weights = [55.30333333  2.01319208  4.82119521  0.07929596  0.15193546 14.00149751], Loss = 0.3509\n",
      "Iteration 9404: Weights = [55.30333333  2.013062    4.8208837   0.07929084  0.15192565 14.00183309], Loss = 0.3508\n",
      "Iteration 9405: Weights = [55.30333333  2.01293194  4.82057221  0.07928571  0.15191583 14.00216864], Loss = 0.3508\n",
      "Iteration 9406: Weights = [55.30333333  2.01280188  4.82026075  0.07928059  0.15190601 14.00250417], Loss = 0.3508\n",
      "Iteration 9407: Weights = [55.30333333  2.01267182  4.8199493   0.07927547  0.1518962  14.00283967], Loss = 0.3507\n",
      "Iteration 9408: Weights = [55.30333333  2.01254178  4.81963787  0.07927035  0.15188639 14.00317516], Loss = 0.3507\n",
      "Iteration 9409: Weights = [55.30333333  2.01241175  4.81932647  0.07926522  0.15187657 14.00351063], Loss = 0.3506\n",
      "Iteration 9410: Weights = [55.30333333  2.01228172  4.81901508  0.0792601   0.15186676 14.00384607], Loss = 0.3506\n",
      "Iteration 9411: Weights = [55.30333333  2.0121517   4.81870371  0.07925498  0.15185695 14.00418149], Loss = 0.3505\n",
      "Iteration 9412: Weights = [55.30333333  2.01202169  4.81839237  0.07924986  0.15184713 14.00451689], Loss = 0.3505\n",
      "Iteration 9413: Weights = [55.30333333  2.01189169  4.81808104  0.07924474  0.15183732 14.00485227], Loss = 0.3504\n",
      "Iteration 9414: Weights = [55.30333333  2.0117617   4.81776974  0.07923962  0.15182751 14.00518762], Loss = 0.3504\n",
      "Iteration 9415: Weights = [55.30333333  2.01163172  4.81745845  0.0792345   0.1518177  14.00552296], Loss = 0.3503\n",
      "Iteration 9416: Weights = [55.30333333  2.01150174  4.81714718  0.07922938  0.15180789 14.00585827], Loss = 0.3503\n",
      "Iteration 9417: Weights = [55.30333333  2.01137177  4.81683594  0.07922426  0.15179809 14.00619356], Loss = 0.3503\n",
      "Iteration 9418: Weights = [55.30333333  2.01124182  4.81652471  0.07921914  0.15178828 14.00652883], Loss = 0.3502\n",
      "Iteration 9419: Weights = [55.30333333  2.01111187  4.81621351  0.07921402  0.15177847 14.00686408], Loss = 0.3502\n",
      "Iteration 9420: Weights = [55.30333333  2.01098192  4.81590232  0.07920891  0.15176866 14.00719931], Loss = 0.3501\n",
      "Iteration 9421: Weights = [55.30333333  2.01085199  4.81559116  0.07920379  0.15175886 14.00753451], Loss = 0.3501\n",
      "Iteration 9422: Weights = [55.30333333  2.01072206  4.81528001  0.07919867  0.15174905 14.00786969], Loss = 0.3500\n",
      "Iteration 9423: Weights = [55.30333333  2.01059215  4.81496889  0.07919355  0.15173925 14.00820486], Loss = 0.3500\n",
      "Iteration 9424: Weights = [55.30333333  2.01046224  4.81465778  0.07918844  0.15172944 14.00854   ], Loss = 0.3499\n",
      "Iteration 9425: Weights = [55.30333333  2.01033234  4.8143467   0.07918332  0.15171964 14.00887511], Loss = 0.3499\n",
      "Iteration 9426: Weights = [55.30333333  2.01020245  4.81403563  0.0791782   0.15170984 14.00921021], Loss = 0.3498\n",
      "Iteration 9427: Weights = [55.30333333  2.01007256  4.81372459  0.07917309  0.15170003 14.00954528], Loss = 0.3498\n",
      "Iteration 9428: Weights = [55.30333333  2.00994269  4.81341356  0.07916797  0.15169023 14.00988034], Loss = 0.3498\n",
      "Iteration 9429: Weights = [55.30333333  2.00981282  4.81310256  0.07916286  0.15168043 14.01021537], Loss = 0.3497\n",
      "Iteration 9430: Weights = [55.30333333  2.00968297  4.81279158  0.07915774  0.15167063 14.01055038], Loss = 0.3497\n",
      "Iteration 9431: Weights = [55.30333333  2.00955312  4.81248061  0.07915263  0.15166083 14.01088537], Loss = 0.3496\n",
      "Iteration 9432: Weights = [55.30333333  2.00942327  4.81216967  0.07914751  0.15165103 14.01122033], Loss = 0.3496\n",
      "Iteration 9433: Weights = [55.30333333  2.00929344  4.81185874  0.0791424   0.15164123 14.01155528], Loss = 0.3495\n",
      "Iteration 9434: Weights = [55.30333333  2.00916362  4.81154784  0.07913729  0.15163144 14.0118902 ], Loss = 0.3495\n",
      "Iteration 9435: Weights = [55.30333333  2.0090338   4.81123696  0.07913217  0.15162164 14.0122251 ], Loss = 0.3494\n",
      "Iteration 9436: Weights = [55.30333333  2.00890399  4.81092609  0.07912706  0.15161184 14.01255998], Loss = 0.3494\n",
      "Iteration 9437: Weights = [55.30333333  2.00877419  4.81061525  0.07912195  0.15160205 14.01289484], Loss = 0.3494\n",
      "Iteration 9438: Weights = [55.30333333  2.0086444   4.81030443  0.07911684  0.15159225 14.01322968], Loss = 0.3493\n",
      "Iteration 9439: Weights = [55.30333333  2.00851462  4.80999362  0.07911172  0.15158246 14.01356449], Loss = 0.3493\n",
      "Iteration 9440: Weights = [55.30333333  2.00838485  4.80968284  0.07910661  0.15157266 14.01389929], Loss = 0.3492\n",
      "Iteration 9441: Weights = [55.30333333  2.00825508  4.80937208  0.0791015   0.15156287 14.01423406], Loss = 0.3492\n",
      "Iteration 9442: Weights = [55.30333333  2.00812532  4.80906133  0.07909639  0.15155308 14.01456881], Loss = 0.3491\n",
      "Iteration 9443: Weights = [55.30333333  2.00799557  4.80875061  0.07909128  0.15154328 14.01490354], Loss = 0.3491\n",
      "Iteration 9444: Weights = [55.30333333  2.00786583  4.80843991  0.07908617  0.15153349 14.01523824], Loss = 0.3490\n",
      "Iteration 9445: Weights = [55.30333333  2.0077361   4.80812922  0.07908106  0.1515237  14.01557293], Loss = 0.3490\n",
      "Iteration 9446: Weights = [55.30333333  2.00760638  4.80781856  0.07907595  0.15151391 14.01590759], Loss = 0.3489\n",
      "Iteration 9447: Weights = [55.30333333  2.00747666  4.80750792  0.07907084  0.15150412 14.01624223], Loss = 0.3489\n",
      "Iteration 9448: Weights = [55.30333333  2.00734695  4.80719729  0.07906573  0.15149433 14.01657685], Loss = 0.3489\n",
      "Iteration 9449: Weights = [55.30333333  2.00721726  4.80688669  0.07906062  0.15148454 14.01691145], Loss = 0.3488\n",
      "Iteration 9450: Weights = [55.30333333  2.00708757  4.80657611  0.07905551  0.15147476 14.01724603], Loss = 0.3488\n",
      "Iteration 9451: Weights = [55.30333333  2.00695788  4.80626555  0.07905041  0.15146497 14.01758059], Loss = 0.3487\n",
      "Iteration 9452: Weights = [55.30333333  2.00682821  4.805955    0.0790453   0.15145518 14.01791512], Loss = 0.3487\n",
      "Iteration 9453: Weights = [55.30333333  2.00669855  4.80564448  0.07904019  0.1514454  14.01824963], Loss = 0.3486\n",
      "Iteration 9454: Weights = [55.30333333  2.00656889  4.80533398  0.07903508  0.15143561 14.01858412], Loss = 0.3486\n",
      "Iteration 9455: Weights = [55.30333333  2.00643924  4.8050235   0.07902998  0.15142583 14.01891859], Loss = 0.3485\n",
      "Iteration 9456: Weights = [55.30333333  2.0063096   4.80471303  0.07902487  0.15141604 14.01925304], Loss = 0.3485\n",
      "Iteration 9457: Weights = [55.30333333  2.00617997  4.80440259  0.07901977  0.15140626 14.01958746], Loss = 0.3485\n",
      "Iteration 9458: Weights = [55.30333333  2.00605034  4.80409217  0.07901466  0.15139648 14.01992187], Loss = 0.3484\n",
      "Iteration 9459: Weights = [55.30333333  2.00592073  4.80378177  0.07900955  0.1513867  14.02025625], Loss = 0.3484\n",
      "Iteration 9460: Weights = [55.30333333  2.00579112  4.80347139  0.07900445  0.15137691 14.02059061], Loss = 0.3483\n",
      "Iteration 9461: Weights = [55.30333333  2.00566152  4.80316102  0.07899935  0.15136713 14.02092495], Loss = 0.3483\n",
      "Iteration 9462: Weights = [55.30333333  2.00553194  4.80285068  0.07899424  0.15135735 14.02125927], Loss = 0.3482\n",
      "Iteration 9463: Weights = [55.30333333  2.00540235  4.80254036  0.07898914  0.15134757 14.02159357], Loss = 0.3482\n",
      "Iteration 9464: Weights = [55.30333333  2.00527278  4.80223006  0.07898403  0.15133779 14.02192784], Loss = 0.3481\n",
      "Iteration 9465: Weights = [55.30333333  2.00514322  4.80191978  0.07897893  0.15132802 14.02226209], Loss = 0.3481\n",
      "Iteration 9466: Weights = [55.30333333  2.00501366  4.80160952  0.07897383  0.15131824 14.02259633], Loss = 0.3480\n",
      "Iteration 9467: Weights = [55.30333333  2.00488411  4.80129927  0.07896872  0.15130846 14.02293053], Loss = 0.3480\n",
      "Iteration 9468: Weights = [55.30333333  2.00475457  4.80098905  0.07896362  0.15129869 14.02326472], Loss = 0.3480\n",
      "Iteration 9469: Weights = [55.30333333  2.00462504  4.80067885  0.07895852  0.15128891 14.02359889], Loss = 0.3479\n",
      "Iteration 9470: Weights = [55.30333333  2.00449552  4.80036867  0.07895342  0.15127913 14.02393303], Loss = 0.3479\n",
      "Iteration 9471: Weights = [55.30333333  2.004366    4.80005851  0.07894832  0.15126936 14.02426716], Loss = 0.3478\n",
      "Iteration 9472: Weights = [55.30333333  2.0042365   4.79974837  0.07894322  0.15125959 14.02460126], Loss = 0.3478\n",
      "Iteration 9473: Weights = [55.30333333  2.004107    4.79943825  0.07893812  0.15124981 14.02493534], Loss = 0.3477\n",
      "Iteration 9474: Weights = [55.30333333  2.00397751  4.79912814  0.07893301  0.15124004 14.0252694 ], Loss = 0.3477\n",
      "Iteration 9475: Weights = [55.30333333  2.00384803  4.79881806  0.07892791  0.15123027 14.02560344], Loss = 0.3476\n",
      "Iteration 9476: Weights = [55.30333333  2.00371856  4.798508    0.07892282  0.1512205  14.02593745], Loss = 0.3476\n",
      "Iteration 9477: Weights = [55.30333333  2.00358909  4.79819796  0.07891772  0.15121073 14.02627145], Loss = 0.3476\n",
      "Iteration 9478: Weights = [55.30333333  2.00345964  4.79788794  0.07891262  0.15120096 14.02660542], Loss = 0.3475\n",
      "Iteration 9479: Weights = [55.30333333  2.00333019  4.79757794  0.07890752  0.15119119 14.02693937], Loss = 0.3475\n",
      "Iteration 9480: Weights = [55.30333333  2.00320075  4.79726796  0.07890242  0.15118142 14.0272733 ], Loss = 0.3474\n",
      "Iteration 9481: Weights = [55.30333333  2.00307132  4.796958    0.07889732  0.15117165 14.02760721], Loss = 0.3474\n",
      "Iteration 9482: Weights = [55.30333333  2.0029419   4.79664805  0.07889222  0.15116188 14.02794109], Loss = 0.3473\n",
      "Iteration 9483: Weights = [55.30333333  2.00281248  4.79633813  0.07888713  0.15115212 14.02827496], Loss = 0.3473\n",
      "Iteration 9484: Weights = [55.30333333  2.00268308  4.79602823  0.07888203  0.15114235 14.0286088 ], Loss = 0.3472\n",
      "Iteration 9485: Weights = [55.30333333  2.00255368  4.79571835  0.07887693  0.15113258 14.02894262], Loss = 0.3472\n",
      "Iteration 9486: Weights = [55.30333333  2.00242429  4.79540849  0.07887184  0.15112282 14.02927642], Loss = 0.3471\n",
      "Iteration 9487: Weights = [55.30333333  2.00229491  4.79509865  0.07886674  0.15111305 14.0296102 ], Loss = 0.3471\n",
      "Iteration 9488: Weights = [55.30333333  2.00216554  4.79478883  0.07886164  0.15110329 14.02994395], Loss = 0.3471\n",
      "Iteration 9489: Weights = [55.30333333  2.00203617  4.79447903  0.07885655  0.15109353 14.03027769], Loss = 0.3470\n",
      "Iteration 9490: Weights = [55.30333333  2.00190682  4.79416925  0.07885145  0.15108377 14.0306114 ], Loss = 0.3470\n",
      "Iteration 9491: Weights = [55.30333333  2.00177747  4.79385949  0.07884636  0.151074   14.0309451 ], Loss = 0.3469\n",
      "Iteration 9492: Weights = [55.30333333  2.00164813  4.79354974  0.07884126  0.15106424 14.03127877], Loss = 0.3469\n",
      "Iteration 9493: Weights = [55.30333333  2.0015188   4.79324002  0.07883617  0.15105448 14.03161241], Loss = 0.3468\n",
      "Iteration 9494: Weights = [55.30333333  2.00138948  4.79293032  0.07883108  0.15104472 14.03194604], Loss = 0.3468\n",
      "Iteration 9495: Weights = [55.30333333  2.00126017  4.79262064  0.07882598  0.15103496 14.03227965], Loss = 0.3467\n",
      "Iteration 9496: Weights = [55.30333333  2.00113086  4.79231098  0.07882089  0.1510252  14.03261323], Loss = 0.3467\n",
      "Iteration 9497: Weights = [55.30333333  2.00100156  4.79200134  0.0788158   0.15101545 14.03294679], Loss = 0.3467\n",
      "Iteration 9498: Weights = [55.30333333  2.00087227  4.79169172  0.07881071  0.15100569 14.03328034], Loss = 0.3466\n",
      "Iteration 9499: Weights = [55.30333333  2.00074299  4.79138212  0.07880561  0.15099593 14.03361385], Loss = 0.3466\n",
      "Iteration 9500: Weights = [55.30333333  2.00061372  4.79107254  0.07880052  0.15098618 14.03394735], Loss = 0.3465\n",
      "Iteration 9501: Weights = [55.30333333  2.00048446  4.79076298  0.07879543  0.15097642 14.03428083], Loss = 0.3465\n",
      "Iteration 9502: Weights = [55.30333333  2.0003552   4.79045344  0.07879034  0.15096667 14.03461428], Loss = 0.3464\n",
      "Iteration 9503: Weights = [55.30333333  2.00022596  4.79014392  0.07878525  0.15095691 14.03494772], Loss = 0.3464\n",
      "Iteration 9504: Weights = [55.30333333  2.00009672  4.78983441  0.07878016  0.15094716 14.03528113], Loss = 0.3463\n",
      "Iteration 9505: Weights = [55.30333333  1.99996749  4.78952493  0.07877507  0.1509374  14.03561452], Loss = 0.3463\n",
      "Iteration 9506: Weights = [55.30333333  1.99983826  4.78921547  0.07876998  0.15092765 14.03594789], Loss = 0.3463\n",
      "Iteration 9507: Weights = [55.30333333  1.99970905  4.78890603  0.07876489  0.1509179  14.03628123], Loss = 0.3462\n",
      "Iteration 9508: Weights = [55.30333333  1.99957985  4.78859661  0.0787598   0.15090815 14.03661456], Loss = 0.3462\n",
      "Iteration 9509: Weights = [55.30333333  1.99945065  4.78828721  0.07875471  0.1508984  14.03694786], Loss = 0.3461\n",
      "Iteration 9510: Weights = [55.30333333  1.99932146  4.78797783  0.07874962  0.15088865 14.03728115], Loss = 0.3461\n",
      "Iteration 9511: Weights = [55.30333333  1.99919228  4.78766847  0.07874453  0.1508789  14.03761441], Loss = 0.3460\n",
      "Iteration 9512: Weights = [55.30333333  1.99906311  4.78735913  0.07873945  0.15086915 14.03794765], Loss = 0.3460\n",
      "Iteration 9513: Weights = [55.30333333  1.99893394  4.78704981  0.07873436  0.1508594  14.03828087], Loss = 0.3459\n",
      "Iteration 9514: Weights = [55.30333333  1.99880479  4.78674051  0.07872927  0.15084966 14.03861406], Loss = 0.3459\n",
      "Iteration 9515: Weights = [55.30333333  1.99867564  4.78643122  0.07872418  0.15083991 14.03894724], Loss = 0.3458\n",
      "Iteration 9516: Weights = [55.30333333  1.9985465   4.78612196  0.0787191   0.15083016 14.03928039], Loss = 0.3458\n",
      "Iteration 9517: Weights = [55.30333333  1.99841737  4.78581272  0.07871401  0.15082042 14.03961352], Loss = 0.3458\n",
      "Iteration 9518: Weights = [55.30333333  1.99828825  4.7855035   0.07870893  0.15081067 14.03994663], Loss = 0.3457\n",
      "Iteration 9519: Weights = [55.30333333  1.99815914  4.7851943   0.07870384  0.15080093 14.04027972], Loss = 0.3457\n",
      "Iteration 9520: Weights = [55.30333333  1.99803003  4.78488512  0.07869875  0.15079119 14.04061279], Loss = 0.3456\n",
      "Iteration 9521: Weights = [55.30333333  1.99790094  4.78457596  0.07869367  0.15078144 14.04094583], Loss = 0.3456\n",
      "Iteration 9522: Weights = [55.30333333  1.99777185  4.78426682  0.07868859  0.1507717  14.04127886], Loss = 0.3455\n",
      "Iteration 9523: Weights = [55.30333333  1.99764277  4.7839577   0.0786835   0.15076196 14.04161186], Loss = 0.3455\n",
      "Iteration 9524: Weights = [55.30333333  1.9975137   4.7836486   0.07867842  0.15075222 14.04194484], Loss = 0.3454\n",
      "Iteration 9525: Weights = [55.30333333  1.99738463  4.78333951  0.07867333  0.15074248 14.0422778 ], Loss = 0.3454\n",
      "Iteration 9526: Weights = [55.30333333  1.99725558  4.78303045  0.07866825  0.15073274 14.04261074], Loss = 0.3454\n",
      "Iteration 9527: Weights = [55.30333333  1.99712653  4.78272141  0.07866317  0.150723   14.04294366], Loss = 0.3453\n",
      "Iteration 9528: Weights = [55.30333333  1.99699749  4.78241239  0.07865808  0.15071326 14.04327655], Loss = 0.3453\n",
      "Iteration 9529: Weights = [55.30333333  1.99686846  4.78210339  0.078653    0.15070352 14.04360943], Loss = 0.3452\n",
      "Iteration 9530: Weights = [55.30333333  1.99673944  4.78179441  0.07864792  0.15069378 14.04394228], Loss = 0.3452\n",
      "Iteration 9531: Weights = [55.30333333  1.99661043  4.78148545  0.07864284  0.15068405 14.04427511], Loss = 0.3451\n",
      "Iteration 9532: Weights = [55.30333333  1.99648142  4.78117651  0.07863776  0.15067431 14.04460792], Loss = 0.3451\n",
      "Iteration 9533: Weights = [55.30333333  1.99635243  4.78086758  0.07863268  0.15066458 14.0449407 ], Loss = 0.3450\n",
      "Iteration 9534: Weights = [55.30333333  1.99622344  4.78055868  0.0786276   0.15065484 14.04527347], Loss = 0.3450\n",
      "Iteration 9535: Weights = [55.30333333  1.99609446  4.7802498   0.07862252  0.15064511 14.04560622], Loss = 0.3450\n",
      "Iteration 9536: Weights = [55.30333333  1.99596549  4.77994094  0.07861744  0.15063537 14.04593894], Loss = 0.3449\n",
      "Iteration 9537: Weights = [55.30333333  1.99583652  4.7796321   0.07861236  0.15062564 14.04627164], Loss = 0.3449\n",
      "Iteration 9538: Weights = [55.30333333  1.99570757  4.77932328  0.07860728  0.15061591 14.04660432], Loss = 0.3448\n",
      "Iteration 9539: Weights = [55.30333333  1.99557862  4.77901447  0.0786022   0.15060618 14.04693698], Loss = 0.3448\n",
      "Iteration 9540: Weights = [55.30333333  1.99544968  4.77870569  0.07859712  0.15059645 14.04726962], Loss = 0.3447\n",
      "Iteration 9541: Weights = [55.30333333  1.99532075  4.77839693  0.07859204  0.15058672 14.04760223], Loss = 0.3447\n",
      "Iteration 9542: Weights = [55.30333333  1.99519183  4.77808819  0.07858696  0.15057699 14.04793483], Loss = 0.3446\n",
      "Iteration 9543: Weights = [55.30333333  1.99506292  4.77777947  0.07858189  0.15056726 14.0482674 ], Loss = 0.3446\n",
      "Iteration 9544: Weights = [55.30333333  1.99493401  4.77747077  0.07857681  0.15055753 14.04859995], Loss = 0.3446\n",
      "Iteration 9545: Weights = [55.30333333  1.99480512  4.77716208  0.07857173  0.1505478  14.04893248], Loss = 0.3445\n",
      "Iteration 9546: Weights = [55.30333333  1.99467623  4.77685342  0.07856665  0.15053807 14.04926499], Loss = 0.3445\n",
      "Iteration 9547: Weights = [55.30333333  1.99454735  4.77654478  0.07856158  0.15052835 14.04959747], Loss = 0.3444\n",
      "Iteration 9548: Weights = [55.30333333  1.99441848  4.77623616  0.0785565   0.15051862 14.04992994], Loss = 0.3444\n",
      "Iteration 9549: Weights = [55.30333333  1.99428961  4.77592755  0.07855143  0.1505089  14.05026238], Loss = 0.3443\n",
      "Iteration 9550: Weights = [55.30333333  1.99416076  4.77561897  0.07854635  0.15049917 14.0505948 ], Loss = 0.3443\n",
      "Iteration 9551: Weights = [55.30333333  1.99403191  4.77531041  0.07854128  0.15048945 14.05092721], Loss = 0.3442\n",
      "Iteration 9552: Weights = [55.30333333  1.99390307  4.77500187  0.0785362   0.15047972 14.05125959], Loss = 0.3442\n",
      "Iteration 9553: Weights = [55.30333333  1.99377424  4.77469335  0.07853113  0.15047    14.05159194], Loss = 0.3442\n",
      "Iteration 9554: Weights = [55.30333333  1.99364542  4.77438484  0.07852605  0.15046028 14.05192428], Loss = 0.3441\n",
      "Iteration 9555: Weights = [55.30333333  1.99351661  4.77407636  0.07852098  0.15045056 14.05225659], Loss = 0.3441\n",
      "Iteration 9556: Weights = [55.30333333  1.9933878   4.7737679   0.07851591  0.15044084 14.05258889], Loss = 0.3440\n",
      "Iteration 9557: Weights = [55.30333333  1.99325901  4.77345945  0.07851083  0.15043112 14.05292116], Loss = 0.3440\n",
      "Iteration 9558: Weights = [55.30333333  1.99313022  4.77315103  0.07850576  0.1504214  14.05325341], Loss = 0.3439\n",
      "Iteration 9559: Weights = [55.30333333  1.99300144  4.77284263  0.07850069  0.15041168 14.05358564], Loss = 0.3439\n",
      "Iteration 9560: Weights = [55.30333333  1.99287267  4.77253425  0.07849562  0.15040196 14.05391785], Loss = 0.3438\n",
      "Iteration 9561: Weights = [55.30333333  1.9927439   4.77222588  0.07849054  0.15039224 14.05425003], Loss = 0.3438\n",
      "Iteration 9562: Weights = [55.30333333  1.99261515  4.77191754  0.07848547  0.15038252 14.0545822 ], Loss = 0.3438\n",
      "Iteration 9563: Weights = [55.30333333  1.9924864   4.77160922  0.0784804   0.15037281 14.05491434], Loss = 0.3437\n",
      "Iteration 9564: Weights = [55.30333333  1.99235766  4.77130091  0.07847533  0.15036309 14.05524646], Loss = 0.3437\n",
      "Iteration 9565: Weights = [55.30333333  1.99222893  4.77099263  0.07847026  0.15035338 14.05557856], Loss = 0.3436\n",
      "Iteration 9566: Weights = [55.30333333  1.99210021  4.77068437  0.07846519  0.15034366 14.05591064], Loss = 0.3436\n",
      "Iteration 9567: Weights = [55.30333333  1.9919715   4.77037612  0.07846012  0.15033395 14.0562427 ], Loss = 0.3435\n",
      "Iteration 9568: Weights = [55.30333333  1.99184279  4.7700679   0.07845505  0.15032423 14.05657473], Loss = 0.3435\n",
      "Iteration 9569: Weights = [55.30333333  1.99171409  4.7697597   0.07844998  0.15031452 14.05690675], Loss = 0.3434\n",
      "Iteration 9570: Weights = [55.30333333  1.9915854   4.76945151  0.07844491  0.15030481 14.05723874], Loss = 0.3434\n",
      "Iteration 9571: Weights = [55.30333333  1.99145672  4.76914335  0.07843984  0.1502951  14.05757071], Loss = 0.3434\n",
      "Iteration 9572: Weights = [55.30333333  1.99132805  4.7688352   0.07843478  0.15028539 14.05790266], Loss = 0.3433\n",
      "Iteration 9573: Weights = [55.30333333  1.99119939  4.76852708  0.07842971  0.15027568 14.05823459], Loss = 0.3433\n",
      "Iteration 9574: Weights = [55.30333333  1.99107073  4.76821898  0.07842464  0.15026597 14.0585665 ], Loss = 0.3432\n",
      "Iteration 9575: Weights = [55.30333333  1.99094209  4.76791089  0.07841957  0.15025626 14.05889838], Loss = 0.3432\n",
      "Iteration 9576: Weights = [55.30333333  1.99081345  4.76760283  0.07841451  0.15024655 14.05923025], Loss = 0.3431\n",
      "Iteration 9577: Weights = [55.30333333  1.99068482  4.76729478  0.07840944  0.15023684 14.05956209], Loss = 0.3431\n",
      "Iteration 9578: Weights = [55.30333333  1.99055619  4.76698676  0.07840437  0.15022713 14.05989391], Loss = 0.3430\n",
      "Iteration 9579: Weights = [55.30333333  1.99042758  4.76667875  0.07839931  0.15021743 14.06022571], Loss = 0.3430\n",
      "Iteration 9580: Weights = [55.30333333  1.99029898  4.76637077  0.07839424  0.15020772 14.06055749], Loss = 0.3430\n",
      "Iteration 9581: Weights = [55.30333333  1.99017038  4.7660628   0.07838918  0.15019802 14.06088925], Loss = 0.3429\n",
      "Iteration 9582: Weights = [55.30333333  1.99004179  4.76575486  0.07838411  0.15018831 14.06122098], Loss = 0.3429\n",
      "Iteration 9583: Weights = [55.30333333  1.98991321  4.76544694  0.07837905  0.15017861 14.0615527 ], Loss = 0.3428\n",
      "Iteration 9584: Weights = [55.30333333  1.98978464  4.76513903  0.07837398  0.15016891 14.06188439], Loss = 0.3428\n",
      "Iteration 9585: Weights = [55.30333333  1.98965607  4.76483114  0.07836892  0.1501592  14.06221606], Loss = 0.3427\n",
      "Iteration 9586: Weights = [55.30333333  1.98952752  4.76452328  0.07836386  0.1501495  14.06254771], Loss = 0.3427\n",
      "Iteration 9587: Weights = [55.30333333  1.98939897  4.76421543  0.07835879  0.1501398  14.06287934], Loss = 0.3426\n",
      "Iteration 9588: Weights = [55.30333333  1.98927043  4.76390761  0.07835373  0.1501301  14.06321095], Loss = 0.3426\n",
      "Iteration 9589: Weights = [55.30333333  1.9891419   4.7635998   0.07834867  0.1501204  14.06354253], Loss = 0.3426\n",
      "Iteration 9590: Weights = [55.30333333  1.98901338  4.76329202  0.07834361  0.1501107  14.0638741 ], Loss = 0.3425\n",
      "Iteration 9591: Weights = [55.30333333  1.98888486  4.76298425  0.07833854  0.150101   14.06420564], Loss = 0.3425\n",
      "Iteration 9592: Weights = [55.30333333  1.98875636  4.76267651  0.07833348  0.1500913  14.06453716], Loss = 0.3424\n",
      "Iteration 9593: Weights = [55.30333333  1.98862786  4.76236878  0.07832842  0.1500816  14.06486866], Loss = 0.3424\n",
      "Iteration 9594: Weights = [55.30333333  1.98849937  4.76206107  0.07832336  0.15007191 14.06520014], Loss = 0.3423\n",
      "Iteration 9595: Weights = [55.30333333  1.98837089  4.76175339  0.0783183   0.15006221 14.0655316 ], Loss = 0.3423\n",
      "Iteration 9596: Weights = [55.30333333  1.98824242  4.76144572  0.07831324  0.15005251 14.06586303], Loss = 0.3422\n",
      "Iteration 9597: Weights = [55.30333333  1.98811395  4.76113807  0.07830818  0.15004282 14.06619445], Loss = 0.3422\n",
      "Iteration 9598: Weights = [55.30333333  1.9879855   4.76083045  0.07830312  0.15003312 14.06652584], Loss = 0.3422\n",
      "Iteration 9599: Weights = [55.30333333  1.98785705  4.76052284  0.07829806  0.15002343 14.06685721], Loss = 0.3421\n",
      "Iteration 9600: Weights = [55.30333333  1.98772861  4.76021525  0.078293    0.15001374 14.06718856], Loss = 0.3421\n",
      "Iteration 9601: Weights = [55.30333333  1.98760018  4.75990769  0.07828794  0.15000404 14.06751989], Loss = 0.3420\n",
      "Iteration 9602: Weights = [55.30333333  1.98747176  4.75960014  0.07828288  0.14999435 14.0678512 ], Loss = 0.3420\n",
      "Iteration 9603: Weights = [55.30333333  1.98734334  4.75929261  0.07827783  0.14998466 14.06818248], Loss = 0.3419\n",
      "Iteration 9604: Weights = [55.30333333  1.98721493  4.7589851   0.07827277  0.14997497 14.06851375], Loss = 0.3419\n",
      "Iteration 9605: Weights = [55.30333333  1.98708654  4.75867762  0.07826771  0.14996528 14.06884499], Loss = 0.3418\n",
      "Iteration 9606: Weights = [55.30333333  1.98695815  4.75837015  0.07826265  0.14995559 14.06917621], Loss = 0.3418\n",
      "Iteration 9607: Weights = [55.30333333  1.98682977  4.7580627   0.0782576   0.1499459  14.06950741], Loss = 0.3418\n",
      "Iteration 9608: Weights = [55.30333333  1.98670139  4.75775527  0.07825254  0.14993621 14.06983859], Loss = 0.3417\n",
      "Iteration 9609: Weights = [55.30333333  1.98657303  4.75744786  0.07824748  0.14992653 14.07016975], Loss = 0.3417\n",
      "Iteration 9610: Weights = [55.30333333  1.98644467  4.75714048  0.07824243  0.14991684 14.07050088], Loss = 0.3416\n",
      "Iteration 9611: Weights = [55.30333333  1.98631632  4.75683311  0.07823737  0.14990715 14.070832  ], Loss = 0.3416\n",
      "Iteration 9612: Weights = [55.30333333  1.98618798  4.75652576  0.07823232  0.14989747 14.07116309], Loss = 0.3415\n",
      "Iteration 9613: Weights = [55.30333333  1.98605965  4.75621843  0.07822726  0.14988778 14.07149416], Loss = 0.3415\n",
      "Iteration 9614: Weights = [55.30333333  1.98593133  4.75591112  0.07822221  0.1498781  14.07182521], Loss = 0.3415\n",
      "Iteration 9615: Weights = [55.30333333  1.98580301  4.75560383  0.07821715  0.14986841 14.07215624], Loss = 0.3414\n",
      "Iteration 9616: Weights = [55.30333333  1.98567471  4.75529656  0.0782121   0.14985873 14.07248725], Loss = 0.3414\n",
      "Iteration 9617: Weights = [55.30333333  1.98554641  4.75498931  0.07820705  0.14984905 14.07281824], Loss = 0.3413\n",
      "Iteration 9618: Weights = [55.30333333  1.98541812  4.75468209  0.07820199  0.14983936 14.0731492 ], Loss = 0.3413\n",
      "Iteration 9619: Weights = [55.30333333  1.98528984  4.75437488  0.07819694  0.14982968 14.07348015], Loss = 0.3412\n",
      "Iteration 9620: Weights = [55.30333333  1.98516156  4.75406769  0.07819189  0.14982    14.07381107], Loss = 0.3412\n",
      "Iteration 9621: Weights = [55.30333333  1.9850333   4.75376052  0.07818684  0.14981032 14.07414197], Loss = 0.3411\n",
      "Iteration 9622: Weights = [55.30333333  1.98490504  4.75345337  0.07818179  0.14980064 14.07447285], Loss = 0.3411\n",
      "Iteration 9623: Weights = [55.30333333  1.98477679  4.75314624  0.07817673  0.14979096 14.07480371], Loss = 0.3411\n",
      "Iteration 9624: Weights = [55.30333333  1.98464855  4.75283913  0.07817168  0.14978129 14.07513454], Loss = 0.3410\n",
      "Iteration 9625: Weights = [55.30333333  1.98452032  4.75253204  0.07816663  0.14977161 14.07546536], Loss = 0.3410\n",
      "Iteration 9626: Weights = [55.30333333  1.9843921   4.75222496  0.07816158  0.14976193 14.07579615], Loss = 0.3409\n",
      "Iteration 9627: Weights = [55.30333333  1.98426388  4.75191791  0.07815653  0.14975225 14.07612692], Loss = 0.3409\n",
      "Iteration 9628: Weights = [55.30333333  1.98413567  4.75161088  0.07815148  0.14974258 14.07645767], Loss = 0.3408\n",
      "Iteration 9629: Weights = [55.30333333  1.98400747  4.75130387  0.07814643  0.1497329  14.0767884 ], Loss = 0.3408\n",
      "Iteration 9630: Weights = [55.30333333  1.98387928  4.75099688  0.07814138  0.14972323 14.07711911], Loss = 0.3407\n",
      "Iteration 9631: Weights = [55.30333333  1.9837511   4.75068991  0.07813633  0.14971355 14.0774498 ], Loss = 0.3407\n",
      "Iteration 9632: Weights = [55.30333333  1.98362293  4.75038296  0.07813129  0.14970388 14.07778047], Loss = 0.3407\n",
      "Iteration 9633: Weights = [55.30333333  1.98349476  4.75007603  0.07812624  0.14969421 14.07811111], Loss = 0.3406\n",
      "Iteration 9634: Weights = [55.30333333  1.9833666   4.74976911  0.07812119  0.14968454 14.07844173], Loss = 0.3406\n",
      "Iteration 9635: Weights = [55.30333333  1.98323845  4.74946222  0.07811614  0.14967487 14.07877233], Loss = 0.3405\n",
      "Iteration 9636: Weights = [55.30333333  1.98311031  4.74915535  0.07811109  0.14966519 14.07910291], Loss = 0.3405\n",
      "Iteration 9637: Weights = [55.30333333  1.98298218  4.7488485   0.07810605  0.14965552 14.07943347], Loss = 0.3404\n",
      "Iteration 9638: Weights = [55.30333333  1.98285406  4.74854166  0.078101    0.14964586 14.07976401], Loss = 0.3404\n",
      "Iteration 9639: Weights = [55.30333333  1.98272594  4.74823485  0.07809595  0.14963619 14.08009453], Loss = 0.3404\n",
      "Iteration 9640: Weights = [55.30333333  1.98259783  4.74792806  0.07809091  0.14962652 14.08042502], Loss = 0.3403\n",
      "Iteration 9641: Weights = [55.30333333  1.98246973  4.74762128  0.07808586  0.14961685 14.08075549], Loss = 0.3403\n",
      "Iteration 9642: Weights = [55.30333333  1.98234164  4.74731453  0.07808082  0.14960718 14.08108595], Loss = 0.3402\n",
      "Iteration 9643: Weights = [55.30333333  1.98221356  4.7470078   0.07807577  0.14959752 14.08141638], Loss = 0.3402\n",
      "Iteration 9644: Weights = [55.30333333  1.98208548  4.74670108  0.07807073  0.14958785 14.08174679], Loss = 0.3401\n",
      "Iteration 9645: Weights = [55.30333333  1.98195742  4.74639439  0.07806568  0.14957819 14.08207717], Loss = 0.3401\n",
      "Iteration 9646: Weights = [55.30333333  1.98182936  4.74608772  0.07806064  0.14956852 14.08240754], Loss = 0.3400\n",
      "Iteration 9647: Weights = [55.30333333  1.98170131  4.74578106  0.0780556   0.14955886 14.08273789], Loss = 0.3400\n",
      "Iteration 9648: Weights = [55.30333333  1.98157327  4.74547443  0.07805055  0.14954919 14.08306821], Loss = 0.3400\n",
      "Iteration 9649: Weights = [55.30333333  1.98144523  4.74516781  0.07804551  0.14953953 14.08339851], Loss = 0.3399\n",
      "Iteration 9650: Weights = [55.30333333  1.98131721  4.74486122  0.07804047  0.14952987 14.08372879], Loss = 0.3399\n",
      "Iteration 9651: Weights = [55.30333333  1.98118919  4.74455464  0.07803542  0.14952021 14.08405905], Loss = 0.3398\n",
      "Iteration 9652: Weights = [55.30333333  1.98106118  4.74424809  0.07803038  0.14951055 14.08438929], Loss = 0.3398\n",
      "Iteration 9653: Weights = [55.30333333  1.98093318  4.74394155  0.07802534  0.14950089 14.08471951], Loss = 0.3397\n",
      "Iteration 9654: Weights = [55.30333333  1.98080519  4.74363504  0.0780203   0.14949123 14.08504971], Loss = 0.3397\n",
      "Iteration 9655: Weights = [55.30333333  1.98067721  4.74332854  0.07801526  0.14948157 14.08537988], Loss = 0.3396\n",
      "Iteration 9656: Weights = [55.30333333  1.98054923  4.74302207  0.07801022  0.14947191 14.08571003], Loss = 0.3396\n",
      "Iteration 9657: Weights = [55.30333333  1.98042126  4.74271561  0.07800518  0.14946225 14.08604017], Loss = 0.3396\n",
      "Iteration 9658: Weights = [55.30333333  1.9802933   4.74240917  0.07800014  0.1494526  14.08637028], Loss = 0.3395\n",
      "Iteration 9659: Weights = [55.30333333  1.98016535  4.74210276  0.0779951   0.14944294 14.08670037], Loss = 0.3395\n",
      "Iteration 9660: Weights = [55.30333333  1.98003741  4.74179636  0.07799006  0.14943328 14.08703043], Loss = 0.3394\n",
      "Iteration 9661: Weights = [55.30333333  1.97990948  4.74148998  0.07798502  0.14942363 14.08736048], Loss = 0.3394\n",
      "Iteration 9662: Weights = [55.30333333  1.97978155  4.74118363  0.07797998  0.14941397 14.08769051], Loss = 0.3393\n",
      "Iteration 9663: Weights = [55.30333333  1.97965363  4.74087729  0.07797494  0.14940432 14.08802051], Loss = 0.3393\n",
      "Iteration 9664: Weights = [55.30333333  1.97952572  4.74057097  0.0779699   0.14939467 14.08835049], Loss = 0.3393\n",
      "Iteration 9665: Weights = [55.30333333  1.97939782  4.74026467  0.07796487  0.14938501 14.08868045], Loss = 0.3392\n",
      "Iteration 9666: Weights = [55.30333333  1.97926993  4.73995839  0.07795983  0.14937536 14.08901039], Loss = 0.3392\n",
      "Iteration 9667: Weights = [55.30333333  1.97914204  4.73965214  0.07795479  0.14936571 14.08934031], Loss = 0.3391\n",
      "Iteration 9668: Weights = [55.30333333  1.97901417  4.7393459   0.07794975  0.14935606 14.08967021], Loss = 0.3391\n",
      "Iteration 9669: Weights = [55.30333333  1.9788863   4.73903968  0.07794472  0.14934641 14.09000009], Loss = 0.3390\n",
      "Iteration 9670: Weights = [55.30333333  1.97875844  4.73873348  0.07793968  0.14933676 14.09032994], Loss = 0.3390\n",
      "Iteration 9671: Weights = [55.30333333  1.97863059  4.7384273   0.07793465  0.14932711 14.09065977], Loss = 0.3389\n",
      "Iteration 9672: Weights = [55.30333333  1.97850275  4.73812114  0.07792961  0.14931746 14.09098959], Loss = 0.3389\n",
      "Iteration 9673: Weights = [55.30333333  1.97837491  4.737815    0.07792458  0.14930781 14.09131938], Loss = 0.3389\n",
      "Iteration 9674: Weights = [55.30333333  1.97824708  4.73750888  0.07791954  0.14929817 14.09164915], Loss = 0.3388\n",
      "Iteration 9675: Weights = [55.30333333  1.97811927  4.73720278  0.07791451  0.14928852 14.09197889], Loss = 0.3388\n",
      "Iteration 9676: Weights = [55.30333333  1.97799145  4.7368967   0.07790947  0.14927887 14.09230862], Loss = 0.3387\n",
      "Iteration 9677: Weights = [55.30333333  1.97786365  4.73659064  0.07790444  0.14926923 14.09263833], Loss = 0.3387\n",
      "Iteration 9678: Weights = [55.30333333  1.97773586  4.7362846   0.0778994   0.14925959 14.09296801], Loss = 0.3386\n",
      "Iteration 9679: Weights = [55.30333333  1.97760807  4.73597858  0.07789437  0.14924994 14.09329767], Loss = 0.3386\n",
      "Iteration 9680: Weights = [55.30333333  1.9774803   4.73567258  0.07788934  0.1492403  14.09362732], Loss = 0.3386\n",
      "Iteration 9681: Weights = [55.30333333  1.97735253  4.7353666   0.07788431  0.14923066 14.09395694], Loss = 0.3385\n",
      "Iteration 9682: Weights = [55.30333333  1.97722477  4.73506064  0.07787927  0.14922101 14.09428653], Loss = 0.3385\n",
      "Iteration 9683: Weights = [55.30333333  1.97709701  4.73475469  0.07787424  0.14921137 14.09461611], Loss = 0.3384\n",
      "Iteration 9684: Weights = [55.30333333  1.97696927  4.73444877  0.07786921  0.14920173 14.09494567], Loss = 0.3384\n",
      "Iteration 9685: Weights = [55.30333333  1.97684153  4.73414287  0.07786418  0.14919209 14.0952752 ], Loss = 0.3383\n",
      "Iteration 9686: Weights = [55.30333333  1.97671381  4.73383699  0.07785915  0.14918245 14.09560472], Loss = 0.3383\n",
      "Iteration 9687: Weights = [55.30333333  1.97658609  4.73353112  0.07785412  0.14917281 14.09593421], Loss = 0.3382\n",
      "Iteration 9688: Weights = [55.30333333  1.97645838  4.73322528  0.07784909  0.14916317 14.09626368], Loss = 0.3382\n",
      "Iteration 9689: Weights = [55.30333333  1.97633067  4.73291946  0.07784406  0.14915354 14.09659313], Loss = 0.3382\n",
      "Iteration 9690: Weights = [55.30333333  1.97620298  4.73261366  0.07783903  0.1491439  14.09692256], Loss = 0.3381\n",
      "Iteration 9691: Weights = [55.30333333  1.97607529  4.73230787  0.077834    0.14913426 14.09725197], Loss = 0.3381\n",
      "Iteration 9692: Weights = [55.30333333  1.97594761  4.73200211  0.07782897  0.14912463 14.09758135], Loss = 0.3380\n",
      "Iteration 9693: Weights = [55.30333333  1.97581994  4.73169636  0.07782394  0.14911499 14.09791072], Loss = 0.3380\n",
      "Iteration 9694: Weights = [55.30333333  1.97569228  4.73139064  0.07781891  0.14910536 14.09824006], Loss = 0.3379\n",
      "Iteration 9695: Weights = [55.30333333  1.97556463  4.73108493  0.07781388  0.14909572 14.09856938], Loss = 0.3379\n",
      "Iteration 9696: Weights = [55.30333333  1.97543698  4.73077925  0.07780886  0.14908609 14.09889869], Loss = 0.3379\n",
      "Iteration 9697: Weights = [55.30333333  1.97530935  4.73047358  0.07780383  0.14907646 14.09922797], Loss = 0.3378\n",
      "Iteration 9698: Weights = [55.30333333  1.97518172  4.73016794  0.0777988   0.14906682 14.09955722], Loss = 0.3378\n",
      "Iteration 9699: Weights = [55.30333333  1.9750541   4.72986231  0.07779377  0.14905719 14.09988646], Loss = 0.3377\n",
      "Iteration 9700: Weights = [55.30333333  1.97492648  4.72955671  0.07778875  0.14904756 14.10021568], Loss = 0.3377\n",
      "Iteration 9701: Weights = [55.30333333  1.97479888  4.72925112  0.07778372  0.14903793 14.10054487], Loss = 0.3376\n",
      "Iteration 9702: Weights = [55.30333333  1.97467129  4.72894555  0.0777787   0.1490283  14.10087405], Loss = 0.3376\n",
      "Iteration 9703: Weights = [55.30333333  1.9745437   4.72864001  0.07777367  0.14901867 14.1012032 ], Loss = 0.3375\n",
      "Iteration 9704: Weights = [55.30333333  1.97441612  4.72833448  0.07776865  0.14900904 14.10153233], Loss = 0.3375\n",
      "Iteration 9705: Weights = [55.30333333  1.97428855  4.72802897  0.07776362  0.14899942 14.10186144], Loss = 0.3375\n",
      "Iteration 9706: Weights = [55.30333333  1.97416098  4.72772349  0.0777586   0.14898979 14.10219053], Loss = 0.3374\n",
      "Iteration 9707: Weights = [55.30333333  1.97403343  4.72741802  0.07775357  0.14898016 14.10251959], Loss = 0.3374\n",
      "Iteration 9708: Weights = [55.30333333  1.97390588  4.72711257  0.07774855  0.14897054 14.10284864], Loss = 0.3373\n",
      "Iteration 9709: Weights = [55.30333333  1.97377835  4.72680714  0.07774353  0.14896091 14.10317766], Loss = 0.3373\n",
      "Iteration 9710: Weights = [55.30333333  1.97365082  4.72650173  0.0777385   0.14895129 14.10350667], Loss = 0.3372\n",
      "Iteration 9711: Weights = [55.30333333  1.97352329  4.72619635  0.07773348  0.14894166 14.10383565], Loss = 0.3372\n",
      "Iteration 9712: Weights = [55.30333333  1.97339578  4.72589098  0.07772846  0.14893204 14.10416461], Loss = 0.3372\n",
      "Iteration 9713: Weights = [55.30333333  1.97326828  4.72558563  0.07772343  0.14892242 14.10449355], Loss = 0.3371\n",
      "Iteration 9714: Weights = [55.30333333  1.97314078  4.7252803   0.07771841  0.14891279 14.10482247], Loss = 0.3371\n",
      "Iteration 9715: Weights = [55.30333333  1.97301329  4.72497499  0.07771339  0.14890317 14.10515136], Loss = 0.3370\n",
      "Iteration 9716: Weights = [55.30333333  1.97288581  4.7246697   0.07770837  0.14889355 14.10548024], Loss = 0.3370\n",
      "Iteration 9717: Weights = [55.30333333  1.97275834  4.72436443  0.07770335  0.14888393 14.10580909], Loss = 0.3369\n",
      "Iteration 9718: Weights = [55.30333333  1.97263087  4.72405918  0.07769833  0.14887431 14.10613793], Loss = 0.3369\n",
      "Iteration 9719: Weights = [55.30333333  1.97250342  4.72375395  0.07769331  0.14886469 14.10646674], Loss = 0.3369\n",
      "Iteration 9720: Weights = [55.30333333  1.97237597  4.72344873  0.07768829  0.14885507 14.10679553], Loss = 0.3368\n",
      "Iteration 9721: Weights = [55.30333333  1.97224853  4.72314354  0.07768327  0.14884546 14.1071243 ], Loss = 0.3368\n",
      "Iteration 9722: Weights = [55.30333333  1.9721211   4.72283837  0.07767825  0.14883584 14.10745305], Loss = 0.3367\n",
      "Iteration 9723: Weights = [55.30333333  1.97199368  4.72253322  0.07767323  0.14882622 14.10778178], Loss = 0.3367\n",
      "Iteration 9724: Weights = [55.30333333  1.97186626  4.72222809  0.07766821  0.14881661 14.10811048], Loss = 0.3366\n",
      "Iteration 9725: Weights = [55.30333333  1.97173886  4.72192297  0.07766319  0.14880699 14.10843917], Loss = 0.3366\n",
      "Iteration 9726: Weights = [55.30333333  1.97161146  4.72161788  0.07765818  0.14879738 14.10876783], Loss = 0.3365\n",
      "Iteration 9727: Weights = [55.30333333  1.97148407  4.72131281  0.07765316  0.14878776 14.10909647], Loss = 0.3365\n",
      "Iteration 9728: Weights = [55.30333333  1.97135669  4.72100775  0.07764814  0.14877815 14.10942509], Loss = 0.3365\n",
      "Iteration 9729: Weights = [55.30333333  1.97122932  4.72070272  0.07764312  0.14876854 14.10975369], Loss = 0.3364\n",
      "Iteration 9730: Weights = [55.30333333  1.97110195  4.72039771  0.07763811  0.14875892 14.11008227], Loss = 0.3364\n",
      "Iteration 9731: Weights = [55.30333333  1.97097459  4.72009271  0.07763309  0.14874931 14.11041083], Loss = 0.3363\n",
      "Iteration 9732: Weights = [55.30333333  1.97084724  4.71978774  0.07762807  0.1487397  14.11073936], Loss = 0.3363\n",
      "Iteration 9733: Weights = [55.30333333  1.9707199   4.71948278  0.07762306  0.14873009 14.11106788], Loss = 0.3362\n",
      "Iteration 9734: Weights = [55.30333333  1.97059257  4.71917785  0.07761804  0.14872048 14.11139637], Loss = 0.3362\n",
      "Iteration 9735: Weights = [55.30333333  1.97046525  4.71887293  0.07761303  0.14871087 14.11172484], Loss = 0.3362\n",
      "Iteration 9736: Weights = [55.30333333  1.97033793  4.71856804  0.07760801  0.14870126 14.11205329], Loss = 0.3361\n",
      "Iteration 9737: Weights = [55.30333333  1.97021062  4.71826316  0.077603    0.14869166 14.11238172], Loss = 0.3361\n",
      "Iteration 9738: Weights = [55.30333333  1.97008333  4.7179583   0.07759798  0.14868205 14.11271013], Loss = 0.3360\n",
      "Iteration 9739: Weights = [55.30333333  1.96995603  4.71765347  0.07759297  0.14867244 14.11303852], Loss = 0.3360\n",
      "Iteration 9740: Weights = [55.30333333  1.96982875  4.71734865  0.07758796  0.14866284 14.11336689], Loss = 0.3359\n",
      "Iteration 9741: Weights = [55.30333333  1.96970148  4.71704385  0.07758294  0.14865323 14.11369523], Loss = 0.3359\n",
      "Iteration 9742: Weights = [55.30333333  1.96957421  4.71673907  0.07757793  0.14864363 14.11402356], Loss = 0.3359\n",
      "Iteration 9743: Weights = [55.30333333  1.96944695  4.71643432  0.07757292  0.14863402 14.11435186], Loss = 0.3358\n",
      "Iteration 9744: Weights = [55.30333333  1.9693197   4.71612958  0.07756791  0.14862442 14.11468014], Loss = 0.3358\n",
      "Iteration 9745: Weights = [55.30333333  1.96919246  4.71582486  0.0775629   0.14861482 14.1150084 ], Loss = 0.3357\n",
      "Iteration 9746: Weights = [55.30333333  1.96906523  4.71552016  0.07755788  0.14860521 14.11533664], Loss = 0.3357\n",
      "Iteration 9747: Weights = [55.30333333  1.968938    4.71521548  0.07755287  0.14859561 14.11566486], Loss = 0.3356\n",
      "Iteration 9748: Weights = [55.30333333  1.96881079  4.71491082  0.07754786  0.14858601 14.11599305], Loss = 0.3356\n",
      "Iteration 9749: Weights = [55.30333333  1.96868358  4.71460618  0.07754285  0.14857641 14.11632123], Loss = 0.3355\n",
      "Iteration 9750: Weights = [55.30333333  1.96855638  4.71430156  0.07753784  0.14856681 14.11664938], Loss = 0.3355\n",
      "Iteration 9751: Weights = [55.30333333  1.96842918  4.71399696  0.07753283  0.14855721 14.11697751], Loss = 0.3355\n",
      "Iteration 9752: Weights = [55.30333333  1.968302    4.71369238  0.07752782  0.14854761 14.11730563], Loss = 0.3354\n",
      "Iteration 9753: Weights = [55.30333333  1.96817482  4.71338782  0.07752281  0.14853801 14.11763372], Loss = 0.3354\n",
      "Iteration 9754: Weights = [55.30333333  1.96804766  4.71308328  0.0775178   0.14852842 14.11796179], Loss = 0.3353\n",
      "Iteration 9755: Weights = [55.30333333  1.9679205   4.71277876  0.07751279  0.14851882 14.11828983], Loss = 0.3353\n",
      "Iteration 9756: Weights = [55.30333333  1.96779334  4.71247425  0.07750779  0.14850922 14.11861786], Loss = 0.3352\n",
      "Iteration 9757: Weights = [55.30333333  1.9676662   4.71216977  0.07750278  0.14849963 14.11894587], Loss = 0.3352\n",
      "Iteration 9758: Weights = [55.30333333  1.96753907  4.71186531  0.07749777  0.14849003 14.11927385], Loss = 0.3352\n",
      "Iteration 9759: Weights = [55.30333333  1.96741194  4.71156087  0.07749276  0.14848044 14.11960181], Loss = 0.3351\n",
      "Iteration 9760: Weights = [55.30333333  1.96728482  4.71125644  0.07748776  0.14847085 14.11992976], Loss = 0.3351\n",
      "Iteration 9761: Weights = [55.30333333  1.96715771  4.71095204  0.07748275  0.14846125 14.12025768], Loss = 0.3350\n",
      "Iteration 9762: Weights = [55.30333333  1.96703061  4.71064765  0.07747774  0.14845166 14.12058558], Loss = 0.3350\n",
      "Iteration 9763: Weights = [55.30333333  1.96690352  4.71034329  0.07747274  0.14844207 14.12091346], Loss = 0.3349\n",
      "Iteration 9764: Weights = [55.30333333  1.96677643  4.71003894  0.07746773  0.14843248 14.12124131], Loss = 0.3349\n",
      "Iteration 9765: Weights = [55.30333333  1.96664935  4.70973462  0.07746273  0.14842289 14.12156915], Loss = 0.3349\n",
      "Iteration 9766: Weights = [55.30333333  1.96652228  4.70943031  0.07745772  0.1484133  14.12189696], Loss = 0.3348\n",
      "Iteration 9767: Weights = [55.30333333  1.96639522  4.70912603  0.07745272  0.14840371 14.12222476], Loss = 0.3348\n",
      "Iteration 9768: Weights = [55.30333333  1.96626817  4.70882176  0.07744771  0.14839412 14.12255253], Loss = 0.3347\n",
      "Iteration 9769: Weights = [55.30333333  1.96614113  4.70851752  0.07744271  0.14838453 14.12288028], Loss = 0.3347\n",
      "Iteration 9770: Weights = [55.30333333  1.96601409  4.70821329  0.07743771  0.14837494 14.12320801], Loss = 0.3346\n",
      "Iteration 9771: Weights = [55.30333333  1.96588706  4.70790908  0.0774327   0.14836536 14.12353572], Loss = 0.3346\n",
      "Iteration 9772: Weights = [55.30333333  1.96576004  4.70760489  0.0774277   0.14835577 14.12386341], Loss = 0.3346\n",
      "Iteration 9773: Weights = [55.30333333  1.96563303  4.70730073  0.0774227   0.14834619 14.12419108], Loss = 0.3345\n",
      "Iteration 9774: Weights = [55.30333333  1.96550603  4.70699658  0.07741769  0.1483366  14.12451872], Loss = 0.3345\n",
      "Iteration 9775: Weights = [55.30333333  1.96537903  4.70669245  0.07741269  0.14832702 14.12484635], Loss = 0.3344\n",
      "Iteration 9776: Weights = [55.30333333  1.96525204  4.70638834  0.07740769  0.14831743 14.12517395], Loss = 0.3344\n",
      "Iteration 9777: Weights = [55.30333333  1.96512506  4.70608425  0.07740269  0.14830785 14.12550153], Loss = 0.3343\n",
      "Iteration 9778: Weights = [55.30333333  1.96499809  4.70578018  0.07739769  0.14829827 14.12582909], Loss = 0.3343\n",
      "Iteration 9779: Weights = [55.30333333  1.96487113  4.70547613  0.07739269  0.14828869 14.12615663], Loss = 0.3342\n",
      "Iteration 9780: Weights = [55.30333333  1.96474418  4.7051721   0.07738769  0.1482791  14.12648415], Loss = 0.3342\n",
      "Iteration 9781: Weights = [55.30333333  1.96461723  4.70486809  0.07738269  0.14826952 14.12681165], Loss = 0.3342\n",
      "Iteration 9782: Weights = [55.30333333  1.96449029  4.7045641   0.07737769  0.14825994 14.12713912], Loss = 0.3341\n",
      "Iteration 9783: Weights = [55.30333333  1.96436336  4.70426013  0.07737269  0.14825036 14.12746658], Loss = 0.3341\n",
      "Iteration 9784: Weights = [55.30333333  1.96423644  4.70395618  0.07736769  0.14824079 14.12779401], Loss = 0.3340\n",
      "Iteration 9785: Weights = [55.30333333  1.96410953  4.70365225  0.07736269  0.14823121 14.12812143], Loss = 0.3340\n",
      "Iteration 9786: Weights = [55.30333333  1.96398262  4.70334833  0.07735769  0.14822163 14.12844882], Loss = 0.3339\n",
      "Iteration 9787: Weights = [55.30333333  1.96385573  4.70304444  0.07735269  0.14821205 14.12877619], Loss = 0.3339\n",
      "Iteration 9788: Weights = [55.30333333  1.96372884  4.70274057  0.07734769  0.14820248 14.12910354], Loss = 0.3339\n",
      "Iteration 9789: Weights = [55.30333333  1.96360196  4.70243671  0.0773427   0.1481929  14.12943087], Loss = 0.3338\n",
      "Iteration 9790: Weights = [55.30333333  1.96347509  4.70213288  0.0773377   0.14818333 14.12975817], Loss = 0.3338\n",
      "Iteration 9791: Weights = [55.30333333  1.96334822  4.70182907  0.0773327   0.14817375 14.13008546], Loss = 0.3337\n",
      "Iteration 9792: Weights = [55.30333333  1.96322137  4.70152527  0.0773277   0.14816418 14.13041272], Loss = 0.3337\n",
      "Iteration 9793: Weights = [55.30333333  1.96309452  4.7012215   0.07732271  0.1481546  14.13073997], Loss = 0.3336\n",
      "Iteration 9794: Weights = [55.30333333  1.96296768  4.70091774  0.07731771  0.14814503 14.13106719], Loss = 0.3336\n",
      "Iteration 9795: Weights = [55.30333333  1.96284085  4.70061401  0.07731272  0.14813546 14.13139439], Loss = 0.3336\n",
      "Iteration 9796: Weights = [55.30333333  1.96271402  4.70031029  0.07730772  0.14812589 14.13172157], Loss = 0.3335\n",
      "Iteration 9797: Weights = [55.30333333  1.96258721  4.70000659  0.07730273  0.14811632 14.13204873], Loss = 0.3335\n",
      "Iteration 9798: Weights = [55.30333333  1.9624604   4.69970292  0.07729773  0.14810675 14.13237587], Loss = 0.3334\n",
      "Iteration 9799: Weights = [55.30333333  1.9623336   4.69939926  0.07729274  0.14809718 14.13270298], Loss = 0.3334\n",
      "Iteration 9800: Weights = [55.30333333  1.96220681  4.69909562  0.07728774  0.14808761 14.13303008], Loss = 0.3333\n",
      "Iteration 9801: Weights = [55.30333333  1.96208003  4.698792    0.07728275  0.14807804 14.13335715], Loss = 0.3333\n",
      "Iteration 9802: Weights = [55.30333333  1.96195326  4.6984884   0.07727776  0.14806847 14.13368421], Loss = 0.3333\n",
      "Iteration 9803: Weights = [55.30333333  1.96182649  4.69818483  0.07727276  0.14805891 14.13401124], Loss = 0.3332\n",
      "Iteration 9804: Weights = [55.30333333  1.96169973  4.69788127  0.07726777  0.14804934 14.13433825], Loss = 0.3332\n",
      "Iteration 9805: Weights = [55.30333333  1.96157298  4.69757773  0.07726278  0.14803977 14.13466524], Loss = 0.3331\n",
      "Iteration 9806: Weights = [55.30333333  1.96144624  4.69727421  0.07725779  0.14803021 14.13499221], Loss = 0.3331\n",
      "Iteration 9807: Weights = [55.30333333  1.96131951  4.69697071  0.07725279  0.14802064 14.13531916], Loss = 0.3330\n",
      "Iteration 9808: Weights = [55.30333333  1.96119279  4.69666723  0.0772478   0.14801108 14.13564608], Loss = 0.3330\n",
      "Iteration 9809: Weights = [55.30333333  1.96106607  4.69636376  0.07724281  0.14800152 14.13597299], Loss = 0.3330\n",
      "Iteration 9810: Weights = [55.30333333  1.96093936  4.69606032  0.07723782  0.14799195 14.13629987], Loss = 0.3329\n",
      "Iteration 9811: Weights = [55.30333333  1.96081266  4.6957569   0.07723283  0.14798239 14.13662674], Loss = 0.3329\n",
      "Iteration 9812: Weights = [55.30333333  1.96068597  4.6954535   0.07722784  0.14797283 14.13695358], Loss = 0.3328\n",
      "Iteration 9813: Weights = [55.30333333  1.96055928  4.69515012  0.07722285  0.14796327 14.1372804 ], Loss = 0.3328\n",
      "Iteration 9814: Weights = [55.30333333  1.96043261  4.69484675  0.07721786  0.14795371 14.1376072 ], Loss = 0.3327\n",
      "Iteration 9815: Weights = [55.30333333  1.96030594  4.69454341  0.07721287  0.14794415 14.13793398], Loss = 0.3327\n",
      "Iteration 9816: Weights = [55.30333333  1.96017928  4.69424009  0.07720788  0.14793459 14.13826074], Loss = 0.3327\n",
      "Iteration 9817: Weights = [55.30333333  1.96005263  4.69393678  0.07720289  0.14792503 14.13858747], Loss = 0.3326\n",
      "Iteration 9818: Weights = [55.30333333  1.95992599  4.6936335   0.07719791  0.14791548 14.13891419], Loss = 0.3326\n",
      "Iteration 9819: Weights = [55.30333333  1.95979935  4.69333023  0.07719292  0.14790592 14.13924088], Loss = 0.3325\n",
      "Iteration 9820: Weights = [55.30333333  1.95967273  4.69302699  0.07718793  0.14789636 14.13956755], Loss = 0.3325\n",
      "Iteration 9821: Weights = [55.30333333  1.95954611  4.69272376  0.07718294  0.14788681 14.13989421], Loss = 0.3324\n",
      "Iteration 9822: Weights = [55.30333333  1.9594195   4.69242055  0.07717796  0.14787725 14.14022084], Loss = 0.3324\n",
      "Iteration 9823: Weights = [55.30333333  1.9592929   4.69211737  0.07717297  0.1478677  14.14054745], Loss = 0.3324\n",
      "Iteration 9824: Weights = [55.30333333  1.9591663   4.6918142   0.07716798  0.14785814 14.14087404], Loss = 0.3323\n",
      "Iteration 9825: Weights = [55.30333333  1.95903972  4.69151105  0.077163    0.14784859 14.1412006 ], Loss = 0.3323\n",
      "Iteration 9826: Weights = [55.30333333  1.95891314  4.69120792  0.07715801  0.14783904 14.14152715], Loss = 0.3322\n",
      "Iteration 9827: Weights = [55.30333333  1.95878657  4.69090482  0.07715303  0.14782948 14.14185368], Loss = 0.3322\n",
      "Iteration 9828: Weights = [55.30333333  1.95866001  4.69060173  0.07714804  0.14781993 14.14218018], Loss = 0.3321\n",
      "Iteration 9829: Weights = [55.30333333  1.95853346  4.69029866  0.07714306  0.14781038 14.14250666], Loss = 0.3321\n",
      "Iteration 9830: Weights = [55.30333333  1.95840691  4.68999561  0.07713807  0.14780083 14.14283313], Loss = 0.3321\n",
      "Iteration 9831: Weights = [55.30333333  1.95828037  4.68969258  0.07713309  0.14779128 14.14315957], Loss = 0.3320\n",
      "Iteration 9832: Weights = [55.30333333  1.95815385  4.68938957  0.0771281   0.14778173 14.14348599], Loss = 0.3320\n",
      "Iteration 9833: Weights = [55.30333333  1.95802733  4.68908658  0.07712312  0.14777218 14.14381239], Loss = 0.3319\n",
      "Iteration 9834: Weights = [55.30333333  1.95790081  4.68878361  0.07711814  0.14776264 14.14413876], Loss = 0.3319\n",
      "Iteration 9835: Weights = [55.30333333  1.95777431  4.68848065  0.07711316  0.14775309 14.14446512], Loss = 0.3318\n",
      "Iteration 9836: Weights = [55.30333333  1.95764781  4.68817772  0.07710817  0.14774354 14.14479146], Loss = 0.3318\n",
      "Iteration 9837: Weights = [55.30333333  1.95752133  4.68787481  0.07710319  0.147734   14.14511777], Loss = 0.3318\n",
      "Iteration 9838: Weights = [55.30333333  1.95739485  4.68757192  0.07709821  0.14772445 14.14544406], Loss = 0.3317\n",
      "Iteration 9839: Weights = [55.30333333  1.95726838  4.68726904  0.07709323  0.14771491 14.14577034], Loss = 0.3317\n",
      "Iteration 9840: Weights = [55.30333333  1.95714191  4.68696619  0.07708825  0.14770536 14.14609659], Loss = 0.3316\n",
      "Iteration 9841: Weights = [55.30333333  1.95701546  4.68666336  0.07708327  0.14769582 14.14642282], Loss = 0.3316\n",
      "Iteration 9842: Weights = [55.30333333  1.95688901  4.68636054  0.07707828  0.14768627 14.14674903], Loss = 0.3315\n",
      "Iteration 9843: Weights = [55.30333333  1.95676257  4.68605775  0.0770733   0.14767673 14.14707521], Loss = 0.3315\n",
      "Iteration 9844: Weights = [55.30333333  1.95663614  4.68575497  0.07706832  0.14766719 14.14740138], Loss = 0.3315\n",
      "Iteration 9845: Weights = [55.30333333  1.95650972  4.68545221  0.07706335  0.14765765 14.14772753], Loss = 0.3314\n",
      "Iteration 9846: Weights = [55.30333333  1.95638331  4.68514948  0.07705837  0.14764811 14.14805365], Loss = 0.3314\n",
      "Iteration 9847: Weights = [55.30333333  1.9562569   4.68484676  0.07705339  0.14763857 14.14837976], Loss = 0.3313\n",
      "Iteration 9848: Weights = [55.30333333  1.9561305   4.68454406  0.07704841  0.14762903 14.14870584], Loss = 0.3313\n",
      "Iteration 9849: Weights = [55.30333333  1.95600411  4.68424138  0.07704343  0.14761949 14.1490319 ], Loss = 0.3312\n",
      "Iteration 9850: Weights = [55.30333333  1.95587773  4.68393873  0.07703845  0.14760995 14.14935794], Loss = 0.3312\n",
      "Iteration 9851: Weights = [55.30333333  1.95575136  4.68363609  0.07703347  0.14760042 14.14968396], Loss = 0.3312\n",
      "Iteration 9852: Weights = [55.30333333  1.95562499  4.68333347  0.0770285   0.14759088 14.15000996], Loss = 0.3311\n",
      "Iteration 9853: Weights = [55.30333333  1.95549864  4.68303087  0.07702352  0.14758134 14.15033594], Loss = 0.3311\n",
      "Iteration 9854: Weights = [55.30333333  1.95537229  4.68272829  0.07701854  0.14757181 14.15066189], Loss = 0.3310\n",
      "Iteration 9855: Weights = [55.30333333  1.95524595  4.68242573  0.07701357  0.14756227 14.15098783], Loss = 0.3310\n",
      "Iteration 9856: Weights = [55.30333333  1.95511961  4.68212319  0.07700859  0.14755274 14.15131374], Loss = 0.3309\n",
      "Iteration 9857: Weights = [55.30333333  1.95499329  4.68182067  0.07700362  0.14754321 14.15163964], Loss = 0.3309\n",
      "Iteration 9858: Weights = [55.30333333  1.95486697  4.68151816  0.07699864  0.14753367 14.15196551], Loss = 0.3309\n",
      "Iteration 9859: Weights = [55.30333333  1.95474067  4.68121568  0.07699367  0.14752414 14.15229136], Loss = 0.3308\n",
      "Iteration 9860: Weights = [55.30333333  1.95461437  4.68091322  0.07698869  0.14751461 14.15261719], Loss = 0.3308\n",
      "Iteration 9861: Weights = [55.30333333  1.95448808  4.68061078  0.07698372  0.14750508 14.152943  ], Loss = 0.3307\n",
      "Iteration 9862: Weights = [55.30333333  1.95436179  4.68030835  0.07697874  0.14749555 14.15326879], Loss = 0.3307\n",
      "Iteration 9863: Weights = [55.30333333  1.95423552  4.68000595  0.07697377  0.14748602 14.15359455], Loss = 0.3306\n",
      "Iteration 9864: Weights = [55.30333333  1.95410925  4.67970356  0.0769688   0.14747649 14.1539203 ], Loss = 0.3306\n",
      "Iteration 9865: Weights = [55.30333333  1.95398299  4.6794012   0.07696382  0.14746696 14.15424602], Loss = 0.3306\n",
      "Iteration 9866: Weights = [55.30333333  1.95385674  4.67909885  0.07695885  0.14745743 14.15457173], Loss = 0.3305\n",
      "Iteration 9867: Weights = [55.30333333  1.9537305   4.67879653  0.07695388  0.1474479  14.15489741], Loss = 0.3305\n",
      "Iteration 9868: Weights = [55.30333333  1.95360426  4.67849422  0.0769489   0.14743838 14.15522307], Loss = 0.3304\n",
      "Iteration 9869: Weights = [55.30333333  1.95347804  4.67819193  0.07694393  0.14742885 14.15554871], Loss = 0.3304\n",
      "Iteration 9870: Weights = [55.30333333  1.95335182  4.67788967  0.07693896  0.14741932 14.15587433], Loss = 0.3303\n",
      "Iteration 9871: Weights = [55.30333333  1.95322561  4.67758742  0.07693399  0.1474098  14.15619993], Loss = 0.3303\n",
      "Iteration 9872: Weights = [55.30333333  1.95309941  4.67728519  0.07692902  0.14740027 14.15652551], Loss = 0.3303\n",
      "Iteration 9873: Weights = [55.30333333  1.95297321  4.67698298  0.07692405  0.14739075 14.15685106], Loss = 0.3302\n",
      "Iteration 9874: Weights = [55.30333333  1.95284703  4.67668079  0.07691908  0.14738123 14.1571766 ], Loss = 0.3302\n",
      "Iteration 9875: Weights = [55.30333333  1.95272085  4.67637862  0.07691411  0.1473717  14.15750211], Loss = 0.3301\n",
      "Iteration 9876: Weights = [55.30333333  1.95259468  4.67607647  0.07690914  0.14736218 14.15782761], Loss = 0.3301\n",
      "Iteration 9877: Weights = [55.30333333  1.95246852  4.67577434  0.07690417  0.14735266 14.15815308], Loss = 0.3300\n",
      "Iteration 9878: Weights = [55.30333333  1.95234237  4.67547223  0.0768992   0.14734314 14.15847853], Loss = 0.3300\n",
      "Iteration 9879: Weights = [55.30333333  1.95221622  4.67517014  0.07689423  0.14733362 14.15880396], Loss = 0.3300\n",
      "Iteration 9880: Weights = [55.30333333  1.95209009  4.67486807  0.07688926  0.1473241  14.15912937], Loss = 0.3299\n",
      "Iteration 9881: Weights = [55.30333333  1.95196396  4.67456601  0.0768843   0.14731458 14.15945476], Loss = 0.3299\n",
      "Iteration 9882: Weights = [55.30333333  1.95183784  4.67426398  0.07687933  0.14730506 14.15978012], Loss = 0.3298\n",
      "Iteration 9883: Weights = [55.30333333  1.95171172  4.67396197  0.07687436  0.14729555 14.16010547], Loss = 0.3298\n",
      "Iteration 9884: Weights = [55.30333333  1.95158562  4.67365997  0.07686939  0.14728603 14.16043079], Loss = 0.3297\n",
      "Iteration 9885: Weights = [55.30333333  1.95145952  4.673358    0.07686443  0.14727651 14.1607561 ], Loss = 0.3297\n",
      "Iteration 9886: Weights = [55.30333333  1.95133344  4.67305604  0.07685946  0.147267   14.16108138], Loss = 0.3297\n",
      "Iteration 9887: Weights = [55.30333333  1.95120736  4.67275411  0.07685449  0.14725748 14.16140664], Loss = 0.3296\n",
      "Iteration 9888: Weights = [55.30333333  1.95108129  4.67245219  0.07684953  0.14724797 14.16173188], Loss = 0.3296\n",
      "Iteration 9889: Weights = [55.30333333  1.95095522  4.6721503   0.07684456  0.14723845 14.1620571 ], Loss = 0.3295\n",
      "Iteration 9890: Weights = [55.30333333  1.95082917  4.67184842  0.0768396   0.14722894 14.1623823 ], Loss = 0.3295\n",
      "Iteration 9891: Weights = [55.30333333  1.95070312  4.67154656  0.07683463  0.14721943 14.16270748], Loss = 0.3294\n",
      "Iteration 9892: Weights = [55.30333333  1.95057708  4.67124472  0.07682967  0.14720991 14.16303264], Loss = 0.3294\n",
      "Iteration 9893: Weights = [55.30333333  1.95045105  4.6709429   0.07682471  0.1472004  14.16335777], Loss = 0.3294\n",
      "Iteration 9894: Weights = [55.30333333  1.95032503  4.67064111  0.07681974  0.14719089 14.16368289], Loss = 0.3293\n",
      "Iteration 9895: Weights = [55.30333333  1.95019901  4.67033933  0.07681478  0.14718138 14.16400798], Loss = 0.3293\n",
      "Iteration 9896: Weights = [55.30333333  1.95007301  4.67003757  0.07680982  0.14717187 14.16433306], Loss = 0.3292\n",
      "Iteration 9897: Weights = [55.30333333  1.94994701  4.66973583  0.07680485  0.14716236 14.16465811], Loss = 0.3292\n",
      "Iteration 9898: Weights = [55.30333333  1.94982102  4.6694341   0.07679989  0.14715285 14.16498314], Loss = 0.3291\n",
      "Iteration 9899: Weights = [55.30333333  1.94969504  4.6691324   0.07679493  0.14714335 14.16530815], Loss = 0.3291\n",
      "Iteration 9900: Weights = [55.30333333  1.94956906  4.66883072  0.07678997  0.14713384 14.16563314], Loss = 0.3291\n",
      "Iteration 9901: Weights = [55.30333333  1.9494431   4.66852906  0.076785    0.14712433 14.16595811], Loss = 0.3290\n",
      "Iteration 9902: Weights = [55.30333333  1.94931714  4.66822742  0.07678004  0.14711483 14.16628305], Loss = 0.3290\n",
      "Iteration 9903: Weights = [55.30333333  1.94919119  4.66792579  0.07677508  0.14710532 14.16660798], Loss = 0.3289\n",
      "Iteration 9904: Weights = [55.30333333  1.94906525  4.66762419  0.07677012  0.14709582 14.16693288], Loss = 0.3289\n",
      "Iteration 9905: Weights = [55.30333333  1.94893932  4.6673226   0.07676516  0.14708631 14.16725777], Loss = 0.3289\n",
      "Iteration 9906: Weights = [55.30333333  1.94881339  4.66702104  0.0767602   0.14707681 14.16758263], Loss = 0.3288\n",
      "Iteration 9907: Weights = [55.30333333  1.94868748  4.66671949  0.07675524  0.14706731 14.16790747], Loss = 0.3288\n",
      "Iteration 9908: Weights = [55.30333333  1.94856157  4.66641797  0.07675028  0.1470578  14.16823229], Loss = 0.3287\n",
      "Iteration 9909: Weights = [55.30333333  1.94843567  4.66611646  0.07674532  0.1470483  14.16855709], Loss = 0.3287\n",
      "Iteration 9910: Weights = [55.30333333  1.94830977  4.66581497  0.07674036  0.1470388  14.16888187], Loss = 0.3286\n",
      "Iteration 9911: Weights = [55.30333333  1.94818389  4.6655135   0.07673541  0.1470293  14.16920663], Loss = 0.3286\n",
      "Iteration 9912: Weights = [55.30333333  1.94805801  4.66521206  0.07673045  0.1470198  14.16953137], Loss = 0.3286\n",
      "Iteration 9913: Weights = [55.30333333  1.94793215  4.66491063  0.07672549  0.1470103  14.16985608], Loss = 0.3285\n",
      "Iteration 9914: Weights = [55.30333333  1.94780629  4.66460922  0.07672053  0.1470008  14.17018078], Loss = 0.3285\n",
      "Iteration 9915: Weights = [55.30333333  1.94768043  4.66430783  0.07671558  0.1469913  14.17050545], Loss = 0.3284\n",
      "Iteration 9916: Weights = [55.30333333  1.94755459  4.66400646  0.07671062  0.14698181 14.17083011], Loss = 0.3284\n",
      "Iteration 9917: Weights = [55.30333333  1.94742876  4.66370511  0.07670566  0.14697231 14.17115474], Loss = 0.3283\n",
      "Iteration 9918: Weights = [55.30333333  1.94730293  4.66340378  0.07670071  0.14696281 14.17147935], Loss = 0.3283\n",
      "Iteration 9919: Weights = [55.30333333  1.94717711  4.66310246  0.07669575  0.14695332 14.17180394], Loss = 0.3283\n",
      "Iteration 9920: Weights = [55.30333333  1.9470513   4.66280117  0.0766908   0.14694382 14.17212851], Loss = 0.3282\n",
      "Iteration 9921: Weights = [55.30333333  1.9469255   4.6624999   0.07668584  0.14693433 14.17245306], Loss = 0.3282\n",
      "Iteration 9922: Weights = [55.30333333  1.9467997   4.66219865  0.07668089  0.14692484 14.17277758], Loss = 0.3281\n",
      "Iteration 9923: Weights = [55.30333333  1.94667391  4.66189741  0.07667593  0.14691534 14.17310209], Loss = 0.3281\n",
      "Iteration 9924: Weights = [55.30333333  1.94654814  4.6615962   0.07667098  0.14690585 14.17342658], Loss = 0.3280\n",
      "Iteration 9925: Weights = [55.30333333  1.94642237  4.661295    0.07666602  0.14689636 14.17375104], Loss = 0.3280\n",
      "Iteration 9926: Weights = [55.30333333  1.9462966   4.66099383  0.07666107  0.14688687 14.17407548], Loss = 0.3280\n",
      "Iteration 9927: Weights = [55.30333333  1.94617085  4.66069267  0.07665612  0.14687738 14.17439991], Loss = 0.3279\n",
      "Iteration 9928: Weights = [55.30333333  1.9460451   4.66039153  0.07665116  0.14686789 14.17472431], Loss = 0.3279\n",
      "Iteration 9929: Weights = [55.30333333  1.94591937  4.66009042  0.07664621  0.1468584  14.17504869], Loss = 0.3278\n",
      "Iteration 9930: Weights = [55.30333333  1.94579364  4.65978932  0.07664126  0.14684891 14.17537305], Loss = 0.3278\n",
      "Iteration 9931: Weights = [55.30333333  1.94566791  4.65948824  0.07663631  0.14683942 14.17569739], Loss = 0.3277\n",
      "Iteration 9932: Weights = [55.30333333  1.9455422   4.65918718  0.07663135  0.14682993 14.1760217 ], Loss = 0.3277\n",
      "Iteration 9933: Weights = [55.30333333  1.94541649  4.65888614  0.0766264   0.14682044 14.176346  ], Loss = 0.3277\n",
      "Iteration 9934: Weights = [55.30333333  1.9452908   4.65858512  0.07662145  0.14681096 14.17667028], Loss = 0.3276\n",
      "Iteration 9935: Weights = [55.30333333  1.94516511  4.65828412  0.0766165   0.14680147 14.17699453], Loss = 0.3276\n",
      "Iteration 9936: Weights = [55.30333333  1.94503943  4.65798314  0.07661155  0.14679199 14.17731876], Loss = 0.3275\n",
      "Iteration 9937: Weights = [55.30333333  1.94491375  4.65768218  0.0766066   0.1467825  14.17764298], Loss = 0.3275\n",
      "Iteration 9938: Weights = [55.30333333  1.94478809  4.65738124  0.07660165  0.14677302 14.17796717], Loss = 0.3275\n",
      "Iteration 9939: Weights = [55.30333333  1.94466243  4.65708031  0.0765967   0.14676354 14.17829134], Loss = 0.3274\n",
      "Iteration 9940: Weights = [55.30333333  1.94453678  4.65677941  0.07659175  0.14675405 14.17861549], Loss = 0.3274\n",
      "Iteration 9941: Weights = [55.30333333  1.94441114  4.65647853  0.0765868   0.14674457 14.17893962], Loss = 0.3273\n",
      "Iteration 9942: Weights = [55.30333333  1.94428551  4.65617766  0.07658186  0.14673509 14.17926373], Loss = 0.3273\n",
      "Iteration 9943: Weights = [55.30333333  1.94415989  4.65587682  0.07657691  0.14672561 14.17958781], Loss = 0.3272\n",
      "Iteration 9944: Weights = [55.30333333  1.94403427  4.65557599  0.07657196  0.14671613 14.17991188], Loss = 0.3272\n",
      "Iteration 9945: Weights = [55.30333333  1.94390866  4.65527519  0.07656701  0.14670665 14.18023593], Loss = 0.3272\n",
      "Iteration 9946: Weights = [55.30333333  1.94378306  4.6549744   0.07656207  0.14669717 14.18055995], Loss = 0.3271\n",
      "Iteration 9947: Weights = [55.30333333  1.94365747  4.65467363  0.07655712  0.14668769 14.18088395], Loss = 0.3271\n",
      "Iteration 9948: Weights = [55.30333333  1.94353189  4.65437288  0.07655217  0.14667821 14.18120794], Loss = 0.3270\n",
      "Iteration 9949: Weights = [55.30333333  1.94340631  4.65407216  0.07654723  0.14666874 14.1815319 ], Loss = 0.3270\n",
      "Iteration 9950: Weights = [55.30333333  1.94328075  4.65377145  0.07654228  0.14665926 14.18185584], Loss = 0.3269\n",
      "Iteration 9951: Weights = [55.30333333  1.94315519  4.65347076  0.07653733  0.14664978 14.18217976], Loss = 0.3269\n",
      "Iteration 9952: Weights = [55.30333333  1.94302963  4.65317009  0.07653239  0.14664031 14.18250366], Loss = 0.3269\n",
      "Iteration 9953: Weights = [55.30333333  1.94290409  4.65286944  0.07652744  0.14663083 14.18282754], Loss = 0.3268\n",
      "Iteration 9954: Weights = [55.30333333  1.94277856  4.6525688   0.0765225   0.14662136 14.18315139], Loss = 0.3268\n",
      "Iteration 9955: Weights = [55.30333333  1.94265303  4.65226819  0.07651756  0.14661189 14.18347523], Loss = 0.3267\n",
      "Iteration 9956: Weights = [55.30333333  1.94252751  4.6519676   0.07651261  0.14660241 14.18379904], Loss = 0.3267\n",
      "Iteration 9957: Weights = [55.30333333  1.942402    4.65166703  0.07650767  0.14659294 14.18412284], Loss = 0.3266\n",
      "Iteration 9958: Weights = [55.30333333  1.9422765   4.65136647  0.07650272  0.14658347 14.18444661], Loss = 0.3266\n",
      "Iteration 9959: Weights = [55.30333333  1.942151    4.65106594  0.07649778  0.146574   14.18477036], Loss = 0.3266\n",
      "Iteration 9960: Weights = [55.30333333  1.94202552  4.65076542  0.07649284  0.14656453 14.18509409], Loss = 0.3265\n",
      "Iteration 9961: Weights = [55.30333333  1.94190004  4.65046493  0.0764879   0.14655506 14.1854178 ], Loss = 0.3265\n",
      "Iteration 9962: Weights = [55.30333333  1.94177457  4.65016445  0.07648295  0.14654559 14.18574149], Loss = 0.3264\n",
      "Iteration 9963: Weights = [55.30333333  1.94164911  4.649864    0.07647801  0.14653612 14.18606516], Loss = 0.3264\n",
      "Iteration 9964: Weights = [55.30333333  1.94152365  4.64956356  0.07647307  0.14652665 14.18638881], Loss = 0.3264\n",
      "Iteration 9965: Weights = [55.30333333  1.94139821  4.64926314  0.07646813  0.14651719 14.18671244], Loss = 0.3263\n",
      "Iteration 9966: Weights = [55.30333333  1.94127277  4.64896274  0.07646319  0.14650772 14.18703604], Loss = 0.3263\n",
      "Iteration 9967: Weights = [55.30333333  1.94114734  4.64866237  0.07645825  0.14649825 14.18735963], Loss = 0.3262\n",
      "Iteration 9968: Weights = [55.30333333  1.94102192  4.64836201  0.07645331  0.14648879 14.18768319], Loss = 0.3262\n",
      "Iteration 9969: Weights = [55.30333333  1.94089651  4.64806167  0.07644837  0.14647932 14.18800674], Loss = 0.3261\n",
      "Iteration 9970: Weights = [55.30333333  1.9407711   4.64776135  0.07644343  0.14646986 14.18833026], Loss = 0.3261\n",
      "Iteration 9971: Weights = [55.30333333  1.9406457   4.64746104  0.07643849  0.14646039 14.18865376], Loss = 0.3261\n",
      "Iteration 9972: Weights = [55.30333333  1.94052031  4.64716076  0.07643355  0.14645093 14.18897724], Loss = 0.3260\n",
      "Iteration 9973: Weights = [55.30333333  1.94039493  4.6468605   0.07642861  0.14644147 14.1893007 ], Loss = 0.3260\n",
      "Iteration 9974: Weights = [55.30333333  1.94026956  4.64656026  0.07642368  0.14643201 14.18962414], Loss = 0.3259\n",
      "Iteration 9975: Weights = [55.30333333  1.9401442   4.64626003  0.07641874  0.14642254 14.18994756], Loss = 0.3259\n",
      "Iteration 9976: Weights = [55.30333333  1.94001884  4.64595983  0.0764138   0.14641308 14.19027095], Loss = 0.3258\n",
      "Iteration 9977: Weights = [55.30333333  1.93989349  4.64565964  0.07640886  0.14640362 14.19059433], Loss = 0.3258\n",
      "Iteration 9978: Weights = [55.30333333  1.93976815  4.64535948  0.07640393  0.14639416 14.19091768], Loss = 0.3258\n",
      "Iteration 9979: Weights = [55.30333333  1.93964282  4.64505933  0.07639899  0.14638471 14.19124102], Loss = 0.3257\n",
      "Iteration 9980: Weights = [55.30333333  1.93951749  4.64475921  0.07639405  0.14637525 14.19156433], Loss = 0.3257\n",
      "Iteration 9981: Weights = [55.30333333  1.93939218  4.6444591   0.07638912  0.14636579 14.19188762], Loss = 0.3256\n",
      "Iteration 9982: Weights = [55.30333333  1.93926687  4.64415901  0.07638418  0.14635633 14.1922109 ], Loss = 0.3256\n",
      "Iteration 9983: Weights = [55.30333333  1.93914157  4.64385894  0.07637925  0.14634688 14.19253415], Loss = 0.3256\n",
      "Iteration 9984: Weights = [55.30333333  1.93901628  4.64355889  0.07637431  0.14633742 14.19285738], Loss = 0.3255\n",
      "Iteration 9985: Weights = [55.30333333  1.93889099  4.64325886  0.07636938  0.14632797 14.19318059], Loss = 0.3255\n",
      "Iteration 9986: Weights = [55.30333333  1.93876572  4.64295885  0.07636444  0.14631851 14.19350377], Loss = 0.3254\n",
      "Iteration 9987: Weights = [55.30333333  1.93864045  4.64265886  0.07635951  0.14630906 14.19382694], Loss = 0.3254\n",
      "Iteration 9988: Weights = [55.30333333  1.93851519  4.64235889  0.07635457  0.1462996  14.19415009], Loss = 0.3253\n",
      "Iteration 9989: Weights = [55.30333333  1.93838994  4.64205894  0.07634964  0.14629015 14.19447321], Loss = 0.3253\n",
      "Iteration 9990: Weights = [55.30333333  1.9382647   4.64175901  0.07634471  0.1462807  14.19479632], Loss = 0.3253\n",
      "Iteration 9991: Weights = [55.30333333  1.93813946  4.64145909  0.07633977  0.14627125 14.1951194 ], Loss = 0.3252\n",
      "Iteration 9992: Weights = [55.30333333  1.93801423  4.6411592   0.07633484  0.1462618  14.19544246], Loss = 0.3252\n",
      "Iteration 9993: Weights = [55.30333333  1.93788902  4.64085932  0.07632991  0.14625235 14.1957655 ], Loss = 0.3251\n",
      "Iteration 9994: Weights = [55.30333333  1.9377638   4.64055947  0.07632498  0.1462429  14.19608853], Loss = 0.3251\n",
      "Iteration 9995: Weights = [55.30333333  1.9376386   4.64025963  0.07632005  0.14623345 14.19641153], Loss = 0.3250\n",
      "Iteration 9996: Weights = [55.30333333  1.93751341  4.63995982  0.07631512  0.146224   14.19673451], Loss = 0.3250\n",
      "Iteration 9997: Weights = [55.30333333  1.93738822  4.63966002  0.07631018  0.14621455 14.19705746], Loss = 0.3250\n",
      "Iteration 9998: Weights = [55.30333333  1.93726304  4.63936024  0.07630525  0.1462051  14.1973804 ], Loss = 0.3249\n",
      "Iteration 9999: Weights = [55.30333333  1.93713787  4.63906048  0.07630032  0.14619566 14.19770332], Loss = 0.3249\n",
      "Iteration 10000: Weights = [55.30333333  1.93701271  4.63876074  0.07629539  0.14618621 14.19802621], Loss = 0.3248\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 1200x1200 with 6 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# =============================================================================\n",
    "# 11. Using all 10,000 iterations ^^\n",
    "# =============================================================================\n",
    "\n",
    "final_weights, weight_history = gradient_descent(X_train_aug, y_train, \n",
    "                                                 initial_weights,\n",
    "                                                 learning_rate=0.01, \n",
    "                                                 iterations=10000)\n",
    "\n",
    "# Convert weight_history list to a NumPy array for plotting\n",
    "weight_history = np.array(weight_history)  # shape: (iterations, number_of_weights)\n",
    "\n",
    "# Define names for each weight: first weight is the Intercept, then the feature names\n",
    "weight_names = ['Intercept'] + features\n",
    "\n",
    "# Create a 3x2 grid of subplots\n",
    "fig, axs = plt.subplots(3, 2, figsize=(12, 12))\n",
    "axs = axs.flatten()  # Flatten the grid for easy indexing\n",
    "\n",
    "# Plot the evolution of each weight over iterations\n",
    "for j in range(weight_history.shape[1]):\n",
    "    axs[j].plot(range(1, len(weight_history) + 1), weight_history[:, j],\n",
    "                marker='o', linestyle='-')\n",
    "    axs[j].set_xlabel('Iteration')\n",
    "    axs[j].set_ylabel(weight_names[j])\n",
    "    axs[j].set_title(f'Variation of {weight_names[j]} over Iterations')\n",
    "    axs[j].grid(True)\n",
    "\n",
    "# Hide any unused subplots (if any)\n",
    "for k in range(weight_history.shape[1], len(axs)):\n",
    "    axs[k].axis('off')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predictions on Test Set:\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([75.97880846, 65.0768064 , 79.89773847, 44.81125131, 53.93732059,\n",
       "       85.5155443 , 61.31391823, 94.28898271, 86.79023854, 76.18032813,\n",
       "       34.75550784, 39.84653892, 48.29053714, 65.79347971, 77.35365527,\n",
       "       29.8793008 , 61.35045232, 26.54260863, 51.49468495, 57.07502385,\n",
       "       33.54961898, 58.1716698 , 27.23169159, 67.18271034, 67.71697752,\n",
       "       34.74397901, 39.84653892, 23.42741847, 59.03189784, 27.37881793])"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Predict using the final weights for all test observations\n",
    "predictions = X_test_aug.dot(final_weights)\n",
    "\n",
    "# Show a few predictions alongside actual values\n",
    "predictions = X_test_aug.dot(final_weights)\n",
    "print(\"Predictions on Test Set:\")\n",
    "predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean Squared Error on Test Set: 0.8128\n",
      "Root Mean Squared Error on Test Set: 0.9015\n"
     ]
    }
   ],
   "source": [
    "mse = mean_squared_error(y_test, predictions)\n",
    "rmse = math.sqrt(mse)\n",
    "print(f\"Mean Squared Error on Test Set: {mse:.4f}\")\n",
    "print(f\"Root Mean Squared Error on Test Set: {rmse:.4f}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
